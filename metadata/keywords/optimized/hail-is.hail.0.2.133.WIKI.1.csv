quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"type(GT=0, AD=[12, 0], DP=12, GQ=36, PL=[0, 36, 420])]. Integrate sample annotations¶; Hail treats variant and sample annotations as first-class citizens.; Annotations are usually a critical part of any genetic study. Sample; annotations are where you’ll store information about sample phenotypes,; ancestry, sex, and covariates. Variant annotations can be used to store; information like gene membership and functional impact for use in QC or; analysis.; In this tutorial, we demonstrate how to take a text file and use it to; annotate the samples in a VDS.; iPython supports various cell “magics”. The %%sh magic is one which; interprets the cell with bash, rather than Python. We can use this to; look at the first few lines of our annotation file. This file contains; the sample ID, the population and “super-population” designations, the; sample sex, and two simulated phenotypes (one binary, one discrete). In [11]:. %%sh; head data/1kg_annotations.txt | column -t. sh: 1: column: not found; head: write error: Broken pipe. This file can be imported into Hail with; HailContext.import_table.; This method produces a; KeyTable; object. Think of this as a Pandas or R dataframe that isn’t limited by; the memory on your machine – behind the scenes, it’s distributed with; Spark. In [12]:. table = hc.import_table('data/1kg_annotations.txt', impute=True)\; .key_by('Sample'). 2018-10-18 01:26:28 Hail: INFO: Reading table to impute column types; 2018-10-18 01:26:28 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed). A good way to peek at the structure of a KeyTable is to look at its; schema. In [13]:. print(table.schema). Struct{Sample:String,Population:String,SuperPopulation",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:6289,error,error,6289,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['error'],['error']
Availability,"type_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:123675,error,errors,123675,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"t’s do a GWAS!¶; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); uncorrelated (not in linkage disequilibrium). Both of these are easy in Hail. In [43]:. common_vds = (vds; .filter_variants_expr('va.qc.AF > 0.01'); .ld_prune(memory_per_core=256, num_cores=4)). 2018-10-18 01:26:50 Hail: INFO: Running LD prune with nSamples=843, nVariants=9085, nPartitions=4, and maxQueueSize=257123.; 2018-10-18 01:26:50 Hail: INFO: LD prune step 1 of 3: nVariantsKept=8478, nPartitions=4, time=351.375ms; 2018-10-18 01:26:51 Hail: INFO: LD prune step 2 of 3: nVariantsKept=8478, nPartitions=12, time=1.184s; 2018-10-18 01:26:52 Hail: INFO: Coerced sorted dataset; 2018-10-18 01:26:52 Hail: INFO: LD prune step 3 of 3: nVariantsKept=8478, time=481.478ms. In [44]:. common_vds.count(). Out[44]:. (843L, 8555L). These filters removed about 15% of sites (we started with a bit over; 10,000). This is NOT representative of most sequencing datasets! We; have already downsampled the full thousand genomes dataset to include; more common variants than we’d expect by chance.; In Hail, the association tests accept sample annotations for the sample; phenotype and covariates. Since we’ve already got our phenotype of; interest (caffeine consumption) in the dataset, we are good to go:. In [45]:. gwas = common_vds.linreg('sa.CaffeineConsumption'); pprint(gwas.variant_schema). 2018-10-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:21466,down,downsampled,21466,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['down'],['downsampled']
Availability,"ue = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; missing_label (str) – Label to use when a point is missing data for a categorical label. Returns:; bokeh.plotting.figure if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.manhattan(pvals, locus=None, title=None, size=4, hover_fields=None, collect_all=None, n_divisions=500, significance_line=5e-08)[source]; Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters:. pvals (Float64Expression) – P-values to be plotted.; locus (LocusExpression, optional) – Locus values to be plotted.; title (str, optional) – Title of the plot.; size (int) – Size of markers in screen space units.; hover_fields (Dict[str, Expression], optional) – Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all (bool, optional) – Deprecated - use n_divisions instead.; n_divisions (int, optional.) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; significance_line (float, optional) – p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If None, no line is added. Returns:; bokeh.models.Plot. hail.plot.output_notebook()[source]; Configure the Bokeh output state to generate output in notebook; cells when bokeh.io.show() is called. Calls; bokeh.io.output_notebook(). hail.plot.visualize_missingness(entry_field, row_field=None, column_field=None, window=6000000, plot_width=1800, plot_height=900)[source]; Visualize missingness in a MatrixTable.; Inspired by naniar.; Row field is windowed by default, and missingness is aggregated over this window to generate a proportion defined.; This windowing is set to 6,000,000 by default, so that the human genome is divided into ~500 rows.; With ~2,000 columns, this function returns a sensibly-sized plot with this windowing. Warning; Generating ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:13114,down,downsample,13114,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['down'],['downsample']
Availability,"ue if that key is not present. items; Returns an array of tuples containing key/value pairs in the dictionary. key_set; Returns the set of keys in the dictionary. keys; Returns an array with all keys in the dictionary. map_values; Transform values of the dictionary according to a function. size; Returns the size of the dictionary. values; Returns an array with all values in the dictionary. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:2016,error,error,2016,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['error'],['error']
Availability,"uester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:; >>> with hfs.open( ; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:5464,error,error,5464,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['error'],['error']
Availability,"ugh=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:15509,toler,tolerance,15509,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['toler'],['tolerance']
Availability,"ult is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str) – The names of the datasets with which to annotate rel. Returns:; MatrixTable or Table – The relational object rel, with the annotations from names; added. property available_datasets; List of names of available annotation datasets. Returns:; list – List of available annotation datasets. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:3473,avail,available,3473,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['avail'],['available']
Availability,"unc:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for a genotype is; the minimum over multiallelic `PL` entries for genotypes that map to that; genotype. `GQ` is recomputed from `PL` if `PL` is provided and is not; missing. If not, it is copied from the original GQ. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split fields in the info field. This means that if a; multiallelic site with `info.AC` value ``[10, 2]`` is split, each split; site will contain the same array ``[10, 2]``. The provided allele index; field `a_index` can be used to select the value corresponding to the split; allele's position:. >>> split_ds = hl.split_multi_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:119765,down,downcoding,119765,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['down'],['downcoding']
Availability,"unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """""". return self.annotate(**{name: hl.scan.count()}). [docs] @typecheck_method(tables=table_type, unify=bool); def union(self, *tables, unify: bool = False) -> 'Table':; """"""Union the rows of multiple tables. Examples; --------. Take the union of rows from two tables:. >>> union_table = table1.union(other_table). Notes; -----; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the `unify` parameter is; ``True``, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters; ----------; tables : varargs of :class:`.Table`; Tables to union.; unify : :obj:`bool`; Attempt to unify table field. Returns; -------; :class:`.Table`; Table with all rows from each component table.; """"""; left_key = self.key.dtype; for (; i,; ht,; ) in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(; f""'union': table {i} has a different key.""; f"" Expected: {left_key}\n""; f"" Table {i}: {ht.key.dtype}""; ). if not (unify or ht.row.dtype == self.row.dtype):; raise ValueError(; f""'union': table {i} has a different row type.\n""; f"" Expected: {self.row.dtype}\n""; f"" Table {i}: {ht.row.dtype}\n""; f"" If the tables have the same fields in different orders, or some\n""; f"" common and some unique fields, then the 'unify' parameter may be\n""; f"" able to coerce the tables to a common type.""; ); all_tables = [self]; all_tables.extend(tables). if unify and not len(set(ht.row_value.dtype for ht in all_tables)) =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:85654,error,error,85654,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['error'],['error']
Availability,"unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:11615,down,downcode,11615,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['down'],['downcode']
Availability,"updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """""". require_row_key_variant(ds, ""split_multi""); new_id = Env.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is_table else ds._rvrow; kept_alleles = hl.range(1, hl.len(old_row.alleles)); if not keep_star:; kept_alleles = kept_alleles.filter(lambda i: old_row.alleles[i] != ""*""). def new_struct(variant, i):; return hl.struct(alleles=variant.alleles, locus=variant.locus, a_index=i, was_split=hl.len(old_row.alleles) > 2). def split_rows(expr, rekey):; if isinstance(ds, MatrixTable):; mt = ds.annotate_rows(**{new_id: expr}).explode_rows(new_id); if rekey:; mt = mt.key_rows_by(); else:; mt = mt.key_rows_by('locus'); new_row_expr = mt._rvrow.annotate(; locus=mt[new_id]['loc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:114164,error,error,114164,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['error'],['error']
Availability,"ur images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:12621,error,error,12621,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['error'],['error']
Availability,"urce mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expected_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0; errors.append(; ExpressionException(; ""scope violation: '{caller}' expects an expression {strictness}indexed by {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}{agg}"".format(; caller=caller,; strictness=strictness,; expected=list(expected_axes),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:2487,error,errors,2487,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,1,['error'],['errors']
Availability,"ure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our dis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1306,avail,available,1306,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['avail'],['available']
Availability,"urns:; MatrixTable – Filtered matrix table. filter_entries(expr, keep=True)[source]; Filter entries of the matrix. Parameters:. expr (bool or BooleanExpression) – Filter expression.; keep (bool) – Keep entries where expr is true. Returns:; MatrixTable – Filtered matrix table. Examples; Keep entries where the sum of AD is greater than 10 and GQ is greater than 20:; >>> dataset_result = dataset.filter_entries((hl.sum(dataset.AD) > 10) & (dataset.GQ > 20)). Warning; When expr evaluates to missing, the entry will be removed regardless of; keep. Note; This method does not support aggregation. Notes; The expression expr will be evaluated for every entry of the table.; If keep is True, then entries where expr evaluates to True; will be kept (the filter removes the entries where the predicate; evaluates to False). If keep is False, then entries where; expr evaluates to True will be removed (the filter keeps the; entries where the predicate evaluates to False).; Filtered entries are removed entirely from downstream operations. This; means that the resulting matrix table has sparsity – that is, that the; number of entries is smaller than the product of count_rows(); and count_cols(). To re-densify a filtered matrix table, use the; unfilter_entries() method to restore filtered entries, populated; all fields with missing values. Below are some properties of an; entry-filtered matrix table. Filtered entries are not included in the entries() table. >>> mt_range = hl.utils.range_matrix_table(10, 10); >>> mt_range = mt_range.annotate_entries(x = mt_range.row_idx + mt_range.col_idx); >>> mt_range.count(); (10, 10). >>> mt_range.entries().count(); 100. >>> mt_filt = mt_range.filter_entries(mt_range.x % 2 == 0); >>> mt_filt.count(); (10, 10). >>> mt_filt.count_rows() * mt_filt.count_cols(); 100. >>> mt_filt.entries().count(); 50. Filtered entries are not included in aggregation. >>> mt_filt.aggregate_entries(hl.agg.count()); 50. >>> mt_filt = mt_filt.annotate_cols(col_n = hl.agg.count",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:33239,down,downstream,33239,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['down'],['downstream']
Availability,"us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 330646029.00001156}. Count¶; The count aggregator is pretty simple - it counts the number of; elements in the aggregable. In [26]:. vds.query_genotypes('gs.count()'). Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:11470,avail,available,11470,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['avail'],['available']
Availability,"us(contig_holder, position, rg). gen_table = gen_table.annotate(locus=locus, alleles=alleles, rsid=rsid, varid=varid); gen_table = gen_table.annotate(; entries=gen_table.data[last_rowf_idx + 1 :]; .map(lambda x: hl.float64(x)); .grouped(3); .map(lambda x: hl.struct(GP=x)); ); if skip_invalid_loci:; gen_table = gen_table.filter(hl.is_defined(gen_table.locus)). sample_table_count = sample_table.count() - 2 # Skipping first 2 unneeded rows in sample file; gen_table = gen_table.annotate_globals(cols=hl.range(sample_table_count).map(lambda x: hl.struct(col_idx=x))); mt = gen_table._unlocalize_entries('entries', 'cols', ['col_idx']). sample_table = sample_table.tail(sample_table_count).add_index(); sample_table = sample_table.annotate(s=sample_table.text.split(' ')[0]); sample_table = sample_table.key_by(sample_table.idx); mt = mt.annotate_cols(s=sample_table[hl.int64(mt.col_idx)].s). mt = mt.annotate_entries(; GP=hl.rbind(; hl.sum(mt.GP),; lambda gp_sum: hl.if_else(; hl.abs(1.0 - gp_sum) > tolerance, hl.missing(hl.tarray(hl.tfloat64)), hl.abs((1 / gp_sum) * mt.GP); ),; ); ); mt = mt.annotate_entries(; GT=hl.rbind(; hl.argmax(mt.GP),; lambda max_idx: hl.if_else(; hl.len(mt.GP.filter(lambda y: y == mt.GP[max_idx])) == 1,; hl.switch(max_idx); .when(0, hl.call(0, 0)); .when(1, hl.call(0, 1)); .when(2, hl.call(1, 1)); .or_error(""error creating gt field.""),; hl.missing(hl.tcall),; ),; ); ); mt = mt.filter_entries(hl.is_defined(mt.GP)). mt = mt.key_cols_by('s').drop('col_idx', 'file', 'data'); mt = mt.key_rows_by('locus', 'alleles').select_entries('GT', 'GP'); return mt. [docs]@typecheck(; paths=oneof(str, sequenceof(str)),; key=table_key_type,; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=oneof(str, sequenceof(str)),; delimiter=str,; missing=oneof(str, sequenceof(str)),; types=dictof(str, hail_type),; quote=nullable(char),; skip_blank_lines=bool,; force_bgz=bool,; filter=nullable(str),; find_replace=nullable(sized_tupleof(str, str)),; force=bool,; sour",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:52729,toler,tolerance,52729,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['toler'],['tolerance']
Availability,"use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTable.; We can use rows along with select to pull out 5 variants. The select method takes either a string refering to a field name in the table, or a Hail Expression. Here, we leave the arguments blank to keep only the row key fields, locus and alleles.; Use the show method to display the variants. [6]:. mt.rows().select().show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""];",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2925,down,downstream,2925,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['down'],['downstream']
Availability,"ut=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; ) -> 'Table':; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`Table`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_table`. It is; possible to read the file at this path later with :func:`.read_table`. Examples; --------; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); _assert_type = self._type; _load_refs = False; else:; _assert_type = None; _load_refs = True; return hl.read_table(; output,; _intervals=_intervals,; _filter_intervals=_filter_intervals,; _assert_type=_assert_type,; _load_refs=_load_refs,; ). [docs] @typecheck_method(output=str, overwrite=bool, stage_locally=bool, _codec_spec=nullable(str)); def write(self, output: str, overwrite=False, stage_locally: bool = False, _codec_spec: Optional[str] = None):; """"""Write to disk. Examples; --------. >>> table1.write('output/table1.ht', overwrite=True). .. include:: _templates/write_warning.rst. See Also; --------; :func:`.read_table`. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major out",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:59540,checkpoint,checkpoint,59540,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['checkpoint'],['checkpoint']
Availability,"ute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-or x and y.; Examples; >>> hl.eval(hl.bit_xor(5, 3)); 6. Notes; See the Python wiki; for more information about bit o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:3975,toler,tolerance,3975,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,"val. If the .bed file has four or more columns, then Hail will store the fourth column in the table:. interval (Interval) - Genomic interval.; target (String) - Fourth column of .bed file. UCSC bed files can have up to 12 fields, ; but Hail will only ever look at the first four. Hail ignores header lines in BED files. Caution; UCSC BED files are 0-indexed and end-exclusive. The line “5 100 105” will contain; locus 5:105 but not 5:100. Details here. Parameters:path (str) – Path to .bed file. Return type:KeyTable. static import_fam(path, quantitative=False, delimiter='\\\\s+', missing='NA')[source]¶; Import PLINK .fam file into a key table.; Examples; Import case-control phenotype data from a tab-separated PLINK .fam file into sample; annotations:; >>> fam_kt = KeyTable.import_fam('data/myStudy.fam'). In Hail, unlike PLINK, the user must explicitly distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype without quantitative=True will return an error; (unless all values happen to be 0, 1, 2, and -9):; >>> fam_kt = KeyTable.import_fam('data/myStudy.fam', quantitative=True). Columns; The column, types, and missing values are shown below. ID (String) – Sample ID (key column); famID (String) – Family ID (missing = “0”); patID (String) – Paternal ID (missing = “0”); matID (String) – Maternal ID (missing = “0”); isFemale (Boolean) – Sex (missing = “NA”, “-9”, “0”). One of:. isCase (Boolean) – Case-control phenotype (missing = “0”, “-9”, non-numeric or the missing argument, if given.; qPheno (Double) – Quantitative phenotype (missing = “NA” or the missing argument, if given. Parameters:; path (str) – Path to .fam file.; quantitative (bool) – If True, .fam phenotype is interpreted as quantitative.; delimiter (str) – .fam file field delimiter regex.; missing (str) – The string used to denote missing values.; For case-control, 0, -9, and non-numeric are also treated; as missing. Returns:Key table with information from .fam file. Return ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:13548,error,error,13548,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['error'],['error']
Availability,"variates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covari",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64138,toler,tolerance,64138,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"variates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:154917,error,errors,154917,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; NumericExpression. View page source. NumericExpression. class hail.expr.NumericExpression[source]; Expression of numeric type.; >>> x = hl.literal(3). >>> y = hl.literal(4.5). Attributes. dtype; The data type of the expression. Methods. __add__(other)[source]; Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other)[source]; Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other)[source]; Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other)[source]; Less-than-or-equals compar,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:1347,error,error,1347,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,1,['error'],['error']
Availability,"ve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83756,checkpoint,checkpoints,83756,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['checkpoint'],['checkpoints']
Availability,"vec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < toleranc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:67875,toler,tolerance,67875,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['toler'],['tolerance']
Availability,"verview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Ext",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Expression-1.html:1433,error,error,1433,docs/0.2/hail.expr.Expression-1.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html,2,['error'],['error']
Availability,"visions=nullable(int),; significance_line=nullable(numeric),; ); def manhattan(; pvals: 'Float64Expression',; locus: 'Optional[LocusExpression]' = None,; title: 'Optional[str]' = None,; size: int = 4,; hover_fields: 'Optional[Dict[str, Expression]]' = None,; collect_all: 'Optional[bool]' = None,; n_divisions: 'Optional[int]' = 500,; significance_line: 'Optional[Union[int, float]]' = 5e-8,; ) -> Plot:; """"""Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters; ----------; pvals : :class:`.Float64Expression`; P-values to be plotted.; locus : :class:`.LocusExpression`, optional; Locus values to be plotted.; title : str, optional; Title of the plot.; size : int; Size of markers in screen space units.; hover_fields : Dict[str, :class:`.Expression`], optional; Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all : bool, optional; Deprecated - use `n_divisions` instead.; n_divisions : int, optional.; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; significance_line : float, optional; p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If ``None``, no line is added. Returns; -------; :class:`bokeh.models.Plot`; """"""; if locus is None:; locus = pvals._indices.source.locus. ref = locus.dtype.reference_genome. if hover_fields is None:; hover_fields = {}. hover_fields['locus'] = hail.str(locus). pvals = -hail.log10(pvals). source_pd = _collect_scatter_plot_data(; ('_global_locus', locus.global_position()),; ('_pval', pvals),; fields=hover_fields,; n_divisions=_downsampling_factor('manhattan', n_divisions, collect_all),; ); source_pd['p_value'] = [10 ** (-p) for p in source_pd['_pval']]; source_pd['_contig'] = [locus.split("":"")[0] for locus in source_pd['locus']]. observed_contigs = [contig for contig in ref.contigs.copy() if contig in set(source_pd['_contig'])]. contig_ticks = [ref.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:52482,down,downsample,52482,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['down'],['downsample']
Availability,"w page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that take",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1424,avail,available,1424,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['avail'],['available']
Availability,"w per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157134,error,error,157134,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['error'],['error']
Availability,"w_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7679,checkpoint,checkpoint,7679,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,1,['checkpoint'],['checkpoint']
Availability,"we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:10507,checkpoint,checkpoint,10507,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['checkpoint'],['checkpoint']
Availability,"where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few ent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77882,error,error,77882,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.ba",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74770,avail,available,74770,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['avail'],['available']
Availability,"will be; determined from their corresponding Hail types. To output a desired; Description, Number, and/or Type value in a FORMAT or INFO field or to; specify FILTER lines, use the metadata parameter to supply a dictionary; with the relevant information. See; get_vcf_metadata() for how to obtain the; dictionary corresponding to the original VCF, and for info on how this; dictionary should be structured.; The output VCF header will also contain CONTIG lines; with ID, length, and assembly fields derived from the reference genome of; the dataset.; The output VCF header will not contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; append_to_header parameter. Warning; INFO fields stored at VCF import are not automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating info, downstream; tools which may produce erroneous results. The solution is to create new; fields in info or overwrite existing fields. For example, in order to; produce an accurate AC field, one can run variant_qc() and copy; the variant_qc.AC field to info.AC as shown below.; >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the head",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:50222,down,downstream,50222,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['down'],['downstream']
Availability,"with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fie",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62201,toler,tolerance,62201,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"wnsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:47387,error,error,47387,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['error'],['error']
Availability,"x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt = mt.annotate_globals(; yvec=(; hl.case(); .when(mt.n - k - 1 >= 1, hl.nd.array(mt.yvec)); .or_error(; hl.format(""_lowered_poisson_regression_rows: insufficient degrees of freedom: n=%s, k=%s"", mt.n, k); ); ),; covmat=hl.nd.array(mt.covmat),; n_complete_samples=mt.n,; ); covmat = mt.covmat; yvec = mt.yvec; n = mt.n_complete_samples. logmean = hl.log(yvec.sum() / n); b = hl.nd.array([logmean, *[0 for _ in range(k - 1)]]); mu = hl.exp(covmat @ b); residual = yvec - mu; score = covmat.T @ residual; fisher = (mu * covmat.T) @ covmat; mt = mt.annotate_globals(null_fit=_poisson_fit(covmat, yvec, b, mu, score, fisher, max_iterations, tolerance)); mt = mt.annotate_globals(; null_fit=hl.case(); .when(mt.null_fit.converged, mt.null_fit); .or_error(; hl.format(; '_lowered_poisson_regression_rows: null model did not converge: %s',; mt.null_fit.select('n_iterations', 'log_lkhd', 'converged', 'exploded'),; ); ); ); mt = mt.annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:66009,toler,tolerance,66009,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['toler'],['tolerance']
Availability,"x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; data = _generate_hist2d_data(x, y, bins, range).to_pandas(). # Use python prettier float -> str function; data['x'] = data['x'].apply(lambda e: str(float(e))); data['y'] = data['y'].apply(lambda e: str(float(e))). mapper: ColorMapper; if log:; mapper = LogColorMapper(palette=colors, low=data.c.min(), high=data.c.max()); else:; mapper = LinearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:18648,avail,available,18648,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['avail'],['available']
Availability,"x_shift=100, keep_star=False)[source]¶; Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. aIndex will never be zero). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:; >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of aIndices because; we are mapping between the old and new allele indices, not; the alternate allele indices.; Notes; If filter_altered_genotypes is true, genotypes that contain filtered-out alleles are set to missing.; filter_alleles() implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. Subset algorithm; The subset algorithm (the default, subset=True) subsets the; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:48426,down,downcode,48426,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcode']
Availability,"x_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:14871,down,down,14871,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,1,['down'],['down']
Availability,"x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of type :class:`.tarray` of :obj:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""gqFromPL"", tint32, pl). [docs]@typecheck(n=expr_int32); def triangle(n) -> Int32Expression:; """"""Returns the triangle number of `n`. Examples; --------. >>> hl.eval(hl.triangle(3)); 6. Notes; -----; The calculation is ``n * (n + 1) / 2``. Parameters; ----------; n : :class:`.Expression` of type :py:data:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""triangle"", tint32, n). [docs]@typecheck(f=func_spec(1, expr_bool), collection=expr_oneof(expr_set(), expr_array())); def filter(f: Callable, collection):; """"""Returns a new",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105751,down,downcode,105751,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['down'],['downcode']
Availability,"xed; the values are found in the range; ``[0, N)``, where ``N`` is the total number of rows. Parameters; ----------; name : :class:`str`; Name for row index field. Returns; -------; :class:`.MatrixTable`; Dataset with new field.; """"""; return self.annotate_rows(**{name: hl.scan.count()}). [docs] @typecheck_method(name=str); def add_col_index(self, name: str = 'col_idx') -> 'MatrixTable':; """"""Add the integer index of each column as a new column field. Examples; --------. >>> dataset_result = dataset.add_col_index(). Notes; -----; The field added is type :py:data:`.tint32`. The column index is 0-indexed; the values are found in the range; ``[0, N)``, where ``N`` is the total number of columns. Parameters; ----------; name: :class:`str`; Name for column index field. Returns; -------; :class:`.MatrixTable`; Dataset with new field.; """"""; return self.annotate_cols(**{name: hl.scan.count()}). @typecheck_method(other=matrix_table_type, tolerance=numeric, absolute=bool, reorder_fields=bool); def _same(self, other, tolerance=1e-6, absolute=False, reorder_fields=False) -> bool:; entries_name = Env.get_uid('entries_'); cols_name = Env.get_uid('columns_'). fd_f = set if reorder_fields else list. if fd_f(self.row) != fd_f(other.row):; print(f'Different row fields: \n {list(self.row)}\n {list(other.row)}'); return False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False; if fd_f(self.col) != fd_f(other.col):; print(f'Different col fields: \n {list(self.col)}\n {list(other.col)}'); return False; if fd_f(self.entry) != fd_f(other.entry):; print(f'Different row fields: \n {list(self.entry)}\n {list(other.entry)}'); return False. if reorder_fields:; entry_order = list(self.entry); if list(other.entry) != entry_order:; other = other.select_entries(*entry_order). globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). col_order",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:113376,toler,tolerance,113376,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['toler'],['tolerance']
Availability,"xperimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:4808,avail,available,4808,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['avail'],['available']
Availability,"xplicitly adding all input files as input resource; files to the batch so to make sure the same code can run in all scenarios. Files that are already; in a Docker image do not need to be read as inputs to the batch. Output Files; All files generated by Batch are temporary files! They are copied as appropriate between jobs; for downstream jobs’ use, but will be removed when the batch has completed. In order to save; files generated by a batch for future use, you need to explicitly call Batch.write_output().; The first argument to Batch.write_output() can be any type of ResourceFile which includes input resource; files and job resource files as well as resource groups as described below. The second argument to write_output; should be either a local file path or a google storage file path when using the LocalBackend.; For the ServiceBackend, the second argument must be a google storage file path.; >>> b = hb.Batch(name='hello-input'); >>> j = b.new_job(name='hello'); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Resource Groups; Many bioinformatics tools treat files as a group with a common file; path and specific file extensions. For example, PLINK; stores genetic data in three files: *.bed has the genotype data,; *.bim has the variant information, and *.fam has the sample information.; PLINK can take as an input the path to the files expecting there will be three; files with the appropriate extensions. It also writes files with a common file root and; specific file extensions including when writing out a new dataset or outputting summary statistics.; To enable Batch to work with file groups, we added a ResourceGroup object; that is essentially a dictionary from file extension name to file path. When creating; a ResourceGroup in a Job (equivalent to a JobResourceFile),; you first need to use the method BashJob.declare_resource_group() to declare the files; in the resource group explicitly before referring t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:13067,echo,echo,13067,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"xpr_any); def str(x) -> StringExpression:; """"""Returns the string representation of `x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of type :class:`.tarray` of :obj:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""gqFromPL"", tint32, pl). [docs]@typecheck(n=expr_int32); def triangle(n) -> Int32Expression:; """"""Returns the triangle number of `n`. Examples; --------. >>> hl.eval(hl.triangle(3)); 6. Notes; -----; The calculation is ``n * (n + 1) / 2``. Parameters; ----------; n : :class:`.Expression` of type :py:data:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""triangle"", tint32, n). [docs]@typecheck(f=func_spec(1, expr_bool), collection=expr_oneof",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105664,down,downcoded,105664,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['down'],['downcoded']
Availability,xpression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.NumericExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). (hail.expr.StructExpression method). (hail.expr.TupleExpression method). (hail.GroupedMatrixTable method). (hail.MatrixTable method). (hail.Table method). diagonal() (hail.linalg.BlockMatrix method). (in module hail.nd). dict() (in module hail.expr.functions). DictExpression (class in hail.expr). difference() (hail.expr.SetExpression method). distinct() (hail.Table method). distinct_by_col() (hail.MatrixTable method). distinct_by_row() (hail.MatrixTable method). dnorm() (in module hail.expr.functions). downcode() (in module hail.expr.functions). downsample() (in module hail.expr.aggregators). dpois() (in module hail.expr.functions). drop() (hail.expr.StructExpression method). (hail.MatrixTable method). (hail.Table method). dtype (hail.expr.ArrayExpression property). (hail.expr.ArrayNumericExpression property). (hail.expr.BooleanExpression property). (hail.expr.CallExpression property). (hail.expr.CollectionExpression property). (hail.expr.DictExpression property). (hail.expr.Expression property). (hail.expr.Float32Expression property). (hail.expr.Float64Expression property). (hail.expr.Int32Expression property). (hail.expr.Int64Expression property). (hail.expr.IntervalExpression property). (hail.expr.LocusExpression property). (hail.expr.NDArrayExpression property). (hail.expr.NDArrayNumericExpression property). (hail.expr.NumericExpression property). (hail.expr.SetExpression property). (hail.expr.StringExpression property). (hail.expr.StructExpression property). (hail.expr.TupleExpression property). dtype() (in module hail.expr.types). E. ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:18200,down,downsample,18200,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['down'],['downsample']
Availability,"xpressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1483,avail,available,1483,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['avail'],['available']
Availability,"y 1 Gi/core, ‘standard’ corresponds to approximately; 4 Gi/core, and ‘highmem’ corresponds to approximately 7 Gi/core.; The default value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the job’s storage size.; Examples; Set the job’s disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7200,echo,echo,7200,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['echo'],['echo']
Availability,"y ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+=========+========+============+=========",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7198,error,errors,7198,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['error'],['errors']
Availability,"y exports the contents of ``va.info`` to the INFO field. No other annotations besides ``va.info`` are exported. The genotype schema must have the type :py:class:`~hail.expr.TGenotype` or :py:class:`~hail.expr.TStruct`. If the type is; :py:class:`~hail.expr.TGenotype`, then the FORMAT fields will be GT, AD, DP, GQ, and PL (or PP if ``export_pp`` is True).; If the type is :py:class:`~hail.expr.TStruct`, then the exported FORMAT fields will be the names of each field of the Struct.; Each field must have a type of String, Char, Int, Double, or Call. Arrays and Sets are also allowed as long as they are not nested.; For example, a field with type ``Array[Int]`` can be exported but not a field with type ``Array[Array[Int]]``.; Nested Structs are also not allowed. .. caution::. If samples or genotypes are filtered after import, the value stored in ``va.info.AC`` value may no longer reflect the number of called alternate alleles in the filtered VDS. If the filtered VDS is then exported to VCF, downstream tools may produce erroneous results. The solution is to create new annotations in ``va.info`` or overwrite existing annotations. For example, in order to produce an accurate ``AC`` field, one can run :py:meth:`~hail.VariantDataset.variant_qc` and copy the ``va.qc.AC`` field to ``va.info.AC``:. >>> (vds.filter_genotypes('g.gq >= 20'); ... .variant_qc(); ... .annotate_variants_expr('va.info.AC = va.qc.AC'); ... .export_vcf('output/example.vcf.bgz')). :param str output: Path of .vcf file to write. :param append_to_header: Path of file to append to VCF header.; :type append_to_header: str or None. :param bool export_pp: If true, export linear-scaled probabilities (Hail's `pp` field on genotype) as the VCF PP FORMAT field. :param bool parallel: If true, return a set of VCF files (one per partition) rather than serially concatenating these files.; """""". self._jvdf.exportVCF(output, joption(append_to_header), export_pp, parallel). [docs] @handle_py4j; @convertVDS; @typecheck_method(o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:58627,down,downstream,58627,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downstream']
Availability,"y list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:17178,error,error,17178,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['error'],['error']
Availability,"y of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, K",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:50288,error,errors,50288,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"y the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. It’s behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[‘identifier’]. If an object for that identifier doesn’t exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1914,echo,echo,1914,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,1,['echo'],['echo']
Availability,"y: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); _assert_type = self._type; _load_refs = False; else:; _assert_type = None; _load_refs = True; return hl.read_matrix_table(; output,; _intervals=_intervals,; _filter_intervals=_filter_intervals,; _drop_cols=_drop_cols,; _drop_rows=_drop_rows,; _assert_type=_assert_type,; _load_refs=_load_refs,; ). [docs] @typecheck_method(; output=str, overwrite=bool, stage_locally=bool, _codec_spec=nullable(str), _partitions=nullable(expr_any); ); def write(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _partitions=None,; ):; """"""Write to disk. Examples; --------. >>> dataset.write('output/dataset.mt'). .. include:: _templates/write_warning.rst. See Also; --------; :func:`.read_matrix_table`. P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:81223,checkpoint,checkpoint,81223,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['checkpoint'],['checkpoint']
Availability,"y:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pn",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71353,fault,fault,71353,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['fault'],['fault']
Availability,"yTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:155052,error,errors,155052,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"y_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7539,checkpoint,checkpoint,7539,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,1,['checkpoint'],['checkpoint']
Availability,"ym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4911,error,errors,4911,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['error'],['errors']
Availability,"{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:56639,error,error,56639,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"{list(other.globals)}'); return False; if fd_f(self.col) != fd_f(other.col):; print(f'Different col fields: \n {list(self.col)}\n {list(other.col)}'); return False; if fd_f(self.entry) != fd_f(other.entry):; print(f'Different row fields: \n {list(self.entry)}\n {list(other.entry)}'); return False. if reorder_fields:; entry_order = list(self.entry); if list(other.entry) != entry_order:; other = other.select_entries(*entry_order). globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). col_order = list(self.col); if list(other.col) != col_order:; other = other.select_cols(*col_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select_rows(*row_order). if list(self.col_key) != list(other.col_key):; print(f'different col keys:\n {list(self.col_key)}\n {list(other.col_key)}'); return False. return self._localize_entries(entries_name, cols_name)._same(; other._localize_entries(entries_name, cols_name), tolerance, absolute; ). @typecheck_method(caller=str, s=expr_struct()); def _select_entries(self, caller, s) -> 'MatrixTable':; base, cleanup = self._process_joins(s); analyze(caller, s, self._entry_indices); return cleanup(MatrixTable(ir.MatrixMapEntries(base._mir, s._ir))). @typecheck_method(caller=str, row=expr_struct()); def _select_rows(self, caller, row) -> 'MatrixTable':; analyze(caller, row, self._row_indices, {self._col_axis}); base, cleanup = self._process_joins(row); return cleanup(MatrixTable(ir.MatrixMapRows(base._mir, row._ir))). @typecheck_method(caller=str, col=expr_struct(), new_key=nullable(sequenceof(str))); def _select_cols(self, caller, col, new_key=None) -> 'MatrixTable':; analyze(caller, col, self._col_indices, {self._row_axis}); base, cleanup = self._process_joins(col); return cleanup(MatrixTable(ir.MatrixMapCols(base._mir, col._ir, new_key))). @typecheck_method(caller=str, s=expr_struct()); def _select_globals(self, caller, s) -> 'MatrixTable':; base,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:114880,toler,tolerance,114880,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['toler'],['tolerance']
Availability,"} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, MatrixTable.annotate_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.agg.linreg with no aggregated data records.; (#9028) Fixed memory; leak affecting Table.annotate with scans,; hl.experimental.densify, and Table.group_by / aggregate.; (#8978) Fixed; aggregation behavior of; MatrixTable.{group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:68825,error,error,68825,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24688,error,error,24688,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['error'],['error']
Availability,"’ll use the Table.parallelize() method to create two small tables, t1 and t2. [4]:. t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). [5]:. t1.show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. abstrint32; ""bar""2; ""bar""2; ""foo""1. [6]:. t2.show(). txstrfloat64; ""bar""2.78e+00; ""bar""-1.00e+00; ""foo""3.14e+00; ""quam""0.00e+00. Now, we can join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new joined table j has a field t2_x that comes from the rows of t2. The tables could be joined, because they shared the same number of keys (1) and the same key type (string). The keys do not need to share the same name. Notice that the rows with keys present in t2 but not in t1 do not show up in the final result.; This join syntax performs a left join. Tables also have a SQL-style inner/left/right/outer join() method.; The magic of keys is that they can be used to create a mapping, like a Python dictionary, between the keys of one table and the row value",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:4536,down,down,4536,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['down'],['down']
Availability,". (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:38605,error,errors,38605,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference genome from a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Returns:; ReferenceGenome. property global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. Returns:; dict – A dictionary of contig names to global genomic positions. has_liftover(dest_reference_genome)[source]; True if a liftover chain file is available from this reference; genome to the destination reference. Parameters:; dest_reference_genome (str or ReferenceGenome). Returns:; bool. has_sequence()[source]; True if the reference sequence has been loaded. Returns:; bool. property lengths; Dict of contig name to contig length. Returns:; dict of str to int. locus_from_global_position(global_pos)[source]; ”; Constructs a locus from a global position in reference genome.; The inverse of Locus.position().; Examples; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters:; global_pos (int) – Zero-based global base position along the reference genome. Returns:; Locus. property mt_contigs; Mi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:7240,avail,available,7240,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['avail'],['available']
Availability,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:812,down,download,812,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,['down'],['download']
Availability,"﻿. . Overview — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:219,down,download,219,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['down'],['download']
Availability,"﻿. . Overview: module code — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Overview: module code. All modules for which code is available; hail.context; hail.dataset; hail.expr; hail.keytable; hail.kinshipMatrix; hail.ldMatrix; hail.representation.annotations; hail.representation.genotype; hail.representation.interval; hail.representation.pedigree; hail.representation.variant; hail.utils. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/index.html:318,avail,available,318,docs/0.1/_modules/index.html,https://hail.is,https://hail.is/docs/0.1/_modules/index.html,1,['avail'],['available']
Availability,"﻿. . Tutorials — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials-landing.html:220,down,download,220,docs/0.1/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html,2,['down'],['download']
Availability,"﻿. . Using the expression language to slice, dice, and query genetic data — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Introduction to the expression language; Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:433,down,download,433,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['down'],['download']
Availability,"﻿. Batch Service — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Loca",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:509,avail,available,509,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['avail'],['available']
Availability,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets.html:737,avail,available,737,docs/0.2/datasets.html,https://hail.is,https://hail.is/docs/0.2/datasets.html,1,['avail'],['available']
Availability,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials-landing.html:835,down,downloading,835,docs/0.2/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html,1,['down'],['downloading']
Availability,"﻿. Hail | ; Numeric functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum elem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:826,toler,tolerance,826,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,﻿. Hail | ; Overview: module code. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Overview: module code. All modules for which code is available; hail.context; hail.experimental.datasets; hail.experimental.db; hail.experimental.export_entries_by_col; hail.experimental.expressions; hail.experimental.filtering_allele_frequency; hail.experimental.full_outer_join_mt; hail.experimental.import_gtf; hail.experimental.ld_score_regression; hail.experimental.ldscore; hail.experimental.ldscsim; hail.experimental.loop; hail.experimental.pca; hail.experimental.phase_by_transmission; hail.experimental.plots; hail.experimental.tidyr; hail.experimental.time; hail.expr.aggregators.aggregators; hail.expr.builders; hail.expr.expressions.base_expression; hail.expr.expressions.expression_utils; hail.expr.expressions.typed_expressions; hail.expr.functions; hail.expr.types; hail.genetics.allele_type; hail.genetics.call; hail.genetics.locus; hail.genetics.pedigree; hail.genetics.reference_genome; hail.ggplot.aes; hail.ggplot.coord_cartesian; hail.ggplot.facets; hail.ggplot.geoms; hail.ggplot.ggplot; hail.ggplot.labels; hail.ggplot.scale; hail.linalg.blockmatrix; hail.linalg.utils.misc; hail.matrixtable; hail.methods.family_methods; hail.methods.impex; hail.methods.misc; hail.methods.pca; hail.methods.qc; hail.methods.relatedness.identity_by_descent; hail.methods.relatedness.king; hail.methods.relatedness.mating_simulation; hail.methods.relatedness.pc_relate; hail.methods.statgen; hail.nd.nd; hail.plot.plots; hail.stats.linear_mixed_model; hail.table; hail.utils.hadoop_utils; hail.utils.interval; hail.utils.misc; hail.utils.struct; hail.utils.tutorial; hail.vds.c,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/index.html:470,avail,available,470,docs/0.2/_modules/index.html,https://hail.is,https://hail.is/docs/0.2/_modules/index.html,1,['avail'],['available']
Availability,"﻿. Hail | ; VariantDatasetCombiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VariantDatasetCombiner. View page source. VariantDatasetCombiner. class hail.vds.combiner.VariantDatasetCombiner[source]; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets.; Examples; A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:813,failure,failure-tolerant,813,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['failure'],['failure-tolerant']
Availability,"﻿. Hail | ; hail.utils.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.misc. Source code for hail.utils.misc; import atexit; import datetime; import difflib; import json; import os; import re; import secrets; import shutil; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:929,error,error,929,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,1,['error'],['error']
Availability,"﻿. Hail | ; hail.vds.split_multi. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.split_multi. View page source. hail.vds.split_multi. hail.vds.split_multi(vds, *, filter_changed_loci=False)[source]; Split the multiallelic variants in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; filter_changed_loci (bool) – If any REF/ALT pair changes locus under min_rep(), filter that; variant instead of throwing an error. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html:1054,error,error,1054,docs/0.2/vds/hail.vds.split_multi.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html,1,['error'],['error']
Availability,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html:1151,down,downstream,1151,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html,1,['down'],['downstream']
Availability,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/batch_api.html:645,avail,available,645,docs/0.2/batch_api.html,https://hail.is,https://hail.is/docs/0.2/batch_api.html,4,['avail'],['available']
Deployability," # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:8350,pipeline,pipeline,8350,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['pipeline'],['pipeline']
Deployability," '3'). >>> hl.eval(t[2]); '3'. Parameters:; iterable (an iterable of Expression) – Tuple elements. Returns:; TupleExpression. hail.expr.functions.array(collection)[source]; Construct an array expression.; Examples; >>> s = {'Bob', 'Charlie', 'Alice'}. >>> hl.eval(hl.array(s)); ['Alice', 'Bob', 'Charlie']. Parameters:; collection (ArrayExpression or SetExpression or DictExpression). Returns:; ArrayExpression. hail.expr.functions.empty_array(t)[source]; Returns an empty array of elements of a type t.; Examples; >>> hl.eval(hl.empty_array(hl.tint32)); []. Parameters:; t (str or HailType) – Type of the array elements. Returns:; ArrayExpression. hail.expr.functions.set(collection)[source]; Convert a set expression.; Examples; >>> s = hl.set(['Bob', 'Charlie', 'Alice', 'Bob', 'Bob']); >>> hl.eval(s) ; {'Alice', 'Bob', 'Charlie'}. Returns:; SetExpression – Set of all unique elements. hail.expr.functions.empty_set(t)[source]; Returns an empty set of elements of a type t.; Examples; >>> hl.eval(hl.empty_set(hl.tstr)); set(). Parameters:; t (str or HailType) – Type of the set elements. Returns:; SetExpression. hail.expr.functions.dict(collection)[source]; Creates a dictionary.; Examples; >>> hl.eval(hl.dict([('foo', 1), ('bar', 2), ('baz', 3)])); {'bar': 2, 'baz': 3, 'foo': 1}. Notes; This method expects arrays or sets with elements of type ttuple; with 2 fields. The first field of the tuple becomes the key, and the second; field becomes the value. Parameters:; collection (DictExpression or ArrayExpression or SetExpression). Returns:; DictExpression. hail.expr.functions.empty_dict(key_type, value_type)[source]; Returns an empty dictionary with key type key_type and value type; value_type.; Examples; >>> hl.eval(hl.empty_dict(hl.tstr, hl.tint32)); {}. Parameters:. key_type (str or HailType) – Type of the keys.; value_type (str or HailType) – Type of the values. Returns:; DictExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/constructors.html:7165,update,updated,7165,docs/0.2/functions/constructors.html,https://hail.is,https://hail.is/docs/0.2/functions/constructors.html,1,['update'],['updated']
Deployability," (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None); show(p). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:7472,update,updated,7472,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['update'],['updated']
Deployability," ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30755,update,update,30755,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability," ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_generation_tup, idx):; prev_size = prev_generation_tup[1]; n_new = hl.int32(hl.floor(prev_size * generation_size_multiplier)); new_generation = hl.range(n_new).map(; lambda i: hl.struct(; s=hl.str('generation_') + hl.str(idx + 1) + hl.str('_idx_') + hl.str(i),; original=hl.missing('str'),; mother=hl.rand_int32(0, prev_size),; father=hl.rand_int32(0, prev_size),; ); ); return (new_generation, (prev_size + n_new) if keep_founders else n_new). ht = ht.annotate_globals(; generations=hl.range(n_rounds).scan(; lambda prev, idx: make_new_generation(prev, idx), (ht.generation_0, hl.len(ht.generation_0)); ); ). def simulate_mating_calls(prev_generation_calls, new_generation):; new_samples = new_generation.map(; lambda samp: hl.call(; prev_generation_calls[samp.mother][hl.rand_int32(0, 2)],; prev_generation_calls[samp.father][hl.rand_int32(0, 2)],; ); ); if keep_founders:; return prev_generation_calls.extend(new_samples); else:; return new_samples. ht = ht.annotate(; __new_entries=hl.fold(; lambda prev_calls, generation_metadata: simulate_mating_calls(prev_calls, generation_metadata[0]),; ht.__entries.GT,; ht.generations[1:],; ).map(lambda gt: hl.struct(GT=gt)); ); ht = ht.annotate_globals(; __new_cols=ht.generations.flatmap(lambda x: x[0]) if keep_founders else ht.generations[-1][0]; ); ht = ht.drop('__entries', '__cols', 'generation_0', 'generations'); return ht._unlocalize_entries('__new_entries', '__new_cols', list('s')). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html:3481,update,updated,3481,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,1,['update'],['updated']
Deployability," +-------+-------+-----+-------+-------+----------+-------+----------+; +----------------+; | HT_DESCRIPTION |; +----------------+; | str |; +----------------+; | ""sixty-five"" |; | ""seventy-two"" |; | ""seventy"" |; | ""sixty"" |; +----------------+. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Expressions for new fields. Returns; -------; :class:`.Table`; Table with new fields. """"""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:42409,pipeline,pipeline,42409,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['pipeline'],['pipeline']
Deployability," 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulip",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:50454,release,release,50454,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability," : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """""". Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name); if dest_reference_genome.name in self._liftovers:; raise KeyError(f""Liftover already exists from {self.name} to {dest_reference_genome.name}.""); if dest_reference_genome.name == self.name:; raise ValueError(f'Destination reference genome cannot have the same name as this reference {self.name}.'); self._liftovers[dest_reference_genome.name] = chain_file. [docs] @typecheck_method(global_pos=int); def locus_from_global_position(self, global_pos: int) -> 'hl.Locus':; """""" ""; Constructs a locus from a global position in reference genome.; The inverse of :meth:`.Locus.position`. Examples; --------; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters; ----------; global_pos : int; Zero-based global base position along the reference genome. Returns; -------; :class:`.Locus`; """"""; if global_pos < 0:; raise ValueError(f""global_pos must be non-negative, got {global_pos}""). if self._global_positions_list is None:; # dicts are in insertion order as of 3.7; self._global_positions_list = list(self.global_positions_dict.values()). global_positions = self._global_positions_list; contig = self.contigs[bisect_right(global_positions, global_pos) - 1]; contig_pos = self.global_positions_dict[contig]. if global_pos >= contig_pos + self.lengths[contig]:; raise ValueError(f""global_pos {global_pos} exceeds length of reference genome {self}.""). return hl.Locus(contig, global_pos - contig_pos + 1, self). rg_type.set(ReferenceGenome). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:16681,update,updated,16681,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['update'],['updated']
Deployability," = right.localize_entries('right_entries', 'right_cols'). ht = left_t.join(right_t, how='outer'); ht = ht.annotate_globals(; left_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.left_cols.map(lambda x: hl.tuple([x[f] for f in left.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; right_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.right_cols.map(lambda x: hl.tuple([x[f] for f in right.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; ); ht = ht.annotate_globals(; key_indices=hl.array(ht.left_keys.key_set().union(ht.right_keys.key_set())); .map(lambda k: hl.struct(k=k, left_indices=ht.left_keys.get(k), right_indices=ht.right_keys.get(k))); .flatmap(; lambda s: hl.case(); .when(; hl.is_defined(s.left_indices) & hl.is_defined(s.right_indices),; hl.range(0, s.left_indices.length()).flatmap(; lambda i: hl.range(0, s.right_indices.length()).map(; lambda j: hl.struct(k=s.k, left_index=s.left_indices[i], right_index=s.right_indices[j]); ); ),; ); .when(; hl.is_defined(s.left_indices),; s.left_indices.map(lambda elt: hl.struct(k=s.k, left_index=elt, right_index=hl.missing('int32'))),; ); .when(; hl.is_defined(s.right_indices),; s.right_indices.map(lambda elt: hl.struct(k=s.k, left_index=hl.missing('int32'), right_index=elt)),; ); .or_error('assertion error'); ); ); ht = ht.annotate(; __entries=ht.key_indices.map(; lambda s: hl.struct(left_entry=ht.left_entries[s.left_index], right_entry=ht.right_entries[s.right_index]); ); ); ht = ht.annotate_globals(; __cols=ht.key_indices.map(; lambda s: hl.struct(; **{f: s.k[i] for i, f in enumerate(left.col_key)},; left_col=ht.left_cols[s.left_index],; right_col=ht.right_cols[s.right_index],; ); ); ); ht = ht.drop('left_entries', 'left_cols', 'left_keys', 'right_entries', 'right_cols', 'right_keys', 'key_indices'); return ht._unlocalize_entries('__entries', '__cols', list(left.col_key)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html:6017,update,updated,6017,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,1,['update'],['updated']
Deployability," Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1361,release,releases,1361,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['release'],['releases']
Deployability," Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Pre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:1071,configurat,configuration,1071,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['configurat'],['configuration']
Deployability," Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89345,pipeline,pipelines,89345,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:7642,release,released,7642,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['released']
Deployability," Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:23784,install,installation,23784,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installation']
Deployability," PositionScaleDiscrete(""y"", name=name, breaks=breaks, labels=labels). [docs]def scale_x_genomic(reference_genome, name=None):; """"""The default genomic x scale. This is used when the ``x`` aesthetic corresponds to a :class:`.LocusExpression`. Parameters; ----------; reference_genome:; The reference genome being used.; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleGenomic(""x"", reference_genome, name=name). [docs]def scale_color_discrete():; """"""The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_color_hue(). [docs]def scale_color_hue():; """"""Map discrete colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""color""). [docs]def scale_color_continuous():; """"""The default continuous color scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""color""). [docs]def scale_color_identity():; """"""A color scale that assumes the expression specified in the ``color`` aesthetic can be used as a color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""color""). [docs]def scale_color_manual(*, values):; """"""A color scale that assigns strings to colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""color"", values=values). [docs]def scale_fill_discrete():; """"""The default discrete fill scale. This maps each discrete value to a fill color. Returns; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:12020,continuous,continuous,12020,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['continuous'],['continuous']
Deployability," This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general, once you know the wall clock time of your job, you can enter your cluster parameters into the; Google Cloud Pricing Calculator. and get a precise estimate; of cost using the latest prices. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:4067,update,updated,4067,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['update'],['updated']
Deployability," ['LA', 'LAD', 'LGT', 'GQ']):; ad_field = 'LAD'; gt_field = 'LGT'; elif all(x in mt.entry for x in ['AD', 'GT', 'GQ']):; ad_field = 'AD'; gt_field = 'GT'; else:; raise ValueError(; f""'compute_charr': require a VDS or MatrixTable with fields LAD/LAD/LGT/GQ/DP or AD/GT/GQ/DP,""; f"" found entry fields {list(mt.entry)}""; ); # Annotate reference allele frequency when it is not defined in the original data, and name it 'ref_AF'.; ref_af_field = '__ref_af'; if ref_AF is None:; n_samples = mt.count_cols(); if n_samples < 10000:; raise ValueError(; ""'compute_charr': with fewer than 10,000 samples, require a reference AF in 'reference_data_source'.""; ). n_alleles = 2 * n_samples; mt = mt.annotate_rows(**{ref_af_field: 1 - hl.agg.sum(mt[gt_field].n_alt_alleles()) / n_alleles}); else:; mt = mt.annotate_rows(**{ref_af_field: ref_AF}). # Filter to autosomal biallelic SNVs with reference allele frequency within the range (min_af, max_af); rg = mt.locus.dtype.reference_genome.name; if rg == 'GRCh37':; mt = hl.filter_intervals(mt, [hl.parse_locus_interval('1-22', reference_genome=rg)]); elif rg == 'GRCh38':; mt = hl.filter_intervals(mt, [hl.parse_locus_interval('chr1-chr22', reference_genome=rg)]); else:; mt = mt.filter_rows(mt.locus.in_autosome()). mt = mt.filter_rows(; (hl.len(mt.alleles) == 2); & hl.is_snp(mt.alleles[0], mt.alleles[1]); & (mt[ref_af_field] > min_af); & (mt[ref_af_field] < max_af); ). # Filter to variant calls with GQ above min_gq and DP within the range (min_dp, max_dp); ad_dp = mt['DP'] if 'DP' in mt.entry else hl.sum(mt[ad_field]); mt = mt.filter_entries(mt[gt_field].is_hom_var() & (mt.GQ >= min_gq) & (ad_dp >= min_dp) & (ad_dp <= max_dp)). # Compute CHARR; mt = mt.select_cols(charr=hl.agg.mean((mt[ad_field][0] / (mt[ad_field][0] + mt[ad_field][1])) / mt[ref_af_field])). mt = mt.select_globals(; af_min=min_af,; af_max=max_af,; dp_min=min_dp,; dp_max=max_dp,; gq_min=min_gq,; ). return mt.cols(). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:63354,update,updated,63354,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['update'],['updated']
Deployability," [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8142,pipeline,pipeline,8142,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['pipeline'],['pipeline']
Deployability," [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud paramete",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:10139,configurat,configuration,10139,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['configurat'],['configuration']
Deployability," allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77668,integrat,integration,77668,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['integrat'],['integration']
Deployability," already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_fro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:26945,pipeline,pipelines,26945,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool, optional) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to down",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:8027,continuous,continuous,8027,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['continuous'],['continuous']
Deployability," between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30551,update,update,30551,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability," bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:36643,pipeline,pipelines,36643,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_discrete()[source]; The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_hue()[source]; Map discrete colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_manual(*, values)[source]; A color scale that assigns strings to colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_identity()[source]; A color scale that assumes the expression specified in the color aesthetic can be used as a color. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_continuous()[source]; The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_discrete()[source]; The default discrete fill scale. This maps each discrete value to a fill color. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_hue()[source]; Map discrete fill colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_manual(*, values)[source]; A color scale that assigns strings to fill colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_identity()[source]; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. Returns:; FigureAttribute – The scale to be applied. Fa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:13684,continuous,continuous,13684,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability," creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2548,install,installed,2548,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['install'],['installed']
Deployability," dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2333,install,installed,2333,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['install'],['installed']
Deployability," diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point.; (#12901); hl.Struct now has a correct and useful implementation of; pprint. Version 0.2.115; Released 2023-04-25. New Features. (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in fa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:34496,release,release,34496,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability," each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; BlockMatrix.write_from_entry_expr().; The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; ld_matrix().; To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:23013,update,updated,23013,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['update'],['updated']
Deployability," evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is eith",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:70416,integrat,integration,70416,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['integrat'],['integration']
Deployability," exists for annotation dataset: {self.name}. Rows have been'; f' annotated with version {indexed_values[1]}.'; ); return indexed_values[0]. [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:9989,configurat,configuration,9989,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['configurat'],['configuration']
Deployability," experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions. View page source. Expressions. eval(expression); Evaluate a Hail expression, returning the result. Expression; Base class for Hail expressions. ArrayExpression; Expression of type tarray. ArrayNumericExpression; Expression of type tarray with a numeric type. BooleanExpression; Expression of type tbool. CallExpression; Expression of type tcall. CollectionExpression; Expression of type tarray or tset. DictExpression; Expression of type tdict. IntervalExpression; Expression of type tinterval. LocusExpression; Expression of type tlocus. NumericExpression; Expression of numeric type. Int32Expression; Expression of type tint32. Int64Expression; Expression of type tint64. Float32Expression; Expression of type tfloat32. Float64Expression; Expression of type tfloat64. SetExpression; Expression of type tset. StringExpression; Expression of type tstr. StructExpression; Expression of type tstruct. TupleExpression; Expression of type ttuple. NDArrayExpression; Expression of type tndarray. NDArrayNumericExpression; Expression of type tndarray with a numeric element type. hail.expr.eval(expression)[source]; Evaluate a Hail expression, returning the result.; This method is extremely useful for learning about Hail expressions and; understanding how to compose them.; The expression must have no indices, but can refer to the globals; of a Table or MatrixTable.; Examples; Evaluate a conditional:; >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters:; expression (Expression) – Any expression, or a Python value that can be implicitly interpreted as an expression. Returns:; Any. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/expressions.html:2327,update,updated,2327,docs/0.2/expressions.html,https://hail.is,https://hail.is/docs/0.2/expressions.html,1,['update'],['updated']
Deployability," filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (locus, alleles) polymorphism.; Examples; >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['TAA', 'TA'])); Struct(locus=Locus(contig=1, position=100000, reference_genome=GRCh37), alleles=['TA', 'T']). >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['AATAA', 'AACAA'])); Struct(locus=Locus(contig=1, position=100002, reference_genome=GRCh37), alleles=['T', 'C']). Notes; Computing the minimal representation can cause the locus shift right (the; position can increase). Parameters:. locus (LocusExpression); alleles (ArrayExpression of type tstr). Returns:; StructExpression – A tstruct expression with two fields, locus; (LocusExpression) and alleles; (ArrayExpression of type tstr). hail.expr.functions.reverse_complement(s, rna=False)[source]; Reverses the string and translates base pairs into their complements; .. rubric:: Examples; >>> bases = hl.literal('NNGATTACA'); >>> hl.eval(hl.reverse_complement(bases)); 'TGTAATCNN'. Parameters:. s (StringExpression) – Base string.; rna (bool) – If True, pair adenine (A) with uracil (U) instead of thymine (T). Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:24707,update,updated,24707,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['update'],['updated']
Deployability," filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155753,update,updated,155753,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['updated']
Deployability," gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11700,install,installed,11700,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability," gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Stomach_all_snp_gene_associations. View page source. GTEx_eQTL_Stomach_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html:9679,update,updated,9679,docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html,1,['update'],['updated']
Deployability," gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Thyroid_all_snp_gene_associations. View page source. GTEx_eQTL_Thyroid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html:9679,update,updated,9679,docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html,1,['update'],['updated']
Deployability," hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20474,install,installed,20474,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['install'],['installed']
Deployability," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:7864,install,installed,7864,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,['install'],"['install', 'installed']"
Deployability," is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; fig = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5419,update,update,5419,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability," is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argumen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:6802,configurat,configuration,6802,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability," lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:19174,integrat,integrate,19174,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integrate']
Deployability," log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:5425,configurat,configuration,5425,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability," missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; paths,; key=key,; min_partitions=min_partitions,; impute=impute,; no_header=no_header,; comment=comment,; missing=missing,; types=types,; skip_blank_lines=skip_blank_lines,; force_bgz=force_bgz,; filter=filter,; find_replace=find_replace,; force=force,; source_file_field=source_file_field,; delimiter="","",; quote=quote,; ); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:119522,update,updated,119522,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['update'],['updated']
Deployability," mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86170,pipeline,pipelines,86170,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:85377,update,updates,85377,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['updates']
Deployability," n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_qual_hists': struct {; gq_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'gnomad_age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; ----------------------------------------; Entry fields:; 'DP': int32; 'GQ': int32; 'MIN_DP': int32; 'PID': str; 'RGQ': int32; 'SB': array<int32>; 'GT': call; 'PGT': call; 'AD': array<int32>; 'PL': array<int32>; 'adj': bool; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html:34895,update,updated,34895,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html,1,['update'],['updated']
Deployability," n_non_ref: int64,; n_snp: int64,; n_transition: int64,; n_transversion: int64,; r_het_hom_var: float64,; r_insertion_deletion: float64,; r_ti_tv: float64; }; 'gnomad_sex_imputation': struct {; chr20_mean_dp: float32,; chrX_mean_dp: float32,; chrY_mean_dp: float32,; chrX_ploidy: float32,; chrY_ploidy: float32,; X_karyotype: str,; Y_karyotype: str,; sex_karyotype: str,; f_stat: float64,; n_called: int64,; expected_homs: float64,; observed_homs: int64; }; 'gnomad_population_inference': struct {; pca_scores: array<float64>,; pop: str,; prob_afr: float64,; prob_ami: float64,; prob_amr: float64,; prob_asj: float64,; prob_eas: float64,; prob_fin: float64,; prob_mid: float64,; prob_nfe: float64,; prob_oth: float64,; prob_sas: float64; }; 'gnomad_sample_qc_residuals': struct {; n_snp_residual: float64,; r_ti_tv_residual: float64,; r_insertion_deletion_residual: float64,; n_insertion_residual: float64,; n_deletion_residual: float64,; r_het_hom_var_residual: float64,; n_transition_residual: float64,; n_transversion_residual: float64; }; 'gnomad_sample_filters': struct {; hard_filters: set<str>,; hard_filtered: bool,; release_related: bool,; qc_metrics_filters: set<str>; }; 'gnomad_high_quality': bool; 'gnomad_release': bool; 'relatedness_inference': struct {; related_samples: set<struct {; s: str,; kin: float64,; ibd0: float64,; ibd1: float64,; ibd2: float64; }>,; related: bool; }; 'hgdp_tgp_meta': struct {; project: str,; study_region: str,; population: str,; genetic_region: str,; latitude: float64,; longitude: float64,; hgdp_technical_meta: struct {; source: str,; library_type: str; },; global_pca_scores: array<float64>,; subcontinental_pca: struct {; pca_scores: array<float64>,; pca_scores_outliers_removed: array<float64>,; outlier: bool; },; gnomad_labeled_subpop: str; }; 'high_quality': bool; ----------------------------------------; Key: ['s']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html:21063,update,updated,21063,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html,1,['update'],['updated']
Deployability," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25555,pipeline,pipelines,25555,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['pipeline'],['pipelines']
Deployability," now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; err",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:38816,pipeline,pipeline,38816,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipeline']
Deployability," object is not mutable""); self.__dict__[key] = value. def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]. raise AttributeError(get_nice_attr_error(self, item)). def _copy_fields_from(self, other: 'ExprContainer'):; self._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same group",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5092,pipeline,pipeline,5092,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['pipeline'],['pipeline']
Deployability," of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current working directory,; * `-t <name>` specifies the image name, and; * `-f <dockerfile>` specifies the Dockerfile file.; * A more complete description may be found `here: <https://docs.docker.com/engine/reference/com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:2066,update,update,2066,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,2,"['install', 'update']","['install', 'update']"
Deployability," panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_female. View page source. UK_Biobank_Rapid_GWAS_female. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html:10497,update,updated,10497,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,1,['update'],['updated']
Deployability," partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22166,pipeline,pipelines,22166,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:94538,patch,patch,94538,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['patch', 'update']","['patch', 'update']"
Deployability," path=str, overwrite=bool); def write_expression(expr, path, overwrite=False):; """"""Write an Expression. In the same vein as Python's pickle, write out an expression; that does not have a source (such as one that comes from; Table.aggregate with _localize=False). Example; -------; >>> ht = hl.utils.range_table(100).annotate(x=hl.rand_norm()); >>> mean_norm = ht.aggregate(hl.agg.mean(ht.x), _localize=False); >>> mean_norm; >>> hl.eval(mean_norm); >>> hl.experimental.write_expression(mean_norm, 'output/expression.he'). Parameters; ----------. expr : :class:`~.Expression`; Expression to write.; path : :class:`str`; Path to which to write expression.; Suggested extension: .he (hail expression).; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination. Returns; -------; None; """"""; source = expr._indices.source; if source is not None:; analyze('write_expression.expr', expr, source._global_indices); source = source.select_globals(__expr=expr); expr = source.index_globals().__expr; hl.utils.range_table(1).filter(False).key_by().drop('idx').annotate_globals(expr=expr).write(; path, overwrite=overwrite; ). [docs]@typecheck(path=str, _assert_type=nullable(hail_type)); def read_expression(path, _assert_type=None):; """"""Read an :class:`~.Expression` written with :func:`.experimental.write_expression`. Example; -------; >>> hl.experimental.write_expression(hl.array([1, 2]), 'output/test_expression.he'); >>> expression = hl.experimental.read_expression('output/test_expression.he'); >>> hl.eval(expression). Parameters; ----------. path : :class:`str`; File to read. Returns; -------; :class:`~.Expression`; """"""; _assert_table_type = None; _load_refs = True; if _assert_type:; _assert_table_type = ttable(hl.tstruct(expr=_assert_type), row_type=hl.tstruct(), row_key=[]); _load_refs = False; return hl.read_table(path, _assert_type=_assert_table_type, _load_refs=_load_refs).index_globals().expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html:2719,update,updated,2719,docs/0.2/_modules/hail/experimental/expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html,1,['update'],['updated']
Deployability," requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_ag",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:73667,pipeline,pipeline,73667,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipeline']
Deployability," running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:5383,install,install,5383,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['install']
Deployability," should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You shou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1824,install,install-on-cluster,1824,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['install-on-cluster']
Deployability," so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162155,update,update,162155,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['update']
Deployability," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:2673,update,updated,2673,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html,1,['update'],['updated']
Deployability," table, use flatten() first. Warning; Do not export to a path that is being read from in the same pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:23944,pipeline,pipeline,23944,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipeline']
Deployability," the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters; ----------; n_alleles : :obj:`int`; Number of total alleles, including the reference. Returns; -------; :obj:`list` of :obj:`int`; """"""; r = [0] * n_alleles; for a in self._alleles:; r[a] += 1; return r. [docs] def unphased_diploid_gt_index(self):; """"""Return the genotype index for unphased, diploid calls. Returns; -------; :obj:`int`; """"""; from hail.utils import FatalError. if self.ploidy != 2 or self.phased:; raise FatalError(; ""'unphased_diploid_gt_index' is only valid for unphased, diploid calls. Found {}."".format(repr(self)); ); a0 = self._alleles[0]; a1 = self._alleles[1]; assert a0 <= a1; return a1 * (a1 + 1) / 2 + a0. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:5872,update,updated,5872,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,1,['update'],['updated']
Deployability," the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8217,pipeline,pipeline,8217,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['pipeline'],['pipeline']
Deployability," the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:92085,release,releases,92085,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['releases']
Deployability," to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None); Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:11277,update,updated,11277,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['update'],['updated']
Deployability," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:43508,update,updated,43508,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['update'],['updated']
Deployability," transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetada",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31233,update,update,31233,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability," two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:7893,install,installed,7893,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['install'],['installed']
Deployability," type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:19313,update,updated,19313,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['update'],['updated']
Deployability," type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:; GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use MatrixTable.select_entries() to rearrange these fields if; necessary.; The following new fields are generated:. old_locus (locus) – The old locus, before filtering and computing; the minimal representation.; old_alleles (array<str>) – The old alleles, before filtering an",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:23919,update,update,23919,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability," wheel 712 Jan 25 17:19 index.tsv; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-00.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-01.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-02.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-03.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-04.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-05.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-06.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-07.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-08.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-09.tsv.bgz. $ zcat output/cols_files/part-00.tsv.bgz; #{""col_idx"":0}; row_idx x; 0 6.2501e-02; 1 7.0083e-01; 2 3.6452e-01; 3 4.4170e-01; 4 7.9177e-02; 5 6.2392e-01; 6 5.9920e-01; 7 9.7540e-01; 8 8.4848e-01; 9 3.7423e-01. Due to overhead and file system limits related to having large numbers; of open files, this function will iteratively export groups of columns.; The `batch_size` parameter can control the size of these groups. Parameters; ----------; mt : :class:`.MatrixTable`; path : :obj:`int`; Path (directory to write to.; batch_size : :obj:`int`; Number of columns to write per iteration.; bgzip : :obj:`bool`; BGZip output files.; header_json_in_file : :obj:`bool`; Include JSON header in each component file (if False, only written to index.tsv); """"""; if use_string_key_as_file_name and not (len(mt.col_key) == 1 and mt.col_key[0].dtype == hl.tstr):; raise ValueError(; f'parameter ""use_string_key_as_file_name"" requires a single string column key, found {list(mt.col_key.dtype.values())}'; ); hl.utils.java.Env.backend().execute(; hl.ir.MatrixToValueApply(; mt._mir,; {; 'name': 'MatrixExportEntriesByCol',; 'parallelism': batch_size,; 'path': path,; 'bgzip': bgzip,; 'headerJsonInFile': header_json_in_file,; 'useStringKeyAsFileName': use_string_key_as_file_name,; },; ); ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html:3668,update,updated,3668,docs/0.2/_modules/hail/experimental/export_entries_by_col.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html,1,['update'],['updated']
Deployability," while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; we could do the following:; >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to MatrixTable.annotate_rows():; >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus, mt.alleles]). Interacting with Matrix Tables Locally; Some useful methods to interact with matrix tables locally are; MatrixTable.describe(), MatrixTable.head(), and; MatrixTable.sample_rows(). describe prints out the schema for all row; fields, column fields, entry fields, and global fields as well as the row keys; and column keys. head returns a new matrix table with only the first N rows.; sample_rows returns a new matrix table where the rows are randomly sampled with; frequency p.; To get the dimensions of the matrix table, use MatrixTable.count_rows(); and MatrixTable.count_cols(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:14109,update,updated,14109,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['update'],['updated']
Deployability," x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor,; target_records=target_records,; gvcf_batch_size=gvcf_batch_size,; contig_recoding=contig_recoding,; call_fields=call_fields,; vdses=vdses,; gvcfs=gvcf_paths,; gvcf_import_intervals=intervals,; gvcf_external_header=gvcf_external_header,; gvcf_sample_names=gvcf_sample_names,; gvcf_info_to_keep=gvcf_info_to_keep,; gvcf_reference_entry_fields_to_keep=gvcf_reference_entry_fields_to_keep,; ); combiner._raise_if_output_exists(); return combiner. [docs]def load_combiner(path: str) -> VariantDatasetCombiner:; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; return VariantDatasetCombiner.load(path). class Encoder(json.JSONEncoder):; def default(self, o):; if isinstance(o, VariantDatasetCombiner):; return o.to_dict(); if isinstance(o, HailType):; return str(o); if isinstance(o, tmatrix):; return o.to_dict(); return json.JSONEncoder.default(self, o). class Decoder(json.JSONDecoder):; def __init__(self, **kwargs):; super().__init__(object_hook=Decoder._object_hook, **kwargs). @staticmethod; def _object_hook(obj):; if 'name' not in obj:; return obj; name = obj['name']; if name == VariantDatasetCombiner.__name__:; del obj['name']; obj['vdses'] = [VDSMetadata(*x) for x in obj['vdses']]; obj['dataset_type'] = CombinerOutType(*(tmatrix._from_json(ty) for ty in obj['dataset_type'])); if 'gvcf_type' in obj and obj['gvcf_type']:; obj['gvcf_type'] = tmatrix._from_json(obj['gvcf_type']). rg = hl.get_reference(obj['reference_genome']); obj['reference_genome'] = rg; intervals_type = hl.tarray(hl.tinterval(hl.tlocus(rg))); intervals = intervals_type._convert_from_json(obj['gvcf_import_intervals']); obj['gvcf_import_intervals'] = intervals. return VariantDatasetCombiner(**obj); return obj. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:34258,update,updated,34258,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['updated']
Deployability," x_range[0]) / x_bins; y_spacing = (y_range[1] - y_range[0]) / y_bins. def frange(start, stop, step):; from itertools import count, takewhile. return takewhile(lambda x: x <= stop, count(start, step)). x_levels = hail.literal(list(frange(x_range[0], x_range[1], x_spacing))[::-1]); y_levels = hail.literal(list(frange(y_range[0], y_range[1], y_spacing))[::-1]); grouped_ht = source.group_by(; x=hail.str(x_levels.find(lambda w: x >= w)), y=hail.str(y_levels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinsta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:22840,update,update,22840,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability," your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:16277,pipeline,pipeline,16277,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipeline']
Deployability,"""""""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:42795,pipeline,pipeline,42795,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['pipeline'],['pipeline']
Deployability,"#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:32579,pipeline,pipelines,32579,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"& (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23472,update,update,23472,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:144427,pipeline,pipelines,144427,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['pipeline'],['pipelines']
Deployability,"'GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {ve",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20273,install,installed,20273,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['install'],['installed']
Deployability,"((1 - K) * P); cas = mt.filter_cols(mt.y_w_asc_bias == 1); con = mt.filter_cols(mt.y_w_asc_bias == 0).add_col_index(name='col_idx_' + uid); keep = round(p * n * (1 - K)) * [1] + round((1 - p) * n * (1 - K)) * [0]; con = con.annotate_cols(**{'keep_' + uid: hl.literal(keep)[hl.int32(con['col_idx_' + uid])]}); con = con.filter_cols(con['keep_' + uid] == 1); con = _clean_fields(con, uid); mt = con.union_cols(cas); return mt. [docs]@typecheck(mt=MatrixTable, y=oneof(expr_int32, expr_float64), K=oneof(int, float), exact=bool); def binarize(mt, y, K, exact=False):; r""""""Binarize phenotype `y` such that it has prevalence `K` = cases/(cases+controls); Uses inverse CDF of Gaussian to set binarization threshold when `exact` = False,; otherwise uses ranking to determine threshold. Parameters; ----------; mt : :class:`.MatrixTable`; :class:`.MatrixTable` containing phenotype to be binarized.; y : :class:`.Expression`; Column field of phenotype.; K : :obj:`int` or :obj:`float`; Desired ""population prevalence"" of phenotype.; exact : :obj:`bool`; Whether to get prevalence as close as possible to `K` (does not use inverse CDF). Returns; -------; :class:`.MatrixTable`; :class:`.MatrixTable` containing binary phenotype with prevalence of approx. `K`; """"""; if exact:; key = list(mt.col_key); uid = Env.get_uid(base=100); mt = mt.annotate_cols(**{'y_' + uid: y}); tb = mt.cols().order_by('y_' + uid); tb = tb.add_index('idx_' + uid); n = tb.count(); # ""+ 1"" because of zero indexing; tb = tb.annotate(y_binarized=tb['idx_' + uid] + 1 <= round(n * K)); tb, mt = tb.key_by('y_' + uid), mt.key_cols_by('y_' + uid); mt = mt.annotate_cols(y_binarized=tb[mt['y_' + uid]].y_binarized); mt = mt.key_cols_by(*map(lambda x: mt[x], key)); else: # use inverse CDF; y_stats = mt.aggregate_cols(hl.agg.stats(y)); threshold = stats.norm.ppf(1 - K, loc=y_stats.mean, scale=y_stats.stdev); mt = mt.annotate_cols(y_binarized=y > threshold); return mt. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:35369,update,updated,35369,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,1,['update'],['updated']
Deployability,"(GQ) scores.; dp_bins : :class:`tuple` of :obj:`int`; Tuple containing cutoffs for depth (DP) scores.; dp_field : :obj:`str`; Name of depth field. If not supplied, DP or MIN_DP will be used, in that order. Returns; -------; :class:`.Table`; Hail Table of results, keyed by sample.; """""". require_first_key_field_locus(vds.reference_data, 'sample_qc'); require_first_key_field_locus(vds.variant_data, 'sample_qc'). if dp_field is not None:; ref_dp_field_to_use = dp_field; elif 'DP' in vds.reference_data.entry:; ref_dp_field_to_use = 'DP'; elif 'MIN_DP' in vds.reference_data.entry:; ref_dp_field_to_use = 'MIN_DP'; else:; ref_dp_field_to_use = None. vmt = vds.variant_data; if 'GT' not in vmt.entry:; vmt = vmt.annotate_entries(GT=hl.vds.lgt_to_gt(vmt.LGT, vmt.LA)); allele_count, atypes = vmt_sample_qc_variant_annotations(global_gt=vmt.GT, alleles=vmt.alleles); variant_ac = Env.get_uid(); variant_atypes = Env.get_uid(); vmt = vmt.annotate_rows(**{variant_ac: allele_count, variant_atypes: atypes}); vmt_dp = vmt['DP'] if ref_dp_field_to_use is not None and 'DP' in vmt.entry else None; variant_results = vmt.select_cols(; **vmt_sample_qc(; global_gt=vmt.GT,; gq=vmt.GQ,; variant_ac=vmt[variant_ac],; variant_atypes=vmt[variant_atypes],; dp=vmt_dp,; gq_bins=gq_bins,; dp_bins=dp_bins,; ); ).cols(). rmt = vds.reference_data; rmt_dp = rmt[ref_dp_field_to_use] if ref_dp_field_to_use is not None else None; reference_results = rmt.select_cols(; **rmt_sample_qc(; locus=rmt.locus,; gq=rmt.GQ,; end=rmt.END,; dp=rmt_dp,; gq_bins=gq_bins,; dp_bins=dp_bins,; ); ).cols(). joined = reference_results[variant_results.key]; dp_bins_field = {}; if ref_dp_field_to_use is not None:; dp_bins_field['dp_bins'] = hl.tuple(dp_bins); joined_results = variant_results.transmute(**combine_sample_qc(joined, variant_results.row)); joined_results = joined_results.annotate_globals(gq_bins=hl.tuple(gq_bins), **dp_bins_field); return joined_results. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html:13298,update,updated,13298,docs/0.2/_modules/hail/vds/sample_qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html,1,['update'],['updated']
Deployability,"(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31089,update,update,31089,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"(hl.missing(rd.entry.dtype), False), lambda acc: keep_last(acc, (elt, False)), keep_last; ),; ht.entries,; ),; ht.entries,; ).map(lambda tup: keep_last(tup[0], (tup[1], False))); ); ht_join = ht. ht = ht.key_by(); ht = ht.select(; to_shuffle=hl.enumerate(ht.prev_block).filter(; lambda idx_and_elt: hl.is_defined(idx_and_elt[1]) & idx_and_elt[1][1]; ); ); ht = ht.explode('to_shuffle'); rg = rd.locus.dtype.reference_genome; ht = ht.transmute(col_idx=ht.to_shuffle[0], entry=ht.to_shuffle[1][0]); ht_shuf = ht.key_by(; locus=hl.locus(hl.literal(rg.contigs)[ht.entry.contig_idx], ht.entry.start_pos, reference_genome=rg); ). ht_shuf = ht_shuf.collect_by_key(""new_starts""); # new_starts can contain multiple records for a collapsed ref block, one for each folded block.; # We want to keep the one with the highest END; ht_shuf = ht_shuf.select(; moved_blocks_dict=hl.group_by(lambda elt: elt.col_idx, ht_shuf.new_starts).map_values(; lambda arr: arr[hl.argmax(arr.map(lambda x: x.entry.END))].entry.drop('contig_idx', 'start_pos'); ); ). ht_joined = ht_join.join(ht_shuf.select_globals(), 'left'). def merge_f(tup):; (idx, original_entry) = tup. return (; hl.case(); .when(; ~(hl.coalesce(ht_joined.prev_block[idx][1], False)),; hl.coalesce(ht_joined.moved_blocks_dict.get(idx), original_entry.drop('contig_idx', 'start_pos')),; ); .or_missing(); ). ht_joined = ht_joined.annotate(new_entries=hl.enumerate(ht_joined.entries).map(lambda tup: merge_f(tup))); ht_joined = ht_joined.drop('moved_blocks_dict', 'entries', 'prev_block', 'contig_idx_row', 'start_pos_row'); new_rd = ht_joined._unlocalize_entries(; entries_field_name='new_entries', cols_field_name='cols', col_key=list(rd.col_key); ). rbml = hl.vds.VariantDataset.ref_block_max_length_field; if rbml in new_rd.globals:; new_rd = new_rd.drop(rbml). if isinstance(ds, VariantDataset):; return VariantDataset(reference_data=new_rd, variant_data=ds.variant_data); return new_rd. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:41193,update,updated,41193,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,1,['update'],['updated']
Deployability,"(loop_ir, ir.If):; if contains_recursive_call(loop_ir.cond):; raise TypeError(""branch condition can't contain recursive call!""); check_tail_recursive(loop_ir.cnsq); check_tail_recursive(loop_ir.altr); elif isinstance(loop_ir, ir.Let):; if contains_recursive_call(loop_ir.value):; raise TypeError(""bound value used in other expression can't contain recursive call!""); check_tail_recursive(loop_ir.body); elif isinstance(loop_ir, ir.TailLoop):; if any(contains_recursive_call(x) for n, x in loop_ir.params):; raise TypeError(""parameters passed to inner loop can't contain recursive call!""); elif not isinstance(loop_ir, ir.Recur) and contains_recursive_call(loop_ir):; raise TypeError(""found recursive expression outside of tail position!""). @typecheck(recur_exprs=expr_any); def make_loop(*recur_exprs):; if len(recur_exprs) != len(args):; raise TypeError('Recursive call in loop has wrong number of arguments'); err = None; for i, (rexpr, expr) in enumerate(zip(recur_exprs, args)):; if rexpr.dtype != expr.dtype:; if err is None:; err = 'Type error in recursive call,'; err += f'\n at argument index {i}, loop arg type: {expr.dtype}, '; err += f'recur arg type: {rexpr.dtype}'; if err is not None:; raise TypeError(err); irs = [expr._ir for expr in recur_exprs]; indices, aggregations = unify_all(*recur_exprs); return construct_expr(ir.Recur(loop_name, irs, typ), typ, indices, aggregations). uid_irs = []; loop_vars = []. for expr in args:; uid = Env.get_uid(); loop_vars.append(construct_variable(uid, expr._type, expr._indices, expr._aggregations)); uid_irs.append((uid, expr._ir)). loop_f = to_expr(f(make_loop, *loop_vars)); if loop_f.dtype != typ:; raise TypeError(f""requested type {typ} does not match inferred type {loop_f.dtype}""); check_tail_recursive(loop_f._ir); indices, aggregations = unify_all(*args, loop_f). return construct_expr(ir.TailLoop(loop_name, loop_f._ir, uid_irs), loop_f.dtype, indices, aggregations). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:6348,update,updated,6348,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,1,['update'],['updated']
Deployability,"(time=expr_str, format=expr_str, zone_id=expr_str); def strptime(time, format, zone_id):; """"""; Interpret a formatted datetime string as a Unix timestamp (number of seconds since 1970-01-01T00:00Z (ISO)). Examples; --------. >>> hl.eval(hl.experimental.strptime(""07/08/19 3:00:01 AM"", ""%D %l:%M:%S %p"", ""America/New_York"")); 1562569201. >>> hl.eval(hl.experimental.strptime(""Saturday, October 11, 1997. 05:45:23 AM"", ""%A, %B %e, %Y. %r"", ""GMT+2"")); 876541523. Notes; -----; The following formatting characters are supported in format strings: A a B b D d e F H I j k l M m n p R r S s T t U u V v W Y y z; See documentation here: https://linux.die.net/man/3/strftime. A zone id can take one of three forms. It can be an explicit offset, like ""+01:00"", a relative offset, like ""GMT+2"",; or a IANA timezone database (TZDB) identifier, like ""America/New_York"". Wikipedia maintains a list of TZDB identifiers here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones. Currently, the parser implicitly uses the ""en_US"" locale. This function will fail if there is not enough information in the string to determine a particular timestamp.; For example, if you have the string `""07/08/09""` and the format string `""%Y.%m.%d""`, this method will fail, since that's not specific; enough to determine seconds from. You can fix this by adding ""00:00:00"" to your date string and ""%H:%M:%S"" to your format string. Parameters; ----------; time : str or :class:`.Expression` of type :py:data:`.tstr`; The string from which to parse the time.; format : str or :class:`.Expression` of type :py:data:`.tstr`; The format string describing how to parse the time.; zone_id: str or :class:`.Expression` of type :py:data:`.tstr`; An id representing the timezone. See notes above. Returns; -------; :class:`~.Int64Expression`; The Unix timestamp associated with the given time string.; """"""; return _func(""strptime"", hl.tint64, time, format, zone_id). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/time.html:4310,update,updated,4310,docs/0.2/_modules/hail/experimental/time.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/time.html,1,['update'],['updated']
Deployability,"))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:6629,configurat,configuration,6629,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"); for version in self.versions; if version.maybe_index(key_expr, all_matches) is not None; ]; if len(compatible_indexed_values) == 0:; versions = [f'{(v.version, v.reference_genome)}' for v in self.versions]; raise ValueError(; f'Could not find compatible version of {self.name} for user'; f' dataset with key {key_expr.dtype}.\n'; f'This annotation dataset is available for the following'; f' versions and reference genome builds: {"", "".join(versions)}.'; ); else:; indexed_values = sorted(compatible_indexed_values, key=lambda x: x[1])[-1]. if len(compatible_indexed_values) > 1:; info(; f'index_compatible_version: More than one compatible version'; f' exists for annotation dataset: {self.name}. Rows have been'; f' annotated with version {indexed_values[1]}.'; ); return indexed_values[0]. [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:9324,configurat,configuration,9324,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['configurat'],['configuration']
Deployability,", P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:5229,install,installed,5229,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['install'],['installed']
Deployability,", ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:4419,configurat,configuration,4419,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,", call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakine",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:31601,pipeline,pipelines,31601,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,", color=None)[source]; Creates a line plot with the area between the line and the x-axis filled in.; Supported aesthetics: x, y, fill, color, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:8491,continuous,continuous,8491,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"-+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper inclu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:73985,integrat,integration,73985,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Deployability,"---+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93214,integrat,integration,93214,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['integrat'],['integration']
Deployability,"-----+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pape",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77492,integrat,integration,77492,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['integrat'],['integration']
Deployability,"------------------------; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>; ----------------------------------------. Common Operations; Like tables, Hail provides a number of methods for manipulating data in a; matrix table.; Filter; MatrixTable has three methods to filter based on expressions:. MatrixTable.filter_rows(); MatrixTable.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:6134,update,update,6134,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['update'],['update']
Deployability,"--; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""color""). [docs]def scale_color_identity():; """"""A color scale that assumes the expression specified in the ``color`` aesthetic can be used as a color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""color""). [docs]def scale_color_manual(*, values):; """"""A color scale that assigns strings to colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""color"", values=values). [docs]def scale_fill_discrete():; """"""The default discrete fill scale. This maps each discrete value to a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_fill_hue(). [docs]def scale_fill_continuous():; """"""The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""fill""). [docs]def scale_fill_identity():; """"""A color scale that assumes the expression specified in the ``fill`` aesthetic can be used as a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""fill""). [docs]def scale_fill_hue():; """"""Map discrete fill colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""fill""). [docs]def scale_fill_manual(*, values):; """"""A color scale that assigns strings to fill colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:13148,continuous,continuous,13148,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['continuous'],['continuous']
Deployability,"-. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryR",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46867,configurat,configuration,46867,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound**3:; return _svd(self.to_numpy(), full_matrices=False, compute_uv=compute_uv, overwrite_a=True); else:; return self._svd_gr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76686,configurat,configuration-dependent,76686,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['configurat'],['configuration-dependent']
Deployability,". Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; Locus. View page source. Locus. class hail.genetics.Locus[source]; An object that represents a location in the genome. Parameters:. contig (str) – Chromosome identifier.; position (int) – Chromosomal position (1-indexed).; reference_genome (str or ReferenceGenome) – Reference genome to use. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.locus.take(5). This is rare; it is much; more common to manipulate the LocusExpression object, which is; constructed using the following functions:. locus(); parse_locus(); locus_from_global_position(). Attributes. contig; Chromosome identifier. position; Chromosomal position (1-based). reference_genome; Reference genome. Methods. parse; Parses a locus object from a CHR:POS string. property contig; Chromosome identifier.; :rtype: str. classmethod parse(string, reference_genome='default')[source]; Parses a locus object from a CHR:POS string.; Examples; >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). Parameters:. string (str) – String to parse.; reference_genome (str or ReferenceGenome) – Reference genome to use. Default is default_reference(). Return type:; Locus. property position; Chromosomal position (1-based).; :rtype: int. property reference_genome; Reference genome. Returns:; ReferenceGenome. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html:2045,update,updated,2045,docs/0.2/genetics/hail.genetics.Locus.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html,1,['update'],['updated']
Deployability,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch. View page source. Batch; Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the Batch Service. Contents. Getting Started; Installation. Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Clumping GWAS Results; Random Forest. Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Exact Match Expression; Partial Match Expression; Keyword Expression; Predefined Keyword Expression; Combining Multiple Statements. Python Version Compatibility Policy; Change Log. Indices and tables. Index; Search Page. Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/index.html:505,pipeline,pipelines,505,docs/batch/index.html,https://hail.is,https://hail.is/docs/batch/index.html,1,['pipeline'],['pipelines']
Deployability,". Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Getting Started; Variant Effect Predictor (VEP). Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1085,deploy,deploying,1085,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['deploy'],['deploying']
Deployability,". Docker Resources — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Docker Resources. View page source. Docker Resources. What is Docker?; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:889,install,install,889,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['install']
Deployability,". Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.ServiceBackend(). Second, we create a Batch and specify the default Python image to; use for Python jobs with default_python_image. image is the return value; from building the Python image above and is the full name of where the newly; built image was pushed to.; b = hb.Batch(name='rf',; default_python_image=image). Next, we read the y dataframe locally in order to get the list of windows; to run. The file path containing the dataframe could be stored on the cloud.; Therefore, we use the function hfs.open to read the data regardless; of where it’s located.; with hfs.open(df_y_path) as f:; local_df_y = pd.read",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:4590,install,installed,4590,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['install'],['installed']
Deployability,". Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1116,install,install,1116,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['install'],['install']
Deployability,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/getting_started.html:683,install,install,683,docs/batch/getting_started.html,https://hail.is,https://hail.is/docs/batch/getting_started.html,3,['install'],['install']
Deployability,". HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.context. Source code for hail.context; from __future__ import print_function # Python 2 and 3 print compatibility. from hail.typecheck import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:1052,configurat,configuration,1052,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,". Python Version Compatibility Policy — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:504,release,released,504,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,2,['release'],['released']
Deployability,". Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:12176,update,updated,12176,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['update'],['updated']
Deployability,". Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ensg': str; 'locus': locus<GRCh37>; 'symbol': str; 'Cells_Transformedfibroblasts': float64; 'Prostate': float64; 'Spleen': float64; 'Brain_FrontalCortex_BA9_': float64; 'SmallIntestine_TerminalIleum': float64; 'MinorSalivaryGland': float64; 'Artery_Coronary': float64; 'Skin_SunExposed_Lowerleg_': float64; 'Cells_EBV_transformedlymphocytes': float64; 'Brain_Hippocampus': float64; 'Esophagus_Muscularis': float64; 'Brain_Nucleusaccumbens_basalganglia_': float64; 'Artery_Tibial': float64; 'Brain_Hypothalamus': float64; 'Adipose_Visceral_Omentum_': float64; 'Cervix_Ectocervix': float64; 'Brain_Spinalcord_cervicalc_1_': float64; 'Brain_CerebellarHemisphere': float64; 'Nerve_Tibial': float64; 'Breast_MammaryTissue': float64; 'Liver': float64; 'Skin_NotSunExposed_Suprapubic_': float64; 'AdrenalGland': float64; 'Vagina': float64; 'Pancreas': float64; 'Lung': float64; 'FallopianTube': float64; 'Pituitary': float64; 'Muscle_Skeletal': float64; 'Colon_Transverse': float64; 'Artery_Aorta': float64; 'Heart_AtrialAppendage': float64; 'Adipose_Subcutaneous': float64; 'Esophagus_Mucosa': float64; 'Heart_LeftVentricle': float64; 'Brain_Cerebellum': float64; 'Brain_Cortex': float64; 'Thyroid': float64; 'Brain_Substantianigra': float64; 'Kidney_Cortex': float64; 'Uterus': float64; 'Stomach': float64; 'WholeBlood': float64; 'Bladder': float64; 'Brain_Anteriorcingulatecortex_BA24_': float64; 'Brain_Putamen_basalganglia_': float64; 'Brain_Caudate_basalganglia_': float64; 'Colon_Sigmoid': float64; 'Cervix_Endocervix': float64; 'Ovary': float64; 'Esophagus_GastroesophagealJunction': float64; 'Testis': float64; 'Brain_Amygdala': float64; 'mean_proportion': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_base_pext.html:10933,update,updated,10933,docs/0.2/datasets/schemas/gnomad_base_pext.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_base_pext.html,1,['update'],['updated']
Deployability,". Tutorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:598,install,installed,598,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['install'],['installed']
Deployability,". hailtop.batch.docker.build_python_image — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; build_python_image(). hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.docker.build_python_image. View page source. hailtop.batch.docker.build_python_image. hailtop.batch.docker.build_python_image(fullname, requirements=None, python_version=None, _tmp_dir='/tmp', *, show_docker_output=False); Build a new Python image with dill and the specified pip packages installed.; Notes; This function is used to build Python images for PythonJob.; Examples; >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) . Parameters:. fullname (str) – Full name of where to build the image including any repository prefix and tags; if desired (default tag is latest).; requirements (Optional[List[str]]) – List of pip packages to install.; python_version (Optional[str]) – String in the format of major_version.minor_version (ex: 3.9). Defaults to; current version of Python that is running.; _tmp_dir (str) – Location to place local temporary files used while building the image.; show_docker_output (bool) – Print the output from Docker when building / pushing the image. Return type:; str. Returns:; Full name where built image is located. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html:783,install,installed,783,docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,2,['install'],"['install', 'installed']"
Deployability,". note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length. original_determine_file_length = DataFileReader.determine_file_length. try:; DataFileReader.determine_file_length = patched_determine_file_length. with DataFileReader(avro_file, DatumReader()) as data_file_reader:; tr = ir.AvroTableReader(avro.schema.parse(data_file_reader.schema), paths, key, intervals). finally:; DataFileReader.determine_file_length = original_determine_file_length. return Table(ir.TableRead(tr)). @typecheck(; paths=oneof(str, sequenceof(str)),; key=table_key_type,; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=oneof(str, sequenceof(str)),; missing=oneof(str, sequenceof(str)),; types=dictof(str, hail_type),; quote=nullable(char),; skip_blank_lines=bool,; f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112846,patch,patch,112846,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['patch'],['patch']
Deployability,".1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'gene': str; 'transcript': str; 'obs_mis': int32; 'exp_mis': float64; 'oe_mis': float64; 'mu_mis': float64; 'possible_mis': int32; 'obs_mis_pphen': int32; 'exp_mis_pphen': float64; 'oe_mis_pphen': float64; 'possible_mis_pphen': int32; 'obs_syn': int32; 'exp_syn': float64; 'oe_syn': float64; 'mu_syn': float64; 'possible_syn': int32; 'obs_lof': int32; 'mu_lof': float64; 'possible_lof': int32; 'exp_lof': float64; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': str; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int32; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int32; 'classic_caf': float64; 'max_af': float64; 'no_lofs': int32; 'obs_het_lof': int32; 'obs_hom_lof': int32; 'defined': int32; 'p': float64; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': int32; 'cds_length': int32; 'num_coding_exons': int32; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': str; 'chromosome': str; 'start_position': int32; 'end_position': int32; ----------------------------------------; Key: ['gene']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html:10970,update,updated,10970,docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html,1,['update'],['updated']
Deployability,".1.1, None); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'gene': str; 'transcript': str; 'obs_mis': int32; 'exp_mis': float64; 'oe_mis': float64; 'mu_mis': float64; 'possible_mis': int32; 'obs_mis_pphen': int32; 'exp_mis_pphen': float64; 'oe_mis_pphen': float64; 'possible_mis_pphen': int32; 'obs_syn': int32; 'exp_syn': float64; 'oe_syn': float64; 'mu_syn': float64; 'possible_syn': int32; 'obs_lof': int32; 'mu_lof': float64; 'possible_lof': int32; 'exp_lof': float64; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': str; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int32; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int32; 'classic_caf': float64; 'max_af': float64; 'no_lofs': int32; 'obs_het_lof': int32; 'obs_hom_lof': int32; 'defined': int32; 'p': float64; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': int32; 'cds_length': int32; 'num_coding_exons': int32; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': str; 'chromosome': str; 'start_position': int32; 'end_position': int32; ----------------------------------------; Key: ['gene']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_lof_metrics.html:10941,update,updated,10941,docs/0.2/datasets/schemas/gnomad_lof_metrics.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_lof_metrics.html,1,['update'],['updated']
Deployability,".131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12361,configurat,configuration,12361,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,".; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39166,install,installing,39166,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['install'],['installing']
Deployability,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111151,pipeline,pipelines,111151,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['pipeline'],['pipelines']
Deployability,".default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21314,pipeline,pipelines,21314,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,".functions). UNKNOWN (hail.genetics.AlleleType attribute). unpersist() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). unphase() (hail.expr.CallExpression method). unphased_diploid_gt_index() (hail.expr.CallExpression method). (hail.genetics.Call method). unphased_diploid_gt_index_call() (in module hail.expr.functions). upper() (hail.expr.StringExpression method). V. validate() (hail.vds.VariantDataset method). values() (hail.expr.DictExpression method). (hail.expr.StructExpression method). variant_qc() (in module hail.methods). variant_str() (in module hail.expr.functions). VariantDataset (class in hail.vds). VariantDatasetCombiner (class in hail.vds.combiner). vars() (in module hail.ggplot). VDSMetadata (class in hail.vds.combiner). vep() (in module hail.methods). VEPConfig (class in hail.methods). VEPConfigGRCh37Version85 (class in hail.methods). VEPConfigGRCh38Version95 (class in hail.methods). version() (in module hail). visualize_missingness() (in module hail.plot). vstack() (in module hail.nd). W. when() (hail.expr.builders.CaseBuilder method). (hail.expr.builders.SwitchBuilder method). when_missing() (hail.expr.builders.SwitchBuilder method). window() (hail.expr.LocusExpression method). write() (hail.genetics.Pedigree method). (hail.genetics.ReferenceGenome method). (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). write_expression() (in module hail.experimental). write_from_entry_expr() (hail.linalg.BlockMatrix static method). write_image() (hail.ggplot.GGPlot method). write_many() (hail.Table method). X. x_contigs (hail.genetics.ReferenceGenome property). xlab() (in module hail.ggplot). Y. y_contigs (hail.genetics.ReferenceGenome property). ylab() (in module hail.ggplot). Z. zeros() (in module hail.nd). zip() (in module hail.expr.functions). zip_with_index() (in module hail.expr.functions). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:52812,update,updated,52812,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['update'],['updated']
Deployability,".locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) # doctest: +SKIP. """""". fd = hl.vds.VariantDataset.ref_block_max_length_field; mts = [vds.reference_data for vds in vdses]; n_with_ref_max_len = len([mt for mt in mts if fd in mt.globals]); any_ref_max = n_with_ref_max_len > 0; all_ref_max = n_with_ref_max_len == len(mts). # if some mts have max ref len but not all, drop it; if all_ref_max:; new_ref_mt = hl.MatrixTable.union_rows(*mts).annotate_globals(**{; fd: hl.max([mt.index_globals()[fd] for mt in mts]); }); else:; if any_ref_max:; mts = [mt.drop(fd) if fd in mt.globals else mt for mt in mts]; new_ref_mt = hl.MatrixTable.union_rows(*mts). new_var_mt = hl.MatrixTable.union_rows(*(vds.variant_data for vds in vdses)); return hl.vds.VariantDataset(new_ref_mt, new_var_mt). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:12847,update,updated,12847,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['update'],['updated']
Deployability,".or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The range includes start, but excludes stop.; If provided exactly one argument, the argument is interpreted as stop and; start is set to zero. This matches the behavior of Python’s range. Parameters:. start (int or Expression of type tint32) – Start of range.; stop (int or Expression of type tint32) – End of range.; step (int or Expression of type tint32) – Step of range. Returns:; ArrayNumericExpression. hail.expr.functions.query_table(path, point_or_interval)[source]; Query records from a table corresponding to a given point or range of keys.; Notes; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in Table.annotate(). Warning; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters:. path (str) – Table path.; point_or_interval – Point or interval to query. Returns:; ArrayExpression. CaseBuilder; Class for chaining multiple if-else statements. SwitchBuilder; Class for generating conditional trees based on value of an expression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:11338,update,updated,11338,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['update'],['updated']
Deployability,.rst.txt; scans.rst.txt; tutorials-landing.rst.txt; types.rst.txt. /_static; . /annotationdb; ; annotationdb.css; annotationdb.js. /cheatsheets; ; hail_matrix_tables_cheat_sheet.pdf; hail_tables_cheat_sheet.pdf. /css; . /fonts; ; fontawesome-webfont.eot; fontawesome-webfont-1.eot; fontawesome-webfont.svg; fontawesome-webfont.ttf; fontawesome-webfont.woff; fontawesome-webfont.woff2; lato-bold.woff; lato-bold.woff2; lato-bold-italic.woff; lato-bold-italic.woff2; lato-normal.woff; lato-normal.woff2; lato-normal-italic.woff; lato-normal-italic.woff2; Roboto-Slab-Bold.woff; Roboto-Slab-Bold.woff2; Roboto-Slab-Regular.woff; Roboto-Slab-Regular.woff2. theme.css. /datasets; ; datasets.js. /js; ; theme.js. _sphinx_javascript_frameworks_compat.js; auto-render.min.js; doctools.js; documentation_options.js; goto.js; hail_version.js; jquery.js; katex.min.js; katex_autorenderer.js; katex-math.css; LeveneHaldane.pdf; nbsphinx-code-cells.css; pygments.css; rtd_modifications.css; sphinx_highlight.js; toggle.js. Hail | Aggregators; Hail | Annotation Database; Hail | Hail Query Python API; Hail | hailtop.batch Python API; Hail | Change Log And Version Policy; Hail | Cheat Sheets. /cloud; ; Hail | Amazon Web Services; Hail | Microsoft Azure; Hail | Databricks; Hail | General Advice; Hail | Google Cloud Platform; Hail | Hail Query-on-Batch. Hail | Configuration Reference. /datasets; . /schemas; ; Hail | 1000_Genomes_autosomes; Hail | 1000_Genomes_chrMT; Hail | 1000_Genomes_chrX; Hail | 1000_Genomes_chrY; Hail | 1000_Genomes_HighCov_autosomes; Hail | 1000_Genomes_HighCov_chrX; Hail | 1000_Genomes_HighCov_chrY; Hail | 1000_Genomes_Retracted_autosomes; Hail | 1000_Genomes_Retracted_chrX; Hail | 1000_Genomes_Retracted_chrY; Hail | CADD; Hail | clinvar_gene_summary; Hail | clinvar_variant_summary; Hail | DANN; Hail | dbNSFP_genes; Hail | dbNSFP_variants; Hail | dbSNP; Hail | dbSNP_rsid; Hail | Ensembl_homo_sapiens_low_complexity_regions; Hail | Ensembl_homo_sapiens_reference_genome; Hail | ge,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index-wcopy.html:21628,toggle,toggle,21628,index-wcopy.html,https://hail.is,https://hail.is/index-wcopy.html,1,['toggle'],['toggle']
Deployability,"0.; seed (int, optional) – Random seed.; size (int or tuple of int, optional). Returns:; Float64Expression. hail.expr.functions.rand_int32(a, b=None, *, seed=None)[source]; Samples from a uniform distribution of 32-bit integers.; If b is None, samples from the uniform distribution over [0, a). Otherwise, sample from the; uniform distribution over [a, b).; Examples; >>> hl.reset_global_randomness(); >>> hl.eval(hl.rand_int32(10)); 9. >>> hl.eval(hl.rand_int32(10, 15)); 14. >>> hl.eval(hl.rand_int32(10, 15)); 12. Parameters:. a (int or Int32Expression) – If b is None, the right boundary of the range; otherwise, the left boundary of range.; b (int or Int32Expression) – If specified, the right boundary of the range.; seed (int, optional) – Random seed. Returns:; Int32Expression. hail.expr.functions.rand_int64(a=None, b=None, *, seed=None)[source]; Samples from a uniform distribution of 64-bit integers.; If a and b are both specified, samples from the uniform distribution over [a, b).; If b is None, samples from the uniform distribution over [0, a).; If both a and b are None samples from the uniform distribution over all; 64-bit integers.; Examples; >>> hl.reset_global_randomness(); >>> hl.eval(hl.rand_int64(10)); 9. >>> hl.eval(hl.rand_int64(1 << 33, 1 << 35)); 33089740109. >>> hl.eval(hl.rand_int64(1 << 33, 1 << 35)); 18195458570. Parameters:. a (int or Int64Expression) – If b is None, the right boundary of the range; otherwise, the left boundary of range.; b (int or Int64Expression) – If specified, the right boundary of the range.; seed (int, optional) – Random seed. Returns:; Int64Expression. hail.expr.functions.shuffle(a, seed=None)[source]; Randomly permute an array; Example; >>> hl.reset_global_randomness(); >>> hl.eval(hl.shuffle(hl.range(5))); [4, 0, 2, 1, 3]. Parameters:. a (ArrayExpression) – Array to permute.; seed (int, optional) – Random seed. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/random.html:12833,update,updated,12833,docs/0.2/functions/random.html,https://hail.is,https://hail.is/docs/0.2/functions/random.html,1,['update'],['updated']
Deployability,"04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also ad",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:37163,release,release,37163,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26844,configurat,configuration,26844,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"1614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52499,release,release,52499,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, the resulting; array will be of that shape. """""". if (nd1.dtype.element_type or nd2.dtype.element_type) == (tfloat64 or tfloat32):; return nd1.map2(; nd2, lambda a, b: hl.if_else(hl.is_nan(a) | hl.is_nan(b), hl.float64(float(""NaN"")), hl.if_else(a > b, a, b)); ); return nd1.map2(nd2, lambda a, b: hl.if_else(a > b, a, b)). [docs]@typecheck(nd1=expr_ndarray(), nd2=oneof(expr_ndarray(), list)); def minimum(nd1, nd2):; """"""Compares elements at corresponding indexes in arrays; and returns an array of the minimum element found; at each compared index. If an array element being compared has the value NaN,; the minimum for that index will be NaN. Examples; --------; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.minimum(a, b)); array([1, 3, 3], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.minimum(a, b)); array([nan, 3., nan]). Parameters; ----------; nd1 : :class:`.NDArrayExpression`; nd2 : class:`.NDArrayExpression`, `.ArrayExpression`, numpy ndarray, or nested python lists/tuples.; nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns; -------; min_array : :class:`.NDArrayExpression`; Element-wise minimums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, resulting array; will be of that shape. """""". if (nd1.dtype.element_type or nd2.dtype.element_type) == (tfloat64 or tfloat32):; return nd1.map2(; nd2, lambda a, b: hl.if_else(hl.is_nan(a) | hl.is_nan(b), hl.float64(float(""NaN"")), hl.if_else(a < b, a, b)); ); return nd1.map2(nd2, lambda a, b: hl.if_else(a < b, a, b)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:22694,update,updated,22694,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['update'],['updated']
Deployability,"2.35e+017.53e+00; "">5%""False3.70e+017.65e+00; "">5%""True3.73e+017.70e+00. We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:. Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease. Epilogue; Congrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the Python API for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on Zulip chat or the discussion forum.; For reference, here’s the full workflow to all tutorial endpoints combined into one cell. [53]:. table = hl.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'). mt = hl.read_matrix_table('data/1kg.mt'); mt = mt.annotate_cols(pheno = table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). mt = mt.annotate_cols(scores = pcs[mt.s].scores); gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 310:> (0 + 1) / 1]. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:27018,update,updated,27018,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['update'],['updated']
Deployability,"22-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49025,configurat,configuration,49025,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"32)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the en",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:93331,pipeline,pipelines,93331,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"32,; AF: float64,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; END: int32,; ExcessHet: float64,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: int32,; MLEAF: float64,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool,; QD: float64,; RAW_MQ: float64,; ReadPosRankSum: float64,; SOR: float64,; VQSLOD: float64,; VariantType: str,; culprit: str,; AN_EAS: int32,; AN_AMR: int32,; AN_EUR: int32,; AN_AFR: int32,; AN_SAS: int32,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EAS: int32,; AC_AMR: int32,; AC_EUR: int32,; AC_AFR: int32,; AC_SAS: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AF_EAS: float64,; AF_AMR: float64,; AF_EUR: float64,; AF_AFR: float64,; AF_SAS: float64,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html:12118,update,updated,12118,docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html,1,['update'],['updated']
Deployability,"32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/a').show(); +-------+-------+; | a | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 2 | 2 |; | 3 | 3 |; | 4 | 4 |; | 5 | 5 |; | 6 | 6 |; | 7 | 7 |; | 8 | 8 |; | 9 | 9 |; +-------+-------+; >>> hl.read_table('output-many/b').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'b': int32; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/b').show(); +-------+-------+; | b | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 4 | 2 |; | 9 | 3 |; | 16 | 4 |; | 25 | 5 |; | 36 | 6 |; | 49 | 7 |; | 64 | 8 |; | 81 | 9 |; +-------+-------+; >>> hl.read_table('output-many/c').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'c': str; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/c').show(); +-----+-------+; | c | idx |; +-----+-------+; | str | int32 |; +-----+-------+; | ""0"" | 0 |; | ""1"" | 1 |; | ""2"" | 2 |; | ""3"" | 3 |; | ""4"" | 4 |; | ""5"" | 5 |; | ""6"" | 6 |; | ""7"" | 7 |; | ""8"" | 8 |; | ""9"" | 9 |; +-----+-------+. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; fields (list of str) – The fields to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:81823,update,updated,81823,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['update'],['updated']
Deployability,"4,; over_15: float64,; over_20: float64,; over_25: float64,; over_30: float64,; over_50: float64,; over_100: float64; }; }; 'gerp': float64; 'tx_annotation': array<struct {; ensg: str,; csq: str,; symbol: str,; lof: str,; lof_flag: str,; Cells_Transformedfibroblasts: float64,; Prostate: float64,; Spleen: float64,; Brain_FrontalCortex_BA9_: float64,; SmallIntestine_TerminalIleum: float64,; MinorSalivaryGland: float64,; Artery_Coronary: float64,; Skin_SunExposed_Lowerleg_: float64,; Cells_EBV_transformedlymphocytes: float64,; Brain_Hippocampus: float64,; Esophagus_Muscularis: float64,; Brain_Nucleusaccumbens_basalganglia_: float64,; Artery_Tibial: float64,; Brain_Hypothalamus: float64,; Adipose_Visceral_Omentum_: float64,; Cervix_Ectocervix: float64,; Brain_Spinalcord_cervicalc_1_: float64,; Brain_CerebellarHemisphere: float64,; Nerve_Tibial: float64,; Breast_MammaryTissue: float64,; Liver: float64,; Skin_NotSunExposed_Suprapubic_: float64,; AdrenalGland: float64,; Vagina: float64,; Pancreas: float64,; Lung: float64,; FallopianTube: float64,; Pituitary: float64,; Muscle_Skeletal: float64,; Colon_Transverse: float64,; Artery_Aorta: float64,; Heart_AtrialAppendage: float64,; Adipose_Subcutaneous: float64,; Esophagus_Mucosa: float64,; Heart_LeftVentricle: float64,; Brain_Cerebellum: float64,; Brain_Cortex: float64,; Thyroid: float64,; Brain_Substantianigra: float64,; Kidney_Cortex: float64,; Uterus: float64,; Stomach: float64,; WholeBlood: float64,; Bladder: float64,; Brain_Anteriorcingulatecortex_BA24_: float64,; Brain_Putamen_basalganglia_: float64,; Brain_Caudate_basalganglia_: float64,; Colon_Sigmoid: float64,; Cervix_Endocervix: float64,; Ovary: float64,; Esophagus_GastroesophagealJunction: float64,; Testis: float64,; Brain_Amygdala: float64,; mean_proportion: float64; }>; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_annotation_pext.html:14370,update,updated,14370,docs/0.2/datasets/schemas/gnomad_annotation_pext.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_annotation_pext.html,1,['update'],['updated']
Deployability,"4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:81472,integrat,integration,81472,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Deployability,"6938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of; >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; The values of a random function are fully determined by three things:. The seed set on the function itself. If not specified, these are simply; generated sequentially.; Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a range_table(),; this data is simply the row id, as suggested by the previous examples.; The global seed. This is fixed for the entire session, and can only be set; using the global_seed argument to init(). To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; reset_global_randomness() at the start of a pipeline, which resets the; counter used to generate seeds.; >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; global_seed in init(). If not specified, the global seed is chosen; randomly. All documentation examples were computed using global_seed=0.; >>> hl.stop() ; >>> hl.init(global_seed=0) ; >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) ; [0.9828239225846387, 0.49094525115847415]. rand_bool(p[, seed]); Returns True with probability p. rand_beta(a, b[, lower, upper, seed]); Samples from a beta distribution with parameters a (alpha) and b (beta). rand_cat(prob[, seed]); Samples from a categorical ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/random.html:4778,pipeline,pipeline,4778,docs/0.2/functions/random.html,https://hail.is,https://hail.is/docs/0.2/functions/random.html,1,['pipeline'],['pipeline']
Deployability,"7). gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_est. View page source. gnomad_ld_scores_est. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_est.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_est.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_est.html,1,['update'],['updated']
Deployability,"7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:87483,update,update-hail-version,87483,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update-hail-version']
Deployability,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:95604,configurat,configurations,95604,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['configurat'],['configurations']
Deployability,": array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: array<str>,; RSID_REMOVED: array<str>,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html:11463,update,updated,11463,docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html,2,['update'],['updated']
Deployability,": int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_autosomes.html:11127,update,updated,11127,docs/0.2/datasets/schemas/1000_Genomes_autosomes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_autosomes.html,2,['update'],['updated']
Deployability,"; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This sho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:25901,install,install-completion,25901,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['install'],"['install', 'install-completion']"
Deployability,"; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invok",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37894,configurat,configuration,37894,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; The returned interval is inclusive of both the start and end; endpoints. Parameters:. before (Expression of type tint32) – Number of bases to include before the locus. Truncates at 1.; after (Expression of type tint32) – Number of bases to include after the locus. Truncates at; contig length. Returns:; IntervalExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.LocusExpression.html:11883,update,updated,11883,docs/0.2/hail.expr.LocusExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html,1,['update'],['updated']
Deployability,"; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'junction_id': str; 'junction_interval': interval<locus<GRCh37>>; 'gene_id': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'gene_symbol': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'TPM': int32; ----------------------------------------; Column key: ['s']; Row key: ['junction_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html:11193,update,updated,11193,docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html,1,['update'],['updated']
Deployability,"; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.filtering_allele_frequency. Source code for hail.experimental.filtering_allele_frequency; from hail.expr.expressions import Float64Expression, expr_float64, expr_int32; from hail.expr.functions import _func; from hail.expr.types import tfloat64; from hail.typecheck import typecheck. [docs]@typecheck(ac=expr_int32, an=expr_int32, ci=expr_float64); def filtering_allele_frequency(ac, an, ci) -> Float64Expression:; """"""; Computes a filtering allele frequency (described below); for `ac` and `an` with confidence `ci`. The filtering allele frequency is the highest true population allele frequency; for which the upper bound of the `ci` (confidence interval) of allele count; under a Poisson distribution is still less than the variant's observed; `ac` (allele count) in the reference sample, given an `an` (allele number). This function defines a ""filtering AF"" that represents; the threshold disease-specific ""maximum credible AF"" at or below which; the disease could not plausibly be caused by that variant. A variant with; a filtering AF >= the maximum credible AF for the disease under consideration; should be filtered, while a variant with a filtering AF below the maximum; credible remains a candidate. This filtering AF is not disease-specific:; it can be applied to any disease of interest by comparing with a; user-defined disease-specific maximum credible AF. For more details, see: `Whiffin et al., 2017 <https://www.nature.com/articles/gim201726>`__. Parameters; ----------; ac : int or :class:`.Expression` of type :py:data:`.tint32`; an : int or :class:`.Expression` of type :py:data:`.tint32`; ci : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""filtering_allele_frequency"", tfloat64, ac, an, ci). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html:2325,update,updated,2325,docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,1,['update'],['updated']
Deployability,"; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1594,configurat,configuration,1594,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['configurat'],['configuration']
Deployability,"; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+------+------+------+------+; showing top 11 rows; showing the first 4 of 10 columns</code></pre>. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial; Learn how to use Hail on Google Cloud. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/try.html:2263,update,updated,2263,docs/0.2/install/try.html,https://hail.is,https://hail.is/docs/0.2/install/try.html,1,['update'],['updated']
Deployability,"; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; 'call_rate': float64; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. [12]:. p = hl.plot.histogram(mt2.call_rate, range=(0,1.0), bins=100,; title='Variant Call Rate Histogram', legend='Call Rate'); show(p). Exercise: GQ vs DP; In this exercise, you’ll use Hail to investigate a strange property of sequencing datasets.; The DP field is the sequencing depth (the number of reads).; Let’s first plot a histogram of DP:. [13]:. p = hl.plot.histogram(mt.DP, range=(0,40), bins=40, title='DP Histogram', legend='DP'); show(p). [Stage 9:> (0 + 1) / 1]. Now, let’s do the same thing for GQ.; The GQ field is the phred-scaled “genotype quality”. The formula to convert to a linear-scale confidence (0 to 1) is 10 ** -(mt.GQ / 10). GQ is truncated to lie between 0 and 99. [14]:. p = hl.plot.histogram(mt.GQ, range=(0,100), bins=100, title='GQ Histogram', legend='GQ'); show(p). [Stage 10:> (0 + 1) / 1]. Whoa! That’s a strange distribution! There’s a big spike at 100. The rest of the values have roughly the same shape as the DP distribution, but form a Dimetrodon. Use Hail to figure out what’s going on!. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/07-matrixtable.html:10599,update,updated,10599,docs/0.2/tutorials/07-matrixtable.html,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html,1,['update'],['updated']
Deployability,"; Schema (2.1.1, GRCh37). gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_fin. View page source. gnomad_ld_scores_fin. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html,1,['update'],['updated']
Deployability,"; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]]) – Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See ServiceBackend for details.; default_timeout (Union[int, float, None]) – Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str]) – Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool]) – If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str]) – DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int]) – Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. ne",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:3750,install,installed,3750,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['install'],['installed']
Deployability,"; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; DANN. View page source. DANN. Versions: None; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (None, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int64,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/DANN.html:9416,update,updated,9416,docs/0.2/datasets/schemas/DANN.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/DANN.html,1,['update'],['updated']
Deployability,"; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gerp_elements. View page source. gerp_elements. Versions: hg19; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (hg19, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh37>>; 'S': float64; 'p_value': float64; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gerp_elements.html:9425,update,updated,9425,docs/0.2/datasets/schemas/gerp_elements.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gerp_elements.html,1,['update'],['updated']
Deployability,"; gnomad_ld_scores_nfe; Schema (2.1.1, GRCh37). gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_nfe. View page source. gnomad_ld_scores_nfe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html,1,['update'],['updated']
Deployability,"; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; Schema (2.1.1, GRCh37). gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_nwe. View page source. gnomad_ld_scores_nwe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html,1,['update'],['updated']
Deployability,"; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; Schema (2.1.1, GRCh37). gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_seu. View page source. gnomad_ld_scores_seu. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html,1,['update'],['updated']
Deployability,"; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_ALL. View page source. giant_bmi_exome_ALL. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html:9563,update,updated,9563,docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html,1,['update'],['updated']
Deployability,"; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html:9724,update,updated,9724,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations. View page source. GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html:9724,update,updated,9724,docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Pituitary_all_snp_gene_associations. View page source. GTEx_sQTL_Pituitary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html:9742,update,updated,9742,docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:74290,update,update,74290,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if Env._hc:; Env.hc().stop(). [docs]def spark_context():; """"""Returns the active Spark context. Returns; -------; :class:`pyspark.SparkContext`; """"""; return Env.spark_backend('spark_context').sc. [docs]def tmp_dir() -> str:; """"""Returns the Hail shared temporary directory. Returns; -------; :class:`str`; """"""; return Env.hc()._tmpdir. class _TemporaryFilenameManager:; def __in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20829,release,releases,20829,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['release'],['releases']
Deployability,"; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_iterations=q_iterations,; oversampling_param=oversampling_param,; block_size=block_size,; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:24481,update,updated,24481,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['update'],['updated']
Deployability,"; movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {users_path} ...'). users = rename_columns(; hl.import_table(user_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'),; ['id', 'age', 'sex', 'occupation', 'zipcode'],; ); users.write(users_path, overwrite=True). info(f'importing movies table and writing to {movies_path} ...'). movies = hl.import_table(movie_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'); movies = rename_columns(; movies, ['id', 'title', 'release date', 'video release date', 'IMDb URL', 'unknown', *genres]; ); movies = movies.drop('release date', 'video release date', 'unknown', 'IMDb URL'); movies = movies.transmute(genres=fields_to_array(movies, genres)); movies.write(movies_path, overwrite=True). info(f'importing ratings table and writing to {ratings_path} ...'). ratings = hl.import_table(ratings_cluster_readable, no_header=True, impute=True); ratings = rename_columns(ratings, ['user_id', 'movie_id', 'rating', 'timestamp']); ratings = ratings.drop('timestamp'); ratings.write(ratings_path, overwrite=True). else:; info('Movie Lens files found!'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:8872,release,release,8872,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,5,"['release', 'update']","['release', 'updated']"
Deployability,"; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Father column fields from `mt`.; - `mother` (``struct``) -- Mother column fields from `mt`.; - `proband_entry` (``struct``) -- Proband entry fields from `mt`.; - `father_entry` (``struct``) -- Father entry fields from `mt`.; - `proband_entry` (``struct``) -- Mother entry fields from `mt`.; - `is_female` (``bool``) -- ``True`` if proband is female.; - `p_de_novo` (``float64``) -- Unfiltered posterior probability; that the event is *de novo* rather than a missed heterozygous; event in a parent.; - `confidence` (``str``) Validation confidence. One of: ``'HIGH'``,; ``'MEDIUM'``, ``'LOW'``. The key of the table is ``['locus', 'alleles', 'id']``. The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:22386,configurat,configuration,22386,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['configurat'],['configuration']
Deployability,"; return annotation. def _typecheck(self, annotation):; if annotation and not isinstance(annotation, Locus):; raise TypeCheckError('TLocus expected type hail.representation.Locus, but found %s' %; type(annotation)). [docs]class TInterval(Type):; """"""; Hail type corresponding to :class:`hail.representation.Interval`. .. include:: hailType.rst. - `expression language documentation <types.html#interval>`__; - in Python, values are instances of :class:`hail.representation.Interval`. """"""; __metaclass__ = SingletonType. def __init__(self):; super(TInterval, self).__init__(scala_object(Env.hail().expr, 'TInterval')). def _convert_to_py(self, annotation):; if annotation:; return Interval._from_java(annotation); else:; return annotation. def _convert_to_j(self, annotation):; if annotation is not None:; return annotation._jrep; else:; return annotation. def _typecheck(self, annotation):; if annotation and not isinstance(annotation, Interval):; raise TypeCheckError('TInterval expected type hail.representation.Interval, but found %s' %; type(annotation)). __singletons__ = {'is.hail.expr.TInt$': TInt,; 'is.hail.expr.TLong$': TLong,; 'is.hail.expr.TFloat$': TFloat,; 'is.hail.expr.TDouble$': TDouble,; 'is.hail.expr.TBoolean$': TBoolean,; 'is.hail.expr.TString$': TString,; 'is.hail.expr.TVariant$': TVariant,; 'is.hail.expr.TAltAllele$': TAltAllele,; 'is.hail.expr.TLocus$': TLocus,; 'is.hail.expr.TGenotype$': TGenotype,; 'is.hail.expr.TCall$': TCall,; 'is.hail.expr.TInterval$': TInterval}. import pprint. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, Type):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/expr.html:20131,patch,patch,20131,docs/0.1/_modules/hail/expr.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html,1,['patch'],['patch']
Deployability,"; sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31501,update,update,31501,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:8410,update,updated,8410,docs/0.2/hail.expr.NDArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html,1,['update'],['updated']
Deployability,"=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23077,configurat,configuration,23077,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region]; for dataset in datasets[name]['versions']; if all([dataset['version'] == version, dataset['reference_genome'] == reference_genome]); ]; assert len(path) == 1; path = path[0]; if path.startswith('s3://'):; try:; dataset = _read_dataset(path); except hl.utils.java.FatalError:; dataset = _read_dataset(path.replace('s3://', 's3a://')); else:; dataset = _read_dataset(path); return dataset. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:4542,update,updated,4542,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['update'],['updated']
Deployability,"={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:8983,continuous,continuous,8983,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"> hl.eval(hl.pgenchisq(40 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of ti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:18503,release,released,18503,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['release'],['released']
Deployability,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.TupleExpression.html:6927,update,updated,6927,docs/0.2/hail.expr.TupleExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_afr. View page source. gnomad_ld_variant_indices_afr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_amr. View page source. gnomad_ld_variant_indices_amr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_asj. View page source. gnomad_ld_variant_indices_asj. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_eas. View page source. gnomad_ld_variant_indices_eas. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_est. View page source. gnomad_ld_variant_indices_est. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_fin. View page source. gnomad_ld_variant_indices_fin. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_nwe. View page source. gnomad_ld_variant_indices_nwe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_seu. View page source. gnomad_ld_variant_indices_seu. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html,1,['update'],['updated']
Deployability,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:3369,release,released,3369,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,2,"['release', 'update']","['released', 'updated']"
Deployability,"Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:60931,pipeline,pipelines,60931,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_male. View page source. UK_Biobank_Rapid_GWAS_male. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html:10491,update,updated,10491,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,1,['update'],['updated']
Deployability,"C_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype': str; 'ZFIN_zebrafish_gene': str; 'ZFIN_zebrafish_structure': str; 'ZFIN_zebrafish_phenotype_quality': str; 'ZFIN_zebrafish_phenotype_tag': str; ----------------------------------------; Key: ['Gene_name']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:12380,update,updated,12380,docs/0.2/datasets/schemas/dbNSFP_genes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html,1,['update'],['updated']
Deployability,"Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The ou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:3818,configurat,configuration,3818,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; 1000_Genomes_chrMT. View page source. 1000_Genomes_chrMT. Versions: phase_3; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (phase_3, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: int32,; VT: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrMT.html:10725,update,updated,10725,docs/0.2/datasets/schemas/1000_Genomes_chrMT.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrMT.html,1,['update'],['updated']
Deployability,"Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides.html:2049,update,updated,2049,docs/0.2/guides.html,https://hail.is,https://hail.is/docs/0.2/guides.html,1,['update'],['updated']
Deployability,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:2803,update,updated,2803,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['update'],['updated']
Deployability,"Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_fl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:70886,integrat,integration,70886,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['integrat'],['integration']
Deployability,"For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Fac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:31818,continuous,continuous,31818,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['continuous'],['continuous']
Deployability,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102953,configurat,configuration,102953,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,3,"['configurat', 'install', 'release']","['configuration', 'installed', 'release']"
Deployability,Installing Hail; Hail | For Software Developers. /ggplot; ; Hail | Plotting With hail.ggplot Overview. /guides; ; Hail | Aggregation; Hail | Annotation; Hail | Genetics. Hail | How-To Guides; Hail | Hadoop Glob Patterns; Hail | ArrayExpression; Hail | ArrayNumericExpression; Hail | BooleanExpression; Hail | CallExpression; Hail | CollectionExpression; Hail | DictExpression. Hail | Expression. Hail | Expression; Hail | Expression. Hail | Float32Expression; Hail | Float64Expression; Hail | Int32Expression; Hail | Int64Expression; Hail | IntervalExpression; Hail | LocusExpression; Hail | NDArrayExpression; Hail | NDArrayNumericExpression; Hail | NumericExpression; Hail | SetExpression; Hail | StringExpression; Hail | StructExpression; Hail | TupleExpression; Hail | GroupedMatrixTable; Hail | GroupedTable; Hail | MatrixTable; Hail | Table; Hail | Hail on the Cloud; Hail | Hail 0.2. /install; ; Hail | Use Hail on Azure HDInsight; Hail | Use Hail on Google Dataproc; Hail | Install Hail on GNU/Linux; Hail | Install Hail on Mac OS X; Hail | Install Hail on a Spark Cluster; Hail | Your First Hail Query. Hail | Libraries. /linalg; . /utils; ; Hail | linalg/utils. Hail | BlockMatrix; Hail | linalg. /methods; ; Hail | Genetics; Hail | Import / Export; Hail | Methods; Hail | Miscellaneous; Hail | Relatedness; Hail | Statistics. /nd; ; Hail | nd. Hail | Other Resources. /overview; ; Hail | Expressions Overview; Hail | Hail Overview. Hail | MatrixTable Overview. Hail | MatrixTable Overview; Hail | MatrixTable Overview. Hail | MatrixTable Overview; Hail | Table Overview. Hail | Plot; Hail | Python API; Hail | Scans; Hail | Search. /stats; ; Hail | LinearMixedModel; Hail | stats. /tutorials; . /iframe_figures; ; figure_11.html; figure_12.html; figure_13.html; figure_15.html; figure_16.html; figure_17.html; figure_18.html; figure_3.html; figure_4.html; figure_5.html; figure_6.html; figure_7.html; figure_8.html. Hail | GWAS Tutorial; Hail | Table Tutorial; Hail | Aggregation Tutorial; ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index-wcopy.html:33391,install,install,33391,index-wcopy.html,https://hail.is,https://hail.is/index-wcopy.html,1,['install'],['install']
Deployability,"K.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2137,install,install,2137,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['install'],['install']
Deployability,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23890,pipeline,pipelines,23890,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['pipeline'],['pipelines']
Deployability,"L2': float64; 'UTR_5_UCSC.flanking.500L2': float64; 'WeakEnhancer_HoffmanL2': float64; 'WeakEnhancer_Hoffman.flanking.500L2': float64; 'GERP.NSL2': float64; 'GERP.RSsup4L2': float64; 'MAFbin1L2': float64; 'MAFbin2L2': float64; 'MAFbin3L2': float64; 'MAFbin4L2': float64; 'MAFbin5L2': float64; 'MAFbin6L2': float64; 'MAFbin7L2': float64; 'MAFbin8L2': float64; 'MAFbin9L2': float64; 'MAFbin10L2': float64; 'MAF_Adj_Predicted_Allele_AgeL2': float64; 'MAF_Adj_LLD_AFRL2': float64; 'Recomb_Rate_10kbL2': float64; 'Nucleotide_Diversity_10kbL2': float64; 'Backgrd_Selection_StatL2': float64; 'CpG_Content_50kbL2': float64; 'MAF_Adj_ASMCL2': float64; 'GTEx_eQTL_MaxCPPL2': float64; 'BLUEPRINT_H3K27acQTL_MaxCPPL2': float64; 'BLUEPRINT_H3K4me1QTL_MaxCPPL2': float64; 'BLUEPRINT_DNA_methylation_MaxCPPL2': float64; 'synonymousL2': float64; 'non_synonymousL2': float64; 'Conserved_Vertebrate_phastCons46wayL2': float64; 'Conserved_Vertebrate_phastCons46way.flanking.500L2': float64; 'Conserved_Mammal_phastCons46wayL2': float64; 'Conserved_Mammal_phastCons46way.flanking.500L2': float64; 'Conserved_Primate_phastCons46wayL2': float64; 'Conserved_Primate_phastCons46way.flanking.500L2': float64; 'BivFlnkL2': float64; 'BivFlnk.flanking.500L2': float64; 'Human_Promoter_VillarL2': float64; 'Human_Promoter_Villar.flanking.500L2': float64; 'Human_Enhancer_VillarL2': float64; 'Human_Enhancer_Villar.flanking.500L2': float64; 'Ancient_Sequence_Age_Human_PromoterL2': float64; 'Ancient_Sequence_Age_Human_Promoter.flanking.500L2': float64; 'Ancient_Sequence_Age_Human_EnhancerL2': float64; 'Ancient_Sequence_Age_Human_Enhancer.flanking.500L2': float64; 'Human_Enhancer_Villar_Species_Enhancer_CountL2': float64; 'Human_Promoter_Villar_ExACL2': float64; 'Human_Promoter_Villar_ExAC.flanking.500L2': float64; 'locus': locus<GRCh37>; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html:12941,update,updated,12941,docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,1,['update'],['updated']
Deployability,"L_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if Env._hc:; Env.hc().stop(). [docs]def spark_context():; """"""Returns the active Spark context. Returns; -------; :class:`pyspark.SparkContext`; """"""; return Env.spark_backend('spark_context').sc. [docs]def tmp_dir() -> str:; """"""Returns the Hail shared temporary directory. Returns; -------; :class:`str`; """"""; return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20777,install,installed,20777,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['install'],['installed']
Deployability,"L_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Clouder",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2400,install,install,2400,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,3,['install'],['install']
Deployability,"MatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See also; aggregate(). Returns:; MatrixTable – Aggregated matrix table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:6747,update,updated,6747,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['update'],['updated']
Deployability,"N=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; hail-script.py. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:2893,configurat,configuration,2893,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"NPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is set to requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. hail.methods.balding_nichols_model(n_populations, n_samples, n_variants, n_partitions=None, pop_dist=None, fst=None, af_dist=None, reference_genome='default', mixture=False, *, phased=False)[source]; Generate a matrix table of variants, samples, and genotypes using the; Balding-Nichols or Pritchard-Stephens-Donnelly model.; Examples; Generate a matrix table of genotypes with 1000 variants and 100 samples; across 3 populations:; >>> hl.r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:7602,configurat,configuration,7602,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"PartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:7458,install,installation,7458,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installation']
Deployability,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106025,integrat,integration,106025,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['integrat'],['integration']
Deployability,"Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.truncate_reference_blocks. View page source. hail.vds.truncate_reference_blocks. hail.vds.truncate_reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None)[source]; Cap reference blocks at a maximum length in order to permit faster interval filtering.; Examples; Truncate reference blocks to 5 kilobases:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) . Truncate the longest 1% of reference blocks to the length of the 99th percentile block:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) . Notes; After this function has been run, the reference blocks have a known maximum length ref_block_max_length,; stored in the global fields, which permits vds.filter_intervals() to filter to intervals of the reference; data by reading ref_block_max_length bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work.; It is also possible to patch an existing VDS to store the max reference block length with vds.store_ref_block_max_length(). See also; vds.store_ref_block_max_length(). Parameters:. vds (VariantDataset or MatrixTable); max_ref_block_base_pairs – Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction – Fraction of reference block length distribution to truncate / winsorize. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html:1714,patch,patch,1714,docs/0.2/vds/hail.vds.truncate_reference_blocks.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html,2,"['patch', 'update']","['patch', 'updated']"
Deployability,"Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries muc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:45902,install,install-on-cluster,45902,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['install-on-cluster']
Deployability,"Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:1948,install,install,1948,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['install']
Deployability,"Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20078,integrat,integration,20078,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Deployability,"Table tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, you",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:18150,configurat,configuration,18150,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"Union[Plot, Column]:; """"""Create an interactive scatter plot. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive scatter plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:30792,continuous,continuous,30792,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['continuous'],['continuous']
Deployability,"Variant Effect Predictor (VEP). Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hai",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1233,install,install,1233,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['install'],['install']
Deployability,"[9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; -----; The `number` parameter matches the `VCF specification <https://samtools.github.io/hts-specs/VCFv4.3.pdf>`__; number definitions:. - ``A`` indicates one value per allele, excluding the reference.; - ``R`` indicates one value per allele, including the reference.; - ``G`` indicates one value per unique diploid genotype. Warning; -------; Using this function can lead to an enormous explosion in data size, without increasing; information capacity. Its appropriate use is to conform to antiquated and badly-scaling; representations (e.g. pVCF), but even so, caution should be exercised. Reindexing local; PLs (or any G-numbered field) at a site with 1000 alleles will produce an array with; more than 5,000 values per sample -- with 100,000 samples, nearly 50GB per variant!. See Also; --------; :func:`.lgt_to_gt`. Parameters; ----------; array : :class:`.ArrayExpression`; Array to reindex.; local_alleles : :class:`.ArrayExpression`; Local alleles array.; n_alleles : :class:`.Int32Expression`; Total number of alleles to reindex to.; fill_value; Value to fill in at global indices with no data.; number : :class:`str`; One of 'A', 'R', 'G'. Returns; -------; :class:`.ArrayExpression`; """"""; try:; fill_value = hl.coercer_from_dtype(array.dtype.element_type).coerce(fill_value); except Exception as e:; raise ValueError(f'fill_value type {fill_value.dtype} is incompatible with array type {array.dtype}') from e. if number == 'G':; return _func(""local_to_global_g"", array.dtype, array, local_alleles, n_alleles, fill_value); elif number == 'R':; omit_first = False; elif number == 'A':; omit_first = True; else:; raise ValueError(f'unrecognized number {number}'). return _func(""local_to_global_a_r"", array.dtype, array, local_alleles, n_alleles, fill_value, hl.bool(omit_first)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/functions.html:3577,update,updated,3577,docs/0.2/_modules/hail/vds/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/functions.html,1,['update'],['updated']
Deployability,"\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = mayb",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30694,update,update,30694,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"__eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expression of the same type. __ge__(other); Return self>=value. __getitem__(item)[source]; Access a field of the struct by name or index.; Examples; >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters:; item (str) – Field name. Returns:; Expression – Struct field. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other)[source]; Return self!=value. annotate(**named_exprs)[source]; Add new fields or recompute existing fields.; Examples; >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; If an expression in named_exprs shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters:; named_exprs (keyword args of Expression) – Fields to add. Returns:; StructExpression – Struct with new or updated fields. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. drop(*fields)[source]; Drop fields from the struct.; Examples; >>> hl.eval(struct.drop('b')); Struct(a=5). Parameters:; fields (varargs of str) – Fields to drop. Returns:; StructExpression – Struct without certain fields. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:3071,update,updated,3071,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['update'],['updated']
Deployability,"_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'vqsr': struct {; AS_VQSLOD: float64,; AS_culprit: str,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool; }; 'region_flag': struct {; lcr: bool,; segdup: bool; }; 'allele_info': struct {; variant_type: str,; allele_type: str,; n_alt_alleles: int32,; was_mixed: bool; }; 'age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_sites.html:14673,update,updated,14673,docs/0.2/datasets/schemas/gnomad_genome_sites.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_sites.html,1,['update'],['updated']
Deployability,"_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gerp_scores. View page source. gerp_scores. Versions: hg19; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (hg19, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'N': float64; 'S': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gerp_scores.html:9397,update,updated,9397,docs/0.2/datasets/schemas/gerp_scores.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gerp_scores.html,1,['update'],['updated']
Deployability,"_apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()[ht.i, ht.j].entry). if statistics in ['kin20', 'all']:; # Get the numerator used in IBD0 (k0) computation (IBS0), compute indicator matrices for homozygotes; hom_alt = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 2.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); hom_ref = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 0.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); ibs0 = _AtB_plus_BtA(hom_alt, hom_ref). # Get the denominator used in IBD0 (k0) computation; mu2 = _replace_nan(mu**2.0, 0.0); one_minus_mu2 = _replace_nan((1.0 - mu) ** 2.0, 0.0); k0_denom = _AtB_plus_BtA(mu2, one_minus_mu2). # Compute IBD0 (k0) estimates, correct the estimates where phi <= k0_cutoff; k0 = ibs0 / k0_denom; k0_cutoff = 2.0 ** (-5.0 / 2.0); ht = ht.annotate(k0=k0.entries()[ht.i, ht.j].entry); ht = ht.annotate(k0=hl.if_else(ht.kin <= k0_cutoff, 1.0 - (4.0 * ht.kin) + ht.k2, ht.k0)). if statistics == 'all':; # Finally, compute IBD1 (k1) estimate; ht = ht.annotate(k1=1.0 - (ht.k2 + ht.k0)). # Filter table to only have one row for each distinct pair of samples; ht = ht.filter(ht.i <= ht.j); ht = ht.rename({'k0': 'ibd0', 'k1': 'ibd1', 'k2': 'ibd2'}). if min_kinship is not None:; ht = ht.filter(ht.kin >= min_kinship). if statistics != 'all':; fields_to_drop = {'kin': ['ibd0', 'ibd1', 'ibd2'], 'kin2': ['ibd0', 'ibd1'], 'kin20': ['ibd1']}; ht = ht.drop(*fields_to_drop[statistics]). if not include_self_kinship:; ht = ht.filter(ht.i == ht.j, keep=False). col_keys = hl.literal(mt.select_cols().key_cols_by().cols().collect(), dtype=hl.tarray(mt.col_key.dtype)); return ht.key_by(i=col_keys[hl.int32(ht.i)], j=col_keys[hl.int32(ht.j)]). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:23163,update,updated,23163,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,1,['update'],['updated']
Deployability,"_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:6973,configurat,configuration,6973,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations. View page source. GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html:9718,update,updated,9718,docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html:9718,update,updated,9718,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations. View page source. GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html:9718,update,updated,9718,docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations. View page source. GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html:9718,update,updated,9718,docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Stomach_all_snp_gene_associations. View page source. GTEx_sQTL_Stomach_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html:9736,update,updated,9736,docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Thyroid_all_snp_gene_associations. View page source. GTEx_sQTL_Thyroid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html:9736,update,updated,9736,docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_exome_C_ALL_Rec; giant_whr_exome_C_EUR_Add; giant_whr_exome_C_EUR_Rec; giant_whr_exome_M_ALL_Add; giant_whr_exome_M_ALL_Rec; giant_whr_exome_M_EUR_Add; giant_whr_exome_M_EUR_Rec; giant_whr_exome_W_ALL_Add; giant_whr_exome_W_ALL_Rec; giant_whr_exome_W_EUR_Add; giant_whr_exome_W_EUR_Rec; gnomad_annotation_pext; gnomad_base_pext; gnomad_chrM_coverage; gnomad_chrM_sites; gnomad_exome_coverage; gnomad_exome_sites; gnomad_genome_coverage; gnomad_genome_sites; gnomad_hgdp_1kg_subset_dense; gnomad_hgdp_1kg_subset_sample_metadata; gnomad_hgdp_1kg_subset_sparse; gnomad_hgdp_1kg_subset_variant_annotations; gnomad_ld_scores_afr; gnomad_ld_scores_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas.html:17268,update,updated,17268,docs/0.2/datasets/schemas.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas.html,1,['update'],['updated']
Deployability,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html:9706,update,updated,9706,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Colon_Transverse_all_snp_gene_associations. View page source. GTEx_eQTL_Colon_Transverse_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html:9706,update,updated,9706,docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html:9706,update,updated,9706,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; Schema (2.2, GRCh37). panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; ldsc_baselineLD_ldscores. View page source. ldsc_baselineLD_ldscores. Versions: 2.2, 1.1; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (2.2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'annotation': str; 'M_5_50': int32; 'M': int32; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'SNP': str; ----------------------------------------; Entry fields:; 'x': float64; ----------------------------------------; Column key: ['annotation']; Row key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html:9646,update,updated,9646,docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. View page source. GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html:9736,update,updated,9736,docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. View page source. GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html:9736,update,updated,9736,docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations. View page source. GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html:9754,update,updated,9754,docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Tibial_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html:9754,update,updated,9754,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations. View page source. GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html:9754,update,updated,9754,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations. View page source. GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html:9754,update,updated,9754,docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_region_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. :param str config: Path to VEP configuration file. :param block_size: Number of variants to annotate per VEP invocation.; :type block_size: int. :param str root: Variant annotation path to store VEP output. :param bool csq: If ``True``, annotates VCF CSQ field as a String.; If ``False``, annotates with the full nested struct schema. :return: An annotated with variant annotations from VEP.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvds.vep(config, root, csq, block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; def variants_table(self):; """"""Convert variants and variant annotations to a KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; v: Variant; va: variant annotations; }. with a single key ``v``. :return: Key table with variants and variant annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.variantsKT()). [docs] @handle_py4j; def samples_table(self):; """"""Convert samples and sample annotations to KeyTable. The resulting",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:227116,configurat,configuration,227116,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['configurat'],['configuration']
Deployability,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Aorta_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html:9694,update,updated,9694,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html:9694,update,updated,9694,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations. View page source. GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html:9694,update,updated,9694,docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html:9805,update,updated,9805,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. View page source. GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html:9805,update,updated,9805,docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; -------; :class:`.MatrixTable`. Notes; -----; Adds a new row field, `row_field`, and a new column field, `col_field`,; each of which are structs with t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58960,pipeline,pipeline,58960,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['pipeline'],['pipeline']
Deployability,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:6775,install,installing,6775,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installing']
Deployability,"a=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; -----. We follow Wikipedia's notational conventions. Some texts refer to the weight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:68494,release,released,68494,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['release'],['released']
Deployability,"abels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the y-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_discrete(name=None, breaks=None, labels=None)[source]; The default discrete y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of str) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_log10(name=None)[source]; Transforms y-axis to be log base 10 scaled. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_reverse(name=None)[source]; Transforms y-axis to be vertically reversed. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_continuous()[source]; The default continuous color scale. This linearly interpolates colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_discrete()[source]; The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_hue()[source]; Map discrete colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_manual(*, values)[source]; A color scale that assigns strings to colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_identity()[source]; A color scale that assumes the expression specified in the color aesthetic can be used as a color. Returns:; FigureAttribute – The scale",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:12622,continuous,continuous,12622,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedTable.html:1909,pipeline,pipeline,1909,docs/0.2/hail.GroupedTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html,3,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"ables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where li",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83904,pipeline,pipelines,83904,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"ache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be queried with :py:attr:`~hail.VariantDataset.variant_schema`. .. code-block:: text. Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223451,release,release-,223451,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['release'],['release-']
Deployability,"ad_ld_scores_afr; gnomad_ld_scores_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_chrM_coverage. View page source. gnomad_chrM_coverage. Versions: 3.1; Reference genome builds: GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'mean': float64; 'median': int32; 'over_100': float64; 'over_1000': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_coverage.html:9371,update,updated,9371,docs/0.2/datasets/schemas/gnomad_chrM_coverage.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_coverage.html,1,['update'],['updated']
Deployability,"ad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_ALL. View page source. giant_height_exome_ALL. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_ALL.html:9572,update,updated,9572,docs/0.2/datasets/schemas/giant_height_exome_ALL.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_ALL.html,1,['update'],['updated']
Deployability,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Aorta_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Aorta_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html:9751,update,updated,9751,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cortex_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html:9751,update,updated,9751,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations. View page source. GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html:9751,update,updated,9751,docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_hgdp_1kg_subset_sparse. View page source. gnomad_hgdp_1kg_subset_sparse. Versions: 3.1.2; Reference genome builds: GRCh38; Type: hail.MatrixTable. Schema (3.1.2, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'DP': int32; 'END': int32; 'GQ': int32; 'LA': array<int32>; 'LAD': array<int32>; 'LGT': call; 'LPGT': call; 'LPL': array<int32>; 'MIN_DP': int32; 'PID': str; 'RGQ': int32; 'SB': array<int32>; 'gvcf_info': struct {; ClippingRankSum: float64,; BaseQRankSum: float64,; MQ: float64,; MQRankSum: float64,; MQ_DP: int32,; QUALapprox: int32,; RAW_MQ: float64,; ReadPosRankSum: float64,; VarDP: int32; }; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html:9925,update,updated,9925,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html,1,['update'],['updated']
Deployability,"ader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1912,install,install,1912,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['install']
Deployability,"ader when importing a VCF and written; to the VCF header when exporting a VCF:. - INFO fields attributes (attached to (`va.info.*`)):. - 'Number': The arity of the field. Can take values. - `0` (Boolean flag),; - `1` (single value),; - `R` (one value per allele, including the reference),; - `A` (one value per non-reference allele),; - `G` (one value per genotype), and; - `.` (any number of values). - When importing: The value in read from the VCF INFO field definition; - When exporting: The default value is `0` for **Boolean**, `.` for **Arrays** and 1 for all other types. - 'Description' (default is ''). - FILTER entries in the VCF header are generated based on the attributes; of `va.filters`. Each key/value pair in the attributes will generate a; FILTER entry in the VCF with ID = key and Description = value. :param str ann_path: Variant annotation path starting with 'va', period-delimited. :param str attribute: The attribute to remove (key). :return: Annotated dataset with the updated variant annotation without the attribute.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(self.hc, self._jvds.deleteVaAttribute(ann_path, attribute)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(propagate_gq=bool,; keep_star_alleles=bool,; max_shift=integral); def split_multi(self, propagate_gq=False, keep_star_alleles=False, max_shift=100):; """"""Split multiallelic variants. .. include:: requireTGenotype.rst. **Examples**. >>> vds.split_multi().write('output/split.vds'). **Notes**. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:207970,update,updated,207970,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['update'],['updated']
Deployability,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:54215,pipeline,pipelines,54215,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipelines']
Deployability,"ailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:1418,update,update,1418,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:49868,pipeline,pipelines,49868,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['pipeline'],['pipelines']
Deployability,"al': array<int64>; 'exp_lof_afr': array<float64>; 'obs_lof_afr': array<int64>; 'exp_lof_amr': array<float64>; 'obs_lof_amr': array<int64>; 'exp_lof_eas': array<float64>; 'obs_lof_eas': array<int64>; 'exp_lof_nfe': array<float64>; 'obs_lof_nfe': array<int64>; 'exp_lof_sas': array<float64>; 'obs_lof_sas': array<int64>; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': set<str>; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int64; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int64; 'n_sites_array': array<int64>; 'classic_caf': float64; 'max_af': float64; 'classic_caf_array': array<float64>; 'no_lofs': int64; 'obs_het_lof': int64; 'obs_hom_lof': int64; 'defined': int64; 'pop_no_lofs': dict<str, int64>; 'pop_obs_het_lof': dict<str, int64>; 'pop_obs_hom_lof': dict<str, int64>; 'pop_defined': dict<str, int64>; 'p': float64; 'pop_p': dict<str, float64>; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': str; 'cds_length': int64; 'num_coding_exons': int64; 'interval': interval<locus<GRCh37>>; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': float64; ----------------------------------------; Key: ['gene', 'transcript']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html:13328,update,updated,13328,docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html,1,['update'],['updated']
Deployability,"al; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:69928,pipeline,pipelines,69928,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"alling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:59554,configurat,configuration,59554,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:35295,update,updated,35295,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['update'],['updated']
Deployability,"ample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry (struct) – Father entry fields from mt.; proband_entry (struct) – Mother entry fields from mt.; is_female (bool) – True if proband is female.; p_de_novo (float64) – Unfiltered posterior probability; that the event is de novo rather than a missed heterozygous; event in a parent.; confidence (str) Validation confidence. One of: 'HIGH',; 'MEDIUM', 'LOW'. The key of the table is ['locus', 'alleles', 'id'].; The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration x = (AA, AA, AB) of calls; occurs, exactly one of the following is true:. d: a de novo mutation occurred in the proband and all calls are; accurate.; m: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. \[\mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}\]; Applying Bayes rule to the numerator and denominator yields. \[\frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}\]; The prior on de novo mutation is estimated from the rate in the literature:. \[\mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}\]; The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. \[\mathrm{P}(m) = 1 - (1 - AF)^4\]; The likelihoods \(\mathrm{P}(x \mid d)\) and \(\mathrm{P}(x \mid m)\); are computed",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:54399,configurat,configuration,54399,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"andas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching hea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:46011,pipeline,pipelines,46011,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"ant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_nfe. View page source. gnomad_ld_variant_indices_nfe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'min_af': float64; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html:10454,update,updated,10454,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html,1,['update'],['updated']
Deployability,"ant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html:9748,update,updated,9748,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. View page source. GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html:9748,update,updated,9748,docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html:9766,update,updated,9766,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ants('variants.map(v => v.altAllele()).filter(aa => aa.isSNP()).counter()'); pprint(Counter(snp_counts).most_common()). [(AltAllele(ref=C, alt=T), 2436L),; (AltAllele(ref=G, alt=A), 2387L),; (AltAllele(ref=A, alt=G), 1944L),; (AltAllele(ref=T, alt=C), 1879L),; (AltAllele(ref=C, alt=A), 496L),; (AltAllele(ref=G, alt=T), 480L),; (AltAllele(ref=T, alt=G), 468L),; (AltAllele(ref=A, alt=C), 454L),; (AltAllele(ref=C, alt=G), 150L),; (AltAllele(ref=G, alt=C), 112L),; (AltAllele(ref=T, alt=A), 79L),; (AltAllele(ref=A, alt=T), 76L)]. It’s nice to see that we can actually uncover something biological from; this small dataset: we see that these frequencies come in pairs. C/T and; G/A are actually the same mutation, just viewed from from opposite; strands. Likewise, T/A and A/T are the same mutation on opposite; strands. There’s a 30x difference between the frequency of C/T and A/T; SNPs. Why?; The same Python, R, and Unix tools could do this work as well, but we’re; starting to hit a wall - the latest gnomAD; release publishes about 250; million variants, and that won’t fit in memory on a single computer.; What about genotypes? Hail can query the collection of all genotypes in; the dataset, and this is getting large even for our tiny dataset. Our; 1,000 samples and 10,000 variants produce 10 million unique genotypes.; The gnomAD dataset has about 5 trillion unique genotypes.; Here we will use the hist aggregator to produce and plot a histogram; of DP values for genotypes in our thousand genomes dataset. In [26]:. dp_hist = vds.query_genotypes('gs.map(g => g.dp).hist(0, 30, 30)'); plt.xlim(0, 31); plt.bar(dp_hist.binEdges[1:], dp_hist.binFrequencies); plt.show(). Quality Control¶; QC is where analysts spend most of their time with sequencing datasets.; QC is an iterative process, and is different for every project: there is; no “push-button” solution for QC. Each time the Broad collects a new; group of samples, it finds new batch effects. However, by practicing; open science an",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:12251,release,release,12251,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['release'],['release']
Deployability,"ape_str(s, backticked=True)). def parsable_strings(strs):; strs = ' '.join(f'""{escape_str(s)}""' for s in strs); return f""({strs})"". def _dumps_partitions(partitions, row_key_type):; parts_type = partitions.dtype; if not (isinstance(parts_type, hl.tarray) and isinstance(parts_type.element_type, hl.tinterval)):; raise ValueError(f'partitions type invalid: {parts_type} must be array of intervals'). point_type = parts_type.element_type.point_type. f1, t1 = next(iter(row_key_type.items())); if point_type == t1:; partitions = hl.map(; lambda x: hl.interval(; start=hl.struct(**{f1: x.start}),; end=hl.struct(**{f1: x.end}),; includes_start=x.includes_start,; includes_end=x.includes_end,; ),; partitions,; ); else:; if not isinstance(point_type, hl.tstruct):; raise ValueError(f'partitions has wrong type: {point_type} must be struct or type of first row key field'); if not point_type._is_prefix_of(row_key_type):; raise ValueError(f'partitions type invalid: {point_type} must be prefix of {row_key_type}'). s = json.dumps(partitions.dtype._convert_to_json(hl.eval(partitions))); return s, partitions.dtype. def default_handler():; try:; from IPython.display import display. return display; except ImportError:; return print. def guess_cloud_spark_provider() -> Optional[Literal['dataproc', 'hdinsight']]:; if 'HAIL_DATAPROC' in os.environ:; return 'dataproc'; if 'AZURE_SPARK' in os.environ or 'hdinsight' in os.getenv('CLASSPATH', ''):; return 'hdinsight'; return None. def no_service_backend(unsupported_feature):; from hail import current_backend; from hail.backend.service_backend import ServiceBackend. if isinstance(current_backend(), ServiceBackend):; raise NotImplementedError(; f'{unsupported_feature!r} is not yet supported on the service backend.'; f'\n If this is a pressing need, please alert the team on the discussion'; f'\n forum to aid in prioritization: https://discuss.hail.is'; ). ANY_REGION = ['any_region']. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:18317,update,updated,18317,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,1,['update'],['updated']
Deployability,"ar = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renaming, _ = deduplicate(list(col_key), already_used=set(col_key)); assert len(renaming) == len(col_key). kinship_between = kinship_between.key_rows_by(; **cols[kinship_between.row_idx].select(*col_key).rename(dict(renaming)); ). kinship_between = kinship_between.annotate_entries(; min_n_hets=hl.min(kinship_between.n_hets_row, kinship_between.n_hets_col); ); return (; kinship_between.select_entries(; phi=(0.5); + (; (2 * kinship_between.het_hom_balance + -kinship_between.n_hets_row - kinship_between.n_hets_col); / (4 * kinship_between.min_n_hets); ); ); .select_rows(); .select_cols(); .select_globals(); ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:10801,update,updated,10801,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,1,['update'],['updated']
Deployability,"aram log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:2453,configurat,configuration,2453,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on :math:`h^2`. **Testing each variant for association**. Fixing a single variant, we define:. - :math:`v = n \\times 1` vector of genotypes, with missing genotypes imputed as the mean of called genotypes; - :math:`X_v = \\left[v | X \\right] = n \\times (1 + c)` matrix concatenating :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the global REML estimate :math:`\\hat{\delta}`, we find the REML estimate :math:`(\\hat{\\beta}_v, \\hat{\si",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:133783,continuous,continuous,133783,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['continuous'],['continuous']
Deployability,ariant_indices_EAS.rst.txt; panukb_ld_variant_indices_EUR.rst.txt; panukb_ld_variant_indices_MID.rst.txt; panukb_meta_analysis_all_ancestries.rst.txt; panukb_meta_analysis_high_quality.rst.txt; panukb_summary_stats.rst.txt; UK_Biobank_Rapid_GWAS_both_sexes.rst.txt; UK_Biobank_Rapid_GWAS_female.rst.txt; UK_Biobank_Rapid_GWAS_male.rst.txt. schemas.rst.txt. /experimental; ; hail.experimental.DB.rst.txt; index.rst.txt; ldscsim.rst.txt. /functions; ; collections.rst.txt; constructors.rst.txt; core.rst.txt; genetics.rst.txt; hail.expr.builders.CaseBuilder.rst.txt; hail.expr.builders.SwitchBuilder.rst.txt; index.rst.txt; numeric.rst.txt; random.rst.txt; stats.rst.txt; string.rst.txt. /genetics; ; hail.genetics.AlleleType.rst.txt; hail.genetics.Call.rst.txt; hail.genetics.Locus.rst.txt; hail.genetics.Pedigree.rst.txt; hail.genetics.ReferenceGenome.rst.txt; hail.genetics.Trio.rst.txt; index.rst.txt. /ggplot; ; index.rst.txt. /guides; ; agg.rst.txt; annotation.rst.txt; genetics.rst.txt. /install; ; azure.rst.txt; dataproc.rst.txt; linux.rst.txt; macosx.rst.txt; other-cluster.rst.txt; try.rst.txt. /linalg; . /utils; ; index.rst.txt. hail.linalg.BlockMatrix.rst.txt; index.rst.txt. /methods; ; genetics.rst.txt; impex.rst.txt; index.rst.txt; misc.rst.txt; relatedness.rst.txt; stats.rst.txt. /nd; ; index.rst.txt. /overview; ; expressions.rst.txt; index.rst.txt; matrix_table.rst.txt; table.rst.txt. /stats; ; hail.stats.LinearMixedModel.rst.txt; index.rst.txt. /tutorials; ; 01-genome-wide-association-study.ipynb.txt; 03-tables.ipynb.txt; 04-aggregation.ipynb.txt; 05-filter-annotate.ipynb.txt; 06-joins.ipynb.txt; 07-matrixtable.ipynb.txt; 08-plotting.ipynb.txt; 09-ggplot.ipynb.txt. /utils; ; index.rst.txt. /vds; ; hail.vds.combiner.load_combiner.rst.txt; hail.vds.combiner.new_combiner.rst.txt; hail.vds.combiner.VariantDatasetCombiner.rst.txt; hail.vds.combiner.VDSMetadata.rst.txt; hail.vds.filter_chromosomes.rst.txt; hail.vds.filter_intervals.rst.txt; hail.vds.filter_samples.rst.txt; ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index-wcopy.html:17786,install,install,17786,index-wcopy.html,https://hail.is,https://hail.is/index-wcopy.html,1,['install'],['install']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_AFR. View page source. giant_height_exome_AFR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'afr_maf': dict<str, float64>; 'exac_afr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AFR.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AFR.html,1,['update'],['updated']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_AMR. View page source. giant_height_exome_AMR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'amr_maf': dict<str, float64>; 'exac_amr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AMR.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_AMR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AMR.html,1,['update'],['updated']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_EAS. View page source. giant_height_exome_EAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eas_maf': dict<str, float64>; 'exac_eas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EAS.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_EAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EAS.html,1,['update'],['updated']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_EUR. View page source. giant_height_exome_EUR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EUR.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_EUR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EUR.html,1,['update'],['updated']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_SAS. View page source. giant_height_exome_SAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'sas_maf': dict<str, float64>; 'exac_sas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_SAS.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_SAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_SAS.html,1,['update'],['updated']
Deployability,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations. View page source. GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html:9697,update,updated,9697,docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html:9697,update,updated,9697,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations. View page source. GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html:9697,update,updated,9697,docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations. View page source. GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html:9697,update,updated,9697,docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ariant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. View page source. GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html:9808,update,updated,9808,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:1980,install,install,1980,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['install']
Deployability,"array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7244,pipeline,pipeline,7244,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['pipeline'],['pipeline']
Deployability,"as; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations. View page source. GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html:9721,update,updated,9721,docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"as_bias_corrected),; __step2_jackknife_variance=(; hl.sum(mt.__step2_block_betas_bias_corrected**2); - hl.sum(mt.__step2_block_betas_bias_corrected) ** 2 / n_blocks; ); / (n_blocks - 1); / n_blocks,; ). # combine step 1 and step 2 block jackknifes; mt = mt.annotate_entries(; __step2_initial_w=1.0; / (mt.__w_initial_floor * 2.0 * (mt.__initial_betas[0] + +mt.__initial_betas[1] * mt.__x_floor) ** 2); ). mt = mt.annotate_cols(; __final_betas=[mt.__step1_betas[0], mt.__step2_betas[1]],; __c=(hl.agg.sum(mt.__step2_initial_w * mt.__x) / hl.agg.sum(mt.__step2_initial_w * mt.__x**2)),; ). mt = mt.annotate_cols(; __final_block_betas=hl.map(; lambda i: (mt.__step2_block_betas[i] - mt.__c * (mt.__step1_block_betas[i][0] - mt.__final_betas[0])),; hl.range(0, n_blocks),; ); ). mt = mt.annotate_cols(; __final_block_betas_bias_corrected=(n_blocks * mt.__final_betas[1] - (n_blocks - 1) * mt.__final_block_betas); ). mt = mt.annotate_cols(; __final_jackknife_mean=[mt.__step1_jackknife_mean[0], hl.mean(mt.__final_block_betas_bias_corrected)],; __final_jackknife_variance=[; mt.__step1_jackknife_variance[0],; (; hl.sum(mt.__final_block_betas_bias_corrected**2); - hl.sum(mt.__final_block_betas_bias_corrected) ** 2 / n_blocks; ); / (n_blocks - 1); / n_blocks,; ],; ). # convert coefficient to heritability estimate; mt = mt.annotate_cols(; phenotype=mt.__y_name,; mean_chi_sq=hl.agg.mean(mt.__y),; intercept=hl.struct(estimate=mt.__final_betas[0], standard_error=hl.sqrt(mt.__final_jackknife_variance[0])),; snp_heritability=hl.struct(; estimate=(M / hl.agg.mean(mt.__n)) * mt.__final_betas[1],; standard_error=hl.sqrt((M / hl.agg.mean(mt.__n)) ** 2 * mt.__final_jackknife_variance[1]),; ),; ). # format and return results; ht = mt.cols(); ht = ht.key_by(ht.phenotype); ht = ht.select(ht.mean_chi_sq, ht.intercept, ht.snp_heritability). ht_tmp_file = new_temp_file(); ht.write(ht_tmp_file); ht = hl.read_table(ht_tmp_file). return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html:18103,update,updated,18103,docs/0.2/_modules/hail/experimental/ld_score_regression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html,1,['update'],['updated']
Deployability,"asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Liver_all_snp_gene_associations. View page source. GTEx_eQTL_Liver_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html:9673,update,updated,9673,docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Ovary_all_snp_gene_associations. View page source. GTEx_eQTL_Ovary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html:9673,update,updated,9673,docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:174466,configurat,configuration,174466,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['configurat'],['configuration']
Deployability,"ate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness estimates between individuals using a KING variant. pc_relate(call_expr, min_individual_maf, *); Compute relatedness estimates between individuals using a variant of the PC-Relate method. Miscellaneous. grep(regex, path[, max_count, show, force, ...]); Searches given paths for all lines containing regex matches. maximal_independent_set(i, j[, keep, ...]); Return a table containing the vertices in a near maximal independent set of an undirected graph whose edges are given by a two-column table. rename_duplicates(dataset[, name]); Rename duplicate column keys. segment_intervals(ht, points); Segment the interval keys of ht at a given set of points. [1]; Purcell, Shaun et al. “PLINK: a tool set for whole-genome association and; population-based linkage analyses.” American journal of human genetics; vol. 81,3 (2007):; 559-75. doi:10.1086/519795. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838/. [2]; Manichaikul, Ani et al. “Robust relationship inference in genome-wide; association studies.” Bioinformatics (Oxford, England) vol. 26,22 (2010):; 2867-73. doi:10.1093/bioinformatics/btq559. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3025716/. [3]; Conomos, Matthew P et al. “Model-free Estimation of Recent Genetic; Relatedness.” American journal of human genetics vol. 98,1 (2016):; 127-48. doi:10.1016/j.ajhg.2015.11.022. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:8850,update,updated,8850,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['update'],['updated']
Deployability,"ategorical label. Returns:; bokeh.models.Plot if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.qq(pvals, label=None, title='Q-Q plot', xlabel='Expected -log10(p)', ylabel='Observed -log10(p)', size=6, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot); If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive qq plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. pvals (NumericExpression) – List of x-values to be plotted.; label (Expression or Dict[str, Expression]]) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:10279,continuous,continuous,10279,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['continuous'],['continuous']
Deployability,"ation of the called alleles. unphased_diploid_gt_index; Return the genotype index for unphased, diploid calls. property alleles; Get the alleles of this call. Returns:; list of int. is_diploid()[source]; True if the ploidy == 2. Return type:; bool. is_haploid()[source]; True if the ploidy == 1. Return type:; bool. is_het()[source]; True if the call contains two different alleles. Return type:; bool. is_het_non_ref()[source]; True if the call contains two different alternate alleles. Return type:; bool. is_het_ref()[source]; True if the call contains one reference and one alternate allele. Return type:; bool. is_hom_ref()[source]; True if the call has no alternate alleles. Return type:; bool. is_hom_var()[source]; True if the call contains identical alternate alleles. Return type:; bool. is_non_ref()[source]; True if the call contains any non-reference alleles. Return type:; bool. n_alt_alleles()[source]; Returns the count of non-reference alleles. Return type:; int. one_hot_alleles(n_alleles)[source]; Returns a list containing the one-hot encoded representation of the; called alleles.; Examples; >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters:; n_alleles (int) – Number of total alleles, including the reference. Returns:; list of int. property phased; True if the call is phased. Returns:; bool. property ploidy; The number of alleles for this call. Returns:; int. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls. Returns:; int. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.Call.html:3966,update,updated,3966,docs/0.2/genetics/hail.genetics.Call.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Call.html,1,['update'],['updated']
Deployability,"ation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""log10""). [docs]def scale_x_reverse(name=None):; """"""Transforms x-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""reverse""). [docs]def scale_y_reverse(name=None):; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""reverse""). [docs]def scale_x_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_y_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Sup",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:8936,continuous,continuous,8936,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['continuous'],['continuous']
Deployability,"ator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_non_ref)); >>> ds_scan.rows().show(); +---------------+------------+-----------+---------------+; | locus | alleles | n_non_ref | cum_n_non_ref |; +---------------+------------+-----------+---------------+; | locus<GRCh37> | array<str> | int64 | int64 |; +---------------+------------+-----------+---------------+; | 20:10579373 | [""C"",""T""] | 1 | 0 |; | 20:10579398 | [""C"",""T""] | 1 | 1 |; | 20:10627772 | [""C"",""T""] | 2 | 2 |; | 20:10633237 | [""G"",""A""] | 69 | 4 |; | 20:10636995 | [""C"",""T""] | 2 | 73 |; | 20:10639222 | [""G"",""A""] | 22 | 75 |; | 20:13763601 | [""A"",""G""] | 2 | 97 |; | 20:16223922 | [""T"",""C""] | 66 | 99 |; | 20:17479617 | [""G"",""A""] | 9 | 165 |; +---------------+------------+-----------+---------------+. Scans over column fields can be done in a similar manner. Danger; Computing the result of certain aggregators, such as; hardy_weinberg_test(), can be very expensive when done; for every row in a scan.”. See the Aggregators module for documentation on the behavior; of specific aggregators. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/scans.html:3063,update,updated,3063,docs/0.2/scans.html,https://hail.is,https://hail.is/docs/0.2/scans.html,1,['update'],['updated']
Deployability,"atsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter; When you are finished with the cluster stop it:; hailctl hdinsight stop MyFirstCluster MyStorageAccount MyResourceGroup. Next Steps. Read more about Hail on Azure HDInsight; Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/azure.html:2390,update,updated,2390,docs/0.2/install/azure.html,https://hail.is,https://hail.is/docs/0.2/install/azure.html,1,['update'],['updated']
Deployability,"atten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95046,update,update,95046,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"ax_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_block_max_length=False). if VariantDataset.ref_block_max_length_field in vds.reference_data.globals:; warning(f""VDS at {vds_path} already contains a global annotation with the max reference block length""); return; rd = vds.reference_data; rd = rd.annotate_rows(__start_pos=rd.locus.position); fs = hl.current_backend().fs; ref_block_max_len = rd.aggregate_entries(hl.agg.max(rd.END - rd.__start_pos + 1))",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2832,patch,patch,2832,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['patch'],['patch']
Deployability,"ay of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; conseque",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:60277,configurat,configuration,60277,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"bals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46039,configurat,configuration,46039,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"bic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X = n \times c\) matrix of sample covariates and intercept column of ones; \(K = n \times n\) kinship matrix; \(I = n \times n\) identity matrix; \(\beta = c \times 1\) vector of covariate coefficients; \(\sigma_g^2 =\) coefficient of genetic variance component \(K\); \(\sigma_e^2 =\) coefficient of environmental variance component \(I\); \(\delta = \frac{\sigma_e^2}{\sigma_g^2} =\) ratio of environmental and genetic variance component coeffici",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:97971,configurat,configuration,97971,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['configurat'],['configuration']
Deployability,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:80169,pipeline,pipelines,80169,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['pipeline'],['pipelines']
Deployability,"bles[0]; if any(head.key.dtype != t.key.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same key type:\n '; + '\n '.join(str(t.key.dtype) for t in tables); ); if any(head.row.dtype != t.row.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same row type\n '; + '\n '.join(str(t.row.dtype) for t in tables); ); if any(head.globals.dtype != t.globals.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same global type\n '; + '\n '.join(str(t.globals.dtype) for t in tables); ); return Table(ir.TableMultiWayZipJoin([t._tir for t in tables], data_field_name, global_field_name)). def _group_within_partitions(self, name, n):; def grouping_func(part):; groups = part.grouped(n); key_names = list(self.key); return groups.map(lambda group: group[0].select(*key_names, **{name: group})). return self._map_partitions(grouping_func). @typecheck_method(f=func_spec(1, expr_stream(expr_struct()))); def _map_partitions(self, f):; rows_uid = 'tmp_rows_' + Env.get_uid(); globals_uid = 'tmp_globals_' + Env.get_uid(); expr = construct_expr(; ir.Ref(rows_uid, hl.tstream(self.row.dtype)), hl.tstream(self.row.dtype), self._row_indices; ); body = f(expr); result_t = body.dtype; if any(k not in result_t.element_type for k in self.key):; raise ValueError('Table._map_partitions must preserve key fields'). body_ir = ir.Let('global', ir.Ref(globals_uid, self._global_type), body._ir); return Table(ir.TableMapPartitions(self._tir, globals_uid, rows_uid, body_ir, len(self.key), len(self.key))). def _calculate_new_partitions(self, n_partitions):; """"""returns a set of range bounds that can be passed to write""""""; return Env.backend().execute(; ir.TableToValueApply(; self.select().select_globals()._tir,; {'name': 'TableCalculateNewPartitions', 'nPartitions': n_partitions},; ); ). table_type.set(Table). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:136644,update,updated,136644,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['update'],['updated']
Deployability,"bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:10884,configurat,configuration,10884,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['configurat'],['configuration']
Deployability,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:8969,install,installed,8969,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,2,"['install', 'update']","['installed', 'updated']"
Deployability,"bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_pays_config=None)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:7204,update,updated,7204,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['update'],['updated']
Deployability,"call_fields:; warning(; ""Mismatch between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30517,update,update,30517,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"ce. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:1485,configurat,configuration,1485,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['configurat'],['configuration']
Deployability,"ce: str,; motif_feature_consequences: array<struct {; allele_num: int32,; consequence_terms: array<str>,; high_inf_pos: str,; impact: str,; minimised: int32,; motif_feature_id: str,; motif_name: str,; motif_pos: int32,; motif_score_change: float64,; strand: int32,; variant_allele: str; }>,; regulatory_feature_consequences: array<struct {; allele_num: int32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'allele_info': struct {; BaseQRankSum: float64,; ClippingRankSum: float64,; DB: bool,; DP: int32,; DS: bool,; END: int32,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MQ: float64,; MQRankSum: float64,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool,; QD: float64,; ReadPosRankSum: float64,; SOR: float64,; VQSLOD: float64,; culprit: str; }; 'rsid': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_sites.html:15398,update,updated,15398,docs/0.2/datasets/schemas/gnomad_exome_sites.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_sites.html,1,['update'],['updated']
Deployability,"ces_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gencode. View page source. gencode. Versions: v19, v31, v35; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (v35, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh38>>; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; 'level': int32; 'gene_id': str; 'gene_type': str; 'ccdsid': str; 'exon_id': str; 'exon_number': int32; 'havana_gene': str; 'transcript_type': str; 'protein_id': str; 'gene_name': str; 'transcript_name': str; 'transcript_id': str; 'transcript_support_level': str; 'hgnc_id': str; 'ont': str; 'havana_transcript': str; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gencode.html:9698,update,updated,9698,docs/0.2/datasets/schemas/gencode.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gencode.html,1,['update'],['updated']
Deployability,"ces_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations. View page source. GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html:9778,update,updated,9778,docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"check_keys('drop', f, protected_key); row_fields = set(table.row); to_drop = [f for f in fields_to_drop if f in row_fields]; table = table._select('drop', table.row.drop(*to_drop)). return table. [docs] @typecheck_method(; output=str, types_file=nullable(str), header=bool, parallel=nullable(ir.ExportType.checker), delimiter=str; ); def export(self, output, types_file=None, header=True, parallel=None, delimiter='\t'):; """"""Export to a text file. Examples; --------; Export to a tab-separated file:. >>> table1.export('output/table1.tsv.bgz'). Note; ----; It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with any Hail method, as well as with Python's ``gzip.open``; and R's ``read.table``. Nested structures will be exported as JSON. In order to export nested struct; fields as separate fields in the resulting table, use :meth:`flatten` first. Warning; -------; Do not export to a path that is being read from in the same pipeline. See Also; --------; :meth:`flatten`, :meth:`write`. Parameters; ----------; output : :class:`str`; URI at which to write exported file.; types_file : :class:`str`, optional; URI at which to write file containing field type information.; header : :obj:`bool`; Include a header in the file.; parallel : :class:`str`, optional; If None, a single file is produced, otherwise a; folder of file shards is produced. If 'separate_header',; the header file is output separately from the file shards. If; 'header_per_shard', each file shard has a header. If set to None; the export will be slower.; delimiter : :class:`str`; Field delimiter.; """"""; hl.current_backend().validate_file(output). parallel = ir.ExportType.default(parallel); Env.backend().execute(; ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter)); ). [docs] def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':; """"""Group by a new key for use with :me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:53565,pipeline,pipeline,53565,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['pipeline'],['pipeline']
Deployability,"ckend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.json_typ. def build_vep_batch(b: bc.aioclient.Batch, vep_input_path: str, vep_output_path: str):; if csq:; local_output_file = '/io/output'; vep_command = vep_config.command(; consequence=csq,; part_id=-1,; input_file=None,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ); env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': str(-1),; 'VEP_OUTPUT_FILE': local_output_file,; 'VEP_COMMAND': vep_command,; }; env.update(vep_config.env); b.create_job(; vep_config.image,; vep_config.batch_run_csq_header_command,; attributes={'name': 'csq-header'},; resources={'cpu': '1', 'memory': 'standard'},; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; output_files=[(local_output_file, f'{vep_output_path}/csq-header')],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). for f in hl.hadoop_ls(vep_input_path):; path = f['path']; part_name = os.path.basename(path); if not part_name.startswith('part-'):; continue; part_id = int(part_name.split('-')[1]). local_input_file = '/io/input'; local_output_file = '/io/output.gz'. vep_command = vep_config.command(; consequence=csq,; part_id=part_id,; input_file=local_input_file,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ). env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tol",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:33515,update,update,33515,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['update'],['update']
Deployability,"cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_comma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28802,configurat,configuration,28802,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True; """"""; return _ALLELE_STRS[self]. @classmethod; def _missing_(cls, value):; if not isinstance(value, str):; return None; return cls.__members__.get(value.upper()). [docs] @staticmethod; def strings():; """"""Returns the names of the allele types, for use with; :func:`~hail.expr.functions.literal`; """"""; return list(_ALLELE_STRS). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:2400,update,updated,2400,docs/0.2/_modules/hail/genetics/allele_type.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html,1,['update'],['updated']
Deployability,"column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. MatrixTable has four select methods that select and create new fields:. MatrixTable.select_globals(); MatrixTable.select_rows(); MatrixTable.select_cols(); MatrixTable.select_entries(). Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; KEY=VALUE to compute new fields. The Python unpack operator ** can be; used to specify that all fields",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:6907,update,update,6907,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['update'],['update']
Deployability,"cores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; Ensembl_homo_sapiens_reference_genome. View page source. Ensembl_homo_sapiens_reference_genome. Versions: release_95; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (release_95, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'reference_allele': str; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html:9490,update,updated,9490,docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html,1,['update'],['updated']
Deployability,"ct type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique.; Structs are very common in Hail. Each component of a Table and MatrixTable; is a struct:. Table.row(); Table.globals(); MatrixTable.row(); MatrixTable.col(); MatrixTable.entry(); MatrixTable.globals(). Structs appear below the top-level component types as well. Consider the following join:; >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to table1 called table2_fields. In the new table,; table2_fields will be a struct containing all the non-key fields from table2. Parameters:; field_types (keyword args of HailType) – Fields. See also; StructExpression, Struct. class hail.expr.types.ttuple(*types)[source]; Hail type for tuples.; In Python, these are represented as tuple. Parameters:; types (varargs of HailType) – Element types. See also; TupleExpression. hail.expr.types.tcall = dtype('call'); Hail type for a diploid genotype.; In Python, these are represented by Call. See also; CallExpression, Call, call(), parse_call(), unphased_diploid_gt_index_call(). class hail.expr.types.tlocus(reference_genome='default')[source]; Hail type for a genomic coordinate with a contig and a position.; In Python, these are represented by Locus. Parameters:; reference_genome (ReferenceGenome or str) – Reference genome to use. See also; LocusExpression, locus(), parse_locus(), Locus. reference_genome; Reference genome. Returns:; ReferenceGenome – Reference genome. class hail.expr.types.tinterval(point_type)[source]; Hail type for intervals of ordered values.; In Python, these are represented by Interval. Parameters:; point_type (HailType) – Interval point type. See also; IntervalExpression, Interval, interval(), parse_locus_interval(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:11849,update,updated,11849,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['update'],['updated']
Deployability,"ction must satisfy the following property:; tie_breaker(l, r) == -tie_breaker(r, l).; When multiple nodes have the same degree, this algorithm will order the; nodes according to tie_breaker and remove the largest node.; If keyed is False, then a node may appear twice in the resulting; table. Parameters:. i (Expression) – Expression to compute one endpoint of an edge.; j (Expression) – Expression to compute another endpoint of an edge.; keep (bool) – If True, return vertices in set. If False, return vertices removed.; tie_breaker (function) – Function used to order nodes with equal degree.; keyed (bool) – If True, key the resulting table by the node field, this requires; a sort. Returns:; Table – Table with the set of independent vertices. The table schema is one row; field node which has the same type as input expressions i and j. hail.methods.rename_duplicates(dataset, name='unique_id')[source]; Rename duplicate column keys. Note; Requires the column key to be one field of type tstr. Examples; >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; This method produces a new column field from the string column key by; appending a unique suffix _N as necessary. For example, if the column; key “NA12878” appears three times in the dataset, the first will produce; “NA12878”, the second will produce “NA12878_1”, and the third will produce; “NA12878_2”. The name of this new field is parameterized by name. Parameters:. dataset (MatrixTable) – Dataset.; name (str) – Name of new field. Returns:; MatrixTable. hail.methods.segment_intervals(ht, points)[source]; Segment the interval keys of ht at a given set of points. Parameters:. ht (Table) – Table with interval keys.; points (Table or ArrayExpression) – Points at which to segment the intervals, a table or an array. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/misc.html:7647,update,updated,7647,docs/0.2/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/methods/misc.html,1,['update'],['updated']
Deployability,"ction.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Rele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:43054,configurat,configuration,43054,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"ctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an import_table or; import_vcf which used a glob expression. In such a case, only the; first 1000 files would have been included in the resulting Table or; MatrixTable.; (#13550); hl.utils.range_table(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:28934,pipeline,pipelines,28934,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['pipeline'],['pipelines']
Deployability,"cussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; To compute an aggregate statistic over the rows of; a dataset, Hail provides an Table.aggregate() method which can be passed; a wide variety of aggregator functions (see Aggregators):; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'F')); 0.5. We also might want to compute the mean value of HT for each sex. This is; possible with a combination of Table.group_by() and; GroupedTable.aggregate():; >>> ht_agg = (ht.group_by(ht.SEX); ... .aggregate(mean = hl.agg.mean(ht.HT))); >>> ht_agg.show(); +-----+----------+; | SEX | mean |; +-----+----------+; | str | float64 |; +-----+----------+; | ""F"" | 6.50e+01 |; | ""M"" | 6.85e+01 |; +-----+----------+. Note that the result of ht.group_by(...).aggregate(...) is a new; Table while the result of ht.aggregate(...) is a Python value. Joi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:4439,update,update,4439,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['update'],['update']
Deployability,"cy'. x_span = data.bin_edges[-1] - data.bin_edges[0]; x_start = data.bin_edges[0] - 0.05 * x_span; x_end = data.bin_edges[-1] + 0.05 * x_span; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; background_fill_color='#EEEEEE',; x_range=(x_start, x_end),; ); q = p.quad(; bottom=0,; top=data.bin_freq,; left=data.bin_edges[:-1],; right=data.bin_edges[1:],; legend_label=legend,; line_color='black',; ); if data.n_larger > 0:; p.quad(; bottom=0,; top=data.n_larger,; left=data.bin_edges[-1],; right=(data.bin_edges[-1] + (data.bin_edges[1] - data.bin_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; --",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:13100,update,update,13100,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"d False is output as '1'. The; default and missing values are 'NA'.; varid (StringExpression, optional) – Expression for the variant ID (2nd column of the BIM file). The default; value is hl.delimit([dataset.locus.contig, hl.str(dataset.locus.position), dataset.alleles[0], dataset.alleles[1]], ':'); cm_position (Float64Expression, optional) – Expression for the 3rd column of the BIM file (position in centimorgans).; The default value is 0.0. The missing value is 0.0. hail.methods.get_vcf_metadata(path)[source]; Extract metadata from VCF header.; Examples; >>> hl.get_vcf_metadata('data/example2.vcf.bgz') ; {'filter': {'LowQual': {'Description': ''}, ...},; 'format': {'AD': {'Description': 'Allelic depths for the ref and alt alleles in the order listed',; 'Number': 'R',; 'Type': 'Integer'}, ...},; 'info': {'AC': {'Description': 'Allele count in genotypes, for each ALT allele, in the same order as listed',; 'Number': 'A',; 'Type': 'Integer'}, ...}}. Notes; This method parses the VCF header to extract the ID, Number,; Type, and Description fields from FORMAT and INFO lines as; well as ID and Description for FILTER lines. For example,; given the following header lines:; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">. The resulting Python dictionary returned would be; metadata = {'filter': {'LowQual': {'Description': 'Low quality'}},; 'format': {'DP': {'Description': 'Read Depth',; 'Number': '1',; 'Type': 'Integer'}},; 'info': {'MQ': {'Description': 'RMS Mapping Quality',; 'Number': '1',; 'Type': 'Float'}}}. which can be used with export_vcf() to fill in the relevant fields in the header. Parameters:; path (str) – VCF file(s) to read. If more than one file is given, the first; file is used. Returns:; dict of str to (dict of str to (dict of str to str)). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:61482,update,updated,61482,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['update'],['updated']
Deployability,"d in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71038,integrat,integration,71038,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['integrat'],['integration']
Deployability,"d return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5111,pipeline,pipeline,5111,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['pipeline'],['pipeline']
Deployability,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39085,configurat,configuration,39085,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['configurat'],['configuration']
Deployability,"d'},; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; output_files=[(local_output_file, f'{vep_output_path}/csq-header')],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). for f in hl.hadoop_ls(vep_input_path):; path = f['path']; part_name = os.path.basename(path); if not part_name.startswith('part-'):; continue; part_id = int(part_name.split('-')[1]). local_input_file = '/io/input'; local_output_file = '/io/output.gz'. vep_command = vep_config.command(; consequence=csq,; part_id=part_id,; input_file=local_input_file,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ). env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': str(-1),; 'VEP_INPUT_FILE': local_input_file,; 'VEP_OUTPUT_FILE': local_output_file,; 'VEP_COMMAND': vep_command,; }; env.update(vep_config.env). b.create_job(; vep_config.image,; vep_config.batch_run_command,; attributes={'name': f'vep-{part_id}'},; resources={'cpu': '1', 'memory': 'standard'},; input_files=[(path, local_input_file)],; output_files=[(local_output_file, f'{vep_output_path}/annotations/{part_name}.tsv.gz')],; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, canc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:34683,update,update,34683,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['update'],['update']
Deployability,"d. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2646,install,installed,2646,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['install'],['installed']
Deployability,"d_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle argume",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:1871,pipeline,pipeline,1871,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['pipeline'],['pipeline']
Deployability,"d_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_exome_coverage. View page source. gnomad_exome_coverage. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_id': int64; 'locus': locus<GRCh37>; 'mean': float64; 'median': int32; 'over_1': float64; 'over_5': float64; 'over_10': float64; 'over_15': float64; 'over_20': float64; 'over_25': float64; 'over_30': float64; 'over_50': float64; 'over_100': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_coverage.html:9527,update,updated,9527,docs/0.2/datasets/schemas/gnomad_exome_coverage.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_coverage.html,1,['update'],['updated']
Deployability,"d_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; clinvar_gene_summary. View page source. clinvar_gene_summary. Versions: 2019-07; Reference genome builds: None; Type: hail.Table. Schema (2019-07, None); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'GeneID': int32; 'Total_submissions': int32; 'Total_alleles': int32; 'Submissions_reporting_this_gene': int32; 'Alleles_reported_Pathogenic_Likely_pathogenic': int32; 'Gene_MIM_number': int32; 'Number_uncertain': int32; 'Number_with_conflicts': int32; 'gene_name': str; ----------------------------------------; Key: ['gene_name']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/clinvar_gene_summary.html:9550,update,updated,9550,docs/0.2/datasets/schemas/clinvar_gene_summary.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/clinvar_gene_summary.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_AFR. View page source. giant_bmi_exome_AFR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'afr_maf': dict<str, float64>; 'exac_afr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_AMR. View page source. giant_bmi_exome_AMR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'amr_maf': dict<str, float64>; 'exac_amr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_EAS. View page source. giant_bmi_exome_EAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eas_maf': dict<str, float64>; 'exac_eas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_EUR. View page source. giant_bmi_exome_EUR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_SAS. View page source. giant_bmi_exome_SAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'sas_maf': dict<str, float64>; 'exac_sas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_all_snp_gene_associations. View page source. GTEx_eQTL_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.MatrixTable. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'tissue': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'tss_distance': int32; ----------------------------------------; Entry fields:; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Column key: ['tissue']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html:9801,update,updated,9801,docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; Schema (2.1, GRCh37). gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d01. View page source. gnomad_mnv_genome_d01. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; Schema (2.1, GRCh37). gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d02. View page source. gnomad_mnv_genome_d02. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; Schema (2.1, GRCh37). gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d03. View page source. gnomad_mnv_genome_d03. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; Schema (2.1, GRCh37). gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d04. View page source. gnomad_mnv_genome_d04. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; Schema (2.1, GRCh37). gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d05. View page source. gnomad_mnv_genome_d05. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; Schema (2.1, GRCh37). gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d06. View page source. gnomad_mnv_genome_d06. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; Schema (2.1, GRCh37). gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d07. View page source. gnomad_mnv_genome_d07. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; Schema (2.1, GRCh37). gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d08. View page source. gnomad_mnv_genome_d08. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; Schema (2.1, GRCh37). gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d09. View page source. gnomad_mnv_genome_d09. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; Schema (2.1, GRCh37). gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d10. View page source. gnomad_mnv_genome_d10. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_EUR_Add. View page source. giant_whr_exome_C_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_EUR_Rec. View page source. giant_whr_exome_C_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_EUR_Add. View page source. giant_whr_exome_M_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_EUR_Rec. View page source. giant_whr_exome_M_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_EUR_Add. View page source. giant_whr_exome_W_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_EUR_Rec. View page source. giant_whr_exome_W_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html,1,['update'],['updated']
Deployability,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"d_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html:9757,update,updated,9757,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be identical, both in type, value, and ordering.; The row key schemas and row schemas must match.; The entry schemas must match. The column fields in the resulting dataset are the column fields from; the first dataset; the column schemas do not need to match.; This method does not deduplicate; if a row exists identically in two; datasets, then it will be duplicated in the result. Warning; This method can trigger a shuffle, if partitions from two datasets; overlap. Parameters:; datasets (varargs of MatrixTable) – Datasets to combine. Returns:; MatrixTable – Dataset with rows from each member of datasets. unpersist()[source]; Unpersists this dataset from memory/disk.; Notes; This function will have no effect on a dataset that was not previously; persisted. Returns:; MatrixTable – Unpersisted dataset. write(output, overwrite=False, stage_locally=False, _codec_spec=None, _partitions=None)[source]; Write to disk.; Examples; >>> dataset.write('output/dataset.mt'). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_matrix_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:69174,update,updated,69174,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['update'],['updated']
Deployability,"def scale_y_reverse(name=None):; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""reverse""). [docs]def scale_x_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_y_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_x_discrete(name=None, breaks=None, labels=None):; """"""The default discrete x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`str`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionS",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:9573,continuous,continuous,9573,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['continuous'],['continuous']
Deployability,"dices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. View page source. GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html:9757,update,updated,9757,docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations. View page source. GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html:9775,update,updated,9775,docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html:9775,update,updated,9775,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations. View page source. GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html:9775,update,updated,9775,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations. View page source. GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html:9775,update,updated,9775,docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif n < 21:; from bokeh.palettes import Category20. _palette = Category20[n]; else:; from bokeh.palettes import viridis. _palette = viridis(n). return CategoricalColorMapper(factors=factors, palette=_palette). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23678,update,update,23678,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by whi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:48076,continuous,continuous,48076,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['continuous'],['continuous']
Deployability,"ds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:5544,install,installed,5544,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installed']
Deployability,"ds: varargs of :class:`str`; Fields to drop. Returns; -------; :class:`.Struct`; Struct without certain fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5, bar=2, apple=10). Drop one field from `s`. >>> s.drop('bar'); Struct(food=8, fruit=5, apple=10). Drop two fields from `s`. >>> s.drop('food', 'fruit'); Struct(bar=2, apple=10); """"""; d = OrderedDict((k, v) for k, v in self.items() if k not in args); return Struct(**d). @typecheck(struct=Struct); def to_dict(struct):; return dict(struct.items()). _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, stream, indent, allowance, context, level, *args, **kwargs):; if isinstance(obj, Struct):; rep = self._repr(obj, context, level); max_width = self._width - indent - allowance; if len(rep) <= max_width:; stream.write(rep); return. stream.write('Struct('); indent += len('Struct('); if all(k.isidentifier() for k in obj):; n = len(obj.items()); for i, (k, v) in enumerate(obj.items()):; is_first = i == 0; is_last = i == n - 1. if not is_first:; stream.write(' ' * indent); stream.write(k); stream.write('='); this_indent = indent + len(k) + len('='); self._format(v, stream, this_indent, allowance, context, level, *args, **kwargs); if not is_last:; stream.write(',\n'); else:; stream.write('**{'); indent += len('**{'); n = len(obj.items()); for i, (k, v) in enumerate(obj.items()):; is_first = i == 0; is_last = i == n - 1. if not is_first:; stream.write(' ' * indent); stream.write(repr(k)); stream.write(': '); this_indent = indent + len(repr(k)) + len(': '); self._format(v, stream, this_indent, allowance, context, level, *args, **kwargs); if not is_last:; stream.write(',\n'); stream.write('}'); stream.write(')'); else:; _old_printer._format(self, obj, stream, indent, allowance, context, level, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:7306,patch,patch,7306,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,2,"['patch', 'update']","['patch', 'updated']"
Deployability,"e 2: Left distinct join: ht[ht2.key] or ht[ht2.field1, ht2.field2]""; ) from e. @property; def key(self) -> StructExpression:; """"""Row key struct. Examples; --------. List of key field names:. >>> list(table1.key); ['ID']. Number of key fields:. >>> len(table1.key); 1. Returns; -------; :class:`.StructExpression`; """"""; return self._key. @property; def _value(self) -> 'StructExpression':; return self.row.drop(*self.key). [docs] def n_partitions(self):; """"""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'NPartitionsTable'})). [docs] def count(self):; """"""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""; return Env.backend().execute(ir.TableCount(self._tir)). async def _async_count(self):; return await Env.backend()._async_execute(ir.TableCount(self._tir)). def _force_count(self):; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). async def _async_force_count(self)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:12755,pipeline,pipeline,12755,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['pipeline'],['pipeline']
Deployability,"e Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nich",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1468,install,install-on-cluster,1468,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['install-on-cluster']
Deployability,"e ValueError(""""""ld_score: entry_expr, locus_expr, coord_expr; (if specified), and annotation_exprs (if; specified) must come from same MatrixTable.""""""). n = mt.count_cols(); r2 = hl.row_correlation(entry_expr, block_size) ** 2; r2_adj = ((n - 1.0) / (n - 2.0)) * r2 - (1.0 / (n - 2.0)). starts, stops = hl.linalg.utils.locus_windows(locus_expr, radius, coord_expr); r2_adj_sparse = r2_adj.sparsify_row_intervals(starts, stops). r2_adj_sparse_tmp = new_temp_file(); r2_adj_sparse.write(r2_adj_sparse_tmp); r2_adj_sparse = BlockMatrix.read(r2_adj_sparse_tmp). if not annotation_exprs:; cols = ['univariate']; col_idxs = {0: 'univariate'}; l2 = r2_adj_sparse.sum(axis=1); else:; ht = mt.select_rows(*wrap_to_list(annotation_exprs)).rows(); ht = ht.annotate(univariate=hl.literal(1.0)); names = [name for name in ht.row if name not in ht.key]. ht_union = Table.union(*[; (ht.annotate(name=hl.str(x), value=hl.float(ht[x])).select('name', 'value')) for x in names; ]); mt_annotations = ht_union.to_matrix_table(row_key=list(ht_union.key), col_key=['name']). cols = mt_annotations.key_cols_by()['name'].collect(); col_idxs = {i: cols[i] for i in range(len(cols))}. a_tmp = new_temp_file(); BlockMatrix.write_from_entry_expr(mt_annotations.value, a_tmp). a = BlockMatrix.read(a_tmp); l2 = r2_adj_sparse @ a. l2_bm_tmp = new_temp_file(); l2_tsv_tmp = new_temp_file(); l2.write(l2_bm_tmp, force_row_major=True); BlockMatrix.export(l2_bm_tmp, l2_tsv_tmp). ht_scores = hl.import_table(l2_tsv_tmp, no_header=True, impute=True); ht_scores = ht_scores.add_index(); ht_scores = ht_scores.key_by('idx'); ht_scores = ht_scores.rename({'f{:}'.format(i): col_idxs[i] for i in range(len(cols))}). ht = mt.select_rows(__locus=locus_expr).rows(); ht = ht.add_index(); ht = ht.annotate(**ht_scores[ht.idx]); ht = ht.key_by('__locus'); ht = ht.select(*[x for x in ht_scores.row if x not in ht_scores.key]); ht = ht.rename({'__locus': 'locus'}). return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:6967,update,updated,6967,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,1,['update'],['updated']
Deployability,"e file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where Z is 3:; >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). ID; HT; SEX; X; Z. int32; int32; str; int32; int32. 2; 3; 72; 70; “M”; “F”; 6; 7; 3; 3. Remove rows where Z is 3:; >>> filtered_ht = ht.filter(ht.Z == 3, keep=False); >>> filtered_ht.show(); +-------+-------+-----+-------+-------+; |",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:24302,pipeline,pipeline,24302,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipeline']
Deployability,"e following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. INFO fields attributes (attached to (va.info.*)):; ‘Number’: The arity of the field. Can take values; 0 (Boolean flag),; 1 (single value),; R (one value per allele, including the reference),; A (one value per non-reference allele),; G (one value per genotype), and; . (any number of values); When importing: The value in read from the VCF INFO field definition; When exporting: The default value is 0 for Boolean, . for Arrays and 1 for all other types. ‘Description’ (default is ‘’). FILTER entries in the VCF header are generated based on the attributes; of va.filters. Each key/value pair in the attributes will generate a; FILTER entry in the VCF with ID = key and Description = value. Parameters:; ann_path (str) – Variant annotation path starting with ‘va’, period-delimited.; attribute (str) – The attribute to remove (key). Returns:Annotated dataset with the updated variant annotation without the attribute. Return type:VariantDataset. drop_samples()[source]¶; Removes all samples from variant dataset.; The variants, variant annotations, and global annnotations will remain,; producing a sites-only variant dataset. Returns:Sites-only variant dataset. Return type:VariantDataset. drop_variants()[source]¶; Discard all variants, variant annotations and genotypes.; Samples, sample annotations and global annotations are retained. This; is the same as filter_variants_expr('false'), but much faster.; Examples; >>> vds_result = vds.drop_variants(). Returns:Samples-only variant dataset. Return type:VariantDataset. export_gen(output, precision=4)[source]¶; Export variant dataset as GEN and SAMPLE file. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Import genotype probability data, filter variants based on INFO score, and export data to a GEN and SAMPLE file:; >>> vds3 = hc.import_bgen(""data/example3.bgen"", sa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:35416,update,updated,35416,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['update'],['updated']
Deployability,"e full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly wor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:92287,release,release,92287,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"e is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. pvals (NumericExpression) – List of x-values to be plotted.; label (Expression or Dict[str, Expression]]) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to downsample (de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:11115,continuous,continuous,11115,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['continuous'],['continuous']
Deployability,"e passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:19145,install,installation,19145,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['install'],['installation']
Deployability,"e run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:4118,pipeline,pipelines,4118,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['pipeline'],['pipelines']
Deployability,"e tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in inverse ppois().; log_p (bool or BooleanExpression) – Exponentiate p before testing. Returns:; Expression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:28031,update,updated,28031,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['update'],['updated']
Deployability,"e {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:31890,configurat,configuration,31890,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"e), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j = 1; while ui < len(x) and li < len(x):; if j == len(x):; ub = 1; lb = 1; xj = max_x; else:; ub = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] - e; new_y[li] = fy; keep[li] = True; j = li + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5996,update,update,5996,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"e: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'gene_id': str; 'gene_symbol': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'read_count': int32; ----------------------------------------; Column key: ['s']; Row key: ['gene_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html:11118,update,updated,11118,docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html,1,['update'],['updated']
Deployability,"e_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=Tr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31301,update,update,31301,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"eatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; AlleleType. View page source. AlleleType. class hail.genetics.AlleleType[source]; An enumeration for allele type.; Notes; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; Attributes. UNKNOWN; Unknown Allele Type. SNP; Single-nucleotide Polymorphism (SNP). MNP; Multi-nucleotide Polymorphism (MNP). INSERTION; Insertion. DELETION; Deletion. COMPLEX; Complex Polymorphism. STAR; Star Allele (alt=*). SYMBOLIC; Symbolic Allele. TRANSITION; Transition SNP. TRANSVERSION; Transversion SNP. pretty_name; A formatted (as opposed to uppercase) version of the member's name, to match allele_type(). Methods. strings; Returns the names of the allele types, for use with literal(). COMPLEX = 5; Complex Polymorphism. DELETION = 4; Deletion. INSERTION = 3; Insertion. MNP = 2; Multi-nucleotide Polymorphism (MNP). SNP = 1; Single-nucleotide Polymorphism (SNP). STAR = 6; Star Allele (alt=*). SYMBOLIC = 7; Symbolic Allele; e.g. alt=<INS>. TRANSITION = 8; Transition SNP; e.g. ref=A alt=G. Note; This is only really used internally in hail.vds.sample_qc() and; hail.methods.sample_qc(). TRANSVERSION = 9; Transversion SNP; e.g. ref=A alt=C. Note; This is only really used internally in hail.vds.sample_qc() and; hail.methods.sample_qc(). UNKNOWN = 0; Unknown Allele Type. property pretty_name; A formatted (as opposed to uppercase) version of the member’s name,; to match allele_type(); Examples; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True. static strings()[source]; Returns the names of the allele types, for use with; literal(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.AlleleType.html:2441,update,updated,2441,docs/0.2/genetics/hail.genetics.AlleleType.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.AlleleType.html,1,['update'],['updated']
Deployability,"econd index is the state on the right.; For example, ``concordance[1][4]`` is the number of ""no call"" genotypes on the left that were called ; homozygous variant on the right. ; ; :param right: right hand variant dataset for concordance; :type right: :class:`.VariantDataset`. :return: The global concordance statistics, a key table with sample concordance; statistics, and a key table with variant concordance statistics.; :rtype: (list of list of int, :py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.concordance(right._jvds); j_global_concordance = r._1(); sample_kt = KeyTable(self.hc, r._2()); variant_kt = KeyTable(self.hc, r._3()); global_concordance = [[j_global_concordance.apply(j).apply(i) for i in xrange(5)] for j in xrange(5)]. return global_concordance, sample_kt, variant_kt. [docs] @handle_py4j; def count(self):; """"""Returns number of samples and variants in the dataset.; ; **Examples**; ; >>> samples, variants = vds.count(); ; **Notes**; ; This is also the fastest way to force evaluation of a Hail pipeline.; ; :returns: The sample and variant counts.; :rtype: (int, int); """""". r = self._jvds.count(). return r._1(), r._2(). [docs] @handle_py4j; def deduplicate(self):; """"""Remove duplicate variants. :return: Deduplicated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.deduplicate()). [docs] @handle_py4j; @typecheck_method(fraction=numeric,; seed=integral); def sample_variants(self, fraction, seed=1):; """"""Downsample variants to a given fraction of the dataset.; ; **Examples**; ; >>> small_vds = vds.sample_variants(0.01); ; **Notes**; ; This method may not sample exactly ``(fraction * n_variants)``; variants from the dataset. :param float fraction: (Expected) fraction of variants to keep. :param int seed: Random seed. :return: Downsampled variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.sampleVariants(fraction, seed)). [docs] @handle_py4j; @re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:45290,pipeline,pipeline,45290,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['pipeline'],['pipeline']
Deployability,"ed by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:; >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function displayHTML with html as its argument.; See Databricks’ Bokeh docs for; more information. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/databricks.html:1780,rolling,rolling,1780,docs/0.2/cloud/databricks.html,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html,2,"['rolling', 'update']","['rolling', 'updated']"
Deployability,"eets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1461,configurat,configuration,1461,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['configurat'],['configuration']
Deployability,"eight=height, width=width); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = False; sp_color_bar.visible = False. # If multiple labels, create JS call back selector; if len(label_cols) > 1:; callback_args: Dict[str, Any]; callback_args = dict(color_mappers=sp_color_mappers, scatter_renderers=sp_scatter_renderers); callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].visible = true; }. """""". if legend:; callback_args.update(dict(legend_items=sp_legend_items, legend=sp_legend, color_bar=sp_color_bar)); callback_code += """"""; if (cb_obj.value in legend_items){; legend.items=legend_items[cb_obj.value]; legend.visible=true; color_bar.visible=false; }else{; legend.visible=false; color_bar.visible=true; }. """""". callback = CustomJS(args=callback_args, code=callback_code); select = Select(title=""Color by"", value=label_cols[0], options=label_cols); select.js_on_change('value', callback); return Column(children=[select, sp]). return sp. @typecheck(; x=oneof(expr_numeric, sized_tupleof(str, expr_numeric)),; y=oneof(expr_numeric, sized_tupleof(str, expr_numeric)),; label=nullable(oneof(dictof(str, expr_any), expr_any)),; title=nullable(str),; xlabel=nullable(str),; ylabel=nullable(str),; size=int,; legend=bool,; hover_fields=nullable(dictof(str, expr_any)),; colors=nullable(oneof(bokeh.models.mappers.ColorMapper, dictof(str, bokeh.models.mappers.ColorMapper))),; width=int,; height=int,; collect_all=nullable(bool),; n_divisions=nullable(int),; missing_label=str,; ); def ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:34929,update,update,34929,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"einberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (ob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:101590,configurat,configuration,101590,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"el text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matm",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:70838,pipeline,pipelines,70838,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"elds. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30583,update,update,30583,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"ele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:74161,integrat,integration,74161,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Deployability,"eles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.ve",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102349,configurat,configuration,102349,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"emp_file, wrap_to_list. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; annotation_exprs=nullable(oneof(expr_numeric, sequenceof(expr_numeric))),; block_size=nullable(int),; ); def ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None) -> Table:; """"""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+00 |; | 20:103517 | 2.04604e+00 | 2.75392e+02 | 4.69239e+00 |; | 20:108286 | 2.06585e+00 | 2.86453e+02 | 5.00124e+00 |; +---------------+-------------------+-----------------------+-------------+. Warning; -------; :func:`.ld_score` will fail if",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:1753,continuous,continuous,1753,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,1,['continuous'],['continuous']
Deployability,"ence genome of; the dataset.; The output VCF header will not contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; append_to_header parameter. Warning; INFO fields stored at VCF import are not automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating info, downstream; tools which may produce erroneous results. The solution is to create new; fields in info or overwrite existing fields. For example, in order to; produce an accurate AC field, one can run variant_qc() and copy; the variant_qc.AC field to info.AC as shown below.; >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:50767,pipeline,pipeline,50767,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['pipeline'],['pipeline']
Deployability,"ene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval))); if verbose:; info(; f'get_gene_intervals found {len(gene_info)} entries:\n'; + ""\n"".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == ""gene"" else x[3]})', gene_info)); ); intervals = list(map(lambda x: x[-1], gene_info)); return intervals. def _load_gencode_gtf(gtf_file=None, reference_genome=None):; """"""; Get Gencode GTF (from file or reference genome). Parameters; ----------; reference_genome : :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :class:`.Table`; """"""; GTFS = {; 'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',; 'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz',; }; if reference_genome is None:; reference_genome = hl.default_reference().name; else:; reference_genome = reference_genome.name; if gtf_file is None:; gtf_file = GTFS.get(reference_genome); if gtf_file is None:; raise ValueError(; 'get_gene_intervals requires a GTF file, or the reference genome be one of GRCh37 or GRCh38 (when on Google Cloud Platform)'; ); ht = hl.experimental.import_gtf(; gtf_file, reference_genome=reference_genome, skip_invalid_contigs=True, min_partitions=12; ); ht = ht.annotate(gene_id=ht.gene_id.split('\\.')[0], transcript_id=ht.transcript_id.split('\\.')[0]); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:10360,update,updated,10360,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,1,['update'],['updated']
Deployability,"enomic interval of interest.; interval_size (int32): Size of interval, in bases. Computes the following entry fields:. bases_over_gq_threshold (tuple of int64): Number of bases in the interval; over each GQ threshold.; fraction_over_gq_threshold (tuple of float64): Fraction of interval (in bases); above each GQ threshold. Computed by dividing each member of bases_over_gq_threshold; by interval_size.; bases_over_dp_threshold (tuple of int64): Number of bases in the interval; over each DP threshold.; fraction_over_dp_threshold (tuple of float64): Fraction of interval (in bases); above each DP threshold. Computed by dividing each member of bases_over_dp_threshold; by interval_size.; sum_dp (int64): Sum of depth values by base across the interval.; mean_dp (float64): Mean depth of bases across the interval. Computed by dividing; sum_dp by interval_size. If the dp_field parameter is not specified, the DP is used for depth; if present. If no DP field is present, the MIN_DP field is used. If no DP; or MIN_DP field is present, no depth statistics will be calculated. Note; The metrics computed by this method are computed only from reference blocks. Most; variant callers produce data where non-reference calls interrupt reference blocks, and; so the metrics computed here are slight underestimates of the true values (which would; include the quality/depth of non-reference calls). This is likely a negligible difference,; but is something to be aware of, especially as it interacts with samples of; ancestral backgrounds with more or fewer non-reference calls. Parameters:. vds (VariantDataset); intervals (Table) – Table of intervals. Must be start-inclusive, and cannot span contigs.; gq_thresholds (tuple of int) – GQ thresholds.; dp_field (str, optional) – Field for depth calculation. Uses DP or MIN_DP by default (with priority for DP if present). Returns:; MatrixTable – Interval-by-sample matrix. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.interval_coverage.html:3031,update,updated,3031,docs/0.2/vds/hail.vds.interval_coverage.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.interval_coverage.html,1,['update'],['updated']
Deployability,"er batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This versi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49903,toggle,toggle,49903,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['toggle'],['toggle']
Deployability,"er images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:51006,pipeline,pipelines,51006,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"er than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ... prs=hl.agg.sum(; ... mt.score * hl.coalesce(; ... hl.if_else(mt.flip, 2 - mt.GT.n_alt_alleles(),; ... mt.GT.n_alt_alleles()), mt.prior))). dependencies:; import_plink(), variant_qc(), import_table(),; coalesce(), case(), cond(), Call.n_alt_alleles(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:14748,update,updated,14748,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['update'],['updated']
Deployability,"er/mother schema,; and the resulting entry schema is the same as the proband_entry/father_entry/mother_entry schema.; If the `keep_trio_cols` option is set, then an additional `source_trio` column is added with the trio column data.; If the `keep_trio_entries` option is set, then an additional `source_trio_entry` column is added with the trio entry data. Note; ----; This assumes that the input MatrixTable is a trio MatrixTable (similar to; the result of :func:`~.trio_matrix`) Its entry schema has to contain; 'proband_entry`, `father_entry` and `mother_entry` all with the same type.; Its column schema has to contain 'proband`, `father` and `mother` all with; the same type. Parameters; ----------; tm : :class:`.MatrixTable`; Trio MatrixTable (entries have to be a Struct with `proband_entry`, `mother_entry` and `father_entry` present); col_keys : :obj:`list` of str; Column key(s) for the resulting sample MatrixTable; keep_trio_cols: bool; Whether to add a `source_trio` column with the trio column data (default `True`); keep_trio_entries: bool; Whether to add a `source_trio_entries` column with the trio entry data (default `False`). Returns; -------; :class:`.MatrixTable`; Sample MatrixTable; """""". select_entries_expr = {'__trio_entries': hl.array([tm.proband_entry, tm.father_entry, tm.mother_entry])}; if keep_trio_entries:; select_entries_expr['source_trio_entry'] = hl.struct(**tm.entry); tm = tm.select_entries(**select_entries_expr). tm = tm.key_cols_by(); select_cols_expr = {'__trio_members': hl.enumerate(hl.array([tm.proband, tm.father, tm.mother]))}; if keep_trio_cols:; select_cols_expr['source_trio'] = hl.struct(**tm.col); tm = tm.select_cols(**select_cols_expr). mt = tm.explode_cols(tm.__trio_members). mt = mt.transmute_entries(**mt.__trio_entries[mt.__trio_members[0]]). mt = mt.key_cols_by(); mt = mt.transmute_cols(**mt.__trio_members[1]). if col_keys:; mt = mt.key_cols_by(*col_keys). return mt. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html:13533,update,updated,13533,docs/0.2/_modules/hail/experimental/phase_by_transmission.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html,1,['update'],['updated']
Deployability,"erimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:2520,update,updated,2520,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['update'],['updated']
Deployability,"ers using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:18984,pipeline,pipelines,18984,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"ers where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first inde",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:94592,install,installs,94592,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installs']
Deployability,"ersion 0.2.114; Version 0.2.113; Version 0.2.112; Version 0.2.111; Version 0.2.110; Version 0.2.109; Version 0.2.108; Version 0.2.107; Version 0.2.106; Version 0.2.105; Version 0.2.104; Version 0.2.103; Version 0.2.102; Version 0.2.101; Version 0.2.100; Version 0.2.99; Version 0.2.98; Version 0.2.97; Version 0.2.96; Version 0.2.95; Version 0.2.94; Version 0.2.93; Version 0.2.92; Version 0.2.91; Version 0.2.90; Version 0.2.89; Version 0.2.88; Version 0.2.87; Version 0.2.86; Version 0.2.85; Version 0.2.84; Version 0.2.83; Version 0.2.82; Version 0.2.81; Version 0.2.80; Version 0.2.79; Version 0.2.78; Version 0.2.77; Version 0.2.76; Version 0.2.75; Version 0.2.74; Version 0.2.73; Version 0.2.72; Version 0.2.71; Version 0.2.70; Version 0.2.69; Version 0.2.68; Version 0.2.67; Version 0.2.66; Version 0.2.65; Version 0.2.64; Version 0.2.63; Version 0.2.62; Version 0.2.61; Version 0.2.60; Version 0.2.59; Version 0.2.58; Version 0.2.57; Version 0.2.56; Version 0.2.55; Version 0.2.54; Version 0.2.53; Version 0.2.52; Version 0.2.51; Version 0.2.50; Version 0.2.49; Version 0.2.48; Version 0.2.47; Version 0.2.46; Version 0.2.45; Version 0.2.44; Version 0.2.43; Version 0.2.42; Version 0.2.41; Version 0.2.40; Version 0.2.39; Version 0.2.38; Version 0.2.37; Version 0.2.36; Version 0.2.35; Version 0.2.34; Version 0.2.33; Version 0.2.32; Version 0.2.31; Version 0.2.30; Version 0.2.29; Version 0.2.28; Version 0.2.27; Version 0.2.26; Version 0.2.25; Version 0.2.24; Version 0.2.23; Version 0.2.22; Version 0.2.21; Version 0.2.20; Version 0.2.19; Version 0.2.18; Version 0.2.17; Version 0.2.16; Version 0.2.15; Version 0.2.14; Version 0.2.13; Version 0.2.12; Version 0.2.11; Version 0.2.10; Version 0.2.9; Version 0.2.8; Version 0.2.7; Version 0.2.6; Version 0.2.5; Version 0.2.4: Beginning of history!. Indices and tables. Index. If you would like to refer to our Hail v0.1 (deprecated) docs, please view Hail 0.1 docs. Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/index.html:4127,update,updated,4127,docs/0.2/index.html,https://hail.is,https://hail.is/docs/0.2/index.html,1,['update'],['updated']
Deployability,"ersion 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:74237,pipeline,pipelines,74237,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:2369,update,updated,2369,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['update'],['updated']
Deployability,"er; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_ro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1570,install,install,1570,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,3,['install'],"['install', 'install-on-cluster', 'installed']"
Deployability,"es. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:7759,release,released,7759,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['released']
Deployability,"es_POPMAX_AF': float64; 'gnomAD_genomes_POPMAX_nhomalt': int32; 'gnomAD_genomes_controls_AC': int32; 'gnomAD_genomes_controls_AN': int32; 'gnomAD_genomes_controls_AF': float64; 'gnomAD_genomes_controls_nhomalt': int32; 'gnomAD_genomes_controls_AFR_AC': int32; 'gnomAD_genomes_controls_AFR_AN': int32; 'gnomAD_genomes_controls_AFR_AF': float64; 'gnomAD_genomes_controls_AFR_nhomalt': int32; 'gnomAD_genomes_controls_AMR_AC': int32; 'gnomAD_genomes_controls_AMR_AN': int32; 'gnomAD_genomes_controls_AMR_AF': float64; 'gnomAD_genomes_controls_AMR_nhomalt': int32; 'gnomAD_genomes_controls_ASJ_AC': int32; 'gnomAD_genomes_controls_ASJ_AN': int32; 'gnomAD_genomes_controls_ASJ_AF': float64; 'gnomAD_genomes_controls_ASJ_nhomalt': int32; 'gnomAD_genomes_controls_EAS_AC': int32; 'gnomAD_genomes_controls_EAS_AN': int32; 'gnomAD_genomes_controls_EAS_AF': float64; 'gnomAD_genomes_controls_EAS_nhomalt': int32; 'gnomAD_genomes_controls_FIN_AC': int32; 'gnomAD_genomes_controls_FIN_AN': int32; 'gnomAD_genomes_controls_FIN_AF': float64; 'gnomAD_genomes_controls_FIN_nhomalt': int32; 'gnomAD_genomes_controls_NFE_AC': int32; 'gnomAD_genomes_controls_NFE_AN': int32; 'gnomAD_genomes_controls_NFE_AF': float64; 'gnomAD_genomes_controls_NFE_nhomalt': int32; 'gnomAD_genomes_controls_POPMAX_AC': int32; 'gnomAD_genomes_controls_POPMAX_AN': int32; 'gnomAD_genomes_controls_POPMAX_AF': float64; 'gnomAD_genomes_controls_POPMAX_nhomalt': int32; 'clinvar_id': int32; 'clinvar_clnsig': str; 'clinvar_trait': str; 'clinvar_review': str; 'clinvar_hgvs': str; 'clinvar_var_source': str; 'clinvar_MedGen_id': str; 'clinvar_OMIM_id': str; 'clinvar_Orphanet_id': str; 'Interpro_domain': str; 'GTEx_V7_gene': str; 'GTEx_V7_tissue': str; 'Geuvadis_eQTL_target_gene': str; 'locus': locus<GRCh37>; 'alleles': array<str>; 'chr': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_variants.html:20991,update,updated,20991,docs/0.2/datasets/schemas/dbNSFP_variants.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_variants.html,1,['update'],['updated']
Deployability,"es_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Type`; """""". return self._point_type. def contains(self, value):; """"""True if `value` is contained within the interval. Examples; --------. >>> interval2.contains(5); True. >>> interval2.contains(6); False. Parameters; ----------; value :; Object with type :meth:`.point_type`. Returns; -------; :obj:`bool`; """""". return hl.eval(hl.literal(self, hl.tinterval(self._point_type)).contains(value)). @typecheck_method(interval=interval_type); def overlaps(self, interval):; """"""True if the the supplied interval contains any value in common with this one. Parameters; ----------; interval : :class:`.Interval`; Interval object with the same point type. Returns; -------; :obj:`bool`; """""". return hl.eval(hl.literal(self, hl.tinterval(self._point_type)).overlaps(interval)). interval_type.set(Interval). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/interval.html:4710,update,updated,4710,docs/0.2/_modules/hail/utils/interval.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html,1,['update'],['updated']
Deployability,"es_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; Ensembl_homo_sapiens_low_complexity_regions. View page source. Ensembl_homo_sapiens_low_complexity_regions. Versions: release_95; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (release_95, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html:9499,update,updated,9499,docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html,1,['update'],['updated']
Deployability,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Spleen_all_snp_gene_associations. View page source. GTEx_sQTL_Spleen_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html:9733,update,updated,9733,docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Testis_all_snp_gene_associations. View page source. GTEx_sQTL_Testis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html:9733,update,updated,9733,docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Uterus_all_snp_gene_associations. View page source. GTEx_sQTL_Uterus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html:9733,update,updated,9733,docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Vagina_all_snp_gene_associations. View page source. GTEx_sQTL_Vagina_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html:9733,update,updated,9733,docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"es_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html:9826,update,updated,9826,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ession refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:11624,update,updated,11624,docs/0.2/hail.expr.CollectionExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html,1,['update'],['updated']
Deployability,"est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html:9766,update,updated,9766,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ethods work. See also; Table.transmute(), MatrixTable.select_globals(), MatrixTable.annotate_globals(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of row",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:64775,pipeline,pipeline,64775,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['pipeline'],['pipeline']
Deployability,"etting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:15424,update,updated,15424,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['update'],['updated']
Deployability,"evels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23287,continuous,continuous,23287,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['continuous'],['continuous']
Deployability,"ew” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipyt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2834,install,installs,2834,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installs']
Deployability,"ey field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:57155,pipeline,pipelines,57155,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"f either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:2066,install,install,2066,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['install']
Deployability,"f the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:24809,pipeline,pipelines,24809,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"f the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175985,release,release-,175985,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['release'],['release-']
Deployability,"f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, MatrixRows]:; """"""Get row lens from relational object. Parameters; ----------; rel : :class:`Table` or :class:`MatrixTable`. Returns; -------; :class:`TableRows` or :class:`MatrixRows`; """"""; if isinstance(rel, MatrixTable):; return MatrixRows(rel); elif isinstance(rel, Table):; return TableRows(rel); else:; rais",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:11965,configurat,configurations,11965,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['configurat'],['configurations']
Deployability,"fault. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:8758,continuous,continuous,8758,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Rele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:24634,pipeline,pipelines,24634,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76454,configurat,configuration,76454,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['configurat'],['configuration']
Deployability,"following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1906,install,install,1906,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['install']
Deployability,"for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:101682,install,installed,101682,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['install'],['installed']
Deployability,"from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52876,pipeline,pipelines,52876,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where larg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:3465,update,update,3465,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['update'],['update']
Deployability,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42449,configurat,configuration,42449,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"g; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:42272,update,updated,42272,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['updated']
Deployability,"gate(is_defined=hail.agg.fraction(hail.is_defined(entry_field))); ); else:; mt = mt._select_all(; row_exprs={'_new_row_key': row_field}, entry_exprs={'is_defined': hail.is_defined(entry_field)}; ); ht = mt.localize_entries('entry_fields', 'phenos'); ht = ht.select(entry_fields=ht.entry_fields.map(lambda entry: entry.is_defined)); data = ht.entry_fields.collect(); if len(data) > 200:; warning(; f'Missingness dataset has {len(data)} rows. '; f'This may take {""a very long time"" if len(data) > 1000 else ""a few minutes""} to plot.'; ); rows = hail.str(ht._new_row_key).collect(). df = pd.DataFrame(data); df = df.rename(columns=dict(enumerate(columns))).rename(index=dict(enumerate(rows))); df.index.name = 'row'; df.columns.name = 'column'. df = pd.DataFrame(df.stack(), columns=['defined']).reset_index(). p = figure(; x_range=columns,; y_range=list(reversed(rows)),; x_axis_location=""above"",; width=plot_width,; height=plot_height,; toolbar_location='below',; tooltips=[('defined', '@defined'), ('row', '@row'), ('column', '@column')],; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_text_font_size = ""5pt""; p.axis.major_label_standoff = 0; colors = [""#75968f"", ""#a5bab7"", ""#c9d9d3"", ""#e2e2e2"", ""#dfccce"", ""#ddb7b1"", ""#cc7878"", ""#933b41"", ""#550b1d""]; from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter. mapper = LinearColorMapper(palette=colors, low=df.defined.min(), high=df.defined.max()). p.rect(; x='column',; y='row',; width=1,; height=1,; source=df,; fill_color={'field': 'defined', 'transform': mapper},; line_color=None,; ). color_bar = ColorBar(; color_mapper=mapper,; major_label_text_font_size=""5pt"",; ticker=BasicTicker(desired_num_ticks=len(colors)),; formatter=PrintfTickFormatter(format=""%d""),; label_standoff=6,; border_line_color=None,; location=(0, 0),; ); p.add_layout(color_bar, 'right'); return p. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:59474,update,updated,59474,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['updated']
Deployability,"ge account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar files are located at; gs://hail-REGION-vep/loftee-beta/GRCh38.tar and; gs://hail-REGION-vep/homo-sapiens/95_GRCh38.tar.; A cluster started without the --vep argument is unable to run VEP and cannot be modified to run; VEP. You must start a new cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:3637,update,updated,3637,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['update'],['updated']
Deployability,"genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'gene_id': str; 'gene_symbol': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'TPM': float64; ----------------------------------------; Column key: ['s']; Row key: ['gene_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html:11092,update,updated,11092,docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html,1,['update'],['updated']
Deployability,"ggregate(; **{rv: hl.agg.take(ht[rv], 1)[0] for rv in ht.row_value if rv not in set([*key, field, value])},; **{; fv: hl.agg.filter(; ht[field] == fv,; hl.rbind(hl.agg.take(ht[value], 1), lambda take: hl.if_else(hl.len(take) > 0, take[0], 'NA')),; ); for fv in field_vals; },; ). ht_tmp = new_temp_file(); ht.write(ht_tmp). return ht. [docs]@typecheck(ht=Table, field=str, into=sequenceof(str), delim=oneof(str, int)); def separate(ht, field, into, delim) -> Table:; """"""Separate a field into multiple fields by splitting on a delimiter; character or position. :func:`.separate` mimics the functionality of the `separate()` function in R's; ``tidyr`` package. This function will create a new table where ``field`` has been split into; multiple new fields, whose names are given by ``into``. If ``delim`` is a ``str`` (including regular expression strings), ``field``; will be separated into columns by that string. In this case, the length; of ``into`` must match the number of resulting fields. If ``delim`` is an ``int``, ``field`` will be separated into two row fields,; where the first field contains the first ``delim`` characters of ``field``; and the second field contains the remaining characters. Parameters; ----------; ht : :class:`.Table`; A Hail table.; field : :class:`str`; The name of the field to separate in ``ht``.; into : list of :class:`str`; The names of the fields to create by separating ``field``.; delimiter : :class:`str` or :obj:`int`; The character or position by which to separate ``field``. Returns; -------; :class:`.Table`; Table with original ``field`` split into fields whose names are defined; by `into`."""""". if isinstance(delim, int):; ht = ht.annotate(**{into[0]: ht[field][:delim], into[1]: ht[field][delim:]}); else:; split = ht[field].split(delim); ht = ht.annotate(**{into[i]: split[i] for i in range(len(into))}); ht = ht.drop(field). ht_tmp = new_temp_file(); ht.write(ht_tmp). return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html:5047,update,updated,5047,docs/0.2/_modules/hail/experimental/tidyr.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html,1,['update'],['updated']
Deployability,"gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_genome_coverage. View page source. gnomad_genome_coverage. Versions: 2.1, 3.0.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_id': int64; 'locus': locus<GRCh37>; 'mean': float64; 'median': int32; 'over_1': float64; 'over_5': float64; 'over_10': float64; 'over_15': float64; 'over_20': float64; 'over_25': float64; 'over_30': float64; 'over_50': float64; 'over_100': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_coverage.html:9545,update,updated,9545,docs/0.2/datasets/schemas/gnomad_genome_coverage.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_coverage.html,1,['update'],['updated']
Deployability,"gression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:4651,update,update,4651,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['update'],['update']
Deployability,"h more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""; if Env.hc()._warn_entries_order and len(self.col_key) > 0:; warning(; ""entries(): Resulting entries table is sorted by '(row_key, col_key)'.""; ""\n To preserve row-major matrix table order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_entries_order = False. return Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:89787,pipeline,pipeline,89787,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['pipeline'],['pipeline']
Deployability,"h was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of typ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:69202,integrat,integrate,69202,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['integrat'],['integrate']
Deployability,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2896,update,updated,2896,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,1,['update'],['updated']
Deployability,"h; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11234,pipeline,pipelines,11234,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"hElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:17814,upgrade,upgrade,17814,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['upgrade'],['upgrade']
Deployability,"hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:2212,install,install-dev-requirements,2212,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['install'],['install-dev-requirements']
Deployability,"hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Sp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:8988,configurat,configuration,8988,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"he facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; _base_scale_mappings: ClassVar = {; ""shared_xaxes"": ""all"",; ""shared_yaxes"": ""all"",; }. _scale_mappings: ClassVar = {; ""fixed"": _base_scale_mappings,; ""free_x"": {; **_base_scale_mappings,; ""shared_xaxes"": False,; },; ""free_y"": {; **_base_scale_mappings,; ""shared_yaxes"": False,; },; ""free"": {; ""shared_xaxes"": False,; ""shared_yaxes"": False,; },; }. def __init__(; self, facets: StructExpression, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ):; if nrow is not None and ncol is not None:; raise ValueError(""Both `nrow` and `ncol` were specified. "" ""Please specify only one of these values.""); if scales not in self._scale_mappings:; raise ValueError(; f""An unsupported value ({scales}) was provided for `scales`. ""; f""Supported values are: {[k for k in self._scale_mappings.keys()]}.""; ); self.nrow = nrow; self.ncol = ncol; self.facets = facets; self.scales = scales. def get_expr_to_group_by(self) -> StructExpression:; return self.facets. def get_facet_nrows_and_ncols(self, num_facet_values: int) -> Tuple[int, int]:; if self.ncol is not None:; return (n_partitions(num_facet_values, self.ncol), self.ncol); elif self.nrow is not None:; return (self.nrow, n_partitions(num_facet_values, self.nrow)); else:; ncol = int(math.ceil(math.sqrt(num_facet_values))); return (n_partitions(num_facet_values, ncol), ncol). def get_shared_axis_kwargs(self) -> Dict[str, str]:; return self._scale_mappings[self.scales]. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:3484,update,updated,3484,docs/0.2/_modules/hail/ggplot/facets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html,1,['update'],['updated']
Deployability,"hema (2.1.1, GRCh37). ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_variant_co-occurrence. View page source. gnomad_variant_co-occurrence. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'max_freq': float64; 'least_consequence': str; 'same_haplotype_em_probability_cutoff': float64; 'different_haplotypes_em_probability_cutoff': float64; 'global_annotation_descriptions': struct {; max_freq: str,; least_consequence: str,; same_haplotype_em_probability_cutoff: str,; different_haplotypes_em_probability_cutoff: str; }; 'row_annotation_descriptions': struct {; locus1: str,; alleles1: str,; locus2: str,; alleles2: str,; phase_info: struct {; description: str,; gt_counts: str,; em: struct {; hap_counts: str,; p_chet: str,; same_haplotype: str,; different_haplotype: str; }; }; }; ----------------------------------------; Row fields:; 'locus1': locus<GRCh37>; 'alleles1': array<str>; 'locus2': locus<GRCh37>; 'alleles2': array<str>; 'phase_info': dict<str, struct {; gt_counts: array<int32>,; em: struct {; hap_counts: array<float64>,; p_chet: float64,; same_haplotype: bool,; different_haplotype: bool; }; }>; ----------------------------------------; Key: ['locus1', 'alleles1', 'locus2', 'alleles2']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html:10199,update,updated,10199,docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html,1,['update'],['updated']
Deployability,"hen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'rsid': set<str>; 'common_low_heteroplasmy': bool; 'base_qual_hist': array<int64>; 'position_hist': array<int64>; 'strand_bias_hist': array<int64>; 'weak_evidence_hist': array<int64>; 'contamination_hist': array<int64>; 'heteroplasmy_below_min_het_threshold_hist': array<int64>; 'excluded_AC': int64; 'AN': int64; 'AC_hom': int64; 'AC_het': int64; 'hl_hist': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_hist_all': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_hist_alt': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_mean': float64; 'mq_mean': float64; 'tlod_mean': float64; 'AF_hom': float32; 'AF_het': float32; 'max_hl': float64; 'hap_AN': array<int64>; 'hap_AC_het': array<int64>; 'hap_AC_hom': array<int64>; 'hap_AF_hom': array<float32>; 'hap_AF_het': array<float32>; 'hap_hl_hist': array<array<int64>>; 'hap_faf_hom': array<float64>; 'hapmax_AF_hom': str; 'hapmax_AF_het': str; 'faf_hapmax_hom': float64; 'pop_AN': array<int64>; 'pop_AC_het': array<int64>; 'pop_AC_hom': array<int64>; 'pop_AF_hom': array<float32>; 'pop_AF_het': array<float32>; 'pop_hl_hist': array<array<int64>>; 'age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_sites.html:19008,update,updated,19008,docs/0.2/datasets/schemas/gnomad_chrM_sites.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_sites.html,1,['update'],['updated']
Deployability,"her bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable) – A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:3673,install,installed,3673,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,1,['install'],['installed']
Deployability,"hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_req",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22519,update,update,22519,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[1][1], elt[0]); ); ). old_to_new = hl.bind(lambda d: mt.alleles.map(lambda a: d.get(a)), old_to_new_dict); mt = mt.annotate_rows(old_to_new=old_to_new, new_to_old=new_to_old); new_locus_alleles = hl.min_rep(mt.locus, mt.new_to_old.map(lambda i: mt.alleles[i])); mt = mt.annotate_rows(__new_locus=new_locus_alleles.locus, __new_alleles=new_locus_alleles.alleles); mt = mt.filter_rows(hl.len(mt.__new_alleles) > 1); left = mt.filter_rows((mt.locus == mt.__new_locus) & (mt.alleles == mt.__new_alleles)). right = mt.filter_rows((mt.locus != mt.__new_locus) | (mt.alleles != mt.__new_alleles)); right = right.key_rows_by(locus=right.__new_locus, alleles=right.__new_alleles); return left.union_rows(right, _check_cols=False).drop('__allele_inclusion', '__new_locus', '__new_alleles'). [docs]@typecheck(mt=MatrixTable, f=anytype, subset=bool); def filter_alleles_hts(mt: MatrixTable, f: Callable, subset: bool = False) -> MatrixTable:; """"""Filter alternate alleles and update standard GATK entry fields. Examples; --------; Filter to SNP alleles using the subset strategy:. >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:. >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; -----; For usage of the `f` argument, see the :func:`.filter_alleles`; documentation. :func:`.filter_alleles_hts` requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:. .. code-block:: text. GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use :meth:`.MatrixTable.select_entries` to rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, befo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:157652,update,update,157652,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['update']
Deployability,"hl.linalg.utils.locus_windows(ht.locus, 1); (array([0, 0, 2, 3, 3, 5]), array([2, 2, 3, 5, 5, 6])). Windows with 1cm radius:; >>> hl.linalg.utils.locus_windows(ht.locus, 1.0, coord_expr=ht.cm); (array([0, 1, 1, 3, 3, 5]), array([1, 3, 3, 5, 5, 6])). Notes; This function returns two 1-dimensional ndarrays of integers,; starts and stops, each of size equal to the number of rows.; By default, for all indices i, [starts[i], stops[i]) is the maximal; range of row indices j such that contig[i] == contig[j] and; position[i] - radius <= position[j] <= position[i] + radius.; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that has; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the function will fail.; The last example above uses centimorgan coordinates, so; [starts[i], stops[i]) is the maximal range of row indices j such; that contig[i] == contig[j] and; cm[i] - radius <= cm[j] <= cm[i] + radius.; Index ranges are start-inclusive and stop-exclusive. This function is; especially useful in conjunction with; BlockMatrix.sparsify_row_intervals(). Parameters:. locus_expr (LocusExpression) – Row-indexed locus expression on a table or matrix table.; radius (int) – Radius of window for row values.; coord_expr (Float64Expression, optional) – Row-indexed numeric expression for the row value.; Must be on the same table or matrix table as locus_expr.; By default, the row value is given by the locus position. Returns:; (numpy.ndarray of int, numpy.ndarray of int) – Tuple of start indices array and stop indices array. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/utils/index.html:4883,update,updated,4883,docs/0.2/linalg/utils/index.html,https://hail.is,https://hail.is/docs/0.2/linalg/utils/index.html,1,['update'],['updated']
Deployability,"hood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on \(h^2\).; Testing each variant for association; Fixing a single variant, we define:. \(v = n \times 1\) vector of genotypes, with missing genotypes imputed as the mean of called genotypes; \(X_v = \left[v | X \right] = n \times (1 + c)\) matrix concatenating \(v\) and \(X\); \(\beta_v = (\beta^0_v, \beta^1_v, \ldots, \beta^c_v) = (1 + c) \times 1\) vector of covariate coefficients. Fixing \(\delta\) at the global REML estimate \(\hat{\delta}\), we find the REML estimate \((\hat{\beta}_v, \hat{\sigma}_{g,v}^2)\) via rotation of the model. \[y \sim \mathrm{N}\left(X_v\b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:104394,continuous,continuous,104394,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['continuous'],['continuous']
Deployability,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:8843,pipeline,pipelines,8843,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"iables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$H",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2121,install,install,2121,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,['install'],['install']
Deployability,"iant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html:9700,update,updated,9700,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ices, aggregations=LinkedList); def construct_expr(; x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; if type is None:; return Expression(x, None, indices, aggregations); x.assign_type(type); if isinstance(type, tarray) and is_numeric(type.element_type):; return ArrayNumericExpression(x, type, indices, aggregations); elif isinstance(type, tarray):; etype = type.element_type; if isinstance(etype, (hl.tarray, hl.tset)):; while isinstance(etype, (hl.tarray, hl.tset)):; etype = etype.element_type; if isinstance(etype, hl.tstruct):; return ArrayStructExpression(x, type, indices, aggregations); else:; return typ_to_expr[type.__class__](x, type, indices, aggregations); elif isinstance(type, tset):; etype = type.element_type; if isinstance(etype, (hl.tarray, hl.tset)):; while isinstance(etype, (hl.tarray, hl.tset)):; etype = etype.element_type; if isinstance(etype, hl.tstruct):; return SetStructExpression(x, type, indices, aggregations); else:; return typ_to_expr[type.__class__](x, type, indices, aggregations); elif isinstance(type, tndarray) and is_numeric(type.element_type):; return NDArrayNumericExpression(x, type, indices, aggregations); elif type in scalars:; return scalars[type](x, type, indices, aggregations); elif type.__class__ in typ_to_expr:; return typ_to_expr[type.__class__](x, type, indices, aggregations); else:; raise NotImplementedError(type). @typecheck(name=str, type=HailType, indices=Indices); def construct_reference(name, type, indices):; assert isinstance(type, hl.tstruct); x = ir.SelectedTopLevelReference(name, type); return construct_expr(x, type, indices). @typecheck(name=str, type=HailType, indices=Indices, aggregations=LinkedList); def construct_variable(name, type, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation)):; return construct_expr(ir.Ref(name, type), type, indices, aggregations). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:118675,update,updated,118675,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['update'],['updated']
Deployability,"ices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html:9712,update,updated,9712,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Liver_all_snp_gene_associations. View page source. GTEx_sQTL_Liver_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html:9730,update,updated,9730,docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Ovary_all_snp_gene_associations. View page source. GTEx_sQTL_Ovary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html:9730,update,updated,9730,docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html:9823,update,updated,9823,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ich_missing_bit = i % 8; if which_missing_bit == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; kwargs[f] = None; else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); kwargs[f] = field_decoded. return Struct(**kwargs). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; keys = list(self.keys()); length = len(keys); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:34727,update,update,34727,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['update'],['update']
Deployability,"ide a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called functio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:2826,release,release,2826,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['release'],['release']
Deployability,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:178201,pipeline,pipelines,178201,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['pipeline'],['pipelines']
Deployability,"ields (Dict[str, Expression], optional) – Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all (bool, optional) – Deprecated - use n_divisions instead.; n_divisions (int, optional.) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; significance_line (float, optional) – p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If None, no line is added. Returns:; bokeh.models.Plot. hail.plot.output_notebook()[source]; Configure the Bokeh output state to generate output in notebook; cells when bokeh.io.show() is called. Calls; bokeh.io.output_notebook(). hail.plot.visualize_missingness(entry_field, row_field=None, column_field=None, window=6000000, plot_width=1800, plot_height=900)[source]; Visualize missingness in a MatrixTable.; Inspired by naniar.; Row field is windowed by default, and missingness is aggregated over this window to generate a proportion defined.; This windowing is set to 6,000,000 by default, so that the human genome is divided into ~500 rows.; With ~2,000 columns, this function returns a sensibly-sized plot with this windowing. Warning; Generating a plot with more than ~1M points takes a long time for Bokeh to render. Consider windowing carefully. Parameters:. entry_field (Expression) – Field for which to check missingness.; row_field (NumericExpression or LocusExpression) – Row field to use for y-axis (can be windowed). If not provided, the row key will be used.; column_field (StringExpression) – Column field to use for x-axis. If not provided, the column key will be used.; window (int, optional) – Size of window to summarize by row_field. If set to None, each field will be used individually.; plot_width (int) – Plot width in px.; plot_height (int) – Plot height in px. Returns:; bokeh.plotting.figure. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:14844,update,updated,14844,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['update'],['updated']
Deployability,"ies per continent, but the number of people living on each continent. We can do this with geom_bar as well by specifying a weight. [13]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent, weight=gp_2007.pop)). [13]:. Histograms are similar to bar plots, except they break a continuous x axis into bins. Let’s import the iris dataset for this. [14]:. iris = hl.Table.from_pandas(plotly.data.iris()); iris.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'sepal_length': float64; 'sepal_width': float64; 'petal_length': float64; 'petal_width': float64; 'species': str; 'species_id': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s make a histogram:. [15]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(). [15]:. By default histogram plots groups stacked on top of each other, which is not always easy to interpret. We can specify the position argument to histogram to get different behavior. ""dodge"" puts the bars next to each other:. [16]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""dodge""). [16]:. And ""identity"" plots them over each other. It helps to set an alpha value to make them slightly transparent in these cases. [17]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""identity"", alpha=0.8). [17]:. Labels and Axes; It’s always a good idea to label your axes. This can be done most easily with xlab and ylab. We can also use ggtitle to add a title. Let’s pull in the same plot from above, and add labels. [18]:. (ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) +; geom_histogram(position=""identity"", alpha=0.8) +; xlab(""Sepal Length"") + ylab(""Number of samples"") + ggtitle(""Sepal length by flower type""); ). [18]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:7590,update,updated,7590,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['update'],['updated']
Deployability,"if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Labe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9467,update,update,9467,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"il/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:. - :math:`y = n \\times 1` vector of phenotypes; - :math:`X = n \\times c` matrix of sample covariates and intercept column of ones; - :math:`K = n \\times n` kinship matrix; - :math:`I = n \\times n` identity matrix; - :math:`\\beta = c \\times 1` vector of covariate coefficients; - :math:`\sigma_g^2 =` coefficient of genetic variance component :math:`K`; - :math:`\sigma_e^2 =` coefficient of environmental variance component :math:`I`; - :math:`\delta = \\frac{",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:126441,configurat,configuration,126441,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['configurat'],['configuration']
Deployability,"iltered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:161986,update,update,161986,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['update']
Deployability,"il’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS envir",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1740,configurat,configuration,1740,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,1,['configurat'],['configuration']
Deployability,"imum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the le",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:103042,integrat,integrate,103042,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['integrat'],['integrate']
Deployability,"in_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_metho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:13711,update,update,13711,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"ing Python’s Counter class. [23]:. from collections import Counter; counts = Counter(snp_counts); counts.most_common(). [23]:. [(Struct(ref='C', alt='T'), 2418),; (Struct(ref='G', alt='A'), 2367),; (Struct(ref='A', alt='G'), 1929),; (Struct(ref='T', alt='C'), 1864),; (Struct(ref='C', alt='A'), 494),; (Struct(ref='G', alt='T'), 477),; (Struct(ref='T', alt='G'), 466),; (Struct(ref='A', alt='C'), 451),; (Struct(ref='C', alt='G'), 150),; (Struct(ref='G', alt='C'), 111),; (Struct(ref='T', alt='A'), 77),; (Struct(ref='A', alt='T'), 75)]. It’s nice to see that we can actually uncover something biological from this small dataset: we see that these frequencies come in pairs. C/T and G/A are actually the same mutation, just viewed from from opposite strands. Likewise, T/A and A/T are the same mutation on opposite strands. There’s a 30x difference between the frequency of C/T and A/T SNPs. Why?; The same Python, R, and Unix tools could do this work as well, but we’re starting to hit a wall - the latest gnomAD release publishes about 250 million variants, and that won’t fit in memory on a single computer.; What about genotypes? Hail can query the collection of all genotypes in the dataset, and this is getting large even for our tiny dataset. Our 284 samples and 10,000 variants produce 10 million unique genotypes. The gnomAD dataset has about 5 trillion unique genotypes.; Hail plotting functions allow Hail fields as arguments, so we can pass in the DP field directly here. If the range and bins arguments are not set, this function will compute the range based on minimum and maximum values of the field and use the default 50 bins. [24]:. p = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p). [Stage 22:> (0 + 1) / 1]. Quality Control; QC is where analysts spend most of their time with sequencing datasets. QC is an iterative process, and is different for every project: there is no “push-button” solution for QC. Each time the Broad collects a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:11825,release,release,11825,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['release'],['release']
Deployability,"ing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """""". if alternative == 'two.sided':; warning(; '""two.sided"" is a deprecated and will be removed in a future '; 'release, please use ""two-sided"" for the `alternative` parameter '; 'to hl.binom_test'; ); alternative = 'two-sided'. alt_enum = {""two-sided"": 0, ""less"": 1, ""greater"": 2}[alternative]; return _func(""binomTest"", tfloat64, x, n, p, to_expr(alt_enum)). [docs]@typecheck(x=expr_float64, df=expr_float64, ncp=nullable(expr_float64), lower_tail=expr_bool, log_p=expr_bool); def pchisqtail(x, df, ncp=None, lower_tail=False, log_p=False) -> Float64Expression:; """"""Returns the probability under the right-tail starting at x for a chi-squared; distribution with df degrees of freedom. Examples; --------. >>> hl.eval(hl.pchisqtail(5, 1)); 0.025347318677468304. >>> hl.eval(hl.pchisqtail(5, 1, ncp=2)); 0.20571085634347097. >>> hl.eval(hl.pchisqtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the CDF.; df : float or :class:`.Expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:62490,release,release,62490,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['release'],['release']
Deployability,"ingular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39272,configurat,configuration-dependent,39272,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['configurat'],['configuration-dependent']
Deployability,"inux; Google Dataproc; Next Steps. Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Google Dataproc. View page source. Use Hail on Google Dataproc; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl dataproc which starts, stops, and; manipulates Hail-enabled Dataproc clusters.; Start a dataproc cluster named “my-first-cluster”. Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes.; hailctl dataproc start my-first-cluster. Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also start a Jupyter Notebook running on the cluster:; hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:; hailctl dataproc stop my-first-cluster. Next Steps. Read more about Hail on Google Cloud; Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/dataproc.html:2162,update,updated,2162,docs/0.2/install/dataproc.html,https://hail.is,https://hail.is/docs/0.2/install/dataproc.html,1,['update'],['updated']
Deployability,"ion. The resulting VDS will be larger and slower in Hail but the genotypes will be accessible from other tools that support Parquet. """""". if self._is_generic_genotype:; self._jvdf.write(output, overwrite); else:; self._jvdf.write(output, overwrite, parquet_genotypes). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=strlike,; annotation=strlike,; subset=bool,; keep=bool,; filter_altered_genotypes=bool,; max_shift=integral,; keep_star=bool); def filter_alleles(self, expr, annotation='va = va', subset=True, keep=True,; filter_altered_genotypes=False, max_shift=100, keep_star=False):; """"""Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. ``aIndex`` will never be zero). .. include:: requireTGenotype.rst. **Examples**. To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:. >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of ``aIndices`` because; we are mapping between the old and new *allele* indices, not; the *alternate allele* indices. **Notes**. If ``filter_altered_genotypes`` is true, genotypes that contain filtered-out alleles are set to missing. :py:meth:`~hail.VariantDataset.filter_alleles` implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. **Subset algorithm**. The subset algorithm (the default, ``subset=True``) subsets the; AD and PL",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:61168,update,update,61168,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['update'],['update']
Deployability,"ion` that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a :class:`.CaseBuilder`. Parameters; ----------; condition: :class:`.BooleanExpression`; then : :class:`.Expression`. Returns; -------; :class:`.CaseBuilder`; Mutates and returns `self`.; """"""; self._unify_type(then.dtype); self._cases.append((condition, then)); return self. [docs] @typecheck_method(then=expr_any); def default(self, then):; """"""Finish the case statement by adding a default case. Notes; -----; If no condition from a :meth:`~.CaseBuilder.when` call is ``True``,; then `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the case statement by returning missing. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:8658,update,updated,8658,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,1,['update'],['updated']
Deployability,"ions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+0",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:5352,continuous,continuous,5352,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['continuous'],['continuous']
Deployability,"ior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2479,install,installed,2479,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['install'],['installed']
Deployability,"is batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:2037,configurat,configurations,2037,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['configurat'],['configurations']
Deployability,"is facing side, none by default. Overrides ``color`` aesthetic. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomArea(mapping, fill=fill, color=color). class GeomRibbon(Geom):; aes_to_arg: ClassVar = {; ""fill"": (""fillcolor"", ""black""),; ""color"": (""line_color"", ""rgba(0, 0, 0, 0)""),; ""tooltip"": (""hovertext"", None),; ""fill_legend"": (""name"", None),; }. def __init__(self, aes, fill, color):; super().__init__(aes); self.fill = fill; self.color = color. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; def plot_group(df):; trace_args_bottom = {; ""x"": df.x,; ""y"": df.ymin,; ""row"": facet_row,; ""col"": facet_col,; ""mode"": ""lines"",; ""showlegend"": False,; }; self._add_aesthetics_to_trace_args(trace_args_bottom, df); self._update_legend_trace_args(trace_args_bottom, legend_cache). trace_args_top = {; ""x"": df.x,; ""y"": df.ymax,; ""row"": facet_row,; ""col"": facet_col,; ""mode"": ""lines"",; ""fill"": 'tonexty',; }; self._add_aesthetics_to_trace_args(trace_args_top, df); self._update_legend_trace_args(trace_args_top, legend_cache). fig_so_far.add_scatter(**trace_args_bottom); fig_so_far.add_scatter(**trace_args_top). for group_df in grouped_data:; plot_group(group_df). def get_stat(self):; return StatIdentity(). [docs]def geom_ribbon(mapping=aes(), fill=None, color=None):; """"""Creates filled in area between two lines specified by x, ymin, and ymax. Supported aesthetics: ``x``, ``ymin``, ``ymax``, ``color``, ``fill``, ``tooltip``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; fill:; Color of fill to draw, black by default. Overrides ``fill`` aesthetic.; color:; Color of line to draw outlining both side, none by default. Overrides ``color`` aesthetic. :return:; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomRibbon(mapping, fill=fill, color=color). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:28060,update,updated,28060,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,1,['update'],['updated']
Deployability,"is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+00 |; | 20:103517 | 2.04604e+00 | 2.75392e+02 | 4.69239e+00 |; | 20:108286 | 2.06585e+00 | 2.86453e+02 | 5.00124e+00 |; +---------------+-------------------+-----------------------+-------------+. Warning; ld_score() will fail if entry_expr results in any missing; va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:5619,continuous,continuous,5619,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['continuous'],['continuous']
Deployability,"it == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; kwargs[f] = None; else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); kwargs[f] = field_decoded. return Struct(**kwargs). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; keys = list(self.keys()); length = len(keys); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:34770,update,update,34770,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['update'],['update']
Deployability,"it multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102008,configurat,configuration,102008,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"iven_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). de_novo_call = (; hl.case(); .when(~het_hom_hom | kid_ad_fail, failure); .when(autosomal, hl.bind(call_auto, kid_pp, dad_pp, mom_pp, kid_ad_ratio)); .when(hemi_x | hemi_mt, hl.bind(call_hemi, kid_pp, mom, mom_pp, kid_ad_ratio)); .when(hemi_y, hl.bind(call_hemi, kid_pp, dad, dad_pp, kid_ad_ratio)); .or_missing(); ). tm = tm.annotate_entries(__call=de_novo_call); tm = tm.filter_entries(hl.is_defined(tm.__call)); entries = tm.entries(); return entries.select(; '__site_freq',; 'proband',; 'father',; 'mother',; 'proband_entry',; 'father_entry',; 'mother_entry',; 'is_female',; **entries.__call,; ).rename({'__site_freq': 'prior'}). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:33740,update,updated,33740,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['update'],['updated']
Deployability,"ix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52702,release,release,52702,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"ixTable. Schema (phase_3, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; DP: int32,; END: int32,; SVTYPE: str,; AA: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrY.html:10939,update,updated,10939,docs/0.2/datasets/schemas/1000_Genomes_chrY.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrY.html,1,['update'],['updated']
Deployability,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Spleen_all_snp_gene_associations. View page source. GTEx_eQTL_Spleen_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html:9676,update,updated,9676,docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Testis_all_snp_gene_associations. View page source. GTEx_eQTL_Testis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html:9676,update,updated,9676,docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Uterus_all_snp_gene_associations. View page source. GTEx_eQTL_Uterus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html:9676,update,updated,9676,docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Vagina_all_snp_gene_associations. View page source. GTEx_eQTL_Vagina_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html:9676,update,updated,9676,docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"j}} := \\frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}\\widehat{\\sigma^2_{is}} \\widehat{\\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \\widehat{k^{(0)}_{ij}} :=; \\begin{cases}; \\frac{\\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \\widehat{\\mu_{is}}^2(1 - \\widehat{\\mu_{js}})^2 + (1 - \\widehat{\\mu_{is}})^2\\widehat{\\mu_{js}}^2}; & \\widehat{\phi_{ij}} > 2^{-5/2} \\\\; 1 - 4 \\widehat{\phi_{ij}} + k^{(2)}_{ij}; & \\widehat{\phi_{ij}} \le 2^{-5/2}; \\end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \\widehat{k^{(1)}_{ij}} := 1 - \\widehat{k^{(2)}_{ij}} - \\widehat{k^{(0)}_{ij}}. **Details**. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_size`` larger than 512 tends to cause memory exhaustion errors. The mini",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:174138,release,release,174138,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['release'],['release']
Deployability,"l be NaN.; Examples; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.maximum(a, b)); array([2, 5, 4], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.maximum(a, b)); array([nan, 5., nan]). Parameters:. nd1 (NDArrayExpression); nd2 (class:.NDArrayExpression, .ArrayExpression, numpy ndarray, or nested python lists/tuples.) – Nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns:; NDArrayExpression – Element-wise maximums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, the resulting; array will be of that shape. hail.nd.minimum(nd1, nd2)[source]; Compares elements at corresponding indexes in arrays; and returns an array of the minimum element found; at each compared index.; If an array element being compared has the value NaN,; the minimum for that index will be NaN.; Examples; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.minimum(a, b)); array([1, 3, 3], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.minimum(a, b)); array([nan, 3., nan]). Parameters:. nd1 (NDArrayExpression); nd2 (class:.NDArrayExpression, .ArrayExpression, numpy ndarray, or nested python lists/tuples.) – nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns:; min_array (NDArrayExpression) – Element-wise minimums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, resulting array; will be of that shape. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:15053,update,updated,15053,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['update'],['updated']
Deployability,"l if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the x-axis label.; """"""; return Labels(xlabel=label). [docs]def ylab(label):; """"""Sets the y-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired y-axis label of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the y-axis label.; """"""; return Labels(ylabel=label). def labs(**group_labels):; """"""Sets the labels for the legend groups of a plot. Examples; --------. Create a scatterplot and label the legend groups according to their field names:. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared=ht.idx ** 2); >>> ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); >>> ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); >>> fig = (; ... hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)); ... + hl.ggplot.geom_point(hl.ggplot.aes(color=ht.even, shape=ht.threeven)); ... + hl.ggplot.labs(color=""Even"", shape=""Threeven""); ... ). Parameters; ----------; group_labels:; Map names of plotly ``legendgroup``s to the desired replacement labels. Returns; -------; :class:`.FigureAttribute`; Label object to change the legend group labels.; """"""; return Labels(group_labels=group_labels). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:3286,update,updated,3286,docs/0.2/_modules/hail/ggplot/labels.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html,1,['update'],['updated']
Deployability,"l.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; >>> hl.eval(call.unphased_diploid_gt_index()); 1. Returns:; Expression of type tint32. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CallExpression.html:11778,update,updated,11778,docs/0.2/hail.expr.CallExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html,1,['update'],['updated']
Deployability,"l.ldscore; import hail as hl; from hail.expr.expressions import expr_float64, expr_locus, expr_numeric; from hail.linalg import BlockMatrix; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; annotation_exprs=nullable(oneof(expr_numeric, sequenceof(expr_numeric))),; block_size=nullable(int),; ); def ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None) -> Table:; """"""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:1486,continuous,continuous,1486,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,1,['continuous'],['continuous']
Deployability,"l.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78474,pipeline,pipelines,78474,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"l_continuous(); elif aesthetic_str == ""shape"" and not is_continuous:; self.scales[""shape""] = scale_shape_auto(); elif aesthetic_str == ""shape"" and is_continuous:; raise ValueError(; ""The 'shape' aesthetic does not support continuous ""; ""types. Specify values of a discrete type instead.""; ); elif is_continuous:; self.scales[aesthetic_str] = ScaleContinuous(aesthetic_str); else:; self.scales[aesthetic_str] = ScaleDiscrete(aesthetic_str). def copy(self):; return GGPlot(self.ht, self.aes, self.geoms[:], self.labels, self.coord_cartesian, self.scales, self.facet). def verify_scales(self):; for aes_key in self.aes.keys():; check_scale_continuity(self.scales[aes_key], self.aes[aes_key].dtype, aes_key); for geom in self.geoms:; aesthetic_dict = geom.aes.properties; for aes_key in aesthetic_dict.keys():; check_scale_continuity(self.scales[aes_key], aesthetic_dict[aes_key].dtype, aes_key). [docs] def to_plotly(self):; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """""". def make_geom_label(geom_idx):; return f""geom{geom_idx}"". def select_table():; fields_to_select = {""figure_mapping"": hl.struct(**self.aes)}; if self.facet is not None:; fields_to_select[""facet""] = self.facet.get_expr_to_group_by(). for geom_idx, geom in enumerate(self.geoms):; geom_label = make_geom_label(geom_idx); fields_to_select[geom_label] = hl.struct(**geom.aes.properties). name, ht = hl.struct(**fields_to_select)._to_table('__fallback'); return ht.select(**{field: ht[name][field] for field in fields_to_select}). def collect_mappings_and_precomputed(selected):; mapping_per_geom = []; precomputes = {}; for geom_idx, geom in enumerate(self.geoms):; geom_label = make_geom_label(geom_idx). combined_mapping = selected[""figure_mapping""].annotate(**selected[geom_label]). for key in combined_mapping:; if key in self.scales:; combined_mapping = combined_mapping.annotate(**{; key: self.scales[key].transform_data(combined_mapping[key]); }); mappin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:4387,update,updated,4387,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,1,['update'],['updated']
Deployability,"l_n.take(5); [5, 5, 5, 5, 5]. >>> mt_filt = mt_filt.annotate_rows(row_n = hl.agg.count()); >>> mt_filt.row_n.take(5); [5, 5, 5, 5, 5]. 3. Annotating a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; --",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58811,pipeline,pipeline,58811,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['pipeline'],['pipeline']
Deployability,"l_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):; """"""Reference genome. :return: :class:`.ReferenceGenome`; """"""; return self._rg. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:3272,update,updated,3272,docs/0.2/_modules/hail/genetics/locus.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html,1,['update'],['updated']
Deployability,"l_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9661,update,update,9661,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"label to show on x-axis; breaks (list of str) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_genomic(reference_genome, name=None)[source]; The default genomic x scale. This is used when the x aesthetic corresponds to a LocusExpression. Parameters:. reference_genome – The reference genome being used.; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_log10(name=None)[source]; Transforms x axis to be log base 10 scaled. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_reverse(name=None)[source]; Transforms x-axis to be vertically reversed. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of float) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the y-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_discrete(name=None, breaks=None, labels=None)[source]; The default discrete y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of str) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_log10(name=None)[source]; Transforms y-axis to be log base 10 scaled. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_reverse(name=None)[source]; Transforms y-axis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:11465,continuous,continuous,11465,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"lass:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:10003,configurat,configuration,10003,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"ld.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log stat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:84570,install,installed,84570,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; CADD. View page source. CADD. Versions: 1.4, 1.6; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (1.4, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int64,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'raw_score': float64; 'PHRED_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/CADD.html:9446,update,updated,9446,docs/0.2/datasets/schemas/CADD.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/CADD.html,1,['update'],['updated']
Deployability,"ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_both_sexes. View page source. UK_Biobank_Rapid_GWAS_both_sexes. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html:10509,update,updated,10509,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,1,['update'],['updated']
Deployability,"ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Whole_Blood_all_snp_gene_associations. View page source. GTEx_eQTL_Whole_Blood_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html:9691,update,updated,9691,docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"le JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72527,install,installation,72527,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installation']
Deployability,"le to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-buc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2286,install,install-gcs-connector,2286,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['install'],['install-gcs-connector']
Deployability,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:15408,update,updated,15408,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,2,['update'],['updated']
Deployability,"le_source(caller, expr):; from hail import MatrixTable. source = expr._indices.source; if not isinstance(source, MatrixTable):; raise ValueError(; ""{}: Expect an expression of 'MatrixTable', found {}"".format(; caller, ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ); return source. @typecheck(caller=str, expr=Expression); def table_source(caller, expr):; from hail import Table. source = expr._indices.source; if not isinstance(source, Table):; raise ValueError(; ""{}: Expect an expression of 'Table', found {}"".format(; caller, ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ); return source. @typecheck(caller=str, expr=Expression); def raise_unless_entry_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be entry-indexed"" f"", found no indices (no source)""); if expr._indices != expr._indices.source._entry_indices:; raise ExpressionException(; f""{caller}: expression must be entry-indexed"" f"", found indices {list(expr._indices.axes)}.""; ). @typecheck(caller=str, expr=Expression); def raise_unless_row_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be row-indexed"" f"", found no indices (no source).""); if expr._indices != expr._indices.source._row_indices:; raise ExpressionException(; f""{caller}: expression must be row-indexed"" f"", found indices {list(expr._indices.axes)}.""; ). @typecheck(caller=str, expr=Expression); def raise_unless_column_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be column-indexed"" f"", found no indices (no source).""); if expr._indices != expr._indices.source._col_indices:; raise ExpressionException(; f""{caller}: expression must be column-indexed"" f"", found indices ({list(expr._indices.axes)}).""; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:9670,update,updated,9670,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,1,['update'],['updated']
Deployability,"leotide variants for which both; individuals \(i\) and \(j\) have a non-missing genotype.; \(X_{i,s}\) be the genotype score matrix. Each entry corresponds to; the genotype of individual \(i\) at variant; \(s\). Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. \(X_{i,s}\) is calculated by invoking; n_alt_alleles() on the call_expr. The three counts above, \(N^{Aa}\), \(N^{Aa,Aa}\), and; \(N^{AA,aa}\), exclude variants where one or both individuals have; missing genotypes.; In terms of the symbols above, we can define \(d\), the genetic distance; between two samples. We can interpret \(d\) as an unnormalized; measurement of the genetic material not shared identically-by-descent:. \[d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2\]; In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. \((X_{i,s} - X_{j,s})^2\); homref; het; homalt. homref; 0; 1; 4. het; 1; 0; 1. homalt; 4; 1; 0. which leads to this expression for genetic distance:. \[d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}\]; The first term, \(4 N^{AA,aa}_{i,j}\), accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case of a pair of heteroyzgous genotypes. We offset this; with the fourth and final term.; The genetic distance, \(d_{i,j}\), ranges between zero and four times; the number of variants in the dataset. In the supplement to Manichaikul,; et. al, the authors demonstrate that the kinship coefficient,; \(\phi_{i,j}\), between two individuals from the same population is; rel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:7191,configurat,configurations,7191,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['configurat'],['configurations']
Deployability,"les.; if aesthetic_str == ""x"":; if is_continuous:; self.scales[""x""] = scale_x_continuous(); elif is_genomic_type(dtype):; self.scales[""x""] = scale_x_genomic(reference_genome=dtype.reference_genome); else:; self.scales[""x""] = scale_x_discrete(); elif aesthetic_str == ""y"":; if is_continuous:; self.scales[""y""] = scale_y_continuous(); elif is_genomic_type(dtype):; raise ValueError(""Don't yet support y axis genomic""); else:; self.scales[""y""] = scale_y_discrete(); elif aesthetic_str == ""color"" and not is_continuous:; self.scales[""color""] = scale_color_discrete(); elif aesthetic_str == ""color"" and is_continuous:; self.scales[""color""] = scale_color_continuous(); elif aesthetic_str == ""fill"" and not is_continuous:; self.scales[""fill""] = scale_fill_discrete(); elif aesthetic_str == ""fill"" and is_continuous:; self.scales[""fill""] = scale_fill_continuous(); elif aesthetic_str == ""shape"" and not is_continuous:; self.scales[""shape""] = scale_shape_auto(); elif aesthetic_str == ""shape"" and is_continuous:; raise ValueError(; ""The 'shape' aesthetic does not support continuous ""; ""types. Specify values of a discrete type instead.""; ); elif is_continuous:; self.scales[aesthetic_str] = ScaleContinuous(aesthetic_str); else:; self.scales[aesthetic_str] = ScaleDiscrete(aesthetic_str). def copy(self):; return GGPlot(self.ht, self.aes, self.geoms[:], self.labels, self.coord_cartesian, self.scales, self.facet). def verify_scales(self):; for aes_key in self.aes.keys():; check_scale_continuity(self.scales[aes_key], self.aes[aes_key].dtype, aes_key); for geom in self.geoms:; aesthetic_dict = geom.aes.properties; for aes_key in aesthetic_dict.keys():; check_scale_continuity(self.scales[aes_key], aesthetic_dict[aes_key].dtype, aes_key). [docs] def to_plotly(self):; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """""". def make_geom_label(geom_idx):; return f""geom{geom_idx}"". def select_table():; fields_to_select = {""figure_mapping",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:3599,continuous,continuous,3599,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,1,['continuous'],['continuous']
Deployability,"lf), hl.agg.count()),; self._summary_aggs(),; )). def _summarize(self, agg_res=None, *, name=None, header=None, top=False):; src = self._indices.source; summary_header = None; if src is None or len(self._indices.axes) == 0:; raise ValueError(""Cannot summarize a scalar expression""); if agg_res is None:; count, agg_res = self._aggregation_method()(hl.tuple((hl.agg.count(), self._all_summary_aggs()))); summary_header = f'{count} records.'; sum_fields, nested = self._summary_fields(agg_res, top); summary = Summary(self._type, agg_res[0], sum_fields, nested, header=summary_header); if name is None and header is None:; return summary; else:; return NamedSummary(summary, name, header). [docs] def summarize(self, handler=None):; """"""Compute and print summary information about the expression. .. include:: _templates/experimental.rst; """""". src = self._indices.source; if self in src._fields:; field_name = src._fields_inverse[self]; prefix = field_name; elif self._ir.is_nested_field:; prefix = self._ir.name; else:; prefix = '<expr>'. if handler is None:; handler = hl.utils.default_handler(); handler(self._summarize(name=prefix)). def _selector_and_agg_method(self):; src = self._indices.source; assert src is not None; assert len(self._indices.axes) > 0; if isinstance(src, hl.MatrixTable):; if self._indices == src._row_indices:; return src.select_rows, lambda t: t.aggregate_rows; elif self._indices == src._col_indices:; return src.select_cols, lambda t: t.aggregate_cols; else:; return src.select_entries, lambda t: t.aggregate_entries; else:; return src.select, lambda t: t.aggregate. def _aggregation_method(self):; return self._selector_and_agg_method()[1](self._indices.source). def _persist(self):; src = self._indices.source; if src is not None:; raise ValueError(""Can only persist a scalar (no Table/MatrixTable source)""); expr = Env.backend().persist_expression(self); assert expr.dtype == self.dtype; return expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:35943,update,updated,35943,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,1,['update'],['updated']
Deployability,"lf):; return str(self). def __str__(self):; if all(k.isidentifier() for k in self._fields):; return 'Struct(' + ', '.join(f'{k}={v!r}' for k, v in self._fields.items()) + ')'; return 'Struct(**{' + ', '.join(f'{k!r}: {v!r}' for k, v in self._fields.items()) + '})'. def __eq__(self, other):; return self._fields == other._fields if isinstance(other, Struct) else NotImplemented. def __hash__(self):; return 37 + hash(tuple(sorted(self._fields.items()))). def __iter__(self):; return iter(self._fields). def __dir__(self):; super_dir = super().__dir__(); return super_dir + list(self._fields.keys()). def annotate(self, **kwargs):; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; d = OrderedDict(self.items()); for k, v in kwargs.items():; d[k] = v; return Struct(**d). @typecheck_method(fields=str, kwargs=anytype); def select(self, *fields, **kwargs):; """"""Select existing fields and compute new ones. Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `kwargs` arguments are new fields to add. Parameters; ----------; fields : varargs of :class:`str`; Field names to keep.; named_exprs : keyword args; New field. Returns; -------; :class:`.Struct`; Struct containing specified",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:3675,update,updated,3675,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,1,['update'],['updated']
Deployability,"lf, fig); if self.scales.get(""y"") is not None:; self.scales[""y""].apply_to_fig(self, fig); if self.coord_cartesian is not None:; self.coord_cartesian.apply_to_fig(fig). fig = fig.update_xaxes(title_font_size=18, ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:10483,install,installed,10483,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,1,['install'],['installed']
Deployability,"lf._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.gro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5355,pipeline,pipeline,5355,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['pipeline'],['pipeline']
Deployability,"lf.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:6938,configurat,configuration,6938,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"lineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats; Schema (0.3, GRCh37). Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_summary_stats. View page source. panukb_summary_stats. Versions: 0.3; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (0.3, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'trait_type': str; 'phenocode': str; 'pheno_sex': str; 'coding': str; 'modifier': str; 'pheno_data': array<struct {; n_cases: int32,; n_controls: int32,; heritability: float64,; saige_version: str,; inv_normalized: bool,; pop: str; }>; 'description': str; 'description_more': str; 'coding_description': str; 'category': str; 'n_cases_full_cohort_both_sexes': int64; 'n_cases_full_cohort_females': int64; 'n_cases_full_cohort_males': int64; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'gene': str; 'annotation': str; ----------------------------------------; Entry fields:; 'summary_stats': array<struct {; AF_Allele2: float64,; imputationInfo: float64,; BETA: float64,; SE: float64,; `p.value.NA`: float64,; `AF.Cases`: float64,; `AF.Controls`: float64,; Pvalue: float64,; low_confidence: bool; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_summary_stats.html:10227,update,updated,10227,docs/0.2/datasets/schemas/panukb_summary_stats.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_summary_stats.html,1,['update'],['updated']
Deployability,"linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:4946,install,installed,4946,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['install'],['installed']
Deployability,"ling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:57643,pipeline,pipelines,57643,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"ll be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus. If a dataset does not have unique rows for each key (consider the; ``gencode`` genes, which may overlap; and ``clinvar_variant_summary``,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters; ----------; rel : :class:`.MatrixTable` or :class:`.Table`; The relational object to which to add annotations.; names : varargs of :class:`str`; The names of the datasets with which to annotate `rel`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; The relational object `rel`, with the annotations from `names`; added.; """"""; rel = self._row_lens(rel); if len(set(names)) != len(names):; raise ValueError(f'cannot annotate same dataset twice,' f' please remove duplicates from: {names}'); self._check_availability(names); datasets = [self._dataset_by_name(name) for name in names]; if any(dataset.is_gene_keyed for dataset in datasets):; gene_field, rel = self._annotate_gene_name(rel); else:; gene_field = None; for dataset in datasets:; if dataset.is_gene_keyed:; genes = rel.select(gene_field).explode(gene_field); genes = genes.annotate(**{dataset.name: dataset.index_compatible_version(genes[gene_field])}); genes = genes.group_by(*genes.key).aggregate(**{; dataset.name: hl.dict(; hl.agg.filter(; hl.is_defined(genes[dataset.name]),; hl.agg.collect((genes[gene_field], genes[dataset.name])),; ); ); }); rel = rel.annotate(**{dataset.name: genes.index(rel.key)[dataset.name]}); else:; indexed_value = dataset.index_compatible_version(rel.key); if isinstance(indexed_value.dtype, hl.tstruct) and len(indexed_value.dtype) == 0:; indexed_value = hl.is_defined(indexed_value); rel = rel.annotate(**{dataset.name: indexed_value}); if gene_field:; rel = rel.drop(gene_field); return rel.unlens(). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:17249,update,updated,17249,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['update'],['updated']
Deployability,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38882,configurat,configuration,38882,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,3,"['configurat', 'install', 'release']","['configuration', 'installed', 'release']"
Deployability,"ll not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, default=None):; """"""Get an item, or return a default value if the item is not found.; ; :param str item: Name of attribute.; ; :param default: Default value.; ; :returns: Value of item if found, or default value if not.; """"""; return self._attrs.get(item, default). @typecheck(struct=Struct); def to_dict(struct):; d = {}; for k, v in struct._attrs.iteritems():; if isinstance(v, Struct):; d[k] = to_dict(v); else:; d[k] = v; return d. import pprint. _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, *args, **kwargs):; if isinstance(obj, Struct):; obj = to_dict(obj); return _old_printer._format(self, obj, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:2737,patch,patch,2737,docs/0.1/_modules/hail/representation/annotations.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html,1,['patch'],['patch']
Deployability,"lled, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VEP and cannot be modified to run VEP. You must start a new; cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:3850,configurat,configuration,3850,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"llele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93390,integrat,integration,93390,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['integrat'],['integration']
Deployability,"lleles of a variant are filtered out, the variant itself; is filtered out.; Using f; The f argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:23545,update,updated,23545,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['updated']
Deployability,"llo”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the Local",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1869,configurat,configurations,1869,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['configurat'],['configurations']
Deployability,"ls. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90385,pipeline,pipelines,90385,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"lt PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: Stri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175825,configurat,configuration,175825,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['configurat'],['configuration']
Deployability,"lt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadings_source.count(). mt = mt.filter_rows(hl.is_defined(mt._loadings) & hl.is_defined(mt._af) & (mt._af > 0) & (mt._af < 1)). gt_norm = (mt._call.n_alt_alleles() - 2 * mt._af) / hl.sqrt(n_variants * 2 * mt._af * (1 - mt._af)). return mt.select_cols(scores=hl.agg.array_sum(mt._loadings * gt_norm)).cols(). def _get_expr_or_join(expr, source, other_source, loc):; if source != other_source:; if isinstance(source, hl.MatrixTable):; source = source.annotate_rows(**{loc: expr}); else:; source = source.annotate(**{loc: expr}); expr = source[other_source.row_key][loc]; return expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:3303,update,updated,3303,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['update'],['updated']
Deployability,"lue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:7841,update,updated,7841,docs/0.2/hail.expr.IntervalExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html,1,['update'],['updated']
Deployability,"ly detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable) – A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any]) – Positional argum",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:3796,install,installed,3796,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,1,['install'],['installed']
Deployability,"m supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30627,update,update,30627,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"mad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; dbSNP_rsid. View page source. dbSNP_rsid. Versions: 154; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (154, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbSNP_rsid.html:9426,update,updated,9426,docs/0.2/datasets/schemas/dbSNP_rsid.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbSNP_rsid.html,1,['update'],['updated']
Deployability,"mad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Pituitary_all_snp_gene_associations. View page source. GTEx_eQTL_Pituitary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html:9685,update,updated,9685,docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html:9796,update,updated,9796,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html:9796,update,updated,9796,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html:9796,update,updated,9796,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"mbl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38244,configurat,configuration,38244,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"mentation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2269,configurat,configuration,2269,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,"mericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif n < 21:; from bokeh.palettes import Category20. _palette = Category20[n]; else:; from bokeh.palettes ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23573,update,update,23573,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"mples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first elemen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:94668,upgrade,upgrade,94668,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['upgrade'],['upgrade']
Deployability,"my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2981,configurat,configuration,2981,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,1,['configurat'],['configuration']
Deployability,"n API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1374,install,installs,1374,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['installs']
Deployability,"n released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:1539,configurat,configuration,1539,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['configurat'],['configuration']
Deployability,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42465,pipeline,pipelines,42465,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['pipeline'],['pipelines']
Deployability,"n the resulting struct in the order they appear in; fields.; The named_exprs arguments are new field expressions. Parameters:. fields (varargs of str) – Field names to keep.; named_exprs (keyword args of Expression) – New field expressions. Returns:; StructExpression – Struct containing specified existing fields and computed fields. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; A list of expressions for each field. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:9200,update,updated,9200,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['update'],['updated']
Deployability,"n(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37572,install,installed,37572,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['install'],['installed']
Deployability,"n-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12212,install,install-gcs-connector,12212,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['install-gcs-connector']
Deployability,"n; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. View page source. GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html:9787,update,updated,9787,docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"n; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html:9787,update,updated,9787,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[1][1], elt[0]); ); ). old_to_new = hl.bind(lambda d: mt.alleles.map(lambda a: d.get(a)), ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155770,update,update,155770,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['update']
Deployability,"nce: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str) – Path to Nirvana configuration file.; block_size (int) – Number of rows to process per Nirvana invocation.; name (str) – Name for resulting row field. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing Nirvana annotations. hail.methods.sample_qc(mt, name='sample_qc')[source]; Compute per-sample metrics useful for quality control. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Compute sample QC metrics and remove low-quality samples:; >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; name parameter.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contai",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:65054,configurat,configuration,65054,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"nced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-acc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1330,configurat,configuration,1330,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,1,['configurat'],['configuration']
Deployability,"ncol is set.; ncol (int) – The number of columns into which the facets will be spread.; scales (str) – Whether the scales are the same across facets. For more information and a list of supported options, see the ggplot documentation. Returns:; FigureAttribute – The faceter. hail.ggplot.vars(*args)[source]. Parameters:; *args (hail.expr.Expression) – Fields to facet by. Returns:; hail.expr.StructExpression – A struct to pass to a faceter. Labels. xlab(label); Sets the x-axis label of a plot. ylab(label); Sets the y-axis label of a plot. ggtitle(label); Sets the title of a plot. hail.ggplot.xlab(label)[source]; Sets the x-axis label of a plot. Parameters:; label (str) – The desired x-axis label of the plot. Returns:; FigureAttribute – Label object to change the x-axis label. hail.ggplot.ylab(label)[source]; Sets the y-axis label of a plot. Parameters:; label (str) – The desired y-axis label of the plot. Returns:; FigureAttribute – Label object to change the y-axis label. hail.ggplot.ggtitle(label)[source]; Sets the title of a plot. Parameters:; label (str) – The desired title of the plot. Returns:; FigureAttribute – Label object to change the title. Classes. class hail.ggplot.GGPlot(ht, aes, geoms=[], labels=<hail.ggplot.labels.Labels object>, coord_cartesian=None, scales=None, facet=None)[source]; The class representing a figure created using the hail.ggplot module.; Create one by using ggplot(). to_plotly()[source]; Turn the hail plot into a Plotly plot. Returns:; A Plotly figure that can be updated with plotly methods. show()[source]; Render and show the plot, either in a browser or notebook. write_image(path)[source]; Write out this plot as an image.; This requires you to have installed the python package kaleido from pypi. Parameters:; path (str) – The path to write the file to. class hail.ggplot.Aesthetic(properties)[source]. class hail.ggplot.FigureAttribute[source]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:16683,update,updated,16683,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,3,"['install', 'update']","['installed', 'updated']"
Deployability,"nd a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1708,install,installing,1708,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['installing']
Deployability,"ndices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html:9709,update,updated,9709,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ndices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Lung_all_snp_gene_associations. View page source. GTEx_sQTL_Lung_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html:9727,update,updated,9727,docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ndices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html:9820,update,updated,9820,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ndle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; Env.hc(); if name == 'default':; return default_reference(); else:; return Env.backend().get_reference(name). [docs]@typecheck(seed=int); def set_global_seed(seed):; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """""". warning(; 'hl.set_global_seed has no effect. See '; 'https://hail.is/docs/0.2/functions/random.html for details on '; 'ensuring reproducibility of randomness.'; ); pass. [docs]@typecheck(); def reset_global_randomness():; """"""Restore global randomness to initial state for test reproducibility."""""". Env.reset_global_randomness(). def _set_flags(**flags):; Env.backend().set_flags(**flags). def _get_flags(*flags):; return Env.backend().get_flags(*flags). @contextmanager; def _with_flags(**flags):; before = _get_flags(*flags); try:; _set_flags(**flags); yield; finally:; _set_flags(**before). def debug_info():; from hail.backend.backend import local_jar_information; from hail.backend.spark_backend import SparkBackend. spark_conf = None; if isinstance(Env.backend(), SparkBackend):; spark_conf = spark_context()._conf.getAll(); return {'spark_conf': spark_conf, 'local_jar_information': local_jar_information(), 'version': version()}. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:27839,update,updated,27839,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['update'],['updated']
Deployability,"ndow); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:11461,pipeline,pipeline,11461,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['pipeline'],['pipeline']
Deployability,"ne entry in some data_field_name array for every row in; the inputs.; The multi_way_zip_join() method assumes that inputs have distinct; keys. If any input has duplicate keys, the row value that is included; in the result array for that key is undefined. Parameters:. tables (list of Table) – A list of tables to combine; data_field_name (str) – The name of the resulting data field; global_field_name (str) – The name of the resulting global field. n_partitions()[source]; Returns the number of partitions in the table.; Examples; Range tables can be constructed with an explicit number of partitions:; >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The min_partitions argument to import_table() forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline.; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> table_result = table1.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; Table – Table with at most max_partitions partitions. order_by(*exprs)[source]; Sort by the specified fields, defaulting",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:47332,pipeline,pipeline,47332,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipeline']
Deployability,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8296,update,updated,8296,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,1,['update'],['updated']
Deployability,"nfo_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. to_dict()[source]; A serializable representation of this combiner. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:6476,update,updated,6476,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['update'],['updated']
Deployability,"ngineer""""Charade (1963)""5.00e+00; ""entertainment""""American in Paris, An (1951)""5.00e+00; ""executive""""A Chef in Love (1996)""5.00e+00; ""healthcare""""39 Steps, The (1935)""5.00e+00; ""homemaker""""Beautiful Girls (1996)""5.00e+00; ""lawyer""""Anastasia (1997)""5.00e+00; showing top 10 rows. Let’s try to get a deeper understanding of this result. Notice that every movie displayed has an average rating of 5, which means that every person gave these movies the highest rating. Is that unlikely? We can determine how many people rated each of these movies by working backwards and filtering our original movie_data table by fields in highest_rated.; Note that in the second line below, we are taking advantage of the fact that Hail tables are keyed. [19]:. highest_rated = highest_rated.key_by(; highest_rated.occupation, highest_rated.movie). counts_temp = movie_data.filter(; hl.is_defined(highest_rated[movie_data.occupation, movie_data.movie])). counts = counts_temp.group_by(counts_temp.occupation, counts_temp.movie).aggregate(; counts = hl.agg.count()). counts.show(). [Stage 108:> (0 + 1) / 1]. occupationmoviecountsstrstrint64; ""administrator""""A Chef in Love (1996)""1; ""artist""""39 Steps, The (1935)""1; ""doctor""""Alien (1979)""1; ""educator""""Aparajito (1956)""2; ""engineer""""Charade (1963)""1; ""entertainment""""American in Paris, An (1951)""1; ""executive""""A Chef in Love (1996)""1; ""healthcare""""39 Steps, The (1935)""1; ""homemaker""""Beautiful Girls (1996)""1; ""lawyer""""Anastasia (1997)""1; showing top 10 rows. So it looks like the highest rated movies, when computed naively, mostly have a single viewer rating them. To get a better understanding of the data, we can recompute this list but only include movies which have more than 1 viewer (left as an exercise). Exercises. What is the favorite movie for each occupation, conditional on there being more than one viewer?; What genres are rated most differently by men and women?. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:14316,update,updated,14316,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['update'],['updated']
Deployability,"nning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:132234,integrat,integrate,132234,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['integrat'],['integrate']
Deployability,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:7939,pipeline,pipelines,7939,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,4,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_ALL_Add. View page source. giant_whr_exome_C_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_ALL_Rec. View page source. giant_whr_exome_C_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_ALL_Add. View page source. giant_whr_exome_M_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_ALL_Rec. View page source. giant_whr_exome_M_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_ALL_Add. View page source. giant_whr_exome_W_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_ALL_Rec. View page source. giant_whr_exome_W_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Pancreas_all_snp_gene_associations. View page source. GTEx_eQTL_Pancreas_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html:9682,update,updated,9682,docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Prostate_all_snp_gene_associations. View page source. GTEx_eQTL_Prostate_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html:9682,update,updated,9682,docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. View page source. GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html:9793,update,updated,9793,docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. View page source. GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html:9793,update,updated,9793,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. :math:`X_{i,s}` is calculated by invoking; :meth:`~.CallExpression.n_alt_alleles` on the `call_expr`. The three counts above, :math:`N^{Aa}`, :math:`N^{Aa,Aa}`, and; :math:`N^{AA,aa}`, exclude variants where one or both individuals have; missing genotypes. In terms of the symbols above, we can define :math:`d`, the genetic distance; between two samples. We can interpret :math:`d` as an unnormalized; measurement of the genetic material not shared identically-by-descent:. .. math::. d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2. In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. +-------------------------------+----------+----------+----------+; |:math:`(X_{i,s} - X_{j,s})^2` |homref |het |homalt |; +-------------------------------+----------+----------+----------+; |homref |0 |1 |4 |; +-------------------------------+----------+----------+----------+; |het |1 |0 |1 |; +-------------------------------+----------+----------+----------+; |homalt |4 |1 |0 |; +-------------------------------+----------+----------+----------+. which leads to this expression for genetic distance:. .. math::. d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}. The first term, :math:`4 N^{AA,aa}_{i,j}`, accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:2844,configurat,configurations,2844,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,1,['configurat'],['configurations']
Deployability,"np_dtype} could not be converted to a hail type.""). def dtypes_from_pandas(pd_dtype):; if type(pd_dtype) == pd.StringDtype:; return hl.tstr; elif pd_dtype == np.int64:; return hl.tint64; elif pd_dtype == np.uint64:; # Hail does *not* support unsigned integers but the next condition,; # pd.api.types.is_integer_dtype(pd_dtype) would return true on unsigned 64-bit ints; return None; # For some reason pandas doesn't have `is_int32_dtype`, so we use `is_integer_dtype` if first branch failed.; elif pd.api.types.is_integer_dtype(pd_dtype):; return hl.tint32; elif pd_dtype == np.float32:; return hl.tfloat32; elif pd_dtype == np.float64:; return hl.tfloat64; elif pd_dtype == bool:; return hl.tbool; return None. class tvariable(HailType):; _cond_map: ClassVar = {; 'numeric': is_numeric,; 'int32': lambda x: x == tint32,; 'int64': lambda x: x == tint64,; 'float32': lambda x: x == tfloat32,; 'float64': lambda x: x == tfloat64,; 'locus': lambda x: isinstance(x, tlocus),; 'struct': lambda x: isinstance(x, tstruct),; 'union': lambda x: isinstance(x, tunion),; 'tuple': lambda x: isinstance(x, ttuple),; }. def __init__(self, name, cond):; self.name = name; self.cond = cond; self.condf = tvariable._cond_map[cond] if cond else None; self.box = Box.from_name(name). def unify(self, t):; if self.condf and not self.condf(t):; return False; return self.box.unify(t). def clear(self):; self.box.clear(). def subst(self):; return self.box.get(). def __str__(self):; s = '?' + self.name; if self.cond:; s = s + ':' + self.cond; return s. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, HailType):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:56984,patch,patch,56984,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,"['patch', 'update']","['patch', 'updated']"
Deployability,"nsecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52619,deploy,deploy,52619,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"nsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1195,update,update,1195,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,2,"['install', 'update']","['install', 'update']"
Deployability,"ntDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2442,patch,patch,2442,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['patch'],['patch']
Deployability,"nt_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Coronary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html:9703,update,updated,9703,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nt_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations. View page source. GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html:9703,update,updated,9703,docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nt_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. View page source. GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html:9814,update,updated,9814,docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ntheses appropriately. A single ‘&’ denotes logical AND and a single ‘|’ denotes logical OR. [9]:. users.aggregate(hl.agg.filter((users.occupation == 'writer') | (users.occupation == 'executive'), hl.agg.count())). [9]:. 77. [10]:. users.aggregate(hl.agg.filter((users.sex == 'F') | (users.occupation == 'executive'), hl.agg.count())). [10]:. 302. hist; As we saw in the first tutorial, hist can be used to build a histogram over numeric data. [11]:. hist = users.aggregate(hl.agg.hist(users.age, 10, 70, 60)); hist. [11]:. Struct(bin_edges=[10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0], bin_freq=[1, 1, 0, 5, 3, 6, 5, 14, 18, 23, 32, 27, 37, 28, 33, 38, 34, 35, 36, 32, 39, 25, 28, 26, 17, 27, 21, 19, 17, 22, 21, 10, 21, 13, 23, 15, 12, 14, 20, 19, 20, 20, 6, 12, 4, 11, 6, 9, 3, 3, 9, 3, 2, 3, 2, 3, 1, 0, 2, 5], n_smaller=1, n_larger=1). [12]:. p = hl.plot.histogram(hist, legend='Age'); show(p). take and collect; There are a few aggregators for collecting values. take localizes a few values into an array. It has an optional ordering.; collect localizes all values into an array.; collect_as_set localizes all unique values into a set. [13]:. users.aggregate(hl.agg.take(users.occupation, 5)). [13]:. ['technician', 'other', 'writer', 'technician', 'other']. [14]:. users.aggregate(hl.agg.take(users.age, 5, ordering=-users.age)). [14]:. [73, 70, 70, 70, 69]. Warning! Aggregators like collect and counter return Python objects and can fail with out of memory errors if you apply them to collections that are too large (e.g. all 50 trillion genotypes in the UK Biobank dataset). [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/04-aggregation.html:6535,update,updated,6535,docs/0.2/tutorials/04-aggregation.html,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html,1,['update'],['updated']
Deployability,"o the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. Se",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28151,update,updated,28151,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['updated']
Deployability,"oat64,; AF_EAS: float64,; AF_AMR: float64,; AF_SAS: float64,; AF_AFR: float64,; HWE_EUR: float64,; HWE_EAS: float64,; HWE_AMR: float64,; HWE_SAS: float64,; HWE_AFR: float64,; HWE: float64,; ExcHet_EUR: float64,; ExcHet_EAS: float64,; ExcHet_AMR: float64,; ExcHet_SAS: float64,; ExcHet_AFR: float64,; ExcHet: float64,; ME: float64,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AC_Hom_EUR_unrel: int32,; AC_Hom_EAS_unrel: int32,; AC_Hom_AMR_unrel: int32,; AC_Hom_SAS_unrel: int32,; AC_Hom_AFR_unrel: int32,; AC_Het_EUR_unrel: int32,; AC_Het_EAS_unrel: int32,; AC_Het_AMR_unrel: int32,; AC_Het_SAS_unrel: int32,; AC_Het_AFR_unrel: int32,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64,; HWE_EUR_unrel: float64,; HWE_EAS_unrel: float64,; HWE_AMR_unrel: float64,; HWE_SAS_unrel: float64,; HWE_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html:12998,update,updated,12998,docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html,2,['update'],['updated']
Deployability,"odegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:75900,install,installation,75900,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installation']
Deployability,"odels genotypes of individuals from a structured; population comprising \(K\) homogeneous modern populations that have; each diverged from a single ancestral population (a star phylogeny). Each; sample is assigned a population by sampling from the categorical; distribution \(\pi\). Note that the actual size of each population is; random.; Variants are modeled as biallelic and unlinked. Ancestral allele; frequencies are drawn independently for each variant from a frequency; spectrum \(P_0\). The extent of genetic drift of each modern population; from the ancestral population is defined by the corresponding \(F_{ST}\); parameter \(F_k\) (here and below, lowercase indices run over a range; bounded by the corresponding uppercase parameter, e.g. \(k = 1, \ldots,; K\)). For each variant and population, allele frequencies are drawn from a; beta distribution; whose parameters are determined by the ancestral allele frequency and; \(F_{ST}\) parameter. The beta distribution gives a continuous; approximation of the effect of genetic drift. We denote sample population; assignments by \(k_n\), ancestral allele frequencies by \(p_m\),; population allele frequencies by \(p_{k, m}\), and diploid, unphased; genotype calls by \(g_{n, m}\) (0, 1, and 2 correspond to homozygous; reference, heterozygous, and homozygous variant, respectively).; The generative model is then given by:. \[\begin{aligned}; k_n \,&\sim\, \pi \\; p_m \,&\sim\, P_0 \\; p_{k,m} \mid p_m\,&\sim\, \mathrm{Beta}(\mu = p_m,\, \sigma^2 = F_k p_m (1 - p_m)) \\; g_{n,m} \mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}); \end{aligned}. \]; The beta distribution by its mean and variance above; the usual parameters; are \(a = (1 - p) \frac{1 - F}{F}\) and \(b = p \frac{1 - F}{F}\) with; \(F = F_k\) and \(p = p_m\).; The resulting dataset has the following fields.; Global fields:. bn.n_populations (tint32) – Number of populations.; bn.n_samples (tint32) – Number of samples.; bn.n_variants (tint32) – Number of v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:12521,continuous,continuous,12521,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['continuous'],['continuous']
Deployability,"oerce_endpoint(point):; if point.dtype == key_typ[0]:; point = hl.struct(**{key_names[0]: point}); ts = point.dtype; if isinstance(ts, tstruct):; i = 0; while i < len(ts):; if i >= len(key_typ):; raise ValueError(; f""query_table: queried with {len(ts)} key field(s), but table only has {len(key_typ)} key field(s)""; ); if key_typ[i] != ts[i]:; raise ValueError(; f""query_table: key mismatch at key field {i} ({list(ts.keys())[i]!r}): query type is {ts[i]}, table key type is {key_typ[i]}""; ); i += 1. if i == 0:; raise ValueError(""query_table: cannot query with empty key""). point_size = builtins.len(point.dtype); return hl.tuple([; hl.struct(**{; key_names[i]: (point[i] if i < point_size else hl.missing(key_typ[i])); for i in builtins.range(builtins.len(key_typ)); }),; hl.int32(point_size),; ]); else:; raise ValueError(; f""query_table: key mismatch: cannot query a table with key ""; f""({', '.join(builtins.str(x) for x in key_typ.values())}) with query point type {point.dtype}""; ). if point_or_interval.dtype != key_typ[0] and isinstance(point_or_interval.dtype, hl.tinterval):; partition_interval = hl.interval(; start=coerce_endpoint(point_or_interval.start),; end=coerce_endpoint(point_or_interval.end),; includes_start=point_or_interval.includes_start,; includes_end=point_or_interval.includes_end,; ); else:; point = coerce_endpoint(point_or_interval); partition_interval = hl.interval(start=point, end=point, includes_start=True, includes_end=True); return construct_expr(; ir.ToArray(ir.ReadPartition(partition_interval._ir, reader=ir.PartitionNativeIntervalReader(path, row_typ))),; type=hl.tarray(row_typ),; indices=partition_interval._indices,; aggregations=partition_interval._aggregations,; ). @typecheck(msg=expr_str, result=expr_any); def _console_log(msg, result):; indices, aggregations = unify_all(msg, result); return construct_expr(ir.ConsoleLog(msg._ir, result._ir), result.dtype, indices, aggregations). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:188705,update,updated,188705,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['update'],['updated']
Deployability,"of .vcf file to write.; append_to_header (str or None) – Path of file to append to VCF header.; export_pp (bool) – If true, export linear-scaled probabilities (Hail’s pp field on genotype) as the VCF PP FORMAT field.; parallel (bool) – If true, return a set of VCF files (one per partition) rather than serially concatenating these files. file_version()[source]¶; File version of variant dataset. Return type:int. filter_alleles(expr, annotation='va = va', subset=True, keep=True, filter_altered_genotypes=False, max_shift=100, keep_star=False)[source]¶; Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. aIndex will never be zero). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:; >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of aIndices because; we are mapping between the old and new allele indices, not; the alternate allele indices.; Notes; If filter_altered_genotypes is true, genotypes that contain filtered-out alleles are set to missing.; filter_alleles() implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. Subset algorithm; The subset algorithm (the default, subset=True) subsets the; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:47865,update,update,47865,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['update'],['update']
Deployability,"of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:64919,pipeline,pipeline,64919,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,2,['pipeline'],['pipeline']
Deployability,"of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.build_python_image; Build a new Python image with dill and the specified pip packages installed. utils.concatenate; Concatenate files using tree aggregation. utils.plink_merge; Merge binary PLINK files using tree aggregation. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:3766,install,installed,3766,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,1,['install'],['installed']
Deployability,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:10105,configurat,configuration,10105,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; Schema (0.2, GRCh37). panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_AFR. View page source. panukb_ld_scores_AFR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,1,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; Schema (0.2, GRCh37). panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_AMR. View page source. panukb_ld_scores_AMR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html,1,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; Schema (0.2, GRCh37). panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_CSA. View page source. panukb_ld_scores_CSA. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html,1,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; Schema (0.2, GRCh37). panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_EAS. View page source. panukb_ld_scores_EAS. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html,1,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; Schema (0.2, GRCh37). panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_EUR. View page source. panukb_ld_scores_EUR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html,1,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; Schema (0.2, GRCh37). panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_MID. View page source. panukb_ld_scores_MID. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_MID.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_MID.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_MID.html,1,['update'],['updated']
Deployability,"omad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. View page source. GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html:9730,update,updated,9730,docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"omad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html:9730,update,updated,9730,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"omad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Whole_Blood_all_snp_gene_associations. View page source. GTEx_sQTL_Whole_Blood_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html:9748,update,updated,9748,docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"omogeneous modern populations that have; each diverged from a single ancestral population (a `star phylogeny`). Each; sample is assigned a population by sampling from the categorical; distribution :math:`\pi`. Note that the actual size of each population is; random. Variants are modeled as biallelic and unlinked. Ancestral allele; frequencies are drawn independently for each variant from a frequency; spectrum :math:`P_0`. The extent of genetic drift of each modern population; from the ancestral population is defined by the corresponding :math:`F_{ST}`; parameter :math:`F_k` (here and below, lowercase indices run over a range; bounded by the corresponding uppercase parameter, e.g. :math:`k = 1, \ldots,; K`). For each variant and population, allele frequencies are drawn from a; `beta distribution <https://en.wikipedia.org/wiki/Beta_distribution>`__; whose parameters are determined by the ancestral allele frequency and; :math:`F_{ST}` parameter. The beta distribution gives a continuous; approximation of the effect of genetic drift. We denote sample population; assignments by :math:`k_n`, ancestral allele frequencies by :math:`p_m`,; population allele frequencies by :math:`p_{k, m}`, and diploid, unphased; genotype calls by :math:`g_{n, m}` (0, 1, and 2 correspond to homozygous; reference, heterozygous, and homozygous variant, respectively). The generative model is then given by:. .. math::; \begin{aligned}; k_n \,&\sim\, \pi \\; p_m \,&\sim\, P_0 \\; p_{k,m} \mid p_m\,&\sim\, \mathrm{Beta}(\mu = p_m,\, \sigma^2 = F_k p_m (1 - p_m)) \\; g_{n,m} \mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}); \end{aligned}. The beta distribution by its mean and variance above; the usual parameters; are :math:`a = (1 - p) \frac{1 - F}{F}` and :math:`b = p \frac{1 - F}{F}` with; :math:`F = F_k` and :math:`p = p_m`. The resulting dataset has the following fields. Global fields:. - `bn.n_populations` (:py:data:`.tint32`) -- Number of populations.; - `bn.n_samples` (:py:data:`.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:146646,continuous,continuous,146646,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['continuous'],['continuous']
Deployability,"on Compatibility Policy; Change Log. Batch. Docker Resources. View page source. Docker Resources. What is Docker?; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1325,install,installed,1325,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['installed']
Deployability,"on-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:101781,install,installing,101781,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['install'],['installing']
Deployability,"on_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. Parameters:; config (str) – Path to VEP configuration file.; block_size (int) – Number of variants to annotate per VEP invocation.; root (str) – Variant annotation path to store VEP output.; csq (bool) – If True, annotates VCF CSQ field as a String.; If False, annotates with the full nested struct schema. Returns:An annotated with variant annotations from VEP. Return type:VariantDataset. was_split()[source]¶; True if multiallelic variants have been split into multiple biallelic variants.; Result is True if split_multi() or filter_multi() has been called on this variant dataset,; or if the variant dataset was imported with import_plink(), import_gen(),; or import_bgen(), or if the variant dataset was simulated with balding_nichols_model(). Return type:bool. write(output, overwrite=False, parquet_genotypes=False)[source]¶; Write variant dataset as VDS file.; Examples; Import data from a VCF file and then write the data to a VDS file:; >>> vds.write(""output/sample.vds""). Parameters:; output (str) – Path of VDS file t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:179572,configurat,configuration,179572,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['configurat'],['configuration']
Deployability,"onal[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word argumen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4297,configurat,configuration,4297,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,1,['configurat'],['configuration']
Deployability,"onary keyed by results of f. hail.expr.functions.fold(f, zero, collection)[source]; Reduces a collection with the given function f, provided the initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; collection (ArrayExpression or SetExpression). Returns:; Expression. hail.expr.functions.array_scan(f, zero, a)[source]; Map each element of a to cumulative value of function f, with initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; a (ArrayExpression). Returns:; ArrayExpression. hail.expr.functions.reversed(x)[source]; Reverses the elements of a collection.; Examples; >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.reversed(a)); ['fox', 'brown', 'quick', 'The']. Parameters:; x (ArrayExpression or StringExpression) – Array or string expression. Returns:; Expression. hail.expr.functions.keyed_intersection(*arrays, key)[source]; Compute the intersection of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. arrays; key. Returns:; ArrayExpression. hail.expr.functions.keyed_union(*arrays, key)[source]; Compute the distinct union of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. exprs; key. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/collections.html:13994,update,updated,13994,docs/0.2/functions/collections.html,https://hail.is,https://hail.is/docs/0.2/functions/collections.html,1,['update'],['updated']
Deployability,"onding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc describe. Deprecations. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:64617,install,install,64617,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['install']
Deployability,"ondrial contigs. Returns:; list of str. property name; Name of reference genome. Returns:; str. property par; Pseudoautosomal regions. Returns:; list of Interval. classmethod read(path)[source]; Load reference genome from a JSON file.; Notes; The JSON file must have the following format:; {""name"": ""my_reference_genome"",; ""contigs"": [{""name"": ""1"", ""length"": 10000000},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. name must be unique and not overlap with Hail’s pre-instantiated; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'.; The contig names in xContigs, yContigs, and mtContigs must be; present in contigs. The intervals listed in par must have contigs in; either xContigs or yContigs and must have positions between 0 and; the contig length given in contigs. Parameters:; path (str) – Path to JSON file. Returns:; ReferenceGenome. remove_liftover(dest_reference_genome)[source]; Remove liftover to dest_reference_genome. Parameters:; dest_reference_genome (str or ReferenceGenome). remove_sequence()[source]; Remove the reference sequence. write(output)[source]; “Write this reference genome to a file in JSON format.; Examples; >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; Use read() to reimport the exported; reference genome in a new HailContext session. Parameters:; output (str) – Path of JSON file to write. property x_contigs; X contigs. Returns:; list of str. property y_contigs; Y contigs. Returns:; list of str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:10210,update,updated,10210,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['update'],['updated']
Deployability,"one, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2748,configurat,configuration,2748,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,"onfiguration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Overview: module code. All modules for which code is available; hail.context; hail.experimental.datasets; hail.experimental.db; hail.experimental.export_entries_by_col; hail.experimental.expressions; hail.experimental.filtering_allele_frequency; hail.experimental.full_outer_join_mt; hail.experimental.import_gtf; hail.experimental.ld_score_regression; hail.experimental.ldscore; hail.experimental.ldscsim; hail.experimental.loop; hail.experimental.pca; hail.experimental.phase_by_transmission; hail.experimental.plots; hail.experimental.tidyr; hail.experimental.time; hail.expr.aggregators.aggregators; hail.expr.builders; hail.expr.expressions.base_expression; hail.expr.expressions.expression_utils; hail.expr.expressions.typed_expressions; hail.expr.functions; hail.expr.types; hail.genetics.allele_type; hail.genetics.call; hail.genetics.locus; hail.genetics.pedigree; hail.genetics.reference_genome; hail.ggplot.aes; hail.ggplot.coord_cartesian; hail.ggplot.facets; hail.ggplot.geoms; hail.ggplot.ggplot; hail.ggplot.labels; hail.ggplot.scale; hail.linalg.blockmatrix; hail.linalg.utils.misc; hail.matrixtable; hail.methods.family_methods; hail.methods.impex; hail.methods.misc; hail.methods.pca; hail.methods.qc; hail.methods.relatedness.identity_by_descent; hail.methods.relatedness.king; hail.methods.relatedness.mating_simulation; hail.methods.relatedness.pc_relate; hail.methods.statgen; hail.nd.nd; hail.plot.plots; hail.stats.linear_mixed_model; hail.table; hail.utils.hadoop_utils; hail.utils.interval; hail.utils.misc; hail.utils.struct; hail.utils.tutorial; hail.vds.combiner.variant_dataset_combiner; hail.vds.functions; hail.vds.methods; hail.vds.sample_qc; hail.vds.variant_dataset; hailtop.frozendict; hailtop.fs.fs_utils. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/index.html:2200,update,updated,2200,docs/0.2/_modules/index.html,https://hail.is,https://hail.is/docs/0.2/_modules/index.html,1,['update'],['updated']
Deployability,"onment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223269,configurat,configuration,223269,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['configurat'],['configuration']
Deployability,"ons (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:84199,update,update,84199,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"ons(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum` (for non-missing values) as:. >>> ht.aggregate(hl.agg.fold(0, lambda accum: accum + ht.idx, lambda comb_left, comb_right: comb_left + comb_right)); 4950. Parameters; ----------; initial_value : :class:`.Expression`; The initial value to start the aggregator with. This is a value of type `A`.; seq_op : function ( (:class:`.Expression`) -> :class:`.Expression`); The function used to combine the current aggregator state with the next element you're aggregating over. Type is; `A => A`; comb_op : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); The function used to combine two aggregator states together and produce final result. Type is `(A, A) => A`.; """""". return _agg_func._fold(initial_value, seq_op, comb_op). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:62023,update,updated,62023,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['update'],['updated']
Deployability,"option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Stor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:5329,configurat,configuration,5329,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,"ore/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:6890,configurat,configuration,6890,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['configurat'],['configuration']
Deployability,"ores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkCo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7160,configurat,configuration,7160,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"ort stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previous",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12460,upgrade,upgrade,12460,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['upgrade'],['upgrade']
Deployability,"ory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12140,update,update,12140,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['install', 'update']","['installing', 'update']"
Deployability,"ot exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return _fses[requester_pays_config].ls(path). [docs]def mkdir(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Ensure files can be created whose dirname is `path`. Warning; -------. On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. """"""; _fses[requester_pays_config].mkdir(path). [docs]def remove(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Removes the file at `path`. If the file does not exist, this function does; nothing. `path` must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters; ----------; path : :class:`str`; """"""; _fses[requester_pays_config].remove(path). [docs]def rmtree(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Recursively remove all files under the given `path`. On a local filesystem,; this removes the directory tree at `path`. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with `path`. As such,; `path` must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters; ----------; path : :class:`str`; """"""; _fses[requester_pays_config].rmtree(path). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:8117,update,updated,8117,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,1,['update'],['updated']
Deployability,"otate_fields[coords] = coord_expr; else:; coords = src._fields_inverse[coord_expr]. if isinstance(src, hl.MatrixTable):; new_src = src.annotate_rows(**annotate_fields); else:; new_src = src.annotate(**annotate_fields). locus_expr = new_src[locus]; if coord_expr is not None:; coord_expr = new_src[coords]. if coord_expr is None:; coord_expr = locus_expr.position. rg = locus_expr.dtype.reference_genome; contig_group_expr = hl.agg.group_by(hl.locus(locus_expr.contig, 1, reference_genome=rg), hl.agg.collect(coord_expr)). # check loci are in sorted order; last_pos = hl.fold(; lambda a, elt: (; hl.case(); .when(a <= elt, elt); .or_error(; hl.str(""locus_windows: 'locus_expr' global position must be in ascending order. ""); + hl.str(a); + hl.str("" was not less then or equal to ""); + hl.str(elt); ); ),; -1,; hl.agg.collect(; hl.case(); .when(hl.is_defined(locus_expr), locus_expr.global_position()); .or_error(""locus_windows: missing value for 'locus_expr'.""); ),; ); checked_contig_groups = (; hl.case().when(last_pos >= 0, contig_group_expr).or_error(""locus_windows: 'locus_expr' has length 0""); ). contig_groups = locus_expr._aggregation_method()(checked_contig_groups, _localize=False). coords = hl.sorted(hl.array(contig_groups)).map(lambda t: t[1]); starts_and_stops = hl._locus_windows_per_contig(coords, radius). if not _localize:; return starts_and_stops. starts, stops = hl.eval(starts_and_stops); return np.array(starts), np.array(stops). def _check_dims(a, name, ndim, min_size=1):; if len(a.shape) != ndim:; raise ValueError(f'{name} must be {ndim}-dimensional, ' f'found {a.ndim}'); for i in range(ndim):; if a.shape[i] < min_size:; raise ValueError(f'{name}.shape[{i}] must be at least ' f'{min_size}, found {a.shape[i]}'). def _ndarray_matmul_ndim(left, right):; if left == 1 and right == 1:; return 0; elif left == 1:; return right - 1; elif right == 1:; return left - 1; else:; assert left == right; return left. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html:8635,update,updated,8635,docs/0.2/_modules/hail/linalg/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html,1,['update'],['updated']
Deployability,"otly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesthetic_name, name, None, None). if isinstance(reference_genome, str):; reference_genome = get_reference(reference_genome); self.reference_genome = reference_genome. def apply_to_fig(self, parent, fig_so_far):; contig_offsets = dict(list(self.reference_genome.global_positions_dict.items())[:24]); breaks = list(contig_offsets.values()); labels = list(contig_offsets.keys()); self.update_axis(fig_so_far)(tickvals=breaks, ticktext=labels). def transform_data(self, field_expr):; return field_expr.global_position(). def is_discrete(self):; r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:1509,continuous,continuous,1509,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['continuous'],['continuous']
Deployability,"ow(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89419,pipeline,pipeline,89419,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipeline']
Deployability,"own ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound*",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76539,install,installing,76539,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['install'],['installing']
Deployability,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:104097,pipeline,pipelines,104097,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:31856,pipeline,pipelines,31856,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"patible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1737,install,install,1737,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['install'],['install']
Deployability,"pe, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; refe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31419,update,update,31419,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"pects a table with interval keys""); point_type = ht.key[0].dtype.point_type; if isinstance(points, Table):; if len(points.key) != 1 or points.key[0].dtype != point_type:; raise ValueError(; ""'segment_intervals' expects points to be a table with a single""; "" key of the same type as the intervals in 'ht', or an array of those points:""; f""\n expect {point_type}, found {list(points.key.dtype.values())}""; ); points = hl.array(hl.set(points.collect(_localize=False))); if points.dtype.element_type != point_type:; raise ValueError(; f""'segment_intervals' expects points to be a table with a single""; f"" key of the same type as the intervals in 'ht', or an array of those points:""; f""\n expect {point_type}, found {points.dtype.element_type}""; ). points = hl._sort_by(points, lambda l, r: hl._compare(l, r) < 0). ht = ht.annotate_globals(__points=points). interval = ht.key[0]; points = ht.__points; lower = hl.expr.functions._lower_bound(points, interval.start); higher = hl.expr.functions._lower_bound(points, interval.end); n_points = hl.len(points); lower = hl.if_else((lower < n_points) & (points[lower] == interval.start), lower + 1, lower); higher = hl.if_else((higher < n_points) & (points[higher] == interval.end), higher - 1, higher); interval_results = hl.rbind(; lower,; higher,; lambda lower, higher: hl.if_else(; lower >= higher,; [interval],; hl.flatten([; [; hl.interval(; interval.start, points[lower], includes_start=interval.includes_start, includes_end=False; ); ],; hl.range(lower, higher - 1).map(; lambda x: hl.interval(points[x], points[x + 1], includes_start=True, includes_end=False); ),; [; hl.interval(; points[higher - 1], interval.end, includes_start=True, includes_end=interval.includes_end; ); ],; ]),; ),; ); ht = ht.annotate(__new_intervals=interval_results, lower=lower, higher=higher).explode('__new_intervals'); return ht.key_by(**{next(iter(ht.key)): ht.__new_intervals}).drop('__new_intervals'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:16992,update,updated,16992,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,1,['update'],['updated']
Deployability,"perimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2582,configurat,configuration,2582,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,"plot.scatter(x, y, label=None, title=None, xlabel=None, ylabel=None, size=4, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create an interactive scatter plot.; x and y must both be either:; - a NumericExpression from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:7072,continuous,continuous,7072,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['continuous'],['continuous']
Deployability,"population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; DP: int32,; END: int32,; SVTYPE: str,; AA: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: str,; RSID_REMOVED: str,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html:11261,update,updated,11261,docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html,1,['update'],['updated']
Deployability,"pruned_table.locus, bp_window_size). entries = r2_bm.sparsify_row_intervals(range(stops.size), stops, blocks_only=True).entries(keyed=False); entries = entries.filter((entries.entry >= r2) & (entries.i < entries.j)); entries = entries.select(i=hl.int32(entries.i), j=hl.int32(entries.j)). if keep_higher_maf:; fields = ['mean', 'locus']; else:; fields = ['locus']. info = locally_pruned_table.aggregate(; hl.agg.collect(locally_pruned_table.row.select('idx', *fields)), _localize=False; ); info = hl.sorted(info, key=lambda x: x.idx). entries = entries.annotate_globals(info=info). entries = entries.filter(; (entries.info[entries.i].locus.contig == entries.info[entries.j].locus.contig); & (entries.info[entries.j].locus.position - entries.info[entries.i].locus.position <= bp_window_size); ). if keep_higher_maf:; entries = entries.annotate(; i=hl.struct(; idx=entries.i, twice_maf=hl.min(entries.info[entries.i].mean, 2.0 - entries.info[entries.i].mean); ),; j=hl.struct(; idx=entries.j, twice_maf=hl.min(entries.info[entries.j].mean, 2.0 - entries.info[entries.j].mean); ),; ). def tie_breaker(left, right):; return hl.sign(right.twice_maf - left.twice_maf). else:; tie_breaker = None. variants_to_remove = hl.maximal_independent_set(; entries.i, entries.j, keep=False, tie_breaker=tie_breaker, keyed=False; ). locally_pruned_table = locally_pruned_table.annotate_globals(; variants_to_remove=variants_to_remove.aggregate(; hl.agg.collect_as_set(variants_to_remove.node.idx), _localize=False; ); ); return (; locally_pruned_table.filter(; locally_pruned_table.variants_to_remove.contains(hl.int32(locally_pruned_table.idx)), keep=False; ); .select(); .persist(); ). def _warn_if_no_intercept(caller, covariates):; if all([e._indices.axes for e in covariates]):; warning(; f'{caller}: model appears to have no intercept covariate.'; '\n To include an intercept, add 1.0 to the list of covariates.'; ); return True; return False. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:172830,update,updated,172830,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['updated']
Deployability,"pulation; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al. “Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services”, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/ar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:9778,integrat,integrate,9778,references.html,https://hail.is,https://hail.is/references.html,1,['integrat'],['integrate']
Deployability,"p. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expression passed to; aggregate. Other column fields and entry fields are dropped. Aggregate Per Row Group. description:; Compute the number of calls with one or more non-reference; alleles per gene group. code:; >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). dependencies:; MatrixTable.group_rows_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the rows of the matrix table by the row-indexed field gene; using MatrixTable.group_rows_by(), which returns a; GroupedMatrixTable. Then use GroupedMatrixTable.aggregate(); to compute an aggregation per grouped row.; The result is a matrix table with an entry field n_non_ref that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to group_rows_by, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to aggregate. Other row fields and entry fields; are dropped. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:7072,update,updated,7072,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['update'],['updated']
Deployability,"quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; dbSNP. View page source. dbSNP. Versions: 154; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (154, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; RS: int32,; GENEINFO: str,; PSEUDOGENEINFO: str,; dbSNPBuildID: int32,; SAO: int32,; SSR: int32,; VC: str,; PM: bool,; NSF: bool,; NSM: bool,; NSN: bool,; SYN: bool,; U3: bool,; U5: bool,; ASS: bool,; DSS: bool,; INT: bool,; R3: bool,; R5: bool,; GNO: bool,; PUB: bool,; FREQ: struct {; _GENOME_DK: float64,; _TWINSUK: float64,; _dbGaP_PopFreq: float64,; _Siberian: float64,; _Chileans: float64,; _FINRISK: float64,; _HapMap: float64,; _Estonian: float64,; _ALSPAC: float64,; _GoESP: float64,; _TOPMED: float64,; _PAGE_STUDY: float64,; _1000Genomes: float64,; _Korea1K: float64,; _ChromosomeY: float64,; _ExAC: float64,; _Qatari: float64,; _GoNL: float64,; _MGP: float64,; _GnomAD: float64,; _Vietnamese: float64,; _GnomAD_exomes: float64,; _PharmGKB: float64,; _KOREAN: float64,; _Daghestan: float64,; _HGDP_Stanford: float64,; _NorthernSweden: float64,; _SGDP_PRJ: float64; },; COMMON: bool,; CLNHGVS: array<str>,; CLNVI: array<str>,; CLNORIGIN: array<str>,; CLNSIG: array<str>,; CLNDISDB: array<str>,; CLNDN: array<str>,; CLNREVSTAT: array<str>,; CLNACC: array<str>; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbSNP.html:10632,update,updated,10632,docs/0.2/datasets/schemas/dbSNP.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbSNP.html,1,['update'],['updated']
Deployability,"r -1, because the; discriminant, y, is missing. Switch Statements; Finally, Hail has the switch() function to build a conditional tree based; on the value of an expression. In the example below, csq is a; StringExpression representing the functional consequence of a; mutation. If csq does not match one of the cases specified by; when(), it is set to missing with; or_missing(). Other switch statements are documented in the; SwitchBuilder class.; >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ... .when(""indel"", True); ... .or_missing()); <BooleanExpression of type bool>. As with case statements, missingness will propagate up through a switch; statement. If we changed the value of csq to the missing value; hl.missing(hl.tstr), then the result of the switch statement above would also; be missing. Missingness; In Hail, all expressions can be missing. An expression representing a missing; value of a given type can be generated with the missing() function, which; takes the type as its single argument.; An example of generating a Float64Expression that is missing is:; >>> hl.missing('float64'); <Float64Expression of type float64>. These can be used with conditional statements to set values to missing if they; don’t satisfy a condition:; >>> hl.if_else(x > 2.0, x, hl.missing(hl.tfloat)); <Float64Expression of type float64>. The Python representation of a missing value is None. For example, if; we define cnull to be a missing value with type tcall, calling; the method is_het will return None and not False.; >>> cnull = hl.missing('call'); >>> hl.eval(cnull.is_het()); None. Functions; In addition to the methods exposed on each Expression, Hail also has; numerous functions that can be applied to expressions, which also return an; expression.; Take a look at the Functions page for full documentation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/expressions.html:9046,update,updated,9046,docs/0.2/overview/expressions.html,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html,1,['update'],['updated']
Deployability,"r gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2580,configurat,configuration,2580,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,1,['configurat'],['configuration']
Deployability,"r read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:81108,update,update,81108,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; Schema (0.2, GRCh37). panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_AFR. View page source. panukb_ld_variant_indices_AFR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,1,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; Schema (0.2, GRCh37). panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_AMR. View page source. panukb_ld_variant_indices_AMR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html,1,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; Schema (0.2, GRCh37). panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_CSA. View page source. panukb_ld_variant_indices_CSA. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html,1,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; Schema (0.2, GRCh37). panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_EAS. View page source. panukb_ld_variant_indices_EAS. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html,1,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; Schema (0.2, GRCh37). panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_EUR. View page source. panukb_ld_variant_indices_EUR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html,1,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; Schema (0.2, GRCh37). panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_MID. View page source. panukb_ld_variant_indices_MID. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html,1,['update'],['updated']
Deployability,"r=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.Se",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:4033,install,installed,4033,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['install'],['installed']
Deployability,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22416,pipeline,pipelined,22416,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['pipeline'],['pipelined']
Deployability,"ral chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20495,integrat,integration,20495,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Deployability,"range.end = y_max_densities[label_cols[0]]. callback_args: Dict[str, Any]; callback_args = dict(; scatter_renderers=sp_scatter_renderers,; color_mappers=sp_color_mappers,; density_renderers=x_renderers + y_renderers,; x_range=xp.y_range,; x_max_densities=x_max_densities,; y_range=yp.x_range,; y_max_densities=y_max_densities,; ). callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].visible = true; }. for (var i = 0; i < density_renderers.length; i++){; density_renderers[i][2].visible = density_renderers[i][0] == cb_obj.value; }. x_range.start = 0; y_range.start = 0; x_range.end = x_max_densities[cb_obj.value]; y_range.end = y_max_densities[cb_obj.value]. """""". if legend:; callback_args.update(dict(legend_items=sp_legend_items, legend=sp_legend, color_bar=sp_color_bar)); callback_code += """"""; if (cb_obj.value in legend_items){; legend.items=legend_items[cb_obj.value]; legend.visible=true; color_bar.visible=false; }else{; legend.visible=false; color_bar.visible=true; }. """""". callback = CustomJS(args=callback_args, code=callback_code); select = Select(title=""Color by"", value=label_cols[0], options=label_cols); select.js_on_change('value', callback); first_row.append(select). return gridplot([first_row, [sp, yp]]). [docs]@typecheck(; pvals=expr_numeric,; label=nullable(oneof(dictof(str, expr_any), expr_any)),; title=nullable(str),; xlabel=nullable(str),; ylabel=nullable(str),; size=int,; legend=bool,; hover_fields=nullable(dictof(str, expr_any)),; colors=nullable(oneof(bokeh.models.mappers.ColorMapper, dictof(str, bokeh.models.mappers.ColorMapper))),; width=int,; height=int,; collect_all=nullable(bool),; n_divisions=nullable(int),; missing_label=str,; ); def qq(; pvals: NumericExpression,; label: Optional[Union[Expression, Dict[s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:44839,update,update,44839,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['update'],['update']
Deployability,"rays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#1375",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21687,pipeline,pipelines,21687,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"rce for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45816,pipeline,pipelined,45816,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['pipeline'],['pipelined']
Deployability,"rce_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VD",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30872,update,update,30872,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"red_axis_kwargs(),; }; else:; n_facet_rows = 1; n_facet_cols = 1; subplot_args = {; ""rows"": 1,; ""cols"": 1,; }; fig = make_subplots(**subplot_args). # Need to know what I've added to legend already so we don't do it more than once.; legend_cache = {}. for geom, geom_label, facet_to_grouped_dfs in geoms_and_grouped_dfs_by_facet_idx:; for facet_idx, grouped_dfs in facet_to_grouped_dfs.items():; scaled_grouped_dfs = []; for df in grouped_dfs:; scales_to_consider = list(df.columns) + list(df.attrs); relevant_aesthetics = [scale_name for scale_name in scales_to_consider if scale_name in self.scales]; scaled_df = df; for relevant_aesthetic in relevant_aesthetics:; scaled_df = transformers[relevant_aesthetic](scaled_df); scaled_grouped_dfs.append(scaled_df). facet_row = facet_idx // n_facet_cols + 1; facet_col = facet_idx % n_facet_cols + 1; geom.apply_to_fig(; scaled_grouped_dfs, fig, precomputed[geom_label], facet_row, facet_col, legend_cache, is_faceted; ). # Important to update axes after labels, axes names take precedence.; self.labels.apply_to_fig(fig); if self.scales.get(""x"") is not None:; self.scales[""x""].apply_to_fig(self, fig); if self.scales.get(""y"") is not None:; self.scales[""y""].apply_to_fig(self, fig); if self.coord_cartesian is not None:; self.coord_cartesian.apply_to_fig(fig). fig = fig.update_xaxes(title_font_size=18, ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:9337,update,update,9337,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,1,['update'],['update']
Deployability,"reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None):; """"""Cap reference blocks at a maximum length in order to permit faster interval filtering. Examples; --------; Truncate reference blocks to 5 kilobases:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) # doctest: +SKIP. Truncate the longest 1% of reference blocks to the length of the 99th percentile block:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) # doctest: +SKIP. Notes; -----; After this function has been run, the reference blocks have a known maximum length `ref_block_max_length`,; stored in the global fields, which permits :func:`.vds.filter_intervals` to filter to intervals of the reference; data by reading `ref_block_max_length` bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work. It is also possible to patch an existing VDS to store the max reference block length with :func:`.vds.store_ref_block_max_length`. See Also; --------; :func:`.vds.store_ref_block_max_length`. Parameters; ----------; vds : :class:`.VariantDataset` or :class:`.MatrixTable`; max_ref_block_base_pairs; Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction; Fraction of reference block length distribution to truncate / winsorize. Returns; -------; :class:`.VariantDataset` or :class:`.MatrixTable`; """"""; if isinstance(ds, VariantDataset):; rd = ds.reference_data; else:; rd = ds. fd_name = hl.vds.VariantDataset.ref_block_max_length_field; if fd_name in rd.globals:; rd = rd.drop(fd_name). if int(ref_block_winsorize_fraction is None) + int(max_ref_block_base_pairs is None) != 1:; raise ValueError(; 'truncate_reference_blocks: require exactly one of ""max_ref_block_base_pairs"", ""ref_block_winsorize_fraction""'; ). if ref_block_winsorize_fraction is not None:; assert (; ref_block_winsorize_fraction > 0 and ref_block_winsoriz",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:33191,patch,patch,33191,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,1,['patch'],['patch']
Deployability,"regate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection – the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using key_cols_by() with no arguments. Warning; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns:; Table – Table with all non-global fields from the matrix, with one row per entry of the matrix. property entry; Returns a struct expression including all row-and-column-indexed fields.; Examples; Get all entry field names:; >>> list(dataset.entry); ['GT', 'AD', 'DP', 'GQ', 'PL']. Returns:; StructExpression – Struct of all entry fields. explode_cols(field_expr)[source]; Explodes a column field of type array or set, copying the entire column for each element.; Examples; Explode columns by annotated cohorts:; >>> dataset_result = dataset.explode_cols(dataset.cohorts). Notes; The new matrix table will have N copies of each column, where N is the; number of elements that column contains for the field denoted by field_expr.; The field referenced in field_expr is replaced in the sequence of duplicated; columns by the sequence of elements in the array or set. All other fields remain; the same, includ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:28666,pipeline,pipeline,28666,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['pipeline'],['pipeline']
Deployability,"related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95258,pipeline,pipelines,95258,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27530,pipeline,pipelines,27530,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"res_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; Schema (3.1, GRCh38). gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_pca_variant_loadings. View page source. gnomad_pca_variant_loadings. Versions: 2.1, 3.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html:9409,update,updated,9409,docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,1,['update'],['updated']
Deployability,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html:9763,update,updated,9763,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Colon_Transverse_all_snp_gene_associations. View page source. GTEx_sQTL_Colon_Transverse_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html:9763,update,updated,9763,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html:9763,update,updated,9763,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixT",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:41977,configurat,configuration,41977,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEP",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:106024,configurat,configuration,106024,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"ritically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11095,pipeline,pipelines,11095,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"rk backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, opt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:3809,pipeline,pipeline,3809,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['pipeline'],['pipeline']
Deployability,"rn construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @deprecated(version=""0.2.58"", reason=""Replaced by first""); def head(self):; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; return self.first(). [docs] def first(self):; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:13923,pipeline,pipeline,13923,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['pipeline'],['pipeline']
Deployability,"rn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; return hl.read_table(self.url)._maybe_flexindex_table_by_expr(indexer_key_expr, all_matches=all_matches). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:5379,configurat,configuration,5379,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['configurat'],"['configuration', 'configurations']"
Deployability,"rocess. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; defa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:11610,configurat,configuration,11610,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"rray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probabili",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20633,integrat,integration,20633,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Deployability,"rror caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89532,update,update,89532,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"rs (keyword args of Expression) – Column-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix, can be used to call GroupedMatrixTable.aggregate(). group_rows_by(*exprs, **named_exprs)[source]; Group rows.; Examples; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:4978,pipeline,pipeline,4978,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['pipeline'],['pipeline']
Deployability,"rs:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; BashJob. new_job(name=None, attributes=None, shell=None); Alias for Batch.new_bash_job(). Return type:; BashJob. new_python_job(name=None, attributes=None); Initialize a new PythonJob object with default; Python image, memory, storage, and CPU settings (defined in Batch); upon batch creation.; Examples; Create and execute a batch b with one job j that prints “hello alice”:; b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; The image to use for Python jobs can be specified by default_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. E",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:7268,install,installed,7268,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['install'],['installed']
Deployability,"rt modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:221769,configurat,configuration,221769,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['configurat'],['configuration']
Deployability,"ruct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_qual_hists': struct {; gq_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'gnomad_age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; 'AS_lowqual': bool; 'telomere_or_centromere': bool; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html:22976,update,updated,22976,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html,1,['update'],['updated']
Deployability,"s 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:2034,install,installed,2034,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['install'],['installed']
Deployability,"s \in S_{ij}}; \widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \widehat{k^{(0)}_{ij}} \coloneqq; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:7764,release,release,7764,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,1,['release'],['release']
Deployability,"s can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in favor of the https scheme and will be removed in a future; release. Bug Fixes. (#12919) An; interactive hail session is no longer unusable after hitting CTRL-C; during a batch execution in Query-on-Batch; (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc j",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:35961,patch,patch,35961,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['patch'],['patch']
Deployability,"s every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:9485,release,released,9485,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['released']
Deployability,"s in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point.; (#12901); hl.Struct now has a correct and useful implementation of; pprint. Version 0.2.115; Released 2023-04-25. New Features. (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in favor of the https scheme and will be removed in a future; release. Bug Fixes. (#12919) An; interactive hail session is no longer unusable after hitting CTRL-C; during a batch execution in Query-on-Batch; (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarray",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:35500,release,release,35500,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"s in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28168,update,update,28168,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"s the modified `alleles`; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155564,update,update,155564,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['update']
Deployability,"s written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77592,pipeline,pipelines,77592,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"s); return self._copy(col_keys=col_key, computed_col_key=computed_key). def _check_bindings(self, caller, new_bindings, indices):; empty = []. def iter_option(o):; return o if o is not None else empty. if indices == self._parent._row_indices:; fixed_fields = [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:7871,pipeline,pipeline,7871,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['pipeline'],['pipeline']
Deployability,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1981,integrat,integrated,1981,index.html,https://hail.is,https://hail.is/index.html,1,['integrat'],['integrated']
Deployability,"s.; The population distribution \(\pi\) defaults to uniform; The \(F_{ST}\) values default to 0.1; The number of partitions defaults to one partition per million genotypes (i.e., samples * variants / 10^6) or 8, whichever is larger. The Balding-Nichols model models genotypes of individuals from a structured population comprising \(K\) homogeneous subpopulations; that have each diverged from a single ancestral population (a star phylogeny). We take \(N\) samples and \(M\) bi-allelic variants in perfect; linkage equilibrium. The relative sizes of the subpopulations are given by a probability vector \(\pi\); the ancestral allele frequencies are; drawn independently from a frequency spectrum \(P_0\); the subpopulations have diverged with possibly different \(F_{ST}\) parameters \(F_k\); (here and below, lowercase indices run over a range bounded by the corresponding uppercase parameter, e.g. \(k = 1, \ldots, K\)).; For each variant, the subpopulation allele frequencies are drawn a beta distribution, a useful continuous approximation of; the effect of genetic drift. We denote the individual subpopulation memberships by \(k_n\), the ancestral allele frequences by \(p_{0, m}\),; the subpopulation allele frequencies by \(p_{k, m}\), and the genotypes by \(g_{n, m}\). The generative model in then given by:. \[ \begin{align}\begin{aligned}k_n \,&\sim\, \pi\\p_{0,m}\,&\sim\, P_0\\p_{k,m}\mid p_{0,m}\,&\sim\, \mathrm{Beta}(\mu = p_{0,m},\, \sigma^2 = F_k p_{0,m}(1 - p_{0,m}))\\g_{n,m}\mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m})\end{aligned}\end{align} \]; We have parametrized the beta distribution by its mean and variance; the usual parameters are \(a = (1 - p)(1 - F)/F,\; b = p(1-F)/F\) with \(F = F_k,\; p = p_{0,m}\).; Annotations; balding_nichols_model() adds the following global, sample, and variant annotations:. global.nPops (Int) – Number of populations; global.nSamples (Int) – Number of samples; global.nVariants (Int) – Number of variants; global.popDist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:5122,continuous,continuous,5122,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['continuous'],['continuous']
Deployability,"s.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); result = result.drop('Z0', 'Z1', 'Z2'); if bounded:; result = result.annotate(ibd=bound_result(result.ibd)); result = result.annotate(ibd=result.ibd.annotate(PI_HAT=result.ibd.Z1 / 2 + result.ibd.Z2)); result = result.filter((result.i < result.j) & (min <= result.ibd.PI_HAT) & (result.ibd.PI_HAT <= max)). samples = hl.literal(dataset.s.collect()); result = result.key_by(i=samples[hl.int32(result.i)], j=samples[hl.int32(result.j)]). return result.persist(). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:9258,update,updated,9258,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,1,['update'],['updated']
Deployability,"s.richUtils.RichDenseMatrixDouble.importFromDoubles(; Env.spark_backend('_breeze_fromfile').fs._jfs, uri, n_rows, n_cols, True; ). def _check_entries_size(n_rows, n_cols):; n_entries = n_rows * n_cols; if n_entries >= 1 << 31:; raise ValueError(f'number of entries must be less than 2^31, found {n_entries}'). def _breeze_from_ndarray(nd):; if any(i == 0 for i in nd.shape):; raise ValueError(f'from_numpy: ndarray dimensions must be non-zero, found shape {nd.shape}'). nd = _ndarray_as_2d(nd); nd = _ndarray_as_float64(nd); n_rows, n_cols = nd.shape. with with_local_temp_file() as path:; uri = local_path_uri(path); nd.tofile(path); return _breeze_fromfile(uri, n_rows, n_cols). def _svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True):; """"""; SciPy supports two Lapack algorithms:; DC: https://software.intel.com/en-us/mkl-developer-reference-fortran-gesdd; GR: https://software.intel.com/en-us/mkl-developer-reference-fortran-gesvd; DC (gesdd) is faster but uses O(elements) memory; lwork may overflow int32; """"""; try:; return spla.svd(; a,; full_matrices=full_matrices,; compute_uv=compute_uv,; overwrite_a=overwrite_a,; check_finite=check_finite,; lapack_driver='gesdd',; ); except ValueError as e:; if 'Too large work array required' in str(e):; return spla.svd(; a,; full_matrices=full_matrices,; compute_uv=compute_uv,; overwrite_a=overwrite_a,; check_finite=check_finite,; lapack_driver='gesvd',; ); else:; raise. def _eigh(a):; """"""; Only the lower triangle is used. Returns eigenvalues, eigenvectors.; NumPy and SciPy apply different Lapack algorithms:; NumPy uses DC: https://software.intel.com/en-us/mkl-developer-reference-fortran-syevd; SciPy uses RRR: https://software.intel.com/en-us/mkl-developer-reference-fortran-syevr; DC (syevd) is faster but uses O(elements) memory; lwork overflows int32 for dim_a > 32766; """"""; return np.linalg.eigh(a) if a.shape[0] <= 32766 else spla.eigh(a). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:83141,update,updated,83141,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['update'],['updated']
Deployability,"s: Optional[Union[ColorMapper, Dict[str, ColorMapper]]] = None,; width: int = 800,; height: int = 800,; collect_all: Optional[bool] = None,; n_divisions: Optional[int] = 500,; missing_label: str = 'NA',; ) -> Union[figure, Column]:; """"""Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot). If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive qq plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xla",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:47198,continuous,continuous,47198,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['continuous'],['continuous']
Deployability,"s; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_fill_hue(). [docs]def scale_fill_continuous():; """"""The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""fill""). [docs]def scale_fill_identity():; """"""A color scale that assumes the expression specified in the ``fill`` aesthetic can be used as a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""fill""). [docs]def scale_fill_hue():; """"""Map discrete fill colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""fill""). [docs]def scale_fill_manual(*, values):; """"""A color scale that assigns strings to fill colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""fill"", values=values). def scale_shape_manual(*, values):; """"""A scale that assigns shapes to discrete aesthetics. See `the plotly documentation <https://plotly.com/python-api-reference/generated/plotly.graph_objects.scatter.html#plotly.graph_objects.scatter.Marker.symbol>`__ for a list of supported shapes. Parameters; ----------; values: :class:`list` of :class:`str`; The shapes from which to choose. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""shape"", values=values). def scale_shape_auto():; """"""A scale that automatically assigns shapes to discrete aesthetics. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleShapeAuto(""shape""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:14974,update,updated,14974,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['update'],['updated']
Deployability,"s; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5242,pipeline,pipeline,5242,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['pipeline'],['pipeline']
Deployability,"s=None, shell=None); Alias for Batch.new_bash_job(). Return type:; BashJob. new_python_job(name=None, attributes=None); Initialize a new PythonJob object with default; Python image, memory, storage, and CPU settings (defined in Batch); upon batch creation.; Examples; Create and execute a batch b with one job j that prints “hello alice”:; b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; The image to use for Python jobs can be specified by default_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str) – File path to read. Return type:; InputResourceFile. rea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:7529,install,installed,7529,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['install'],['installed']
Deployability,"s_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Lung_all_snp_gene_associations. View page source. GTEx_eQTL_Lung_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html:9670,update,updated,9670,docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"s_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html:9763,update,updated,9763,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"s_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html:9781,update,updated,9781,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"s_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations. View page source. GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html:9781,update,updated,9781,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"sertions_dict.get(field); if e is None:; e = self._fields[field]; return e.dtype. new_type = hl.tstruct(**{f: get_type(f) for f in field_order}); indices, aggregations = unify_all(self, *insertions_dict.values()); return construct_expr(; ir.InsertFields.construct_with_deduplication(; self._ir, [(field, expr._ir) for field, expr in insertions_dict.items()], field_order; ),; new_type,; indices,; aggregations,; ). [docs] @typecheck_method(named_exprs=expr_any); def annotate(self, **named_exprs):; """"""Add new fields or recompute existing fields. Examples; --------. >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; -----; If an expression in `named_exprs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Fields to add. Returns; -------; :class:`.StructExpression`; Struct with new or updated fields.; """"""; new_types = {n: t for (n, t) in self.dtype.items()}. for f, e in named_exprs.items():; new_types[f] = e.dtype. result_type = tstruct(**new_types); indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]). return construct_expr(; ir.InsertFields.construct_with_deduplication(; self._ir, list(map(lambda x: (x[0], x[1]._ir), named_exprs.items())), None; ),; result_type,; indices,; aggregations,; ). [docs] @typecheck_method(fields=str, named_exprs=expr_any); def select(self, *fields, **named_exprs):; """"""Select existing fields and compute new ones. Examples; --------. >>> hl.eval(struct.select('a', c=['bar', 'baz'])); Struct(a=5, c=['bar', 'baz']). Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `named_exprs` arguments are new field expressions. Parameters; ----------; fields : varargs of :class:`str`; Field names to keep.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:47645,update,updated,47645,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['update'],['updated']
Deployability,"sion Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; librar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:8476,release,release,8476,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"sp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.all",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:51732,configurat,configuration,51732,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"ss facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; Eagerly checks necessary representational properties of the VDS. write(path, **kwargs)[source]; Write to path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:2785,update,updated,2785,docs/0.2/vds/hail.vds.VariantDataset.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html,1,['update'],['updated']
Deployability,"ssing_occupations = hl.set(['other', 'none']). t = users.select(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idcleaned_occupationint32str; 1""technician""; 2NA; 3""writer""; 4""technician""; 5NA; 6""executive""; 7""administrator""; 8""administrator""; 9""student""; 10""lawyer""; showing top 10 rows. [11]:. missing_occupations = hl.set(['other', 'none']). t = users.transmute(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idagesexzipcodecleaned_occupationint32int32strstrstr; 124""M""""85711""""technician""; 253""F""""94043""NA; 323""M""""32067""""writer""; 424""M""""43537""""technician""; 533""F""""15213""NA; 642""M""""98101""""executive""; 757""M""""91344""""administrator""; 836""M""""05201""""administrator""; 929""M""""01002""""student""; 1053""M""""90703""""lawyer""; showing top 10 rows. Global Fields; Finally, you can add global fields with annotate_globals. Globals are useful for storing metadata about a dataset or storing small data structures like sets and maps. [12]:. t = users.annotate_globals(cohort = 5, cloudable = hl.set(['sample1', 'sample10', 'sample15'])); t.describe(). ----------------------------------------; Global fields:; 'cohort': int32; 'cloudable': set<str>; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. [13]:. t.cloudable. [13]:. <SetExpression of type set<str>>. [14]:. hl.eval(t.cloudable). [14]:. {'sample1', 'sample10', 'sample15'}. Exercises. Z-score normalize the age field of users.; Convert zip to an integer. Hint: Not all zipcodes are US zipcodes! Use hl.int32 to convert a string to an integer. Use StringExpression.matches to see if a string matches a regular expression. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:7289,update,updated,7289,docs/0.2/tutorials/05-filter-annotate.html,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html,1,['update'],['updated']
Deployability,"ssions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12260,update,update,12260,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['install', 'update']","['installing', 'update']"
Deployability,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_afr. View page source. gnomad_ld_scores_afr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_amr. View page source. gnomad_ld_scores_amr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_asj. View page source. gnomad_ld_scores_asj. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_eas. View page source. gnomad_ld_scores_eas. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Pancreas_all_snp_gene_associations. View page source. GTEx_sQTL_Pancreas_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Prostate_all_snp_gene_associations. View page source. GTEx_sQTL_Prostate_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"stically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Releas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78542,configurat,configuration,78542,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.ServiceBackend(). Second, we create a Batch and specify the default Python image to; use for Python jobs with default_python_image. image is the return value; from building the Python image above and is the full name of where the newly; built image was pushed to.; b = hb.Batch(name='rf',; default_python_image=image). Next, we read the y dataframe locally in order to get the list of windows; to run. The file path containing the dataframe could be stored on the cloud.; Therefore, we use t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:4454,install,installed,4454,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['install'],['installed']
Deployability,"submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11299,configurat,configuration,11299,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"t = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30807,update,update,30807,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"t variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113156,update,updates,113156,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['updates']
Deployability,"t())))""; ). duplicates = [k for k, count in counts.items() if count > 1]; if duplicates:; raise ValueError(f""column keys must be unique, found duplicates: {', '.join(duplicates)}""). entries_uid = Env.get_uid(); cols_uid = Env.get_uid(). t = self; t = t._localize_entries(entries_uid, cols_uid). def fmt(f, col_key):; if f:; return col_key + separator + f; else:; return col_key. t = t.annotate(**{; fmt(f, col_keys[i]): t[entries_uid][i][j] for i in range(len(col_keys)) for j, f in enumerate(self.entry); }); t = t.drop(cols_uid, entries_uid). return t. [docs] @typecheck_method(rows=bool, cols=bool, entries=bool, handler=nullable(anyfunc)); def summarize(self, *, rows=True, cols=True, entries=True, handler=None):; """"""Compute and print summary information about the fields in the matrix table. .. include:: _templates/experimental.rst. Parameters; ----------; rows : :obj:`bool`; Compute summary for the row fields.; cols : :obj:`bool`; Compute summary for the column fields.; entries : :obj:`bool`; Compute summary for the entry fields.; """""". if handler is None:; handler = default_handler(); if cols:; handler(self.col._summarize(header='Columns', top=True)); if rows:; handler(self.row._summarize(header='Rows', top=True)); if entries:; handler(self.entry._summarize(header='Entries', top=True)). def _write_block_matrix(self, path, overwrite, entry_field, block_size):; mt = self; mt = mt._select_all(entry_exprs={entry_field: mt[entry_field]}). writer = ir.MatrixBlockMatrixWriter(path, overwrite, entry_field, block_size); Env.backend().execute(ir.MatrixWrite(self._mir, writer)). def _calculate_new_partitions(self, n_partitions):; """"""returns a set of range bounds that can be passed to write""""""; ht = self.rows(); ht = ht.select().select_globals(); return Env.backend().execute(; ir.TableToValueApply(ht._tir, {'name': 'TableCalculateNewPartitions', 'nPartitions': n_partitions}); ). matrix_table_type.set(MatrixTable). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:135577,update,updated,135577,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['update'],['updated']
Deployability,"t(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2218,integrat,integrate,2218,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['integrat'],['integrate']
Deployability,"t. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Ke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1569,install,install,1569,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,2,['install'],['install']
Deployability,"t32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'freq': array<struct {; pop: str,; ac: float64,; af: float64,; an: int64,; gnomad_exomes_ac: int32,; gnomad_exomes_af: float64,; gnomad_exomes_an: int32,; gnomad_genomes_ac: int32,; gnomad_genomes_af: float64,; gnomad_genomes_an: int32; }>; 'pass_gnomad_exomes': bool; 'pass_gnomad_genomes': bool; 'n_passing_populations': int32; 'high_quality': bool; 'nearest_genes': str; 'info': float64; ----------------------------------------; Entry fields:; 'meta_analysis': array<struct {; BETA: float64,; SE: float64,; Pvalue: float64,; Q: float64,; Pvalue_het: float64,; N: int32,; N_pops: int32,; AF_Allele2: float64,; AF_Cases: float64,; AF_Controls: float64; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html:14753,update,updated,14753,docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,2,['update'],['updated']
Deployability,"t:; """"""Create an interactive scatter plot with marginal densities on the side. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. This function returns a :class:`bokeh.models.layouts.Column` containing two :class:`figure.Row`:; - The first row contains the X-axis marginal density and a selection widget if multiple entries are specified in the ``label``; - The second row contains the scatter plot and the y-axis marginal density. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:37647,continuous,continuous,37647,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['continuous'],['continuous']
Deployability,"t; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.local_to_global. View page source. hail.vds.local_to_global. hail.vds.local_to_global(array, local_alleles, n_alleles, fill_value, number)[source]; Reindex a locally-indexed array to globally-indexed.; Examples; >>> local_alleles = hl.array([0, 2]); >>> local_ad = hl.array([9, 10]); >>> local_pl = hl.array([94, 0, 123]). >>> hl.eval(local_to_global(local_ad, local_alleles, n_alleles=3, fill_value=0, number='R')); [9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; The number parameter matches the VCF specification; number definitions:. A indicates one value per allele, excluding the reference.; R indicates one value per allele, including the reference.; G indicates one value per unique diploid genotype. Warning; Using this function can lead to an enormous explosion in data size, without increasing; information capacity. Its appropriate use is to conform to antiquated and badly-scaling; representations (e.g. pVCF), but even so, caution should be exercised. Reindexing local; PLs (or any G-numbered field) at a site with 1000 alleles will produce an array with; more than 5,000 values per sample – with 100,000 samples, nearly 50GB per variant!. See also; lgt_to_gt(). Parameters:. array (ArrayExpression) – Array to reindex.; local_alleles (ArrayExpression) – Local alleles array.; n_alleles (Int32Expression) – Total number of alleles to reindex to.; fill_value – Value to fill in at global indices with no data.; number (str) – One of ‘A’, ‘R’, ‘G’. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.local_to_global.html:2325,update,updated,2325,docs/0.2/vds/hail.vds.local_to_global.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.local_to_global.html,1,['update'],['updated']
Deployability,"t; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html:9769,update,updated,9769,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"t_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_geno",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31445,update,update,31445,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"t_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. View page source. GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html:9751,update,updated,9751,docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"t_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html:9769,update,updated,9769,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"tb, coef_dict=None, str_expr=None, axis='rows')[source]; Aggregates by linear combination fields matching either keys in coef_dict; or str_expr. Outputs the aggregation in a MatrixTable or Table; as a new row field “agg_annot” or a new column field “agg_cov”. Parameters:. tb (MatrixTable or Table) – MatrixTable or Table containing fields to be aggregated.; coef_dict (dict, optional) – Coefficients to multiply each field. The coefficients are specified by; coef_dict value, the row (or col) field name is specified by coef_dict key.; If not included, coefficients are assumed to be 1.; str_expr (str, optional) – String expression to match against row (or col) field names.; axis (str) – Either ‘rows’ or ‘cols’. If ‘rows’, this aggregates across row fields.; If ‘cols’, this aggregates across col fields. If tb is a Table, axis = ‘rows’. Returns:; MatrixTable or Table – MatrixTable or Table containing aggregation field. hail.experimental.ldscsim.get_coef_dict(tb, str_expr=None, ref_coef_dict=None, axis='rows')[source]; Gets either col or row fields matching str_expr and take intersection; with keys in coefficient reference dict. Parameters:. tb (MatrixTable or Table) – MatrixTable or Table containing row (or col) for coef_dict.; str_expr (str, optional) – String expression pattern to match against row (or col) fields. If left; unspecified, the intersection of field names is only between existing; row (or col) fields in mt and keys of ref_coef_dict.; ref_coef_dict (dict, optional) – Reference coefficient dictionary with keys that are row (or col) field; names from which to subset. If not included, coefficients are assumed to be 1.; axis (str) – Field type in which to search for field names. Options: ‘rows’, ‘cols’. Returns:; coef_dict (dict) – Coefficients to multiply each field. The coefficients are specified by; coef_dict value, the row (or col) field name is specified by coef_dict key. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/ldscsim.html:17602,update,updated,17602,docs/0.2/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html,1,['update'],['updated']
Deployability,"tch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:25631,configurat,configuration,25631,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14866,configurat,configurations,14866,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['configurat'],['configurations']
Deployability,"te |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[f]), repr(f0), repr(f); ); ); else:; seen[f] = f0; new_field_types[f] = t. return tstruct(**new_field_types). def unify(self, t):; if not (isinstance(t, tstruct) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tstruct(**{f: t.subst() for f, t in self.items()}). def clear(self):; for f, t in self.items():; t.clear(). def _ge",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:35255,update,update,35255,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['update'],['update']
Deployability,"tem, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:2110,install,installed,2110,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"tems in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Fac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:38721,continuous,continuous,38721,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['continuous'],['continuous']
Deployability,"ters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:5511,install,installed,5511,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installed']
Deployability,"th - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[f]), repr(f0), repr(f); ); ); else:; seen[f] = f0; new_field_types[f] = t. return tstruct(**new_field_types). def unify(self, t):; if not (isinstance(t, tstruct) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tstruct(**{f: t.subst() for f, t in self.items()}).",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:35212,update,update,35212,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['update'],['update']
Deployability,"th one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:2225,configurat,configuration,2225,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['configurat'],['configuration']
Deployability,"the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homoz",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28003,update,update,28003,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""; assert isinstance(mapping, Aesthetic); return GGPlot(table, mapping). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:11676,update,updated,11676,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,1,['update'],['updated']
Deployability,"tig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = [",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30982,update,update,30982,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['update'],['update']
Deployability,"ting by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read fro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72697,install,installation,72697,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installation']
Deployability,"tions.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37673,install,installing,37673,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['install'],['installing']
Deployability,"tions.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37453,configurat,configuration,37453,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"titions defaults to one partition per million genotypes (i.e., samples * variants / 10^6) or 8, whichever is larger. The Balding-Nichols model models genotypes of individuals from a structured population comprising :math:`K` homogeneous subpopulations; that have each diverged from a single ancestral population (a `star phylogeny`). We take :math:`N` samples and :math:`M` bi-allelic variants in perfect; linkage equilibrium. The relative sizes of the subpopulations are given by a probability vector :math:`\pi`; the ancestral allele frequencies are; drawn independently from a frequency spectrum :math:`P_0`; the subpopulations have diverged with possibly different :math:`F_{ST}` parameters :math:`F_k`; (here and below, lowercase indices run over a range bounded by the corresponding uppercase parameter, e.g. :math:`k = 1, \ldots, K`).; For each variant, the subpopulation allele frequencies are drawn a `beta distribution <https://en.wikipedia.org/wiki/Beta_distribution>`__, a useful continuous approximation of; the effect of genetic drift. We denote the individual subpopulation memberships by :math:`k_n`, the ancestral allele frequences by :math:`p_{0, m}`,; the subpopulation allele frequencies by :math:`p_{k, m}`, and the genotypes by :math:`g_{n, m}`. The generative model in then given by:. .. math::; k_n \,&\sim\, \pi. p_{0,m}\,&\sim\, P_0. p_{k,m}\mid p_{0,m}\,&\sim\, \mathrm{Beta}(\mu = p_{0,m},\, \sigma^2 = F_k p_{0,m}(1 - p_{0,m})). g_{n,m}\mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}). We have parametrized the beta distribution by its mean and variance; the usual parameters are :math:`a = (1 - p)(1 - F)/F,\; b = p(1-F)/F` with :math:`F = F_k,\; p = p_{0,m}`. **Annotations**. :py:meth:`~hail.HailContext.balding_nichols_model` adds the following global, sample, and variant annotations:. - **global.nPops** (*Int*) -- Number of populations; - **global.nSamples** (*Int*) -- Number of samples; - **global.nVariants** (*Int*) -- Number of variants; - **glob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:29951,continuous,continuous,29951,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['continuous'],['continuous']
Deployability,"titions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112139,configurat,configuration,112139,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['configurat'],['configuration']
Deployability,"to as_tsv is result and we assign the output to tsv_result.; Lastly in the for loop for each window, we append the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8030,pipeline,pipeline,8030,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['pipeline'],['pipeline']
Deployability,"to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52549,release,release,52549,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For mor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1503,install,installed,1503,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['installed']
Deployability,"top=row_file_sizes_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ). rows_grid = gridplot([[p_rows_per_partition, p_stats], [p, p_file_size]]). if 'entry_file_sizes' in all_data:; title = f'Statistics for {data_type}: {t_path}'. msg = f""Rows: {sum(all_data['rows_per_partition']):,}<br/>Partitions: {len(all_data['rows_per_partition']):,}<br/>Size: {total_entry_file_size}<br/>""; if success_file[0]:; msg += success_file[0]. source = ColumnDataSource(pd.DataFrame(all_data)); p = figure(tools=tools, width=panel_size, height=panel_size); p.title.text = title; p.xaxis.axis_label = 'Number of rows'; p.yaxis.axis_label = f'File size ({entry_scale}B)'; color_map = factor_cmap('spans_chromosome', palette=Spectral8, factors=list(set(all_data['spans_chromosome']))); p.scatter('rows_per_partition', 'entry_file_sizes', color=color_map, legend='spans_chromosome', source=source); p.legend.location = 'bottom_right'; p.select_one(HoverTool).tooltips = [; (x, f'@{x}') for x in ('rows_per_partition', 'entry_file_sizes_human', 'partition_bounds', 'index'); ]. p_stats = Div(text=msg); p_rows_per_partition = figure(x_range=p.x_range, width=panel_size, height=subpanel_size); p_rows_per_partition.quad(; top=rows_per_partition_hist,; bottom=0,; left=rows_per_partition_edges[:-1],; right=rows_per_partition_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ); p_file_size = figure(y_range=p.y_range, width=subpanel_size, height=panel_size). row_file_sizes_hist, row_file_sizes_edges = np.histogram(all_data['entry_file_sizes'], bins=50); p_file_size.quad(; right=row_file_sizes_hist,; left=0,; bottom=row_file_sizes_edges[:-1],; top=row_file_sizes_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ); entries_grid = gridplot([[p_rows_per_partition, p_stats], [p, p_file_size]]). return Tabs(tabs=[TabPanel(child=entries_grid, title='Entries'), TabPanel(child=rows_grid, title='Rows')]); else:; return rows_grid. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html:11564,update,updated,11564,docs/0.2/_modules/hail/experimental/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html,1,['update'],['updated']
Deployability,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:22331,pipeline,pipelines,22331,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['pipeline'],['pipelines']
Deployability,"tring,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing VEP annotations. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:107039,configurat,configuration,107039,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"truct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:106471,configurat,configuration,106471,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['configurat'],['configuration']
Deployability,"ts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:9186,release,release,9186,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"tter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:9540,pipeline,pipelines,9540,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['pipeline'],['pipelines']
Deployability,"tures. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integer or floating point type.; (#12277) Fixed bug; in reading tables/matrixtables with partition intervals that led to; error or segfault. Version 0.2.101; Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:45164,pipeline,pipelines,45164,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"tures. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key beh",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:65980,update,update,65980,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to searc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:92129,install,installations,92129,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installations']
Deployability,"type tint32. hail.expr.functions.parse_int32(x)[source]; Parse a string as a 32-bit integer.; Examples; >>> hl.eval(hl.parse_int32('154')); 154. >>> hl.eval(hl.parse_int32('15.4')); None. >>> hl.eval(hl.parse_int32('asdf')); None. Notes; If the input is an invalid integer, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tint32. hail.expr.functions.parse_int64(x)[source]; Parse a string as a 64-bit integer.; Examples; >>> hl.eval(hl.parse_int64('154')); 154. >>> hl.eval(hl.parse_int64('15.4')); None. >>> hl.eval(hl.parse_int64('asdf')); None. Notes; If the input is an invalid integer, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tint64. hail.expr.functions.parse_float(x)[source]; Parse a string as a 64-bit floating point number.; Examples; >>> hl.eval(hl.parse_float('1.1')); 1.1. >>> hl.eval(hl.parse_float('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat64. hail.expr.functions.parse_float32(x)[source]; Parse a string as a 32-bit floating point number.; Examples; >>> hl.eval(hl.parse_float32('1.1')); 1.100000023841858. >>> hl.eval(hl.parse_float32('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat32. hail.expr.functions.parse_float64(x)[source]; Parse a string as a 64-bit floating point number.; Examples; >>> hl.eval(hl.parse_float64('1.1')); 1.1. >>> hl.eval(hl.parse_float64('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/string.html:6999,update,updated,6999,docs/0.2/functions/string.html,https://hail.is,https://hail.is/docs/0.2/functions/string.html,1,['update'],['updated']
Deployability,"ud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:43123,configurat,configuration,43123,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['configurat'],['configuration']
Deployability,"ul concordance statistics. This value is the number of genotypes ; which were called (homozygous reference, heterozygous, or homozygous variant) in both datasets, ; but where the call did not match between the two.; The column concordance matches the structure of the global summmary, which is detailed above. Once again,; the first index into this array is the state on the left, and the second index is the state on the right.; For example, concordance[1][4] is the number of “no call” genotypes on the left that were called ; homozygous variant on the right. Parameters:right (VariantDataset) – right hand variant dataset for concordance. Returns:The global concordance statistics, a key table with sample concordance; statistics, and a key table with variant concordance statistics. Return type:(list of list of int, KeyTable, KeyTable). count()[source]¶; Returns number of samples and variants in the dataset.; Examples; >>> samples, variants = vds.count(). Notes; This is also the fastest way to force evaluation of a Hail pipeline. Returns:The sample and variant counts. Return type:(int, int). count_variants()[source]¶; Count number of variants in variant dataset. Return type:long. deduplicate()[source]¶; Remove duplicate variants. Returns:Deduplicated variant dataset. Return type:VariantDataset. delete_va_attribute(ann_path, attribute)[source]¶; Removes an attribute from a variant annotation field.; Attributes are key/value pairs that can be attached to a variant annotation field.; The following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. INFO fields attributes (attached to (va.info.*)):; ‘Number’: The arity of the field. Can take values; 0 (Boolean flag),; 1 (single value),; R (one value per allele, including the reference),; A (one value per non-reference allele),; G (one value per genotype), and; . (any number of values); When importing: The value in read from the VCF INFO field definition; When expor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:33950,pipeline,pipeline,33950,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['pipeline'],['pipeline']
Deployability,"ult is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str) – The names of the datasets with which to annotate rel. Returns:; MatrixTable or Table – The relational object rel, with the annotations from names; added. property available_datasets; List of names of available annotation datasets. Returns:; list – List of available annotation datasets. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:3617,update,updated,3617,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,1,['update'],['updated']
Deployability,"ult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any]) – Positional arguments to the Python function. Must be either a builtin; Python object, a Resource, or a Dill serializable object.; kwargs (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any]) – Key-word arguments to the Python function. Must be either a builtin; Python object, a Resource, or a Dill serializable object. Return type:; PythonResult. Returns:; resource.PythonResult. image(image); Set the job’s docker image.; Notes; image must already exist and have the same version of Python as what is; being used on the computer submitting the Batch. It also must have the; dill Python package installed. You can use the function docker.build_python_image(); to build a new image containing dill and additional Python packages.; Examples; Set the job’s docker image to hailgenetics/python-dill:3.9-slim:; >>> b = Batch(); >>> j = b.new_python_job(); >>> (j.image('hailgenetics/python-dill:3.9-slim'); ... .call(print, 'hello')); >>> b.run() . Parameters:; image (str) – Docker image to use. Return type:; PythonJob. Returns:; Same job object with docker image set. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:5780,install,installed,5780,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,1,['install'],['installed']
Deployability,"unction or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:; GT: call; AD: array<int32>; DP: ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:23562,update,update,23562,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"uration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; Trio. View page source. Trio. class hail.genetics.Trio[source]; Class containing information about nuclear family relatedness and sex. Parameters:. s (str) – Sample ID of proband.; fam_id (str or None) – Family ID.; pat_id (str or None) – Sample ID of father.; mat_id (str or None) – Sample ID of mother.; is_female (bool or None) – Sex of proband. Attributes. fam_id; Family ID. is_female; Returns True if the proband is a reported female, False if reported male, and None if no sex is defined. is_male; Returns True if the proband is a reported male, False if reported female, and None if no sex is defined. mat_id; ID of mother in trio, may be missing. pat_id; ID of father in trio, may be missing. s; ID of proband in trio, never missing. Methods. is_complete; Returns True if the trio has a defined mother and father. property fam_id; Family ID. Return type:; str or None. is_complete()[source]; Returns True if the trio has a defined mother and father.; The considered fields are mat_id() and pat_id().; Recall that s may never be missing. The fam_id(); and is_female() fields may be missing in a complete trio. Return type:; bool. property is_female; Returns True if the proband is a reported female,; False if reported male, and None if no sex is defined. Return type:; bool or None. property is_male; Returns True if the proband is a reported male,; False if reported female, and None if no sex is defined. Return type:; bool or None. property mat_id; ID of mother in trio, may be missing. Return type:; str or None. property pat_id; ID of father in trio, may be missing. Return type:; str or None. property s; ID of proband in trio, never missing. Return type:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.Trio.html:2389,update,updated,2389,docs/0.2/genetics/hail.genetics.Trio.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Trio.html,1,['update'],['updated']
Deployability,"ureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of float) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the x-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_discrete(name=None, breaks=None, labels=None)[source]; The default discrete x scale. Parameters:. na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:9418,continuous,continuous,9418,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"urns:; DictExpression – Dictionary with transformed values. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of the dictionary.; Examples; >>> hl.eval(d.size()); 3. Returns:; Expression of type tint32 – Size of the dictionary. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; Returns an array with all values in the dictionary.; Examples; >>> hl.eval(d.values()) ; [33, 44, 43]. Returns:; ArrayExpression – All values in the dictionary. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:9095,update,updated,9095,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['update'],['updated']
Deployability,"v_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; clinvar_variant_summary. View page source. clinvar_variant_summary. Versions: 2019-07; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (2019-07, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'Type': str; 'Name': str; 'GeneID': int32; 'GeneSymbol': str; 'HGNC_ID': str; 'ClinicalSignificance': str; 'ClinSigSimple': int32; 'LastEvaluated': str; 'RS# (dbSNP)': int32; 'nsv/esv (dbVar)': str; 'RCVaccession': str; 'PhenotypeIDS': str; 'PhenotypeList': str; 'Origin': str; 'OriginSimple': str; 'Assembly': str; 'ChromosomeAccession': str; 'ReferenceAllele': str; 'AlternateAllele': str; 'Cytogenetic': str; 'ReviewStatus': str; 'NumberSubmitters': int32; 'Guidelines': str; 'TestedInGTR': str; 'OtherIDs': str; 'SubmitterCategories': int32; 'VariationID': int32; 'interval': interval<locus<GRCh37>>; 'AlleleID': int32; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/clinvar_variant_summary.html:9926,update,updated,9926,docs/0.2/datasets/schemas/clinvar_variant_summary.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/clinvar_variant_summary.html,1,['update'],['updated']
Deployability,"val(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >>> hl.eval(s.startswith('The')); True. >>> hl.eval(s.startswith('the')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. strip()[source]; Returns a copy of the string with whitespace removed from the start; and end.; Examples; >>> s2 = hl.str(' once upon a time\n'); >>> hl.eval(s2.strip()); 'once upon a time'. Returns:; StringExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. translate(mapping)[source]; Translates characters of the string using mapping.; Examples; >>> string = hl.literal('ATTTGCA'); >>> hl.eval(string.translate({'T': 'U'})); 'AUUUGCA'. Parameters:; mapping (DictExpression) – Dictionary of character-character translations. Returns:; StringExpression. See also; replace(). upper()[source]; Returns a copy of the string, but with lower case letters converted; to upper case.; Examples; >>> hl.eval(s.upper()); 'THE QUICK BROWN FOX'. Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StringExpression.html:14632,update,updated,14632,docs/0.2/hail.expr.StringExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html,1,['update'],['updated']
Deployability,"variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Coronary_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Coronary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html:9760,update,updated,9760,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations. View page source. GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html:9760,update,updated,9760,docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"verse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of float) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the x-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_discrete(name=None, breaks=None, labels=None)[source]; The default discrete x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of str) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_genomic(reference_genome, name=None)[source]; The default genomic x scale. This is used when the x aesthetic corresponds to a LocusExpression. Parameters:. reference_genome – The reference genome being used.; name (str) – The label to show on y",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:9934,continuous,continuous,9934,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"vided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:12206,configurat,configuration,12206,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"w allele index to; the old allele index. Its length is the same as the modified alleles; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out.; Using f; The f argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; Fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:23360,update,update,23360,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"w_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configur",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:12769,release,release,12769,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['release'],['release']
Deployability,"will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:19950,pipeline,pipeline,19950,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['pipeline'],['pipeline']
Deployability,"with kinship lower than min_kinship are excluded; from the results.; statistics (str) – Set of statistics to compute.; If 'kin', only estimate the kinship statistic.; If 'kin2', estimate the above and IBD2.; If 'kin20', estimate the above and IBD0.; If 'all', estimate the above and IBD1.; block_size (int, optional) – Block size of block matrices used in the algorithm.; Default given by BlockMatrix.default_block_size().; include_self_kinship (bool) – If True, include entries for an individual’s estimated kinship with; themselves. Defaults to False. Returns:; Table – A Table mapping pairs of samples to their pair-wise statistics. hail.methods.simulate_random_mating(mt, n_rounds=1, generation_size_multiplier=1.0, keep_founders=True)[source]; Simulate random diploid mating to produce new individuals. Parameters:. mt; n_rounds (int) – Number of rounds of mating.; generation_size_multiplier (float) – Ratio of number of offspring to current population for each round of mating.; keep_founders :obj:`bool` – If true, keep all founders and intermediate generations in the final sample list. If; false, keep only offspring in the last generation. Returns:; MatrixTable. [1]; Purcell, Shaun et al. “PLINK: a tool set for whole-genome association and; population-based linkage analyses.” American journal of human genetics; vol. 81,3 (2007):; 559-75. doi:10.1086/519795. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838/. [2]; Manichaikul, Ani et al. “Robust relationship inference in genome-wide; association studies.” Bioinformatics (Oxford, England) vol. 26,22 (2010):; 2867-73. doi:10.1093/bioinformatics/btq559. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3025716/. [3]; Conomos, Matthew P et al. “Model-free Estimation of Recent Genetic; Relatedness.” American journal of human genetics vol. 98,1 (2016):; 127-48. doi:10.1016/j.ajhg.2015.11.022. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:23253,update,updated,23253,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['update'],['updated']
Deployability,"wnload the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:3366,install,installed,3366,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installed']
Deployability,"wnsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:47829,update,updated,47829,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['update'],['updated']
Deployability,"ws.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.M",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:37312,pipeline,pipelines,37312,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"w¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:1617,patch,patches,1617,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['patch'],['patches']
Deployability,"x_chr_ploidy_from_interval_coverage. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.impute_sex_chr_ploidy_from_interval_coverage. View page source. hail.vds.impute_sex_chr_ploidy_from_interval_coverage. hail.vds.impute_sex_chr_ploidy_from_interval_coverage(mt, normalization_contig)[source]; Impute sex chromosome ploidy from a precomputed interval coverage MatrixTable.; The input MatrixTable must have the following row fields:. interval (interval): Genomic interval of interest.; interval_size (int32): Size of interval, in bases. And the following entry fields:. sum_dp (int64): Sum of depth values by base across the interval. Returns a Table with sample ID keys, with the following fields:. autosomal_mean_dp (float64): Mean depth on calling intervals on normalization contig.; x_mean_dp (float64): Mean depth on calling intervals on X chromosome.; x_ploidy (float64): Estimated ploidy on X chromosome. Equal to 2 * x_mean_dp / autosomal_mean_dp.; y_mean_dp (float64): Mean depth on calling intervals on chromosome.; y_ploidy (float64): Estimated ploidy on Y chromosome. Equal to 2 * y_mean_db / autosomal_mean_dp. Parameters:. mt (MatrixTable) – Interval-by-sample MatrixTable with sum of depth values across the interval.; normalization_contig (str) – Autosomal contig for depth comparison. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html:2002,update,updated,2002,docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html,1,['update'],['updated']
Deployability,"xploded table.; Missing arrays or sets are treated as empty.; Currently, the name argument may not be used if field is not a; top-level field of the table (e.g. name may be used with ht.foo; but not ht.foo.bar). Parameters:. field (str or Expression) – Top-level field name or expression.; name (str or None) – If not None, rename the exploded field to name. Returns:; Table. export(output, types_file=None, header=True, parallel=None, delimiter='\t')[source]; Export to a text file.; Examples; Export to a tab-separated file:; >>> table1.export('output/table1.tsv.bgz'). Note; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with any Hail method, as well as with Python’s gzip.open; and R’s read.table.; Nested structures will be exported as JSON. In order to export nested struct; fields as separate fields in the resulting table, use flatten() first. Warning; Do not export to a path that is being read from in the same pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:23036,pipeline,pipeline,23036,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipeline']
Deployability,"xporting missing genotypes without trailing; fields. Bug fixes. (#5306) Fix; ReferenceGenome.add_sequence causing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; opti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:100809,pipeline,pipelines,100809,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"y: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. You can count the rows of a table. [6]:. users.count(). [6]:. 943. You can access fields of tables with the Python attribute notation table.field, or with index notation table['field']. The latter is useful when the field names are not valid Python identifiers (if a field name includes a space, for example). [7]:. users.occupation.describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. [8]:. users['occupation'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. users.occupation and users['occupation'] are Hail Expressions; Lets peak at their using show. Notice that the key is shown as well!. [9]:. users.occupation.show(). idoccupationint32str; 1""technician""; 2""other""; 3""writer""; 4""technician""; 5""other""; 6""executive""; 7""administrator""; 8""administrator""; 9""student""; 10""lawyer""; showing top 10 rows. Exercise; The movie dataset has two other tables: movies.ht and ratings.ht. Load these tables and have a quick look around. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:4956,update,updated,4956,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['update'],['updated']
Deployability,"ype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162138,update,updated,162138,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['update'],['updated']
Deployability,"zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:71938,configurat,configuration,71938,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"| str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. union(s)[source]; Return the union of the set and set s.; Examples; >>> hl.eval(s1.union(s2)); {1, 2, 3, 5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.SetExpression.html:14910,update,updated,14910,docs/0.2/hail.expr.SetExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html,1,['update'],['updated']
Deployability,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:10176,update,updated,10176,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,6,['update'],['updated']
Deployability,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Expression-1.html:6621,update,updated,6621,docs/0.2/hail.expr.Expression-1.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html,2,['update'],['updated']
Deployability,"}]"".format(; missing_sex_count, missing_sex_values; ); ). return Pedigree(trios). @property; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """"""; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father and mother. :rtype: list of :class:`.Trio`; """"""; return list(filter(lambda t: t.is_complete(), self.trios)). [docs] @typecheck_method(samples=sequenceof(nullable(str))); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **Notes**. For any trio, the following steps will be applied:. - If the proband is not in the list of samples provided, the trio is removed.; - If the father is not in the list of samples provided, `pat_id` is set to ``None``.; - If the mother is not in the list of samples provided, `mat_id` is set to ``None``. Parameters; ----------; samples: :obj:`list` [:obj:`str`]; Sample IDs to keep. Returns; -------; :class:`.Pedigree`; """"""; sample_set = set(samples). filtered_trios = []; for trio in self._trios:; restricted_trio = trio._restrict_to(sample_set); if restricted_trio is not None:; filtered_trios.append(restricted_trio). return Pedigree(filtered_trios). [docs] @typecheck_method(path=str); def write(self, path):; """"""Write a .fam file to the given path. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). **Notes**. This method writes a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. .. caution::. Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use :func:`~.import_fam` to; manipulate this information. :param path: output path; :type path: str; """""". lines = [t._to_fam_file_line() for t in self._trios]. with Env.fs().open(path, mode=""w"") as file:; for line in lines:; file.write(line + ""\n""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:8020,update,updated,8020,docs/0.2/_modules/hail/genetics/pedigree.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html,1,['update'],['updated']
Deployability,"’s filter the data to 2007 for our first experiments. [10]:. gp_2007 = gp.filter(gp.year == 2007). If we want to see how many countries from each continent we have, we can use geom_bar, which just takes in an x aesthetic and then implicitly counts how many values of each x there are. [11]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(). [11]:. To make it a little prettier, let’s color per continent as well. We use fill to specify color of shapes (as opposed to color for points and lines. color on a bar chart sets the color of the bar outline.). [12]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent)). [12]:. Maybe we instead want to see not the number of countries per continent, but the number of people living on each continent. We can do this with geom_bar as well by specifying a weight. [13]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent, weight=gp_2007.pop)). [13]:. Histograms are similar to bar plots, except they break a continuous x axis into bins. Let’s import the iris dataset for this. [14]:. iris = hl.Table.from_pandas(plotly.data.iris()); iris.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'sepal_length': float64; 'sepal_width': float64; 'petal_length': float64; 'petal_width': float64; 'species': str; 'species_id': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s make a histogram:. [15]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(). [15]:. By default histogram plots groups stacked on top of each other, which is not always easy to interpret. We can specify the position argument to histogram to get different behavior. ""dodge"" puts the bars next to each other:. [16]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""dodge""). [16]:. And ""identity"" plots them over each other. It he",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:5923,continuous,continuous,5923,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['continuous'],['continuous']
Deployability,". class hail.genetics.Pedigree[source]; Class containing a list of trios, with extra functionality. Parameters:; trios (list of Trio) – list of trio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. complete_trios; List of trio objects that have a defined father and mother. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a PLINK .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]; List of trio objects that have a defined father and mother. Return type:; list of Trio. filter_to(samples)[source]; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, pat_id is set to None.; If the mother is not in the list of samples provided, mat_id is set to None. Parameters:; samples (list [str]) – Sample IDs to keep. Returns:; Pedigree. classmethod read(fam_path, delimiter='\\s+')[source]; Read a PLINK .fam file and return a pedigree object.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'). Notes; See PLINK .fam file for; the required format. Parameters:. fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:; Pedigree. property trios; List of trio objects in this pedigree. Return type:; list of Trio. write(path)[source]; Write a .fam file to the given path.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use import_fam() to; manipulate this information. Parameters:; path (str) – output path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html:2658,update,updated,2658,docs/0.2/genetics/hail.genetics.Pedigree.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html,1,['update'],['updated']
Deployability,"﻿. . Annotation Database — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/annotationdb.html:581,pipeline,pipelines,581,docs/0.1/annotationdb.html,https://hail.is,https://hail.is/docs/0.1/annotationdb.html,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:773,continuous,continuous,773,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,"['continuous', 'integrat']","['continuous', 'integration']"
Deployability,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:900,configurat,configuration,900,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['configurat'],['configuration']
Deployability,"﻿. Batch Service — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Loca",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:676,install,installed,676,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['install'],['installed']
Deployability,"﻿. Hail | ; Amazon Web Services. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Amazon Web Services. View page source. Amazon Web Services; While Hail does not have any built-in tools for working with Amazon EMR, there are two approaches maintained by third parties:. AWS maintains a Hail on AWS quickstart.; The Avillach Lab at Harvard Medical School maintains an open-source tool. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/amazon_web_services.html:891,update,updated,891,docs/0.2/cloud/amazon_web_services.html,https://hail.is,https://hail.is/docs/0.2/cloud/amazon_web_services.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Annotation Database. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the pan",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:718,pipeline,pipelines,718,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"﻿. Hail | ; Annotation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Annotation. View page source. Annotation; Annotations are Hail’s way of adding data fields to Hail’s tables and matrix; tables. Create a nested annotation. description:; Add a new field gq_mean as a nested field inside info. code:; >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). dependencies:; StructExpression.annotate(), MatrixTable.annotate_rows(). understanding:. To add a new field gq_mean as a nested field inside info,; instead of a top-level field, we need to annotate the info field itself.; Construct an expression mt.info.annotate(gq_mean=...) which adds the field; to info. Then, reassign this expression to info using; MatrixTable.annotate_rows(). Remove a nested annotation. description:; Drop a field AF, which is nested inside the info field. To drop a nested field AF, construct an expression mt.info.drop('AF'); which drops the field from its parent field, info. Then, reassign this; expression to info using MatrixTable.annotate_rows(). code:; >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). dependencies:; StructExpression.drop(), MatrixTable.annotate_rows(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/annotation.html:1703,update,updated,1703,docs/0.2/guides/annotation.html,https://hail.is,https://hail.is/docs/0.2/guides/annotation.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Cheat Sheets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Cheat Sheets. View page source. Cheat Sheets. Note; Hail’s cheat sheets are relatively new. We welcome suggestions; for additional cheatsheets, as well as feedback about our documentation. If; you’d like to add a cheatsheet to the documentation, make a pull request!. Hail Tables Cheat Sheet; Hail MatrixTables Cheat Sheet. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cheatsheets.html:790,update,updated,790,docs/0.2/cheatsheets.html,https://hail.is,https://hail.is/docs/0.2/cheatsheets.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:953,configurat,configuration,953,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['configurat'],['configuration']
Deployability,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:884,configurat,configuration,884,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['configurat'],['configuration']
Deployability,"﻿. Hail | ; Databricks. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Databricks. View page source. Databricks; The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_lo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/databricks.html:754,install,installed,754,docs/0.2/cloud/databricks.html,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html,1,['install'],['installed']
Deployability,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets.html:983,pipeline,pipeline,983,docs/0.2/datasets.html,https://hail.is,https://hail.is/docs/0.2/datasets.html,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"﻿. Hail | ; For Software Developers. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:806,install,installation,806,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,3,['install'],"['install', 'installation', 'installed']"
Deployability,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:971,pipeline,pipelines,971,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['pipeline'],['pipelines']
Deployability,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:804,configurat,configuration,804,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['configurat'],['configuration']
Deployability,"﻿. Hail | ; Google Cloud Platform. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:906,install,installations,906,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['install'],['installations']
Deployability,"﻿. Hail | ; Hadoop Glob Patterns. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Hadoop Glob Patterns. Change Log And Version Policy. menu; Hail. Other Resources; Hadoop Glob Patterns. View page source. Hadoop Glob Patterns. Pattern; Description. ?. Matches any single character. *. Matches zero or more characters. [abc]. Matches a single character from character set {a,b,c}. [a-b]. Matches a single character from the character range {a…b}. Note that the “^”; character must occur immediately to the right of the opening bracket. [^a]. Matches a single character that is not from character set or range {a}. Note that; the “^”character must occur immediately to the right of the opening bracket. \c. Removes (escapes) any special meaning of character c. {ab,cd}. Matches a string from the string set {ab, cd}. {ab,c{de, fh}}. Matches a string from the string set {ab, cde, cfh}. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hadoop_glob_patterns.html:1232,update,updated,1232,docs/0.2/hadoop_glob_patterns.html,https://hail.is,https://hail.is/docs/0.2/hadoop_glob_patterns.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/index.html:717,install,installation,717,docs/0.2/index.html,https://hail.is,https://hail.is/docs/0.2/index.html,2,['install'],['installation']
Deployability,"﻿. Hail | ; Hail Overview. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; Expressions; Tables; MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview. View page source. Hail Overview; Hail is a library for analyzing structured tabular and matrix data. Hail; contains a collection of primitives for operating on data in parallel, as well; as a suite of functionality for processing genetic data.; This section of Hail’s documentation contains detailed explanations of Hail’s; architecture, primitives, classes, and libraries. Expressions; What is an Expression?; Boolean Logic; Conditional Expressions; Missingness; Functions. Tables; Import; Global Fields; Keys; Referencing Fields; Updating Fields; Aggregation; Joins; Interacting with Tables Locally. MatrixTables; Keys; Referencing Fields; Import; Common Operations; Aggregation; Group-By; Joins; Interacting with Matrix Tables Locally. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/index.html:1256,update,updated,1256,docs/0.2/overview/index.html,https://hail.is,https://hail.is/docs/0.2/overview/index.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials-landing.html:900,install,install,900,docs/0.2/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html,2,"['install', 'update']","['install', 'updated']"
Deployability,"﻿. Hail | ; Hail on the Cloud. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud. View page source. Hail on the Cloud; Public clouds are a natural place to run Hail, offering the ability to run on-demand workloads with; high elasticity. Microsoft Azure, Google Cloud Platform, Databricks and Amazon Web Services make it; possible to rent Spark clusters with thousands of cores on-demand, providing for the elastic compute; requirements of scientific research without an up-front capital investment in hardware. General Advice; Start Small; Estimating time; Estimating cost. Query-on-Batch; Getting Started; Variant Effect Predictor (VEP). Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail_on_the_cloud.html:1421,update,updated,1421,docs/0.2/hail_on_the_cloud.html,https://hail.is,https://hail.is/docs/0.2/hail_on_the_cloud.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Install Hail on GNU/Linux. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on GNU/Linux. View page source. Install Hail on GNU/Linux. Install Java 11.; Install Python 3.9 or later.; Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; Install BLAS and LAPACK.; Install Hail using pip. On a recent Debian-like system, the following should suffice:; apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. Now let’s take Hail for a spin!. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/linux.html:256,install,installation,256,docs/0.2/install/linux.html,https://hail.is,https://hail.is/docs/0.2/install/linux.html,4,"['install', 'update']","['install', 'installation', 'updated']"
Deployability,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/macosx.html:290,install,installation,290,docs/0.2/install/macosx.html,https://hail.is,https://hail.is/docs/0.2/install/macosx.html,8,"['install', 'update']","['install', 'install-completion', 'installation', 'updated']"
Deployability,"﻿. Hail | ; Install Hail on a Spark Cluster. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like sy",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:274,install,installation,274,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['installation']
Deployability,"﻿. Hail | ; Installing Hail. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail. View page source. Installing Hail. Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started.html:246,install,installation,246,docs/0.2/getting_started.html,https://hail.is,https://hail.is/docs/0.2/getting_started.html,3,"['install', 'update']","['installation', 'updated']"
Deployability,"﻿. Hail | ; Libraries. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Libraries. View page source. Libraries; This pages lists any external libraries we are aware of that are built on top of Hail. These libraries are not developed by the Hail team so we cannot necessarily answer; questions about them, but they may provide useful functions not included in base Hail. gnomad (Hail Utilities for gnomAD); This repo contains a number of Hail utility functions and scripts for the gnomAD project and the Translational Genomics Group.; Install with pip install gnomad.; More info can be found in the documentation or on the PyPI project page. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/libraries.html:922,install,install,922,docs/0.2/libraries.html,https://hail.is,https://hail.is/docs/0.2/libraries.html,2,"['install', 'update']","['install', 'updated']"
Deployability,"﻿. Hail | ; LinearMixedModel. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; stats; LinearMixedModel. View page source. LinearMixedModel. class hail.stats.LinearMixedModel[source]; Class representing a linear mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. Attributes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/stats/hail.stats.LinearMixedModel.html:946,update,updated,946,docs/0.2/stats/hail.stats.LinearMixedModel.html,https://hail.is,https://hail.is/docs/0.2/stats/hail.stats.LinearMixedModel.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Microsoft Azure. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list run",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:676,install,installations,676,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['install'],['installations']
Deployability,"﻿. Hail | ; Other Resources. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Hadoop Glob Patterns. Change Log And Version Policy. menu; Hail. Other Resources. View page source. Other Resources. Hadoop Glob Patterns. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/other_resources.html:565,update,updated,565,docs/0.2/other_resources.html,https://hail.is,https://hail.is/docs/0.2/other_resources.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API. View page source. Python API; This is the Python API documentation for all Hail Python libraries including Query (hail), a cloud-agnostic; file system implementation (hailtop.fs), and Batch (hailtop.batch). hail; hailtop.fs; hailtop.batch. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/root_api.html:749,update,updated,749,docs/0.2/root_api.html,https://hail.is,https://hail.is/docs/0.2/root_api.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/scans.html:765,rolling,rolling,765,docs/0.2/scans.html,https://hail.is,https://hail.is/docs/0.2/scans.html,1,['rolling'],['rolling']
Deployability,"﻿. Hail | ; Search. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Search. Please activate JavaScript to enable the search functionality.; . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/search.html:516,update,updated,516,docs/0.2/search.html,https://hail.is,https://hail.is/docs/0.2/search.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Use Hail on Azure HDInsight. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/azure.html:270,install,installation,270,docs/0.2/install/azure.html,https://hail.is,https://hail.is/docs/0.2/install/azure.html,2,['install'],"['install', 'installation']"
Deployability,"﻿. Hail | ; Use Hail on Google Dataproc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Next Steps. Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Google Dataproc. View page source. Use Hail on Google Dataproc; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl dataproc which starts, stops, and; manipulates Hail-enabled Dataproc clusters.; Start a dataproc cluster named “my-first-cluster”. Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes.; hailctl dataproc start my-first-cluster. Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also start a Jupyter Notebook running on the cluster:; hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:; hailctl dataproc stop my-first-cluster. Next",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/dataproc.html:270,install,installation,270,docs/0.2/install/dataproc.html,https://hail.is,https://hail.is/docs/0.2/install/dataproc.html,2,['install'],"['install', 'installation']"
Deployability,"﻿. Hail | ; VDSMetadata. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VDSMetadata. View page source. VDSMetadata. class hail.vds.combiner.VDSMetadata[source]; The path to a Variant Dataset and the number of samples within. Parameters:. path (str) – Path to the variant dataset.; n_samples (int) – Number of samples contained within the Variant Dataset at path. Attributes. n_samples; Alias for field number 1. path; Alias for field number 0. Methods. __add__(value, /); Return self+value. __class_getitem__(); See PEP 585. __contains__(key, /); Return key in self. __eq__(value, /); Return self==value. __ge__(value, /); Return self>=value. __getitem__(key, /); Return self[key]. __getnewargs__(); Return self as a plain tuple. Used by copy and pickle. __gt__(value, /); Return self>value. __iter__(); Implement iter(self). __le__(value, /); Return self<=value. __len__(); Return len(self). __lt__(value, /); Return self<value. __mul__(value, /); Return self*value. __ne__(value, /); Return self!=value. __rmul__(value, /); Return value*self. count(value, /); Return number of occurrences of value. index(value, start=0, stop=9223372036854775807, /); Return first index of value.; Raises ValueError if the value is not present. n_samples; Alias for field number 1. path; Alias for field number 0. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VDSMetadata.html:1965,update,updated,1965,docs/0.2/vds/hail.vds.combiner.VDSMetadata.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VDSMetadata.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; Your First Hail Query. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+---",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/try.html:252,install,installation,252,docs/0.2/install/try.html,https://hail.is,https://hail.is/docs/0.2/install/try.html,2,['install'],"['install', 'installation']"
Deployability,"﻿. Hail | ; genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics. View page source. genetics. Classes. AlleleType; An enumeration for allele type. Call; An object that represents an individual's call at a genomic locus. Locus; An object that represents a location in the genome. Pedigree; Class containing a list of trios, with extra functionality. ReferenceGenome; An object that represents a reference genome. Trio; Class containing information about nuclear family relatedness and sex. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/index.html:1131,update,updated,1131,docs/0.2/genetics/index.html,https://hail.is,https://hail.is/docs/0.2/genetics/index.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:1477,update,updated,1477,docs/0.2/_modules/hail/ggplot/aes.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:1291,update,updated,1291,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.stats.linear_mixed_model. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.stats.linear_mixed_model. Source code for hail.stats.linear_mixed_model; [docs]class LinearMixedModel(object):; r""""""Class representing a linear mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94. """""". def __init__(self, py, px, s, y=None, x=None, p_path=None):; raise NotImplementedError(""LinearMixedModel is no longer implemented/supported as of Hail 0.2.94""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html:893,update,updated,893,docs/0.2/_modules/hail/stats/linear_mixed_model.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.combiner.load_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.load_combiner. View page source. hail.vds.combiner.load_combiner. hail.vds.combiner.load_combiner(path)[source]; Load a VariantDatasetCombiner from path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.load_combiner.html:911,update,updated,911,docs/0.2/vds/hail.vds.combiner.load_combiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.load_combiner.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.combiner.new_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.new_combiner. View page source. hail.vds.combiner.new_combiner. hail.vds.combiner.new_combiner(*, output_path, temp_path, save_path=None, gvcf_paths=None, vds_paths=None, vds_sample_counts=None, intervals=None, import_interval_size=None, use_genome_default_intervals=False, use_exome_default_intervals=False, gvcf_external_header=None, gvcf_sample_names=None, gvcf_info_to_keep=None, gvcf_reference_entry_fields_to_keep=None, call_fields=['PGT'], branch_factor=100, target_records=24000, gvcf_batch_size=None, batch_size=None, reference_genome='default', contig_recoding=None, force=False)[source]; Create a new VariantDatasetCombiner or load one from save_path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html:1419,update,updated,1419,docs/0.2/vds/hail.vds.combiner.new_combiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.filter_chromosomes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_chromosomes. View page source. hail.vds.filter_chromosomes. hail.vds.filter_chromosomes(vds, *, keep=None, remove=None, keep_autosomes=False)[source]; Filter chromosomes of a VariantDataset in several possible modes.; Notes; There are three modes for filter_chromosomes(), based on which argument is passed; to the function. Exactly one of the below arguments must be passed by keyword. keep: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a VariantDataset with only those; chromosomes.; remove: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a VariantDataset with those chromosomes; removed.; keep_autosomes: This argument expects the value True, and returns a dataset without; sex and mitochondrial chromosomes. Parameters:. vds (VariantDataset) – Dataset.; keep – Keep a specified list of contigs.; remove – Remove a specified list of contigs; keep_autosomes – If true, keep only autosomal chromosomes. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.filter_chromosomes.html:1821,update,updated,1821,docs/0.2/vds/hail.vds.filter_chromosomes.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_chromosomes.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.filter_intervals. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_intervals. View page source. hail.vds.filter_intervals. hail.vds.filter_intervals(vds, intervals, *, split_reference_blocks=False, keep=True)[source]; Filter intervals in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; intervals (Table or ArrayExpression of type tinterval) – Intervals to filter on.; split_reference_blocks (bool) – If true, remove reference data outside the given intervals by segmenting reference; blocks at interval boundaries. Results in a smaller result, but this filter mode; is more computationally expensive to evaluate.; keep (bool) – Whether to keep, or filter out (default) rows that fall within any; interval in intervals. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.filter_intervals.html:1476,update,updated,1476,docs/0.2/vds/hail.vds.filter_intervals.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_intervals.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.filter_samples. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_samples. View page source. hail.vds.filter_samples. hail.vds.filter_samples(vds, samples, *, keep=True, remove_dead_alleles=False)[source]; Filter samples in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; samples (Table or list of str) – Samples to keep or remove.; keep (bool) – Whether to keep (default), or filter out the samples from samples_table.; remove_dead_alleles (bool) – If true, remove alleles observed in no samples. Alleles with AC == 0 will be; removed, and LA values recalculated. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.filter_samples.html:1321,update,updated,1321,docs/0.2/vds/hail.vds.filter_samples.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_samples.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.filter_variants. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_variants. View page source. hail.vds.filter_variants. hail.vds.filter_variants(vds, variants_table, *, keep=True)[source]; Filter variants in a VariantDataset, without removing reference; data. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; variants_table (Table) – Variants to filter on.; keep (bool) – Whether to keep (default), or filter out the variants from variants_table. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.filter_variants.html:1185,update,updated,1185,docs/0.2/vds/hail.vds.filter_variants.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_variants.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.impute_sex_chromosome_ploidy. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.impute_sex_chromosome_ploidy. View page source. hail.vds.impute_sex_chromosome_ploidy. hail.vds.impute_sex_chromosome_ploidy(vds, calling_intervals, normalization_contig, use_variant_dataset=False)[source]; Impute sex chromosome ploidy from depth of reference or variant data within calling intervals.; Returns a Table with sample ID keys, with the following fields:. autosomal_mean_dp (float64): Mean depth on calling intervals on normalization contig.; x_mean_dp (float64): Mean depth on calling intervals on X chromosome.; x_ploidy (float64): Estimated ploidy on X chromosome. Equal to 2 * x_mean_dp / autosomal_mean_dp.; y_mean_dp (float64): Mean depth on calling intervals on chromosome.; y_ploidy (float64): Estimated ploidy on Y chromosome. Equal to 2 * y_mean_db / autosomal_mean_dp. Parameters:. vds (vds: VariantDataset) – Dataset.; calling_intervals (Table or ArrayExpression) – Calling intervals with consistent read coverage (for exomes, trim the capture intervals).; normalization_contig (str) – Autosomal contig for depth comparison.; use_variant_dataset (bool) – Whether to use depth of variant data within calling intervals instead of reference data. Default will use reference data. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html:1972,update,updated,1972,docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.lgt_to_gt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.lgt_to_gt. View page source. hail.vds.lgt_to_gt. hail.vds.lgt_to_gt(lgt, la)[source]; Transform LGT into GT using local alleles array. Parameters:. lgt (CallExpression) – LGT value.; la (ArrayExpression) – Local alleles array. Returns:; CallExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.lgt_to_gt.html:988,update,updated,988,docs/0.2/vds/hail.vds.lgt_to_gt.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.lgt_to_gt.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.merge_reference_blocks. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.merge_reference_blocks. View page source. hail.vds.merge_reference_blocks. hail.vds.merge_reference_blocks(ds, equivalence_function, merge_functions=None)[source]; Merge adjacent reference blocks according to user equivalence criteria.; Examples; Coarsen GQ granularity into bins of 10 and merges blocks with the same GQ in order to; compress reference data.; >>> rd = vds.reference_data ; >>> vds.reference_data = rd.annotate_entries(GQ = rd.GQ - rd.GQ % 10) ; >>> vds2 = hl.vds.merge_reference_blocks(vds,; ... equivalence_function=lambda block1, block2: block1.GQ == block2.GQ),; ... merge_functions={'MIN_DP': 'min'}) . Notes; The equivalence_function argument expects a function from two reference blocks to a; boolean value indicating whether they should be combined. Adjacency checks are builtin; to the method (two reference blocks are ‘adjacent’ if the END of one block is one base; before the beginning of the next).; The merge_functions. Parameters:; ds (VariantDataset or MatrixTable) – Variant dataset or reference block matrix table. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.merge_reference_blocks.html:1837,update,updated,1837,docs/0.2/vds/hail.vds.merge_reference_blocks.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.merge_reference_blocks.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.read_vds. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.read_vds. View page source. hail.vds.read_vds. hail.vds.read_vds(path, *, intervals=None, n_partitions=None, _assert_reference_type=None, _assert_variant_type=None, _warn_no_ref_block_max_length=True)[source]; Read in a VariantDataset written with VariantDataset.write(). Parameters:; path (str). Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.read_vds.html:1057,update,updated,1057,docs/0.2/vds/hail.vds.read_vds.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.read_vds.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.sample_qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.sample_qc. View page source. hail.vds.sample_qc. hail.vds.sample_qc(vds, *, gq_bins=(0, 20, 60), dp_bins=(0, 1, 10, 20, 30), dp_field=None)[source]; Compute sample quality metrics about a VariantDataset.; If the dp_field parameter is not specified, the DP is used for depth; if present. If no DP field is present, the MIN_DP field is used. If no DP; or MIN_DP field is present, no depth statistics will be calculated. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; gq_bins (tuple of int) – Tuple containing cutoffs for genotype quality (GQ) scores.; dp_bins (tuple of int) – Tuple containing cutoffs for depth (DP) scores.; dp_field (str) – Name of depth field. If not supplied, DP or MIN_DP will be used, in that order. Returns:; Table – Hail Table of results, keyed by sample. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.sample_qc.html:1546,update,updated,1546,docs/0.2/vds/hail.vds.sample_qc.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.sample_qc.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.split_multi. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.split_multi. View page source. hail.vds.split_multi. hail.vds.split_multi(vds, *, filter_changed_loci=False)[source]; Split the multiallelic variants in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; filter_changed_loci (bool) – If any REF/ALT pair changes locus under min_rep(), filter that; variant instead of throwing an error. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html:1144,update,updated,1144,docs/0.2/vds/hail.vds.split_multi.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html:1059,patch,patch,1059,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html,2,"['patch', 'update']","['patch', 'updated']"
Deployability,"﻿. Hail | ; hail.vds.to_dense_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.to_dense_mt. View page source. hail.vds.to_dense_mt. hail.vds.to_dense_mt(vds)[source]; Creates a single, dense MatrixTable from the split; VariantDataset representation. Parameters:; vds (VariantDataset) – Dataset in VariantDataset representation. Returns:; MatrixTable – Dataset in dense MatrixTable representation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.to_dense_mt.html:1055,update,updated,1055,docs/0.2/vds/hail.vds.to_dense_mt.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.to_dense_mt.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hail.vds.to_merged_sparse_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.to_merged_sparse_mt. View page source. hail.vds.to_merged_sparse_mt. hail.vds.to_merged_sparse_mt(vds, *, ref_allele_function=None)[source]; Creates a single, merged sparse MatrixTable from the split; VariantDataset representation. Parameters:; vds (VariantDataset) – Dataset in VariantDataset representation. Returns:; MatrixTable – Dataset in the merged sparse MatrixTable representation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html:1136,update,updated,1136,docs/0.2/vds/hail.vds.to_merged_sparse_mt.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/batch_api.html:873,deploy,deploy,873,docs/0.2/batch_api.html,https://hail.is,https://hail.is/docs/0.2/batch_api.html,2,"['deploy', 'update']","['deploy', 'updated']"
Deployability,"﻿. Hail | ; hailtop.frozendict. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.frozendict. Source code for hailtop.frozendict; from collections.abc import Mapping; from typing import Dict, Generic, TypeVar. T = TypeVar(""T""); U = TypeVar(""U""). [docs]class frozendict(Mapping, Generic[T, U]):; """"""; An object representing an immutable dictionary. >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a `frozendict`:. >>> dict(frozendict({'a': 1, 'b': 2})). Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.my_dict.take(5)``. This is rare; it is much; more common to manipulate the :class:`.DictExpression` object, which is; constructed using :func:`.dict`. This class is necessary because hail; supports using dicts as keys to other dicts or as elements in sets, while; python does not. """""". def __init__(self, d: Dict[T, U]):; self.d = d.copy(). def __getitem__(self, k: T) -> U:; return self.d[k]. def __hash__(self) -> int:; return hash(frozenset(self.items())). def __len__(self) -> int:; return len(self.d). def __iter__(self):; return iter(self.d). def __repr__(self):; return f'frozendict({self.d!r})'. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html:1631,update,updated,1631,docs/0.2/_modules/hailtop/frozendict.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html,1,['update'],['updated']
Deployability,"﻿. Hail | ; linalg. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg. View page source. linalg; File formats and interface for numeric matrices are experimental.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. Classes. BlockMatrix; Hail's block-distributed matrix of tfloat64 elements. Modules. utils. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/index.html:792,pipeline,pipelines,792,docs/0.2/linalg/index.html,https://hail.is,https://hail.is/docs/0.2/linalg/index.html,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"﻿. Hail | ; stats. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; stats. View page source. stats. Classes. LinearMixedModel; Class representing a linear mixed model. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/stats/index.html:795,update,updated,795,docs/0.2/stats/index.html,https://hail.is,https://hail.is/docs/0.2/stats/index.html,1,['update'],['updated']
Deployability,"﻿. Hail | Index . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scala",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:787,install,install,787,index.html,https://hail.is,https://hail.is/index.html,2,['install'],"['install', 'installed']"
Deployability,"﻿. Hail | References . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226–237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336–347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333–342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leońska-Duniec, A., Borczyk, M., Korostyński, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 2",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:308,install,installed,308,references.html,https://hail.is,https://hail.is/references.html,2,"['install', 'update']","['installed', 'updated']"
Energy Efficiency," 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""ceil"", x.dtype, x). [docs]@typecheck(n_hom_ref=expr_int32, n_het=expr_int32, n_hom_var=expr_int32, one_sided=expr_bool); def hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:34323,efficient,efficient,34323,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['efficient'],['efficient']
Energy Efficiency," : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""; if gene_symbols is None and gene_ids is None and transcript_ids is None:; raise ValueError('get_gene_intervals requires at least one of gene_symbols, gene_ids, or transcript_ids'); ht = _load_gencode_gtf(gtf_file, reference_genome); criteria = []; if gene_symbols:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_name == y), gene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval))); if verbose:; info(; f'get_gene_intervals found {len(gene_info)} entries:\n'; + ""\n"".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == ""gene"" else x[3]})', gene_info)); ); intervals = list(map(lambda x: x[-1], gene_info)); return intervals. def _load_gencode_gtf(gtf_file=None, reference_genome=None):; """"""; Get Gencode GTF (from file or reference genome). Parameters; ----------; reference_genome : :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :class:`.Table`; """"""; GTFS = {; 'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',; 'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annota",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:8694,reduce,reduce,8694,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,1,['reduce'],['reduce']
Energy Efficiency," >>> hl.eval(hl.any([False, True, False])); True. >>> hl.eval(hl.any([False, False, False])); False. The third form:. >>> a = ['The', 'quick', 'brown', 'fox']; >>> s = {1, 3, 5, 6, 7, 9}. >>> hl.eval(hl.any(lambda x: x[-1] == 'x', a)); True. >>> hl.eval(hl.any(lambda x: x % 4 == 0, s)); False. Notes; -----; :func:`~.any` returns ``False`` when given an empty array or empty argument list.; """"""; base = hl.literal(False); if builtins.len(args) == 0:; return base; if builtins.len(args) == 1:; arg = arg_check(args[0], 'any', 'collection', oneof(collection_type, expr_bool)); if arg.dtype == hl.tbool:; return arg; return arg.any(lambda x: x); if builtins.len(args) == 2:; if callable(args[0]):; f = arg_check(args[0], 'any', 'f', any_to_bool_type); collection = arg_check(args[1], 'any', 'collection', collection_type); return collection.any(f); n_args = builtins.len(args); args = [args_check(x, 'any', 'exprs', i, n_args, expr_bool) for i, x in builtins.enumerate(args)]; return functools.reduce(operator.ior, args, base). [docs]def all(*args) -> BooleanExpression:; """"""Check for all ``True`` in boolean expressions or collections of booleans. :func:`~.all` comes in three forms:. 1. ``hl.all(boolean, ...)``. Are all arguments ``True``?. 2. ``hl.all(collection)``. Are all elements of the collection ``True``?. 3. ``hl.all(function, collection)``. Does ``function`` return ``True`` for; all values in this collection?. Examples; --------. The first form:. >>> hl.eval(hl.all()); True. >>> hl.eval(hl.all(True)); True. >>> hl.eval(hl.all(False)); False. >>> hl.eval(hl.all(True, True, True)); True. >>> hl.eval(hl.all(False, False, True, False)); False. The second form:. >>> hl.eval(hl.all([False, True, False])); False. >>> hl.eval(hl.all([True, True, True])); True. The third form:. >>> a = ['The', 'quick', 'brown', 'fox']; >>> s = {1, 3, 5, 6, 7, 9}. >>> hl.eval(hl.all(lambda x: hl.len(x) > 3, a)); False. >>> hl.eval(hl.all(lambda x: x < 10, s)); True. Notes; -----; :func:`~.all` returns `",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:109361,reduce,reduce,109361,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['reduce'],['reduce']
Energy Efficiency," >>> hl.eval(y // 2); 2.0. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; The floor of the left number divided by the right.; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(other=expr_bool); def __and__(self, other):; """"""Return ``True`` if the left and right arguments are ``True``. Examples; --------. >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59413,power,power,59413,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['power'],['power']
Energy Efficiency," Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:6224,allocate,allocated,6224,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['allocate'],['allocated']
Energy Efficiency," Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from a diagonal band. By default,; all elements outside the band but inside blocks that overlap the; band are set to zero as well.; The band is defined in terms of inclusive lower and upper indices; relative to the diagonal. For example, the indices -1, 0, and 1; correspond to the sub-diagonal, diagonal, and super-diagonal,; respectively. The diagonal band contains the elements at positions; \((i, j)\) such that. \[\mathrm{lower} \leq j - i \leq \mathrm{upper}.\]; lower must be less than or equal to upper, but their values may; exceed the dimensions of the matrix, the band need not include the; diagonal, and the matrix need not be square. Parameters:. lower (int) – Index of lowest band relative to the diagonal.; upper (int) – Index of highest band relative to the diagonal.; blocks_only (bool) – If False, set all elements outside the band to zero.; If True, only set all blocks outside the band to blocks; of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sparsify_rectangles(rectangles)[source]; Filter to blocks overlapping the union of rectangular regions.; Examples; Consider the following block matrix:; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to blocks covering three rectangles and collect to NumPy:; >>> bm.sparsify_rectangles([[0, 1, 0, 1], [0, 3, 0, 2], [1, 2, 0, 4]]).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 9., 10., 0., 0.],; [13., 14., 0., 0.]]). Notes; This method creates a block-sparse matrix by zeroing out (dropping); all blocks which are disjoint from the union of a set of rectangular; regions. Partially overlapping blocks are not modified.; Each rectangle is encoded as a list of length four of; the form [row_start, row_stop, col_start, col_s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:29944,efficient,efficient,29944,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency," Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:2299,efficient,efficient,2299,index.html,https://hail.is,https://hail.is/index.html,1,['efficient'],['efficient']
Energy Efficiency," Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). [docs]@typecheck(nd=expr_ndarray(), full_matrices=bool, compute_uv=bool); def svd(nd, full_matrices=True, compute_uv=True):; """"""Performs a singular value decomposition. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv : :class:`.bool`; If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns; -------; - u: :class:`.NDArrayNumericExpression`; The left singular vectors.; - s: :class:`.NDArrayNumericExpression`; The singular values.; - vt: :class:`.NDArrayNumericExpression`; The right singular vectors.; """"""; float_nd = nd.ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:11084,reduce,reduced,11084,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['reduce'],['reduced']
Energy Efficiency," This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general, once you know the wall clock time of your job, you can enter your cluster parameters into the; Google Cloud Pricing Calculator. and get a precise estimate; of cost using the latest prices. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:3479,charge,charges,3479,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['charge'],['charges']
Energy Efficiency," already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_fro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:26929,reduce,reduce,26929,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['reduce'],['reduce']
Energy Efficiency," alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate al",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:7140,reduce,reduced,7140,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['reduce'],['reduced']
Energy Efficiency," by zeroing out all blocks; which are disjoint from a diagonal band. By default,; all elements outside the band but inside blocks that overlap the; band are set to zero as well. The band is defined in terms of inclusive `lower` and `upper` indices; relative to the diagonal. For example, the indices -1, 0, and 1; correspond to the sub-diagonal, diagonal, and super-diagonal,; respectively. The diagonal band contains the elements at positions; :math:`(i, j)` such that. .. math::. \mathrm{lower} \leq j - i \leq \mathrm{upper}. `lower` must be less than or equal to `upper`, but their values may; exceed the dimensions of the matrix, the band need not include the; diagonal, and the matrix need not be square. Parameters; ----------; lower: :obj:`int`; Index of lowest band relative to the diagonal.; upper: :obj:`int`; Index of highest band relative to the diagonal.; blocks_only: :obj:`bool`; If ``False``, set all elements outside the band to zero.; If ``True``, only set all blocks outside the band to blocks; of zeros; this is more efficient. Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if lower > upper:; raise ValueError(f'sparsify_band: lower={lower} is greater than upper={upper}'). bounds = hl.literal((lower, upper), hl.ttuple(hl.tint64, hl.tint64)); return BlockMatrix(BlockMatrixSparsify(self._bmir, bounds._ir, BandSparsifier(blocks_only))). [docs] @typecheck_method(lower=bool, blocks_only=bool); def sparsify_triangle(self, lower=False, blocks_only=False):; """"""Filter to the upper or lower triangle. Examples; --------; Consider the following block matrix:. >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:. >>> bm.sparsify_triangle().to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:32202,efficient,efficient,32202,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['efficient'],['efficient']
Energy Efficiency," each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; BlockMatrix.write_from_entry_expr().; The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; ld_matrix().; To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:22403,efficient,efficiently,22403,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['efficient'],['efficiently']
Energy Efficiency," fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Expl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:3568,efficient,efficient,3568,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['efficient'],['efficient']
Energy Efficiency," importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:1751,efficient,efficient,1751,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['efficient'],['efficient']
Energy Efficiency," locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic varia",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117823,efficient,efficient,117823,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['efficient'],['efficient']
Energy Efficiency," not the genotypes. Use; split_multi_hts() if possible, or split the genotypes yourself using; one of the entry modification methods: MatrixTable.annotate_entries(),; MatrixTable.select_entries(), MatrixTable.transmute_entries().; The resulting dataset will be keyed by the split locus and alleles.; split_multi() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2.; old_locus (locus) – The original, unsplit locus.; old_alleles (array<str>) – The original, unsplit alleles. All other fields are left unchanged. Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:84444,efficient,efficient,84444,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['efficient'],['efficient']
Energy Efficiency," num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion SNP.; This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. num_mismatch()[source]¶; Returns the number of mismatched bases in this alternate allele.; Fails if the ref and alt alleles are not the same length. Return type:int. ref¶; Reference allele. Return type:str. stripped_snp()[source]¶; Returns the one-character reduced SNP.; Fails if called on an alternate allele that is not a SNP. Return type:str, str. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:3413,reduce,reduced,3413,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,1,['reduce'],['reduced']
Energy Efficiency," simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:2697,charge,charges,2697,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['charge'],['charges']
Energy Efficiency," the entire Batch and are not outputs of a BashJob.; vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). We use the gwas function defined above to create a new job on the batch to; perform a GWAS that outputs a binary PLINK file and association results:; g = gwas(batch, vcf, phenotypes). We call the clump function once per chromosome and aggregate a list of the; clumping results files passing the outputs from the g job defined above; as inputs to the clump function:; results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). Finally, we use the merge function to concatenate the results into a single file; and then write this output to a permanent location using Batch.write_output().; The inputs to the merge function are the clumped output files from each of the clump; jobs.; m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). The last thing we do is submit the Batch to the service and then close the Backend:; batch.run(open=True, wait=False) # doctest: +SKIP; backend.close(). Synopsis; We provide the code used above in one place for your reference:. run_gwas.py; import argparse. import hail as hl. def run_gwas(vcf_file, phenotypes_file, output_file):; table = hl.import_table(phenotypes_file, impute=True).key_by('Sample'). hl.import_vcf(vcf_file).write('tmp.mt'); mt = hl.read_matrix_table('tmp.mt'). mt = mt.annotate_cols(pheno=table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = (; (mt.GT.is_hom_ref() & (ab <= 0.1)); | (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)); | (mt.GT.is_hom_var() & (ab >= 0.9)); ); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). eigenvalues, pcs",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:11572,consumption,consumption,11572,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['consumption'],['consumption']
Energy Efficiency," with height 0, those cannot be log transformed and were left as 0s.""; ). changes = {; ""bin_freq"": bin_freq,; ""n_larger"": math.log10(data.n_larger) if data.n_larger > 0.0 else data.n_larger,; ""n_smaller"": math.log10(data.n_smaller) if data.n_smaller > 0.0 else data.n_smaller,; }; data = data.annotate(**changes); y_axis_label = 'log10 Frequency'; else:; y_axis_label = 'Frequency'. x_span = data.bin_edges[-1] - data.bin_edges[0]; x_start = data.bin_edges[0] - 0.05 * x_span; x_end = data.bin_edges[-1] + 0.05 * x_span; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; background_fill_color='#EEEEEE',; x_range=(x_start, x_end),; ); q = p.quad(; bottom=0,; top=data.bin_freq,; left=data.bin_edges[:-1],; right=data.bin_edges[1:],; legend_label=legend,; line_color='black',; ); if data.n_larger > 0:; p.quad(; bottom=0,; top=data.n_larger,; left=data.bin_edges[-1],; right=(data.bin_edges[-1] + (data.bin_edges[1] - data.bin_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_inte",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:12776,green,green,12776,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['green'],['green']
Energy Efficiency," with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}\]; The eigenvalues of \(Z Z^T\) and \(Z^T Z\) are the squared singular values of \(Z\);; therefore, we instead focus on \(Z^T Z\). In the expressions below, we elide transpositions; of symmetric matrices:. \[\begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}\]; Before substituting the definition of \(P_0\), simplify it using the reduced QR; decomposition:. \[\begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:70529,reduce,reduced,70529,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['reduce'],['reduced']
Energy Efficiency," x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:3867,power,power,3867,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,2,['power'],['power']
Energy Efficiency," yourself using; one of the entry modification methods: :meth:`.MatrixTable.annotate_entries`,; :meth:`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. - `old_locus` (*locus*) -- The original, unsplit locus. - `old_alleles` (*array<str>*) -- The original, unsplit alleles. All other fields are left unchanged. Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:112204,efficient,efficient,112204,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['efficient'],['efficient']
Energy Efficiency,"""""""; return _func(""dnorm"", tfloat64, x, mu, sigma, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, log_p=expr_bool); def dpois(x, lamb, log_p=False) -> Float64Expression:; """"""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""; return _func(""dpois"", tfloat64, x, lamb, log_p). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def exp(x) -> Float64Expression:; """"""Computes `e` raised to the power `x`. Examples; --------. >>> hl.eval(hl.exp(2)); 7.38905609893065. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""exp"", tfloat64, x). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32); def fisher_exact_test(c1, c2, c3, c4) -> StructExpression:; """"""Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher's exact test for a 2x2 table. Examples; --------. >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.100747675033819). >>> hl.eval(hl.fisher_exact_test(51, 43, 22, 92)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967,; ci_95_lower=2.5659373368",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:30544,power,power,30544,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['power'],['power']
Energy Efficiency,"',; u'HG02282',; u'HG02477']},; {u'at': u'8:95863909:A:T', u'homvars': []},; {u'at': u'8:97172671:C:T', u'homvars': []}]. takeBy¶; takeBy is an aggregator that takes elements of an aggregable ordered; by a lambda function (smallest to largest). We can easily select the; variants with the lowest p-values after regression:. In [42]:. top_5_pvals = (vds.linreg('sa.metadata.CaffeineConsumption'); .query_variants('variants.map(v => {at: str(v), pval: va.linreg.pval}).takeBy(x => x.pval, 5)')); pprint(top_5_pvals). 2018-10-18 01:26:07 Hail: INFO: Running linear regression on 1000 samples with 1 covariate including intercept... [{u'at': u'10:56025604:A:C', u'pval': 5.595049078641033e-05},; {u'at': u'20:55431571:A:C', u'pval': 0.00010899661736561121},; {u'at': u'10:91099630:T:C', u'pval': 0.00013497679316886596},; {u'at': u'4:149350527:T:C', u'pval': 0.00017786066989195366},; {u'at': u'7:152600817:G:A', u'pval': 0.0002252314501866726}]. Aggregating by key¶; The; aggregate_by_key; method is likely the most powerful piece of query functionality in Hail.; It’s a method on KeyTable.; You can produce key tables from a; VariantDataset with; three methods:. variants_table():; a key table with the variant and variant annotations as columns.; There is one row per variant.; samples_table():; a key table with the sample and sample annotations as columns. There; is one row per sample.; genotypes_table():; a key table that is the coordinate representation of the genetic; matrix. The columns are the variant, variant annotations, sample,; sample annotations, and genotype. There is one row per variant/sample; combination: (N * M) total rows!. Using; aggregate_by_key; with; genotypes_table; can produce counts of loss of function variants in cases and controls; per gene, compute the mean depth per sample per exon, and much more. You; define the aggregation keys, and you define how to combine the rows.; This method produces another; KeyTable.; We use it here to compute the mean depth and quali",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:19041,power,powerful,19041,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['power'],['powerful']
Energy Efficiency,"(self, other):; return self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Positionally divide by an array or a scalar using floor division. Examples; --------. >>> hl.eval(a1 // 2); [0, 0, 1, 1, 2, 2]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Positionally compute the left modulo the right. Examples; --------. >>> hl.eval(a1 % 2); [0, 1, 0, 1, 0, 1]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, other):; """"""Positionally raise to the power of an array or a scalar. Examples; --------. >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('**', other, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class SetExpression(CollectionExpression):; """"""Expression of type :class:`.tset`. >>> s1 = hl.literal({1, 2, 3}); >>> s2 = hl.literal({1, 3, 5}). See Also; --------; :class:`.CollectionExpression`; """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(SetExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tset); self._ec = ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:25273,power,power,25273,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['power'],['power']
Energy Efficiency,"(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable) – The function to execute.; iterables (Iterable[Any]) – The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None]) – This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator’s; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int) – The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool) – If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the underlying Docker image. For more; details see the default_image argument to BatchPoolExecutor; This function does not return the function’s output, it returns a; BatchPoolFuture whose BatchPoolFuture.result() method; can be used to access ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5933,reduce,reduce,5933,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['reduce'],['reduce']
Energy Efficiency,"(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Float32Expression.html:4073,power,power,4073,docs/0.2/hail.expr.Float32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html,4,['power'],['power']
Energy Efficiency,") – estimated standard error, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Double) – \(p\)-value. Parameters:; y (str) – Response expression; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosages genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg3(ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter on allele count or allele; frequency. Warning; linreg3() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covaria",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:82594,efficient,efficient,82594,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficient']
Energy Efficiency,")). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(threshold=numeric,; tiebreaking_expr=nullable(strlike),; maf=nullable(strlike),; bounded=bool); def ibd_prune(self, threshold, tiebreaking_expr=None, maf=None, bounded=True):; """"""; Prune samples from the :py:class:`.VariantDataset` based on :py:meth:`~hail.VariantDataset.ibd` PI_HAT measures of relatedness. .. include:: requireTGenotype.rst. **Examples**; ; Prune samples so that no two have a PI_HAT value greater than or equal to 0.6.; ; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:. >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). **Notes**. The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted. ; ; The tiebreaking_expr namespace has the following variables available:; ; - ``s1``: The first sample id.; - ``sa1``: The annotations associated with s1.; - ``s2``: The second sample id. ; - ``sa2``: The annotations associated with s2. ; ; The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping ``s1``. Any positive integer expresses a preference for keeping ``s2``. A zero expresses no preference. This function must induce a `preorder <https://en.wikipedia.org/wiki/Preorder>`__ on the samples, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:84977,efficient,efficient,84977,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficient']
Energy Efficiency,"); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:9344,reduce,reduced,9344,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['reduce'],['reduced']
Energy Efficiency,"); null_mu = mt.covmat_Q @ (mt.covmat_Q.T @ mt.yvec); y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=y_residual @ y_residual.T / (n - k)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= max_size, ht.G_take)).T,; ); ht = ht.annotate(Q=((ht.y_residual @ ht.G).map(lambda x: x**2) * ht.weight).sum(0)). # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:83030,reduce,reduced,83030,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['reduce'],['reduced']
Energy Efficiency,", and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1851,consumption,consumption,1851,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['consumption'],['consumption']
Energy Efficiency,",; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) # doctest: +SKIP. Only compute the kinship statistic. This is more efficient than; computing all statistics. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') # doctest: +SKIP. Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using :meth:`.Table.filter`. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) # doctest: +SKIP. One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:2506,efficient,efficient,2506,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,1,['efficient'],['efficient']
Energy Efficiency,"----------+------------+---------------+; | 20:10019093 | [""A"",""G""] | 1 |; | 20:10019093 | [""A"",""G""] | 2 |; | 20:10026348 | [""A"",""G""] | 1 |; | 20:10026348 | [""A"",""G""] | 2 |; | 20:10026357 | [""T"",""C""] | 1 |; | 20:10026357 | [""T"",""C""] | 2 |; | 20:10030188 | [""T"",""A""] | 1 |; | 20:10030188 | [""T"",""A""] | 2 |; | 20:10030452 | [""G"",""A""] | 1 |; | 20:10030452 | [""G"",""A""] | 2 |; +---------------+------------+---------------+; showing top 10 rows. Aggregation; MatrixTable has three methods to compute aggregate statistics. MatrixTable.aggregate_rows(); MatrixTable.aggregate_cols(); MatrixTable.aggregate_entries(). These methods take an aggregated expression and evaluate it, returning; a Python value.; An example of querying entries is to compute the global mean of field GQ:; >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) ; 67.73196915777027. It is possible to compute multiple values simultaneously by; creating a tuple or struct. This is encouraged, because grouping two; computations together is far more efficient by traversing the dataset only once; rather than twice.; >>> mt.aggregate_entries((hl.agg.stats(mt.DP), hl.agg.stats(mt.GQ))) ; (Struct(mean=41.83915800445897, stdev=41.93057654787303, min=0.0, max=450.0, n=34537, sum=1444998.9999999995),; Struct(mean=67.73196915777027, stdev=29.80840934057741, min=0.0, max=99.0, n=33720, sum=2283922.0000000135)). See the Aggregators page for the complete list of aggregator; functions. Group-By; Matrix tables can be aggregated along the row or column axis to produce a new; matrix table. MatrixTable.group_rows_by(); MatrixTable.group_cols_by(). First let’s add a random phenotype as a new column field case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:10347,efficient,efficient,10347,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['efficient'],['efficient']
Energy Efficiency,"------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.; The pca function produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The hwe_normalized_pca function does the same, using HWE-normalized genotypes for the PCA. [41]:. eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). [Stage 158:> (0 + 1) / 1]. [42]:. pprint(eigenvalues). [18.084111467840707,; 9.9840764",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:20336,consumption,consumption,20336,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['consumption'],['consumption']
Energy Efficiency,"-------; :class:`numpy.ndarray`; """""". def parse_rects(fname):; rect_idx_and_bounds = [int(i) for i in re.findall(r'\d+', fname)]; if len(rect_idx_and_bounds) != 5:; raise ValueError(f'Invalid rectangle file name: {fname}'); return rect_idx_and_bounds. rect_files = [file['path'] for file in hl.utils.hadoop_ls(path) if not re.match(r'.*\.crc', file['path'])]; rects = [parse_rects(os.path.basename(file_path)) for file_path in rect_files]. n_rows = max(rects, key=lambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \time",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72554,reduce,reduced,72554,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['reduce'],['reduced']
Energy Efficiency,". RunningBatchType — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; RunningBatchType. Backend; LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; RunningBatchType. View page source. RunningBatchType. class hailtop.batch.backend.RunningBatchType; The type of value returned by Backend._run(). The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; alias of TypeVar(‘RunningBatchType’). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html:606,monitor,monitor,606,docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,1,['monitor'],['monitor']
Energy Efficiency,".01'); .ld_prune(memory_per_core=256, num_cores=4)). 2018-10-18 01:26:50 Hail: INFO: Running LD prune with nSamples=843, nVariants=9085, nPartitions=4, and maxQueueSize=257123.; 2018-10-18 01:26:50 Hail: INFO: LD prune step 1 of 3: nVariantsKept=8478, nPartitions=4, time=351.375ms; 2018-10-18 01:26:51 Hail: INFO: LD prune step 2 of 3: nVariantsKept=8478, nPartitions=12, time=1.184s; 2018-10-18 01:26:52 Hail: INFO: Coerced sorted dataset; 2018-10-18 01:26:52 Hail: INFO: LD prune step 3 of 3: nVariantsKept=8478, time=481.478ms. In [44]:. common_vds.count(). Out[44]:. (843L, 8555L). These filters removed about 15% of sites (we started with a bit over; 10,000). This is NOT representative of most sequencing datasets! We; have already downsampled the full thousand genomes dataset to include; more common variants than we’d expect by chance.; In Hail, the association tests accept sample annotations for the sample; phenotype and covariates. Since we’ve already got our phenotype of; interest (caffeine consumption) in the dataset, we are good to go:. In [45]:. gwas = common_vds.linreg('sa.CaffeineConsumption'); pprint(gwas.variant_schema). 2018-10-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:21734,consumption,consumption,21734,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['consumption'],['consumption']
Energy Efficiency,"0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:; >>> bm.sparsify_triangle().to_numpy() ; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:; >>> bm.sparsify_triangle(blocks_only=True).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triangle. By; default, all elements outside the triangle but inside blocks that; overlap the triangle are set to zero as well. Parameters:. lower (bool) – If False, keep the upper triangle.; If True, keep the lower triangle.; blocks_only (bool) – If False, set all elements outside the triangle to zero.; If True, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sqrt()[source]; Element-wise square root. Returns:; BlockMatrix. sum(axis=None)[source]; Sums array elements over one or both axes.; Examples; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:34646,efficient,efficient,34646,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"01822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Double; }; }. Now that we’ve got principal components per sample, we may as well plot; them! Human history exerts a strong effect in genetic datasets. Even; with a 50MB sequencing dataset, we can recover the major human; populations. In [51]:. pca_table = pca.samples_table().to_pandas(); colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}; plt.scatter(pca_table[""sa.pca.PC1""], pca_table[""sa.pca.PC2""],; c = pca_table[""sa.SuperPopulation""].map(colors),; alpha = .5); plt.xlim(-0.6, 0.6); plt.xlabel(""PC1""); plt.ylabel(""PC2""); legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]; plt.legend(handles=legend_entries, loc=2); plt.show(). Now we can rerun our linear regression, controlling for the first few; principal components and sample sex. In [52]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including intercept... In [53]:. qqplot(pvals, 5, 6). In [54]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:25716,green,green,25716,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['green'],['green']
Energy Efficiency,"1, 0, 2, 2],; ... stops= [2, 0, 3, 4],; ... blocks_only=True); ... .to_numpy()) ; array([[ 1., 2., 0., 0.],; [ 5., 6., 0., 0.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from all row intervals. By default, all elements; outside the row intervals but inside blocks that overlap the row; intervals are set to zero as well.; starts and stops must both have length equal to the number of; rows. The interval for row i is [starts[i], stops[i]). In; particular, 0 <= starts[i] <= stops[i] <= n_cols is required; for all i.; This method requires the number of rows to be less than \(2^{31}\). Parameters:. starts (list of int, or numpy.ndarray of int) – Start indices for each row (inclusive).; stops (list of int, or numpy.ndarray of int) – Stop indices for each row (exclusive).; blocks_only (bool) – If False, set all elements outside row intervals to zero.; If True, only set all blocks outside row intervals to blocks; of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sparsify_triangle(lower=False, blocks_only=False)[source]; Filter to the upper or lower triangle.; Examples; Consider the following block matrix:; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:; >>> bm.sparsify_triangle().to_numpy() ; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:; >>> bm.sparsify_triangle(blocks_only=True).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triang",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:33301,efficient,efficient,33301,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"6-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis converge to implicate negative selection and neuronal processes in the; 	 aetiology of schizophrenia. Mol Psychiatry 27, 3699–3707; 	 (2022). https://doi.org/10.1038/s41380-022-01621-8 https://www.nature.com/articles/s41380-022-01621-8. 	 Mitja, K.I., et al. FinnGen: Unique genetic insights from combining isolated population; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders wit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:9398,power,power,9398,references.html,https://hail.is,https://hail.is/references.html,1,['power'],['power']
Energy Efficiency,": {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; entries are structs of a single field `element`. Parameters; ----------; n_partitions : int or None; Number of partitions of the matrix table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.MatrixTable`; Matrix table where each entry corresponds to an entry in the block matrix.; """"""; t = self.to_table_row_major(n_partitions, maximum_cache_memory_in_bytes); t = t.transmute(entries=t.entries.map(lambda i: hl.struct(element=i))); t = t.annotate_globals(cols=hl.range(self.n_cols).map(lambda i: hl.struct(col_idx=hl.int64(i)))); return t._unlocalize_entries('entries', 'cols', ['col_idx']). [docs] @staticmethod; @typecheck(; path_in=str,; path_out=str,; delimiter=str,; header=nullable(str),; add_index=bool,; parallel=nullable(ExportType.checker),; partition_size=nullable(int),; entries=enumeration('full', 'lower', 'strict_lower', 'upper', 'strict_upper'),; ); def export(; path_in,; path_out,; delimiter='\t',; header=None,; add_index=False,; paral",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:57213,reduce,reduce,57213,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['reduce'],['reduce']
Energy Efficiency,":`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :math:`X`. For; :math:`n > m`, the three stages are:. 1. Compute (and localize) the gramian matrix :math:`X^T X`,. 2. Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition :math:`X^T X = V S V^T` with; :func:`numpy.linalg.eigh` or :func:`scipy.linalg.eigh`,. 3. Compute the singular values as :math:`\Sigma = S^\frac{1}{2}` and the; the left singular vectors as the block matrix; :math:`U = X V \Sigma^{-1}`. In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice :math:`U` (e.g. based on the singular values), or; discard :math:`U` entirely. If :math:`n \leq m`, the three stages instead use the gramian; :math:`X X^T = U S U^T` and return :math:`V^T` as the; block matrix :math:`\Sigma^{-1} U^T X`. Warning; -------; Computing reduced SVD via the gramian presents an added wrinkle when; :math:`X` is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:75214,reduce,reduced,75214,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistic",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:7515,efficient,efficient,7515,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['efficient'],['efficient']
Energy Efficiency,"; >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; This method leverages distributed matrix multiplication to compute; reduced singular value decomposition (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300.; Let \(X\) be an \(n \times m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in descending order,; \(V^T\) is an \(r \times m\) matrix whose rows are; (orthonormal) right singular vectors. If the singular values in \(\Sigma\) are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:35878,reduce,reduced,35878,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"; >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:5064,power,power,5064,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,2,['power'],['power']
Energy Efficiency,"; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:6882,reduce,reduced,6882,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['reduce'],['reduced']
Energy Efficiency,"; [ 5., 6., 0., 0.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; -----; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from all row intervals. By default, all elements; outside the row intervals but inside blocks that overlap the row; intervals are set to zero as well. `starts` and `stops` must both have length equal to the number of; rows. The interval for row ``i`` is ``[starts[i], stops[i])``. In; particular, ``0 <= starts[i] <= stops[i] <= n_cols`` is required; for all ``i``. This method requires the number of rows to be less than :math:`2^{31}`. Parameters; ----------; starts: :obj:`list` of :obj:`int`, or :class:`numpy.ndarray` of :obj:`int`; Start indices for each row (inclusive).; stops: :obj:`list` of :obj:`int`, or :class:`numpy.ndarray` of :obj:`int`; Stop indices for each row (exclusive).; blocks_only: :obj:`bool`; If ``False``, set all elements outside row intervals to zero.; If ``True``, only set all blocks outside row intervals to blocks; of zeros; this is more efficient.; Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if isinstance(starts, np.ndarray):; if starts.dtype not in (np.int32, np.int64):; raise ValueError(""sparsify_row_intervals: starts ndarray must have dtype 'int32' or 'int64'""); starts = [int(s) for s in starts]; if isinstance(stops, np.ndarray):; if stops.dtype not in (np.int32, np.int64):; raise ValueError(""sparsify_row_intervals: stops ndarray must have dtype 'int32' or 'int64'""); stops = [int(s) for s in stops]. n_rows = self.n_rows; n_cols = self.n_cols; if n_rows >= (1 << 31):; raise ValueError(f'n_rows must be less than 2^31, found {n_rows}'); if len(starts) != n_rows or len(stops) != n_rows:; raise ValueError(f'starts and stops must both have length {n_rows} (the number of rows)'); if any([start < 0 for start in starts]):; raise ValueError('all start values must be non-negative'); if any([stop > self.n_cols for stop in stops]):; raise ValueError(f'all stop valu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:36875,efficient,efficient,36875,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statisti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33668,reduce,reduces,33668,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['reduce'],['reduces']
Energy Efficiency,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54124,reduce,reduces,54124,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['reduce'],['reduces']
Energy Efficiency,"AT above this value will; not be included in the output. Must be in [0,1]. Returns:A KeyTable mapping pairs of samples to their IBD; statistics. Return type:KeyTable. ibd_prune(threshold, tiebreaking_expr=None, maf=None, bounded=True)[source]¶; Prune samples from the VariantDataset based on ibd() PI_HAT measures of relatedness. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Prune samples so that no two have a PI_HAT value greater than or equal to 0.6.; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:; >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). Notes; The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted.; The tiebreaking_expr namespace has the following variables available:. s1: The first sample id.; sa1: The annotations associated with s1.; s2: The second sample id.; sa2: The annotations associated with s2. The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping s1. Any positive integer expresses a preference for keeping s2. A zero expresses no preference. This function must induce a preorder on the samples, in particular:. tiebreaking_expr(sample1, sample2) must equal -1 * tie breaking_expr(sa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:68980,efficient,efficient,68980,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficient']
Energy Efficiency,"Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis plat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1009,power,powerful,1009,index.html,https://hail.is,https://hail.is/index.html,1,['power'],['powerful']
Energy Efficiency,"If True, keep the lower triangle.; blocks_only (bool) – If False, set all elements outside the triangle to zero.; If True, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sqrt()[source]; Element-wise square root. Returns:; BlockMatrix. sum(axis=None)[source]; Sums array elements over one or both axes.; Examples; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; This method leverages distributed matrix multiplication to compute; reduced singular value decomposition (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300.; Let \(X\) be an \(n \times m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in desce",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:35464,reduce,reduced,35464,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"Local SSD; Average number of days per month = 365.25 / 12 = 30.4375. Cost per GB per month = $0.048. Cost per core per hour = $0.048 * 375 / 30.4375 / 24 / 16. = $0.001685 per core per hour. Storage; Average number of days per month = 365.25 / 12 = 30.4375. Cost per GB per month = $0.17. Cost per GB per hour = $0.17 / 30.4375 / 24. IP network cost= $0.0003125 per core per hour for nonpreemptible worker types; = $0.00015625 per core per hour for spot worker types. Service cost= $0.01 per core per hour. Logs, Specs, and Firewall Fee= $0.005 per core per hour. The sum of these costs is $0.02684125 per core/hour for standard spot workers, $0.02929425 per core/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:6250,schedul,scheduled,6250,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['schedul'],['scheduled']
Energy Efficiency,"Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size.; To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to float64. One-dimensional ndarrays; of shape (n) are promoted to two-dimensional ndarrays of shape (1,; n), i.e. a single row.; Block matrices support broadcasting of +, -, *, and /; between matrices of different shapes, consistent with the NumPy; broadcasting rules.; There is one exception: block matrices do not currently support element-wise; “outer product” of a single row and a single column",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:1701,power,power,1701,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['power'],['power']
Energy Efficiency,"STER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2709,charge,charges,2709,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['charge'],['charges']
Energy Efficiency,"The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21176,power,power,21176,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['power'],['power']
Energy Efficiency,"True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource) – Resource to be written to a file.; dest (str) – Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11979,charge,charges,11979,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['charge'],['charges']
Energy Efficiency,"])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable) – The function to execute.; iterables (Iterable[Any]) – The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None]) – This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator’s; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int) – The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool) – If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the under",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5722,schedul,schedule,5722,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['schedul'],['schedule']
Energy Efficiency,"a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2525,charge,charges,2525,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['charge'],['charges']
Energy Efficiency,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedTable.html:2386,reduce,reduces,2386,docs/0.2/hail.GroupedTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html,1,['reduce'],['reduces']
Energy Efficiency,"achine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2732,allocate,allocate,2732,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['allocate'],['allocate']
Energy Efficiency,"ackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:10178,charge,charges,10178,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['charge'],['charges']
Energy Efficiency,"am bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linregMultiPheno(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac,; min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; variant_block_size=integral); def linreg3(self, ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes; more efficiently than looping over :py:meth:`.linreg`. This; method is more efficient than :py:meth:`.linreg_multi_pheno`; but doesn't implicitly filter on allele count or allele; frequency. .. warning::. :py:meth:`.linreg3` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of ``y``. - **va.linreg.nCompleteSamples** (*Int*) -- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:112499,efficient,efficient,112499,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficient']
Energy Efficiency,"ambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \times r` matrix whose columns are; (orthonormal) left singular vectors,. - :math:`\Sigma` is an :math:`r \times r` diagonal matrix of non-negative; singular values in descending order,. - :math:`V^T` is an :math:`r \times m` matrix whose rows are; (orthonormal) right singular vectors. If the singular values in :math:`\Sigma` are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computationa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72984,reduce,reduced,72984,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['reduce'],['reduced']
Energy Efficiency,"ant annotations are added. va.linreg.beta (Double) – fit genotype coefficient, \(\hat\beta_1\); va.linreg.se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Double) – \(p\)-value. Parameters:; y (str) – Response expression; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosages genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg3(ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter on allele count or allele; frequency. Warning; linreg3() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:82533,efficient,efficiently,82533,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"ar matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (ndarrays of float64) – The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors. hail.nd.svd(nd, full_matrices=True, compute_uv=True)[source]; Performs a singular value decomposition. Parameters:. nd (NDArrayNumericExpression) – A 2 dimensional ndarray, shape(M, N).; full_matrices (bool) – If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv (bool) – If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns:. - u (NDArrayNumericExpression) – The left singular vectors.; - s (NDArrayNumericExpression) – The singular values.; - vt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:7619,reduce,reduced,7619,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['reduce'],['reduced']
Energy Efficiency,"arameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:4200,power,power,4200,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,1,['power'],['power']
Energy Efficiency,"array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7319,efficient,efficient,7319,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['efficient'],['efficient']
Energy Efficiency,"atus (both split or both multi-allelic). Parameters:right (VariantDataset) – right-hand variant dataset. Returns:Joined variant dataset. Return type:VariantDataset. ld_matrix(force_local=False)[source]¶; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you’ll want to reduce the number of variants with methods like; sample_variants(), filter_variants_expr(), or ld_prune() before; calling this unless your dataset is very small. Parameters:force_local (bool) – If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds \(5000^2\) and locally otherwise. Returns:Matrix of r values between pairs of variants. Return type:LDMatrix. ld_prune(r2=0.2, window=1000000, memory_per_core=256, num_cores=1)[source]¶; Prune variants in linkage disequilibrium (LD). Important; The genotype_schema() must be of type TGenotype in order to use this method. Requires was_split equals True.; Examples; Export the set of common LD pruned variants to a file:; >>> vds_result = (vds.variant_qc(); ... .filter_variants_expr(""va.qc.AF >= 0.05 && va.qc.AF <= 0.95""); ... .ld_prune(); ... .export_variants(""output/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:74848,reduce,reduce,74848,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['reduce'],['reduce']
Energy Efficiency,"ble spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:75966,charge,charges,75966,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['charge'],['charges']
Energy Efficiency,"ble(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symme",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:29746,schedul,schedule,29746,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['schedul'],['schedule']
Energy Efficiency,"butes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:1736,reduce,reduced,1736,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,1,['reduce'],['reduced']
Energy Efficiency,"calize=True) -> int:; """"""Count the number of columns in the matrix. Examples; --------. Count the number of columns:. >>> n_cols = dataset.count_cols(). Returns; -------; :obj:`int`; Number of columns in the matrix.; """"""; count_ir = ir.TableCount(ir.MatrixColsTable(self._mir)); if _localize:; return Env.backend().execute(count_ir); else:; return construct_expr(ir.LiftMeOut(count_ir), hl.tint64). [docs] def count(self) -> Tuple[int, int]:; """"""Count the number of rows and columns in the matrix. Examples; --------. >>> dataset.count(). Returns; -------; :obj:`int`, :obj:`int`; Number of rows, number of cols.; """"""; count_ir = ir.MatrixCount(self._mir); return Env.backend().execute(count_ir). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; _drop_cols=bool,; _drop_rows=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:80522,efficient,efficient,80522,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['efficient'],['efficient']
Energy Efficiency,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:12094,schedul,scheduled,12094,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,2,['schedul'],['scheduled']
Energy Efficiency,"cs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(); results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig); .aggregate(n_het = hl.agg.count_where(entries.GT.is_het()))). [50]:. results.show(). [Stage 184:> (0 + 1) / 1]. popchromosomen_hetstrstrint64; ""AFR""""1""11039; ""AFR""""10""7123; ""AFR""""11""6777; ""AFR""""12""7016; ""AFR""""13""4650; ""AFR""""14""4262; ""AFR""""15""3847; ""AFR""""16""4564; ""AFR""""17""3607; ""AFR""""18""4133; showing top 10 rows. We use the MatrixTable.entries method to convert our matrix table to a table (with one row for each sample for each variant). In this representation, it is easy to aggregate over any fields we like, which is often ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:23335,consumption,consumption,23335,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['consumption'],['consumption']
Energy Efficiency,"ct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on which; scipy.linalg.svd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) the gramian matrix \(X^T X\),; Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition \(X^T X = V S V^T\) with; numpy.linalg.eigh() or scipy.linalg.eigh(),; Compute the singular values as \(\Sigma = S^\frac{1}{2}\) and the; the left singular vectors as the block matrix; \(U = X V \Sigma^{-1}\). In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice \(U\) (e.g. based on the singular values), or; discard \(U\) entirely.; If \(n \leq m\), the three stages instead use the gramian; \(X X^T = U S U^T\) and return \(V^T\) as the; block matrix \(\Sigma^{-1} U^T X\). Warning; Computing reduced SVD via the gramian presents an added wrinkle when; \(X\) is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:37611,efficient,efficient,37611,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"d form:. >>> hl.eval(hl.all([False, True, False])); False. >>> hl.eval(hl.all([True, True, True])); True. The third form:. >>> a = ['The', 'quick', 'brown', 'fox']; >>> s = {1, 3, 5, 6, 7, 9}. >>> hl.eval(hl.all(lambda x: hl.len(x) > 3, a)); False. >>> hl.eval(hl.all(lambda x: x < 10, s)); True. Notes; -----; :func:`~.all` returns ``True`` when given an empty array or empty argument list.; """"""; base = hl.literal(True); if builtins.len(args) == 0:; return base; if builtins.len(args) == 1:; arg = arg_check(args[0], 'any', 'collection', oneof(collection_type, expr_bool)); if arg.dtype == hl.tbool:; return arg; return arg.all(lambda x: x); if builtins.len(args) == 2:; if callable(args[0]):; f = arg_check(args[0], 'all', 'f', any_to_bool_type); collection = arg_check(args[1], 'all', 'collection', collection_type); return collection.all(f); n_args = builtins.len(args); args = [args_check(x, 'all', 'exprs', i, n_args, expr_bool) for i, x in builtins.enumerate(args)]; return functools.reduce(operator.iand, args, base). [docs]@typecheck(f=func_spec(1, expr_bool), collection=expr_oneof(expr_set(), expr_array())); def find(f: Callable, collection):; """"""Returns the first element where `f` returns ``True``. Examples; --------. >>> a = ['The', 'quick', 'brown', 'fox']; >>> s = {1, 3, 5, 6, 7, 9}. >>> hl.eval(hl.find(lambda x: x[-1] == 'x', a)); 'fox'. >>> hl.eval(hl.find(lambda x: x % 4 == 0, s)); None. Notes; -----; If `f` returns ``False`` for every element, then the result is missing. Sets are unordered. If `collection` is of type :class:`.tset`, then the; element returned comes from no guaranteed ordering. Parameters; ----------; f : function ( (arg) -> :class:`.BooleanExpression`); Function to evaluate for each element of the collection. Must return a; :class:`.BooleanExpression`.; collection : :class:`.ArrayExpression` or :class:`.SetExpression`; Collection expression. Returns; -------; :class:`.Expression`; Expression whose type is the element type of the collection.; """""";",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:111027,reduce,reduce,111027,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['reduce'],['reduce']
Energy Efficiency,"d return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5186,efficient,efficient,5186,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['efficient'],['efficient']
Energy Efficiency,"dTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5582,reduce,reduces,5582,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['reduce'],['reduces']
Energy Efficiency,"d_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:10194,reduce,reduced,10194,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['reduce'],['reduced']
Energy Efficiency,"ding left and; right singular vectors by -1. The computational complexity of SVD is; roughly :math:`nmr`. We now describe the implementation in more detail.; If :math:`\sqrt[3]{nmr}` is less than or equal to `complexity_bound`,; then :math:`X` is localized to an ndarray on which; :func:`scipy.linalg.svd` is called. In this case, all components are; returned as ndarrays. If :math:`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :math:`X`. For; :math:`n > m`, the three stages are:. 1. Compute (and localize) the gramian matrix :math:`X^T X`,. 2. Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition :math:`X^T X = V S V^T` with; :func:`numpy.linalg.eigh` or :func:`scipy.linalg.eigh`,. 3. Compute the singular values as :math:`\Sigma = S^\frac{1}{2}` and the; the left singular vectors as the block matrix; :math:`U = X V \Sigma^{-1}`. In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice :math:`U` (e.g. based on the singular values), or; discard :math:`U` entirely. If :math:`n \leq m`, the three stages instead use the gramian; :math:`X X^T = U S U^T` and return :math:`V^T` as the; block matrix :math:`\Sigma^{-1} U^T X`. Warning; -------; Computing reduced SVD via the gramian presents an added wrinkle when; :math:`X` is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:74916,efficient,efficient,74916,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"e UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantile",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90114,reduce,reduces,90114,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['reduce'],['reduces']
Energy Efficiency,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:11568,efficient,efficient,11568,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['efficient'],['efficient']
Energy Efficiency,"e; },; linreg: Struct{; beta: Double,; se: Double,; tstat: Double,; pval: Double; }; }. Looking at the bottom of the above printout, you can see the linear; regression adds new variant annotations for the beta, standard error,; t-statistic, and p-value. In [46]:. def qqplot(pvals, xMax, yMax):; spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals)); exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of these labels due to human error. Genomes don’t have that; problem! Instead of using reported ancestry, we will use genetic; ancestry by including computed principal components in our model.; The; pca; method produces sample PCs in sample annotations, and can also produce; variant loadings and global eigenvalues when asked. In [48]:. pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'). 2018-10-18 01:26:55 Hail: INFO: Running PCA with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:23720,consumption,consumption,23720,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['consumption'],['consumption']
Energy Efficiency,"e=2). Filter to the upper triangle and collect to NumPy:. >>> bm.sparsify_triangle().to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:. >>> bm.sparsify_triangle(blocks_only=True).to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; -----; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triangle. By; default, all elements outside the triangle but inside blocks that; overlap the triangle are set to zero as well. Parameters; ----------; lower: :obj:`bool`; If ``False``, keep the upper triangle.; If ``True``, keep the lower triangle.; blocks_only: :obj:`bool`; If ``False``, set all elements outside the triangle to zero.; If ``True``, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if lower:; lower_band = 1 - self.n_rows; upper_band = 0; else:; lower_band = 0; upper_band = self.n_cols - 1. return self.sparsify_band(lower_band, upper_band, blocks_only). @typecheck_method(intervals=expr_tuple([expr_array(expr_int64), expr_array(expr_int64)]), blocks_only=bool); def _sparsify_row_intervals_expr(self, intervals, blocks_only=False):; return BlockMatrix(BlockMatrixSparsify(self._bmir, intervals._ir, RowIntervalSparsifier(blocks_only))). @typecheck_method(indices=expr_array(expr_int32)); def _sparsify_blocks(self, indices):; return BlockMatrix(BlockMatrixSparsify(self._bmir, indices._ir, PerBlockSparsifier())). [docs] @typecheck_method(; starts=oneof(sequenceof(int), np.ndarray), stops=oneof(sequenceof(int), np.ndarray), blocks_only=bool; ); def sparsify_row_intervals(self, starts, stops, blocks_only=False):; """"""Creates a block-sparse matrix by filterin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:34006,efficient,efficient,34006,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['efficient'],['efficient']
Energy Efficiency,"eBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str) – File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and it’s index (file extensions matter!):; >>> fasta = b.read_input_group(**{'fasta': 'data/example.fasta',; ... 'fasta.idx': 'data/example.fasta.idx'}). Create a resource group where the identifiers don’t match the file extensions:; >>> rg = b.read_input_group(foo='data/foo.txt',; ... bar='data/bar.txt'). rg.foo and rg.bar will not have the .txt file extension and; instead will be {root}.foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8643,charge,charges,8643,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['charge'],['charges']
Energy Efficiency,"e_level, with_local_temp_file; from hail.utils.java import Env. block_matrix_type = lazy(). [docs]class BlockMatrix(object):; """"""Hail's block-distributed matrix of :py:data:`.tfloat64` elements. .. include:: ../_templates/experimental.rst. A block matrix is a distributed analogue of a two-dimensional; `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html>`__ with; shape ``(n_rows, n_cols)`` and NumPy dtype ``float64``.; Import the class with:. >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; :meth:`default_block_size`. **Operations and broadcasting**. The core operations are consistent with NumPy: ``+``, ``-``, ``*``, and; ``/`` for element-wise addition, subtraction, multiplication, and division;; ``@`` for matrix multiplication; ``T`` for transpose; and ``**`` for; element-wise exponentiation to a scalar power. For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (:obj:`int` or :obj:`float`). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size. To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to ``float64``. One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exceptio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:3127,power,power,3127,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['power'],['power']
Energy Efficiency,"ean): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; logP (Boolean) – If true, probabilities are returned as log(p). dpois(x: Double, lambda: Double): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s number e raised to the power of the given value x.; Arguments. x (Double) – the exponent to raise e to. fet(a: Int, b: Int, c: Int, d: Int): Struct{pValue:Double,oddsRatio:Double,ci95Lower:Double,ci95Upper:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio; ci95Lower (Double) – lower bound for 95% confidence interval; ci95Upper (Double) – upper bound for 95% confidence interval. Calculates the p-value, odds ratio, and 95% confidence interval with Fisher’s exact test (FET) for 2x2 tables.; Examples; Annotate each variant with Fisher’s exact test association results (assumes minor/major allele count variant annotations have been computed):; >>> (vds.annotate_variants_expr(; ... 'va.fet = let macCase = gs.filter(g => sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'macControl = gs.filter(g => !sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'majCase = gs.filter(g => sa.pheno.isCase).map(g => 2 - g.nNonRefAlleles()).sum() and '; ... 'majControl = gs.filter(g =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:4451,power,power,4451,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['power'],['power']
Energy Efficiency,"ear regression key table is:. gene; beta; se; tstat; pval. geneA; -0.084; 0.368; -0.227; 0.841. geneB; -0.542; 0.335; -1.617; 0.247. geneC; 0.075; 0.515; 0.145; 0.898. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of linear regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). linreg_multi_pheno(ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes more efficiently; than looping over linreg(). Warning; linreg_multi_pheno() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of y. va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; min_ac (int)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:90181,efficient,efficiently,90181,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"ed 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:66134,monitor,monitoring,66134,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['monitor'],['monitoring']
Energy Efficiency,"ed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:130248,efficient,efficient,130248,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficient']
Energy Efficiency,"equires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:8288,efficient,efficient,8288,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['efficient'],['efficient']
Energy Efficiency,"eric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(other=expr_bool); def __and__(self, other):; """"""Return ``True`` if the left and right arguments are ``True``. Examples; --------. >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval(f & na); False. The ``&`` and ``|`` operators have higher priority than comparison; operators like ``==``, ``<``, or ``>``. Parentheses are often; necessary:. >>> x = hl.literal(5). >>> hl.eval((x < 10) & (x > 2)); True. Para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59632,power,power,59632,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['power'],['power']
Energy Efficiency,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Float32Expression.html:3748,power,power,3748,docs/0.2/hail.expr.Float32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html,6,['power'],['power']
Energy Efficiency,"ession. __mul__(other)[source]; Positionally multiply by an array or a scalar.; Examples; >>> hl.eval(a2 * 5); [5, -5, 5, -5, 5, -5]. >>> hl.eval(a1 * a2); [0, -1, 2, -3, 4, -5]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to multiply by. Returns:; ArrayNumericExpression – Array of positional products. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the array.; Examples; >>> hl.eval(-a1); [0, -1, -2, -3, -4, -5]. Returns:; ArrayNumericExpression – Array expression of the same type. __pow__(other)[source]; Positionally raise to the power of an array or a scalar.; Examples; >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positional differences. __truediv__(other)[source]; Positionally divide by an array or a scalar.; Examples; >>> hl.eval(a1 / 10) ; [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. >>> hl.eval(a2 / a1) ; [inf, -1.0, 0.5, -0.3333333333333333, 0.25, -0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to divide by. Returns:; ArrayNumericExpression – Array of positional quotients. aggregate(f); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:4125,power,power,4125,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['power'],['power']
Energy Efficiency,"est')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str) – Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str) – The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool) – If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:3720,power,power,3720,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['power'],['power']
Energy Efficiency,"et = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column order:; >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:; >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters:; indices (list of int) – List of old column indices. Returns:; MatrixTable. property col; Returns a struct expression of all column-indexed fields, including keys.; Examples; Get all column field names:; >>> list(dataset.col) ; ['s', 'sample_qc', 'is_case', 'pheno', 'cov', 'cov1', 'cov2', 'cohorts', 'pop']. Returns:; StructExpression – Struct of all column fields. property col_key; Column key struct.; Examples; Get the column key field names:; >>> list(dataset.col_key)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:19003,efficient,efficient,19003,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['efficient'],['efficient']
Energy Efficiency,"etric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of :math:`Q` is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call :math:`Z Z^T`:. .. math::. \begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}. The eigenvalues of :math:`Z Z^T` and :math:`Z^T Z` are the squared singular values of :math:`Z`;; therefore, we instead focus on :math:`Z^T Z`. In the expressions below, we elide transpositions; of symmetric matrices:. .. math::. \begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}. Before substituting the definition of :math:`P_0`, simplify it using the reduced QR; decomposition:. .. math::. \begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:89697,reduce,reduced,89697,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['reduce'],['reduced']
Energy Efficiency,"eturns; -------; :class:`.NDArrayNumericExpression`; A 1-dimensional ndarray from `start` to `stop` by `step`.; """"""; return _ndarray(hl.range(start, stop, step)). [docs]@typecheck(shape=shape_type, value=expr_any, dtype=nullable(HailType)); def full(shape, value, dtype=None):; """"""Creates a hail :class:`.NDArrayNumericExpression` full of the specified value. Examples; --------. Create a 5 by 7 NDArray of type :py:data:`.tfloat64` 9s. >>> hl.nd.full((5, 7), 9). It is possible to specify a type other than :py:data:`.tfloat64` with the `dtype` argument. >>> hl.nd.full((5, 7), 9, dtype=hl.tint32). Parameters; ----------; shape : `tuple` or :class:`.TupleExpression`; Desired shape.; value : :class:`.Expression` or python value; Value to fill ndarray with.; dtype : :class:`.HailType`; Desired hail type. Returns; -------; :class:`.NDArrayNumericExpression`; An ndarray of the specified shape filled with the specified value.; """"""; if isinstance(shape, Int64Expression):; shape_product = shape; else:; shape_product = reduce(lambda a, b: a * b, shape); return arange(hl.int32(shape_product)).map(lambda x: cast_expr(value, dtype)).reshape(shape). [docs]@typecheck(shape=shape_type, dtype=HailType); def zeros(shape, dtype=tfloat64):; """"""Creates a hail :class:`.NDArrayNumericExpression` full of zeros. Examples; --------. Create a 5 by 7 NDArray of type :py:data:`.tfloat64` zeros. >>> hl.nd.zeros((5, 7)). It is possible to specify a type other than :py:data:`.tfloat64` with the `dtype` argument. >>> hl.nd.zeros((5, 7), dtype=hl.tfloat32). Parameters; ----------; shape : `tuple` or :class:`.TupleExpression`; Desired shape.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. See Also; --------; :func:`.full`. Returns; -------; :class:`.NDArrayNumericExpression`; ndarray of the specified size full of zeros.; """"""; return full(shape, 0, dtype). [docs]@typecheck(shape=shape_type, dtype=HailType); def ones(shape, dtype=tfloat64):; """"""Creates a hail :class:`.NDArrayNumericExpre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:4234,reduce,reduce,4234,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['reduce'],['reduce']
Energy Efficiency,"f quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper fu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:114640,reduce,reduces,114640,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['reduce'],['reduces']
Energy Efficiency,"float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def pc_relate(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) # doctest: +SKIP. Only compute the kinship statistic. This is more efficient than; computing all statistics. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') # doctest: +SKIP. Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using :meth:`.Table.filter`. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) # doctest: +SKIP. One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:2292,efficient,efficient,2292,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,1,['efficient'],['efficient']
Energy Efficiency,"g.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9159,power,power,9159,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['power'],['power']
Energy Efficiency,"hape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (nd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:6807,reduce,reduced,6807,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['reduce'],['reduced']
Energy Efficiency,"he block matrix must be stored in row-major format, as results; from :meth:`.BlockMatrix.write` with ``force_row_major=True`` and from; :meth:`.BlockMatrix.write_from_entry_expr`. Otherwise,; :meth:`export` will fail. Notes; -----; The five options for `entries` are illustrated below. Full:. .. code-block:: text. 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:. .. code-block:: text. 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:. .. code-block:: text. 0.8; 0.7 0.3. Upper triangle:. .. code-block:: text. 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:. .. code-block:: text. 0.8 0.7; 0.3. The number of columns must be less than :math:`2^{31}`. The number of partitions (file shards) exported equals the ceiling; of ``n_rows / partition_size``. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data. If `parallel` is ``None``, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below. It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with Python's ``gzip.open`` and R's ``read.table``. Parameters; ----------; path_in: :class:`str`; Path to input block matrix, stored row-major on disk.; path_out: :class:`str`; Path for export.; Use extension ``.gz`` for gzip or ``.bgz`` for block gzip.; delimiter: :class:`str`; Column delimiter.; header: :class:`str`, optional; If provided, `header` is prepended before the first row of data.; add_index: :obj:`bool`; If ``True``, add an initial column with the absolute row index.; parallel: :class:`str`, optional; If ``'header_per_shard'``, create a folder with one file per; partition, each with a header if provid",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:60535,reduce,reduces,60535,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['reduce'],['reduces']
Energy Efficiency,"hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Int32Expression.html:3746,power,power,3746,docs/0.2/hail.expr.Int32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html,2,['power'],['power']
Energy Efficiency,"iants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-allelic sites. Important; The genotype_schema() must be of type TGenotype in order to use this method. This method is much less computationally expensive than; split_multi(), and can also be used to produce; a variant dataset that can be used with methods that do not; support multiallelic variants. Returns:Dataset with no multiallelic sites, which can; be used for biallelic-only methods. Return type:VariantDataset. filter_samples_expr(expr, keep=True)[source]¶; Filter samples with the expression language.; Examples; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:54956,efficient,efficiently,54956,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"iants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:75142,efficient,efficiently,75142,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['efficient'],['efficiently']
Energy Efficiency,"index; - **entries** (:py:class:`.tarray` of :py:data:`.tfloat64`) -- Entries for the row. Examples; --------; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters; ----------; n_partitions : int or None; Number of partitions of the table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:55708,reduce,reduce,55708,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['reduce'],['reduce']
Energy Efficiency,"indexed call expression.; block_size (int, optional) – Block size of block matrices used in the algorithm.; Default given by BlockMatrix.default_block_size(). Returns:; MatrixTable – A MatrixTable whose rows and columns are keys are taken from; call-expr’s column keys. It has one entry field, phi. hail.methods.pc_relate(call_expr, min_individual_maf, *, k=None, scores_expr=None, min_kinship=None, statistics='all', block_size=None, include_self_kinship=False)[source]; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Note; Requires the dataset to contain only diploid genotype calls. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) . Only compute the kinship statistic. This is more efficient than; computing all statistics.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') . Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using Table.filter().; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) . One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:; >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\rig",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:12472,efficient,efficient,12472,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['efficient'],['efficient']
Energy Efficiency,"ion Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:1291,charge,charges,1291,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['charge'],['charges']
Energy Efficiency,"ition_size=2). This produces two compressed files which uncompress to:; idx A B C; 0 1.0 0.8 0.7; 1 0.8 1.0 0.3. idx A B C; 2 0.7 0.3 1.0. Warning; The block matrix must be stored in row-major format, as results; from BlockMatrix.write() with force_row_major=True and from; BlockMatrix.write_from_entry_expr(). Otherwise,; export() will fail. Notes; The five options for entries are illustrated below.; Full:; 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:; 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:; 0.8; 0.7 0.3. Upper triangle:; 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:; 0.8 0.7; 0.3. The number of columns must be less than \(2^{31}\).; The number of partitions (file shards) exported equals the ceiling; of n_rows / partition_size. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data.; If parallel is None, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below.; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with Python’s gzip.open and R’s read.table. Parameters:. path_in (str) – Path to input block matrix, stored row-major on disk.; path_out (str) – Path for export.; Use extension .gz for gzip or .bgz for block gzip.; delimiter (str) – Column delimiter.; header (str, optional) – If provided, header is prepended before the first row of data.; add_index (bool) – If True, add an initial column with the absolute row index.; parallel (str, optional) – If 'header_per_shard', create a folder with one file per; partition, each with a header if provided.; If 'separate_header', create a folder with one file per; partition without a hea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:14884,reduce,reduces,14884,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['reduce'],['reduces']
