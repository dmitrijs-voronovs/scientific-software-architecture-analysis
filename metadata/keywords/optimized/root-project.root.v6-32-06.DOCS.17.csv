quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Performance," not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store atomic/; atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_gl0_inv.; - Ensures that the; acquire-fence-paired; ato",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:370419,load,load,370419,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," not the case when going the other way since the; track has first to exit the extruding node before checking the mother.; In other words, an extrusion behavior is dependent on the track; parameters, which is a highly undesirable effect. *B)* We will call ""overlaps"" only the regions in space contained by; more than one node inside the same container. The owner of such regions; cannot be determined based on hierarchical considerations; therefore; they will be considered as belonging to the node from which the current; track is coming from. When coming from their container, the ownership is totally; unpredictable. Again, the ownership of overlapping regions highly; depends on the current track parameters. We must say that even the overlaps of type *A)* and *B)* are allowed in case; the corresponding nodes are created using; TGeoVolume::AddNodeOverlap() method. Navigation is performed in such; cases by giving priority to the non-overlapping nodes. The modeller has; to perform an additional search through the overlapping candidates.; These are detected automatically during the geometry closing procedure; in order to optimize the algorithm, but we will stress that extensive; usage of this feature leads to a drastic deterioration of performance.; In the following we will focus on the non-declared overlaps of type *A)*; and *B)* since this is the main source of errors during tracking. These; are generally non-intended overlaps due to coding mistakes or bad; geometry design. The checking package is loaded together with the; painter classes and contains an automated overlap checker. \image html geometry008.png ""Overlap checking"". This can be activated both at volume level (checking for illegal; overlaps only one level inside a given volume) and from the geometry; manager level (checking full geometry):. ~~~{.cpp}; myVolume->CheckOverlaps(precision, option);; gGeoManager->CheckOverlaps(precision);; myNode->CheckOverlaps(precision);; ~~~. Here precision represents the desired m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:92223,perform,perform,92223,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['perform']
Performance," notify the context of failure. In ORC,; reported failures are propagated to queries pending on definitions provided by; the failing link, and also through edges of the dependence graph to any queries; waiting on dependent symbols. .. _connection_to_orc_runtime:. Connection to the ORC Runtime; =============================. The ORC Runtime (currently under development) aims to provide runtime support; for advanced JIT features, including object format features that require; non-trivial action in the executor (e.g. running initializers, managing thread; local storage, registering with language runtimes, etc.). ORC Runtime support for object format features typically requires cooperation; between the runtime (which executes in the executor process) and JITLink (which; runs in the JIT process and can inspect LinkGraphs to determine what actions; must be taken in the executor). For example: Execution of MachO static; initializers in the ORC runtime is performed by the ``jit_dlopen`` function,; which calls back to the JIT process to ask for the list of address ranges of; ``__mod_init`` sections to walk. This list is collated by the; ``MachOPlatformPlugin``, which installs a pass to record this information for; each object as it is linked into the target. .. _constructing_linkgraphs:. Constructing LinkGraphs; =======================. Clients usually access and manipulate ``LinkGraph`` instances that were created; for them by an ``ObjectLinkingLayer`` instance, but they can be created manually:. #. By directly constructing and populating a ``LinkGraph`` instance. #. By using the ``createLinkGraph`` family of functions to create a; ``LinkGraph`` from an in-memory buffer containing an object file. This is how; ``ObjectLinkingLayer`` usually creates ``LinkGraphs``. #. ``createLinkGraph_<Object-Format>_<Architecture>`` can be used when; both the object format and architecture are known ahead of time. #. ``createLinkGraph_<Object-Format>`` can be used when the object format is; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:33343,perform,performed,33343,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['perform'],['performed']
Performance," objCMessageExpr(hasSelector(""methodA:"", ""methodB:""));; matches both of the expressions below:; [myObj methodA:argA];; [myObj methodB:argB];. Matcher<ObjCMessageExpr>hasKeywordSelector; Matches when the selector is a keyword selector. objCMessageExpr(hasKeywordSelector()) matches the generated setFrame; message expression in. UIWebView *webView = ...;; CGRect bodyFrame = webView.frame;; bodyFrame.size.height = self.bodyContentHeight;; webView.frame = bodyFrame;; // ^---- matches here. Matcher<ObjCMessageExpr>hasNullSelector; Matches when the selector is the empty selector. Matches only when the selector of the objCMessageExpr is NULL. This may; represent an error condition in the tree!. Matcher<ObjCMessageExpr>hasSelectorstd::string BaseName; Matches when BaseName == Selector.getAsString(). matcher = objCMessageExpr(hasSelector(""loadHTMLString:baseURL:""));; matches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>hasUnarySelector; Matches when the selector is a Unary Selector. matcher = objCMessageExpr(matchesSelector(hasUnarySelector());; matches self.bodyView in the code below, but NOT the outer message; invocation of ""loadHTMLString:baseURL:"".; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>isClassMessage; Returns true when the Objective-C message is sent to a class. Example; matcher = objcMessageExpr(isClassMessage()); matches; [NSString stringWithFormat:@""format""];; but not; NSString *x = @""hello"";; [x containsString:@""h""];. Matcher<ObjCMessageExpr>isInstanceMessage; Returns true when the Objective-C message is sent to an instance. Example; matcher = objcMessageExpr(isInstanceMessage()); matches; NSString *x = @""hello"";; [x containsString:@""h""];; but not; [NSString stringWithFormat:@""format""];. Matcher<ObjCMessageExpr>matchesSelectorStringRef RegExp, Regex::RegexFlags Flags = NoFlags; Matches ObjC selectors whose na",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:106881,load,loadHTMLString,106881,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,1,['load'],['loadHTMLString']
Performance," object being returned.; This is important to preserve object identity as well as to make casting,; a pure C++ feature after all, superfluous.; Example:. .. code-block:: python. >>> from cppyy.gbl import Abstract, Concrete; >>> c = Concrete(); >>> Concrete.show_autocast.__doc__; 'Abstract* Concrete::show_autocast()'; >>> d = c.show_autocast(); >>> type(d); <class '__main__.Concrete'>; >>>. As a consequence, if your C++ classes should only be used through their; interfaces, then no bindings should be provided to the concrete classes; (e.g. by excluding them using a :ref:`selection file <selection-files>`).; Otherwise, more functionality will be available in Python than in C++. Sometimes, however, full control over a cast is needed.; For example, if the instance is bound by another tool or even a 3rd party,; hand-written, extension library.; Assuming the object supports the ``PyCapsule`` or ``CObject`` abstraction,; then a C++-style reinterpret_cast (i.e. without implicitly taking offsets; into account), can be done by taking and rebinding the address of an; object:. .. code-block:: python. >>> from cppyy import addressof, bind_object; >>> e = bind_object(addressof(d), Abstract); >>> type(e); <class '__main__.Abstract'>; >>>. `Operators`; -----------. If conversion operators are defined in the C++ class and a Python equivalent; exists (i.e. all builtin integer and floating point types, as well as; ``bool``), then these will map onto those Python conversions.; Note that ``char*`` is mapped onto ``__str__``.; Example:. .. code-block:: python. >>> from cppyy.gbl import Concrete; >>> print(Concrete()); Hello operator const char*!; >>>. C++ code can overload conversion operators by providing methods in a class or; global functions.; Special care needs to be taken for the latter: first, make sure that they are; actually available in some header file.; Second, make sure that headers are loaded in the desired order.; I.e. that these global overloads are available before use. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst:2652,load,loaded,2652,bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst,1,['load'],['loaded']
Performance," object. ``clang-offload-bundler`` always includes; this entry as the first bundled code object entry. For an; embedded bundled code object this entry is not used by the; runtime and so is generally an empty code object. hip Offload code object for the HIP language. Used for all; HIP language offload code objects when the; ``clang-offload-bundler`` is used to bundle code objects as; intermediate steps of the tool chain. Also used for AMD GPU; code objects before ABI version V4 when the; ``clang-offload-bundler`` is used to create a *fat binary*; to be loaded by the HIP runtime. The fat binary can be; loaded directly from a file, or be embedded in the host code; object as a data section with the name ``.hip_fatbin``. hipv4 Offload code object for the HIP language. Used for AMD GPU; code objects with at least ABI version V4 when the; ``clang-offload-bundler`` is used to create a *fat binary*; to be loaded by the HIP runtime. The fat binary can be; loaded directly from a file, or be embedded in the host code; object as a data section with the name ``.hip_fatbin``. openmp Offload code object for the OpenMP language extension.; ============= ==============================================================. **target-triple**; The target triple of the code object. See `Target Triple; <https://clang.llvm.org/docs/CrossCompilation.html#target-triple>`_. The bundler accepts target triples with or without the optional environment; field:. ``<arch><sub>-<vendor>-<sys>``, or; ``<arch><sub>-<vendor>-<sys>-<env>``. However, in order to standardize outputs for tools that consume bitcode; bundles, bundles written by the bundler internally use only the 4-field; target triple:. ``<arch><sub>-<vendor>-<sys>-<env>``. **target-id**; The canonical target ID of the code object. Present only if the target; supports a target ID. See :ref:`clang-target-id`. .. _code-object-composition:. Bundled Code Object Composition; -------------------------------. * Each entry of a bundled code object must ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst:9148,load,loaded,9148,interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,1,['load'],['loaded']
Performance," of EH,; before cleanups run, making it very difficult to build a faithful control flow; graph. For now, the new EH instructions cannot represent SEH filter; expressions, and frontends must outline them ahead of time. Local variables of; the parent function can be escaped and accessed using the ``llvm.localescape``; and ``llvm.localrecover`` intrinsics. New exception handling instructions; ------------------------------------. The primary design goal of the new EH instructions is to support funclet; generation while preserving information about the CFG so that SSA formation; still works. As a secondary goal, they are designed to be generic across MSVC; and Itanium C++ exceptions. They make very few assumptions about the data; required by the personality, so long as it uses the familiar core EH actions:; catch, cleanup, and terminate. However, the new instructions are hard to modify; without knowing details of the EH personality. While they can be used to; represent Itanium EH, the landingpad model is strictly better for optimization; purposes. The following new instructions are considered ""exception handling pads"", in that; they must be the first non-phi instruction of a basic block that may be the; unwind destination of an EH flow edge:; ``catchswitch``, ``catchpad``, and ``cleanuppad``.; As with landingpads, when entering a try scope, if the; frontend encounters a call site that may throw an exception, it should emit an; invoke that unwinds to a ``catchswitch`` block. Similarly, inside the scope of a; C++ object with a destructor, invokes should unwind to a ``cleanuppad``. New instructions are also used to mark the points where control is transferred; out of a catch/cleanup handler (which will correspond to exits from the; generated funclet). A catch handler which reaches its end by normal execution; executes a ``catchret`` instruction, which is a terminator indicating where in; the function control is returned to. A cleanup handler which reaches its end; by normal",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExceptionHandling.rst:25555,optimiz,optimization,25555,interpreter/llvm-project/llvm/docs/ExceptionHandling.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExceptionHandling.rst,1,['optimiz'],['optimization']
Performance," of an item affects its representation in memory only. In a register, a number is just a sequence of bits - 64 bits in the case of AArch64 general purpose registers. Memory, however, is a sequence of addressable units of 8 bits in size. Any number greater than 8 bits must therefore be split up into 8-bit chunks, and endianness describes the order in which these chunks are laid out in memory. A ""little endian"" layout has the least significant byte first (lowest in memory address). A ""big endian"" layout has the *most* significant byte first. This means that when loading an item from big endian memory, the lowest 8-bits in memory must go in the most significant 8-bits, and so forth. ``LDR`` and ``LD1``; ===================. .. figure:: ARM-BE-ldr.png; :align: right. Big endian vector load using ``LDR``. A vector is a consecutive sequence of items that are operated on simultaneously. To load a 64-bit vector, 64 bits need to be read from memory. In little endian mode, we can do this by just performing a 64-bit load - ``LDR q0, [foo]``. However if we try this in big endian mode, because of the byte swapping the lane indices end up being swapped! The zero'th item as laid out in memory becomes the n'th lane in the vector. .. figure:: ARM-BE-ld1.png; :align: right. Big endian vector load using ``LD1``. Note that the lanes retain the correct ordering. Because of this, the instruction ``LD1`` performs a vector load but performs byte swapping not on the entire 64 bits, but on the individual items within the vector. This means that the register content is the same as it would have been on a little endian system. It may seem that ``LD1`` should suffice to perform vector loads on a big endian machine. However there are pros and cons to the two approaches that make it less than simple which register format to pick. There are two options:. 1. The content of a vector register is the same *as if* it had been loaded with an ``LDR`` instruction.; 2. The content of a vector register is t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:2892,perform,performing,2892,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,2,"['load', 'perform']","['load', 'performing']"
Performance," of parallelism can; be reduced to ``N`` via:. - gold:; ``-Wl,-plugin-opt,jobs=N``; - ld64:; ``-Wl,-mllvm,-threads=N``; - ld.lld, ld64.lld:; ``-Wl,--thinlto-jobs=N``; - lld-link:; ``/opt:lldltojobs=N``. Other possible values for ``N`` are:. - 0:; Use one thread per physical core (default); - 1:; Use a single thread only (disable multi-threading); - all:; Use one thread per logical core (uses all hyper-threads). Incremental; -----------; .. _incremental:. ThinLTO supports fast incremental builds through the use of a cache,; which currently must be enabled through a linker option. - gold (as of LLVM 4.0):; ``-Wl,-plugin-opt,cache-dir=/path/to/cache``; - ld64 (supported since clang 3.9 and Xcode 8) and Mach-O ld64.lld (as of LLVM; 15.0):; ``-Wl,-cache_path_lto,/path/to/cache``; - ELF ld.lld (as of LLVM 5.0):; ``-Wl,--thinlto-cache-dir=/path/to/cache``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocache:/path/to/cache``. Cache Pruning; -------------. To help keep the size of the cache under control, ThinLTO supports cache; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value pairs separated by ``:`` characters.; Possible key-value pairs are:. - ``cache_size=X%``: The maximum size for the cache directory is ``X`` percent; of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is invalid. A value of 0 disables the percentage; size-based pruning. The default is 75%. - ``cache_size_bytes=X``, ``cache_size_bytes=Xk``, ``cache_size_b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:5102,cache,cache,5102,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,2,['cache'],['cache']
Performance," of restrictions placed on precompiled; headers. In particular, there can only be one precompiled header and it must; be included at the beginning of the translation unit. The extensions to the; AST file format required for modules are discussed in the section on; :ref:`modules <pchinternals-modules>`. Clang's AST files are designed with a compact on-disk representation, which; minimizes both creation time and the time required to initially load the AST; file. The AST file itself contains a serialized representation of Clang's; abstract syntax trees and supporting data structures, stored using the same; compressed bitstream as `LLVM's bitcode file format; <https://llvm.org/docs/BitCodeFormat.html>`_. Clang's AST files are loaded ""lazily"" from disk. When an AST file is initially; loaded, Clang reads only a small amount of data from the AST file to establish; where certain important data structures are stored. The amount of data read in; this initial load is independent of the size of the AST file, such that a; larger AST file does not lead to longer AST load times. The actual header data; in the AST file --- macros, functions, variables, types, etc. --- is loaded; only when it is referenced from the user's code, at which point only that; entity (and those entities it depends on) are deserialized from the AST file.; With this approach, the cost of using an AST file for a translation unit is; proportional to the amount of code actually used from the AST file, rather than; being proportional to the size of the AST file itself. When given the `-print-stats` option, Clang produces statistics; describing how much of the AST file was actually loaded from disk. For a; simple ""Hello, World!"" program that includes the Apple ``Cocoa.h`` header; (which is built as a precompiled header), this option illustrates how little of; the actual precompiled header is required:. .. code-block:: none. *** AST File Statistics:; 895/39981 source location entries read (2.238563%); 19/15315 type",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:3642,load,load,3642,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,2,['load'],['load']
Performance," of target features, that affect ISA generation. It is target; specific if a target ID is supported, or if the target triple alone is; sufficient to specify the ISA generation. It is used with the ``-mcpu=<target-id>`` and ``--offload-arch=<target-id>``; Clang compilation options to specify the kind of code to generate. It is also used as part of the bundle entry ID to identify the code object. See; :ref:`clang-bundle-entry-id`. Target ID syntax is defined by the following BNF syntax:. .. code::. <target-id> ::== <processor> ( "":"" <target-feature> ( ""+"" | ""-"" ) )*. Where:. **processor**; Is a the target specific processor or any alternative processor name. **target-feature**; Is a target feature name that is supported by the processor. Each target; feature must appear at most once in a target ID and can have one of three; values:. *Any*; Specified by omitting the target feature from the target ID.; A code object compiled with a target ID specifying the default; value of a target feature can be loaded and executed on a processor; configured with the target feature on or off. *On*; Specified by ``+``, indicating the target feature is enabled. A code; object compiled with a target ID specifying a target feature on; can only be loaded on a processor configured with the target feature on. *Off*; specified by ``-``, indicating the target feature is disabled. A code; object compiled with a target ID specifying a target feature off; can only be loaded on a processor configured with the target feature off. .. _compatibility-target-id:. Compatibility Rules for Target ID; ---------------------------------. A code object compiled for a Target ID is considered compatible for a; target, if:. * Their processor is same.; * Their feature set is compatible as defined above. There are two forms of target ID:. *Non-Canonical Form*; The non-canonical form is used as the input to user commands to allow the user; greater convenience. It allows both the primary and alternative processor na",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst:12067,load,loaded,12067,interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,1,['load'],['loaded']
Performance," of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is invalid. A value of 0 disables the percentage; size-based pruning. The default is 75%. - ``cache_size_bytes=X``, ``cache_size_bytes=Xk``, ``cache_size_bytes=Xm``,; ``cache_size_bytes=Xg``:; Sets the maximum size for the cache directory to ``X`` bytes (or KB, MB,; GB respectively). A value over the amount of available space on the disk; will be reduced to the amount of available space. A value of 0 disables; the byte size-based pruning. The default is no byte size-based pruning. Note that ThinLTO will apply both size-based pruning policies simultaneously,; and changing one does not affect the other. For example, a policy of; ``cache_size_bytes=1g`` on its own will cause both the 1GB and default 75%; policies to be applied unless the default ``cache_size`` is overridden. - ``cache_size_files=X``:; Set the maximum number of files in the cache directory. Set to 0 to indicate; no limit. The default is 1000000 files. - ``prune_after=Xs``, ``prune_after=Xm``, ``prune_after=Xh``: Sets the; expiration time for cache files to ``X`` seconds (or minutes, hours; respectively). When a file hasn't been accessed for ``prune_after`` seconds,; it is removed from the cache. A value of 0 disables the expiration-based; pruning. The default is 1 week. - ``prune_interval=Xs``, ``prune_interval=Xm``, ``prune_interval=Xh``:; Sets the pruning interval to ``X`` seconds (or minutes, hours; respectively). This is intended to be used to avoid scanning the directory; too often. It does not impact the decision of which files to prune. A; value of 0 forces the scan to occur. The default is every 20 minutes. Clang Bootstrap; ---------------. To `bootstrap clang/LLVM <https://llvm.org/docs/AdvancedBuilds.html#bootstrap-builds>`_; with ThinLTO, follow these steps:. 1. The host compiler_ must be a version of clang that support",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:6808,cache,cache,6808,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache']
Performance," of the local variable `a`.; The only information remained is the value of a 32 bit integer. In this simple; case, it seems to be pretty clear that `__int_32_0` represents `a`. However, it; is not true. An important note with optimization is that the value of a variable may not; properly express the intended value in the source code. For example:. .. code-block:: c++. static task coro_task(int v) {; int a = v;; co_await await_counter{};; a++; // __int_32_0 is 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here!; std::cout << a << ""\n"";; co_await await_counter{};; a++; // __int_32_0 is still 43 here!!; std::cout << a << ""\n"";; a++; // Why is __int_32_0 still 43 here?; std::cout << a << ""\n"";; }. When debugging step-by-step, the value of `__int_32_0` seemingly does not; change, despite being frequently incremented, and instead is always `43`.; While this might be surprising, this is a result of the optimizer recognizing; that it can eliminate most of the load/store operations. The above code gets; optimized to the equivalent of:. .. code-block:: c++. static task coro_task(int v) {; store v to __int_32_0 in the frame; co_await await_counter{};; a = load __int_32_0; std::cout << a+1 << ""\n"";; std::cout << a+2 << ""\n"";; std::cout << a+3 << ""\n"";; co_await await_counter{};; a = load __int_32_0; std::cout << a+4 << ""\n"";; std::cout << a+5 << ""\n"";; }. It should now be obvious why the value of `__int_32_0` remains unchanged; throughout the function. It is important to recognize that `__int_32_0`; does not directly correspond to `a`, but is instead a variable generated; to assist the compiler in code generation. The variables in an optimized; coroutine frame should not be thought of as directly representing the; variables in the C++ source. Get the suspended points; ========================. An important requirement for debugging coroutines is to understand suspended; points, which are where the c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:9599,optimiz,optimizer,9599,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,2,"['load', 'optimiz']","['load', 'optimizer']"
Performance," of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6606,load,load,6606,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['load'],['load']
Performance," of type `SHT_NOTE`, if the name; starts with "".note"". Otherwise, it will have type `SHT_PROGBITS`. Can be; specified multiple times to add multiple sections. For MachO objects, ``<section>`` must be formatted as; ``<segment name>,<section name>``. .. option:: --binary-architecture <arch>, -B. Ignored for compatibility. .. option:: --disable-deterministic-archives, -U. Use real values for UIDs, GIDs and timestamps when updating archive member; headers. .. option:: --discard-all, -x. Remove most local symbols from the output. Different file formats may limit; this to a subset of the local symbols. For example, file and section symbols in; ELF objects will not be discarded. Additionally, remove all debug sections. .. option:: --dump-section <section>=<file>. Dump the contents of section ``<section>`` into the file ``<file>``. Can be; specified multiple times to dump multiple sections to different files.; ``<file>`` is unrelated to the input and output files provided to; :program:`llvm-objcopy` and as such the normal copying and editing; operations will still be performed. No operations are performed on the sections; prior to dumping them. For MachO objects, ``<section>`` must be formatted as; ``<segment name>,<section name>``. .. option:: --enable-deterministic-archives, -D. Enable deterministic mode when copying archives, i.e. use 0 for archive member; header UIDs, GIDs and timestamp fields. On by default. .. option:: --help, -h. Print a summary of command line options. .. option:: --only-keep-debug. Produce a debug file as the output that only preserves contents of sections; useful for debugging purposes. For ELF objects, this removes the contents of `SHF_ALLOC` sections that are not; `SHT_NOTE` by making them `SHT_NOBITS` and shrinking the program headers where; possible. .. option:: --only-section <section>, -j. Remove all sections from the output, except for sections named ``<section>``.; Can be specified multiple times to keep multiple sections. For MachO object",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst:2493,perform,performed,2493,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,1,['perform'],['performed']
Performance," of zero. This has been implemented by Jeromy Tompkins <Tompkins@nscl.msu.edu>. ## Geometry Libraries; A new module geom/vecgeom was introduced to give transparent access to VecGeom ; solid primitives. VecGeom is a high performance geometry package (link) providing ; SIMD vectorization for the CPU-intensive geometry algorithms used for geometry; navigation. The module creates a new library libConverterVG.so depending on the; VecGeom main library and loaded using the ROOT plug-in mechanism. The main functionality provided by the new vecgeom module is to make a conversion ; in memory of all the shapes in a loaded TGeo geometry into a special adapter; shape TGeoVGShape, redirecting all navigation calls to the corresponding VecGeom ; solid. The library loading and geometry conversion can be done with a single call ; `TVirtualGeoConverter::Instance()->ConvertGeometry()`; . After the conversion is done, all existing TGeo functionality is available as for; a native geometry, only that most of the converted solids provide better navigation ; performance, despite the overhead introduced by the new adapter shape. Prerequisites: installation of VecGeom. ; The installation instructions are available at <http://geant.web.cern.ch/content/installation>; Due to the fact that VecGeom provides for the moment static libraries ; and depends on ROOT, is is advised to compile first ROOT without VecGeom support, ; then compile VecGeom against this ROOT version, then re-configure ROOT to enable ; VecGeom and Vc support, using the flags -Dvc=ON -Dvecgeom=on; ; This has been implemented by Mihaela Gheata <Mihaela.Gheata@cern.ch>. ## Database Libraries. * Fix `TPgSQLStatement::SetBinary` to actually handle binary data (previous limited to ascii). ## Networking Libraries. * When seeing too many requested ranges, Apache 2.4 now simply sends the whole file; (MaxRanges configuration parameter). TWebFile can handle this case now, but this can; trigger multiple transmissions of the full file. TWebF",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:23994,perform,performance,23994,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['perform'],['performance']
Performance," older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:347927,load,load,347927,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,"['cache', 'load']","['caches', 'load']"
Performance," omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; gl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:273961,load,load,273961,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," on LLVM build using CMake.; This document aims to provide a brief overview of CMake for developers modifying; LLVM projects or building their own projects on top of LLVM. The official CMake language references is available in the cmake-language; manpage and `cmake-language online documentation; <https://cmake.org/cmake/help/v3.4/manual/cmake-language.7.html>`_. 10,000 ft View; ==============. CMake is a tool that reads script files in its own language that describe how a; software project builds. As CMake evaluates the scripts it constructs an; internal representation of the software project. Once the scripts have been; fully processed, if there are no errors, CMake will generate build files to; actually build the project. CMake supports generating build files for a variety; of command line build tools as well as for popular IDEs. When a user runs CMake it performs a variety of checks similar to how autoconf; worked historically. During the checks and the evaluation of the build; description scripts CMake caches values into the CMakeCache. This is useful; because it allows the build system to skip long-running checks during; incremental development. CMake caching also has some drawbacks, but that will be; discussed later. Scripting Overview; ==================. CMake's scripting language has a very simple grammar. Every language construct; is a command that matches the pattern _name_(_args_). Commands come in three; primary types: language-defined (commands implemented in C++ in CMake), defined; functions, and defined macros. The CMake distribution also contains a suite of; CMake modules that contain definitions for useful functionality. The example below is the full CMake build for building a C++ ""Hello World""; program. The example uses only CMake language-defined functions. .. code-block:: cmake. cmake_minimum_required(VERSION 3.20.0); project(HelloWorld); add_executable(HelloWorld HelloWorld.cpp). The CMake language provides control flow constructs in the form o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMakePrimer.rst:1427,cache,caches,1427,interpreter/llvm-project/llvm/docs/CMakePrimer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMakePrimer.rst,1,['cache'],['caches']
Performance," on each agent each sharing separate L2; caches.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel for its associated L2.; Therefore, the vector and scalar memory operations performed by wavefronts; executing with different L1 caches and the same L2 cache can be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To en",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:288410,cache,cache,288410,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,3,['cache'],"['cache', 'caches']"
Performance," on local; values. These casts are required in order to transfer objects in and out of ARC; control; see the rationale in the section on :ref:`conversion of retainable; object pointers <arc.objects.restrictions.conversion>`. Using a ``__bridge_retained`` or ``__bridge_transfer`` cast purely to convince; ARC to emit an unbalanced retain or release, respectively, is poor form. .. _arc.objects.restrictions:. Restrictions; ------------. .. _arc.objects.restrictions.conversion:. Conversion of retainable object pointers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. In general, a program which attempts to implicitly or explicitly convert a; value of retainable object pointer type to any non-retainable type, or; vice-versa, is ill-formed. For example, an Objective-C object pointer shall; not be converted to ``void*``. As an exception, cast to ``intptr_t`` is; allowed because such casts are not transferring ownership. The :ref:`bridged; casts <arc.objects.operands.casts>` may be used to perform these conversions; where necessary. .. admonition:: Rationale. We cannot ensure the correct management of the lifetime of objects if they; may be freely passed around as unmanaged types. The bridged casts are; provided so that the programmer may explicitly describe whether the cast; transfers control into or out of ARC. However, the following exceptions apply. .. _arc.objects.restrictions.conversion.with.known.semantics:. Conversion to retainable object pointer type of expressions with known semantics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. :when-revised:`[beginning Apple 4.0, LLVM 3.1]`; :revision:`These exceptions have been greatly expanded; they previously applied; only to a much-reduced subset which is difficult to categorize but which; included null pointers, message sends (under the given rules), and the various; global constants.`. An unbridged conversion to a retainable object pointer type from a type other; than a retainable object poin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:24856,perform,perform,24856,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['perform']
Performance," on the pipeline (similar to -verify-each).; $ opt -debugify-each -O2 sample.ll. In order for ``check-debugify`` to work, the DI must be coming from; ``debugify``. Thus, modules with existing DI will be skipped. ``debugify`` can be used to test a backend, e.g:. .. code-block:: bash. $ opt -debugify < sample.ll | llc -o -. There is also a MIR-level debugify pass that can be run before each backend; pass, see:; :ref:`Mutation testing for MIR-level transformations<MIRDebugify>`. ``debugify`` in regression tests; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The output of the ``debugify`` pass must be stable enough to use in regression; tests. Changes to this pass are not allowed to break existing tests. .. note::. Regression tests must be robust. Avoid hardcoding line/variable numbers in; check lines. In cases where this can't be avoided (say, if a test wouldn't; be precise enough), moving the test to its own file is preferred. .. _MIRDebugify:. Test original debug info preservation in optimizations; ------------------------------------------------------. In addition to automatically generating debug info, the checks provided by; the ``debugify`` utility pass can also be used to test the preservation of; pre-existing debug info metadata. It could be run as follows:. .. code-block:: bash. # Run the pass by checking original Debug Info preservation.; $ opt -verify-debuginfo-preserve -pass-to-test sample.ll. # Check the preservation of original Debug Info after each pass.; $ opt -verify-each-debuginfo-preserve -O2 sample.ll. Limit number of observed functions to speed up the analysis:. .. code-block:: bash. # Test up to 100 functions (per compile unit) per pass.; $ opt -verify-each-debuginfo-preserve -O2 -debugify-func-limit=100 sample.ll. Please do note that running ``-verify-each-debuginfo-preserve`` on big projects; could be heavily time consuming. Therefore, we suggest using; ``-debugify-func-limit`` with a suitable limit number to prevent extremely long; builds. Furthermore, the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst:13658,optimiz,optimizations,13658,interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,1,['optimiz'],['optimizations']
Performance," on; some versions of library, and absent in others. One cannot easily; express this with a single module map file in the library:. .. parsed-literal::. module Foo {; header ""Foo.h""; ...; }. module Foo_Private {; header ""Foo_Private.h""; ...; }. because the header ``Foo_Private.h`` won't always be available. The; module map file could be customized based on whether; ``Foo_Private.h`` is available or not, but doing so requires custom; build machinery. Private module map files, which are named ``module.private.modulemap``; (or, for backward compatibility, ``module_private.map``), allow one to; augment the primary module map file with an additional modules. For; example, we would split the module map file above into two module map; files:. .. code-block:: c. /* module.modulemap */; module Foo {; header ""Foo.h""; }. /* module.private.modulemap */; module Foo_Private {; header ""Foo_Private.h""; }. When a ``module.private.modulemap`` file is found alongside a; ``module.modulemap`` file, it is loaded after the ``module.modulemap``; file. In our example library, the ``module.private.modulemap`` file; would be available when ``Foo_Private.h`` is available, making it; easier to split a library's public and private APIs along header; boundaries. When writing a private module as part of a *framework*, it's recommended that:. * Headers for this module are present in the ``PrivateHeaders`` framework; subdirectory.; * The private module is defined as a *top level module* with the name of the; public framework prefixed, like ``Foo_Private`` above. Clang has extra logic; to work with this naming, using ``FooPrivate`` or ``Foo.Private`` (submodule); trigger warnings and might not work as expected. Modularizing a Platform; =======================; To get any benefit out of modules, one needs to introduce module maps for software libraries starting at the bottom of the stack. This typically means introducing a module map covering the operating system's headers and the C standard library h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:51231,load,loaded,51231,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['load'],['loaded']
Performance," one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in bui",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2951,perform,performing,2951,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,1,['perform'],['performing']
Performance," one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16481,load,load,16481,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['load'],['load']
Performance," only one multi-line lambda in a statement, and there are no expressions; lexically after it in the statement, drop the indent to the standard two space; indent for a block of code, as if it were an if-block opened by the preceding; part of the statement:. .. code-block:: c++. std::sort(foo.begin(), foo.end(), [&](Foo a, Foo b) -> bool {; if (a.blah < b.blah); return true;; if (a.baz < b.baz); return true;; return a.bam < b.bam;; });. To take best advantage of this formatting, if you are designing an API which; accepts a continuation or single callable argument (be it a function object, or; a ``std::function``), it should be the last argument if at all possible. If there are multiple multi-line lambdas in a statement, or additional; parameters after the lambda, indent the block two spaces from the indent of the; ``[]``:. .. code-block:: c++. dyn_switch(V->stripPointerCasts(),; [] (PHINode *PN) {; // process phis...; },; [] (SelectInst *SI) {; // process selects...; },; [] (LoadInst *LI) {; // process loads...; },; [] (AllocaInst *AI) {; // process allocas...; });. Braced Initializer Lists; """""""""""""""""""""""""""""""""""""""""""""""". Starting from C++11, there are significantly more uses of braced lists to; perform initialization. For example, they can be used to construct aggregate; temporaries in expressions. They now have a natural way of ending up nested; within each other and within function calls in order to build up aggregates; (such as option structs) from local variables. The historically common formatting of braced initialization of aggregate; variables does not mix cleanly with deep nesting, general expression contexts,; function arguments, and lambdas. We suggest new code use a simple rule for; formatting braced initialization lists: act as-if the braces were parentheses; in a function call. The formatting rules exactly match those already well; understood for formatting nested function calls. Examples:. .. code-block:: c++. foo({a, b, c}, {1, 2, 3});. llvm::Constant *Mask",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodingStandards.rst:20004,load,loads,20004,interpreter/llvm-project/llvm/docs/CodingStandards.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodingStandards.rst,1,['load'],['loads']
Performance," optimal number) do.                     ; proof->SetParameter(""PROOF_UseMergers"", S). The new functionality can be tested in tutorials by adding the argument; 'submergers' to runProof, e.g. .        ;        ;      root [0] .L; tutorials/proof/runProof.C+ ;        ;        ;      root [1]; runProof(""simple(nhist=10000,submergers)"") . (see the top of tutorials/proof/runProof.C for additional options). A test for the submerger functionality has also been added to; test/stressProof.cxx .; In PROOF-Lite, add the possibility for the administrator; to control the number of workers. This is done using; the rootrc variable ProofLite.MaxWorkers, which is read out of; /etc/system.rootrc and cannot be overwritten by users. Setting the; value to 0 disables PROOF-Lite. Improvements. TFileMerger. A few improvements on the way to make TFileMerger and; hadd totally equivalent:. import from hadd an optimization of key hashing; import from hadd a better way to invoke Merge for; generic objects; add option to merge histograms in one go, instead of; one-by-one as for generic objects (this option is not yet supported by; hadd). TProofOutputFile. Add support for the placeholder <file>; the definition of the outputfile. This allows to have complete URL and; to pass options to TFile::Open. XrdProofd plugin. Add automatically the line 'Path.ForceRemote 1' to the; session rootrc file if the ROOT version is < 5.24/00 ; this acts; as a workaround for the wrong TTreeCache initialization at the; transition between local and remote files fixed in 5.24/00 . Enable mass storage domain settings when working with; TChain's; in multi-master mode. The Mass Storage Domain must be specified as; option in the URL.              ; chain.AddFile(""root:// .....?msd=CERN"").  and the string must match the value specified in defining the; submaster node.; Improved performance monitoring: the 'Rate plot' button; in the dialog box has been renamed 'Performance Plot' and now shows up; to 4 plots as a function of t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:3659,optimiz,optimization,3659,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,1,['optimiz'],['optimization']
Performance," optimistic assumptions made during compilation. The semantics of; ``@llvm.experimental.guard`` is defined in terms of; ``@llvm.experimental.deoptimize`` -- its body is defined to be; equivalent to:. .. code-block:: text. define void @llvm.experimental.guard(i1 %pred, <args...>) {; %realPred = and i1 %pred, undef; br i1 %realPred, label %continue, label %leave [, !make.implicit !{}]. leave:; call void @llvm.experimental.deoptimize(<args...>) [ ""deopt""() ]; ret void. continue:; ret void; }. with the optional ``[, !make.implicit !{}]`` present if and only if it; is present on the call site. For more details on ``!make.implicit``,; see :doc:`FaultMaps`. In words, ``@llvm.experimental.guard`` executes the attached; ``""deopt""`` continuation if (but **not** only if) its first argument; is ``false``. Since the optimizer is allowed to replace the ``undef``; with an arbitrary value, it can optimize guard to fail ""spuriously"",; i.e. without the original condition being false (hence the ""not only; if""); and this allows for ""check widening"" type optimizations. ``@llvm.experimental.guard`` cannot be invoked. After ``@llvm.experimental.guard`` was first added, a more general; formulation was found in ``@llvm.experimental.widenable.condition``.; Support for ``@llvm.experimental.guard`` is slowly being rephrased in; terms of this alternate. '``llvm.experimental.widenable.condition``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.experimental.widenable.condition(). Overview:; """""""""""""""""". This intrinsic represents a ""widenable condition"" which is; boolean expressions with the following property: whether this; expression is `true` or `false`, the program is correct and; well-defined. Together with :ref:`deoptimization operand bundles <deopt_opbundles>`,; ``@llvm.experimental.widenable.condition`` allows frontends to; express guards or checks on optimistic assumptions made during; compilation and represent them as branch instruc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:946202,optimiz,optimizations,946202,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance," option with internal functions to directly adjust their API to; accept the predicate as an argument and return it. This is likely to be; marginally cheaper than embedding into `%rsp` for entering functions. ##### Use `lfence` to guard function transitions. An `lfence` instruction can be used to prevent subsequent loads from; speculatively executing until all prior mispredicted predicates have resolved.; We can use this broader barrier to speculative loads executing between; functions. We emit it in the entry block to handle calls, and prior to each; return. This approach also has the advantage of providing the strongest degree; of mitigation when mixed with unmitigated code by halting all misspeculation; entering a function which is mitigated, regardless of what occurred in the; caller. However, such a mixture is inherently more risky. Whether this kind of; mixture is a sufficient mitigation requires careful analysis. Unfortunately, experimental results indicate that the performance overhead of; this approach is very high for certain patterns of code. A classic example is; any form of recursive evaluation engine. The hot, rapid call and return; sequences exhibit dramatic performance loss when mitigated with `lfence`. This; component alone can regress performance by 2x or more, making it an unpleasant; tradeoff even when only used in a mixture of code. ##### Use an internal TLS location to pass predicate state. We can define a special thread-local value to hold the predicate state between; functions. This avoids direct ABI implications by using a side channel between; callers and callees to communicate the predicate state. It also allows implicit; zero-initialization of the state, which allows non-checked code to be the first; code executed. However, this requires a load from TLS in the entry block, a store to TLS; before every call and every ret, and a load from TLS after every call. As a; consequence it is expected to be substantially more expensive even than usin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:42004,perform,performance,42004,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance," option:: -fcx-limited-range:. This option enables the naive mathematical formulas for complex division and; multiplication with no NaN checking of results. The default is; ``-fno-cx-limited-range``, but this option is enabled by the ``-ffast-math``; option. .. option:: -fcx-fortran-rules:. This option enables the naive mathematical formulas for complex; multiplication and enables application of Smith's algorithm for complex; division. See SMITH, R. L. Algorithm 116: Complex division. Commun.; ACM 5, 8 (1962). The default is ``-fno-cx-fortran-rules``. .. _floating-point-environment:. Accessing the floating point environment; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; Many targets allow floating point operations to be configured to control things; such as how inexact results should be rounded and how exceptional conditions; should be handled. This configuration is called the floating point environment.; C and C++ restrict access to the floating point environment by default, and the; compiler is allowed to assume that all operations are performed in the default; environment. When code is compiled in this default mode, operations that depend; on the environment (such as floating-point arithmetic and `FLT_ROUNDS`) may have; undefined behavior if the dynamic environment is not the default environment; for; example, `FLT_ROUNDS` may or may not simply return its default value for the target; instead of reading the dynamic environment, and floating-point operations may be; optimized as if the dynamic environment were the default. Similarly, it is undefined; behavior to change the floating point environment in this default mode, for example; by calling the `fesetround` function.; C provides two pragmas to allow code to dynamically modify the floating point environment:. - ``#pragma STDC FENV_ACCESS ON`` allows dynamic changes to the entire floating; point environment. - ``#pragma STDC FENV_ROUND FE_DYNAMIC`` allows dynamic changes to just the floating; point rounding mode. Thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:69418,perform,performed,69418,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performed']
Performance," optional ``!dereferenceable`` metadata must reference a single metadata; name ``<deref_bytes_node>`` corresponding to a metadata node with one ``i64``; entry.; See ``dereferenceable`` metadata :ref:`dereferenceable <md_dereferenceable>`. The optional ``!dereferenceable_or_null`` metadata must reference a single; metadata name ``<deref_bytes_node>`` corresponding to a metadata node with one; ``i64`` entry.; See ``dereferenceable_or_null`` metadata :ref:`dereferenceable_or_null; <md_dereferenceable_or_null>`. The optional ``!align`` metadata must reference a single metadata name; ``<align_node>`` corresponding to a metadata node with one ``i64`` entry.; The existence of the ``!align`` metadata on the instruction tells the; optimizer that the value loaded is known to be aligned to a boundary specified; by the integer value in the metadata node. The alignment must be a power of 2.; This is analogous to the ''align'' attribute on parameters and return values.; This metadata can only be applied to loads of a pointer type. If the returned; value is not appropriately aligned at runtime, a poison value is returned; instead. The optional ``!noundef`` metadata must reference a single metadata name; ``<empty_node>`` corresponding to a node with no entries. The existence of; ``!noundef`` metadata on the instruction tells the optimizer that the value; loaded is known to be :ref:`well defined <welldefinedvalues>`.; If the value isn't well defined, the behavior is undefined. If the ``!noundef``; metadata is combined with poison-generating metadata like ``!nonnull``,; violation of that metadata constraint will also result in undefined behavior. Semantics:; """""""""""""""""""". The location of memory pointed to is loaded. If the value being loaded; is of scalar type then the number of bytes read does not exceed the; minimum number of bytes needed to hold all bits of the type. For; example, loading an ``i24`` reads at most three bytes. When loading a; value of a type like ``i20`` with a size t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:417442,load,loads,417442,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance," optionally specify a :ref:`linkage type <linkage>`. Either global variable definitions or declarations may have an explicit section; to be placed in and may have an optional explicit alignment specified. If there; is a mismatch between the explicit or inferred section information for the; variable declaration and its definition the resulting behavior is undefined. A variable may be defined as a global ``constant``, which indicates that; the contents of the variable will **never** be modified (enabling better; optimization, allowing the global data to be placed in the read-only; section of an executable, etc). Note that variables that need runtime; initialization cannot be marked ``constant`` as there is a store to the; variable. LLVM explicitly allows *declarations* of global variables to be marked; constant, even if the final definition of the global is not. This; capability can be used to enable slightly better optimization of the; program, but requires the language definition to guarantee that; optimizations based on the 'constantness' are valid for the translation; units that do not include the definition. As SSA values, global variables define pointer values that are in scope; (i.e. they dominate) all basic blocks in the program. Global variables; always define a pointer to their ""content"" type because they describe a; region of memory, and all memory objects in LLVM are accessed through; pointers. Global variables can be marked with ``unnamed_addr`` which indicates; that the address is not significant, only the content. Constants marked; like this can be merged with other constants if they have the same; initializer. Note that a constant with significant address *can* be; merged with a ``unnamed_addr`` constant, the result being a constant; whose address is significant. If the ``local_unnamed_addr`` attribute is given, the address is known to; not be significant within the module. A global variable may be declared to reside in a target-specific; numbered addr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:31591,optimiz,optimization,31591,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance," or Legal Entity authorized to submit on behalf of; the copyright owner. For the purposes of this definition, ""submitted""; means any form of electronic, verbal, or written communication sent; to the Licensor or its representatives, including but not limited to; communication on electronic mailing lists, source code control systems,; and issue tracking systems that are managed by, or on behalf of, the; Licensor for the purpose of discussing and improving the Work, but; excluding communication that is conspicuously marked or otherwise; designated in writing by the copyright owner as ""Not a Contribution."". ""Contributor"" shall mean Licensor and any individual or Legal Entity; on behalf of whom a Contribution has been received by Licensor and; subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of; this License, each Contributor hereby grants to You a perpetual,; worldwide, non-exclusive, no-charge, royalty-free, irrevocable; copyright license to reproduce, prepare Derivative Works of,; publicly display, publicly perform, sublicense, and distribute the; Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of; this License, each Contributor hereby grants to You a perpetual,; worldwide, non-exclusive, no-charge, royalty-free, irrevocable; (except as stated in this section) patent license to make, have made,; use, offer to sell, sell, import, and otherwise transfer the Work,; where such license applies only to those patent claims licensable; by such Contributor that are necessarily infringed by their; Contribution(s) alone or by combination of their Contribution(s); with the Work to which such Contribution(s) was submitted. If You; institute patent litigation against any entity (including a; cross-claim or counterclaim in a lawsuit) alleging that the Work; or a Contribution incorporated within the Work constitutes direct; or contributory patent infrin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/LICENSE.TXT:3682,perform,perform,3682,interpreter/llvm-project/clang/LICENSE.TXT,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/LICENSE.TXT,4,['perform'],['perform']
Performance," or Twine as a scratch buffer, but then use std::string to persist; the result. .. _ds_set:. Set-Like Containers (std::set, SmallSet, SetVector, etc); --------------------------------------------------------. Set-like containers are useful when you need to canonicalize multiple values; into a single representation. There are several different choices for how to do; this, providing various trade-offs. .. _dss_sortedvectorset:. A sorted 'vector'; ^^^^^^^^^^^^^^^^^. If you intend to insert a lot of elements, then do a lot of queries, a great; approach is to use an std::vector (or other sequential container) with; std::sort+std::unique to remove duplicates. This approach works really well if; your usage pattern has these two distinct phases (insert then query), and can be; coupled with a good choice of :ref:`sequential container <ds_sequential>`. This combination provides the several nice properties: the result data is; contiguous in memory (good for cache locality), has few allocations, is easy to; address (iterators in the final vector are just indices or pointers), and can be; efficiently queried with a standard binary search (e.g.; ``std::lower_bound``; if you want the whole range of elements comparing; equal, use ``std::equal_range``). .. _dss_smallset:. llvm/ADT/SmallSet.h; ^^^^^^^^^^^^^^^^^^^. If you have a set-like data structure that is usually small and whose elements; are reasonably small, a ``SmallSet<Type, N>`` is a good choice. This set has; space for N elements in place (thus, if the set is dynamically smaller than N,; no malloc traffic is required) and accesses them with a simple linear search.; When the set grows beyond N elements, it allocates a more expensive; representation that guarantees efficient access (for most types, it falls back; to :ref:`std::set <dss_set>`, but for pointers it uses something far better,; :ref:`SmallPtrSet <dss_smallptrset>`. The magic of this class is that it handles small sets extremely efficiently, but; gracefully handles",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:77579,cache,cache,77579,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['cache'],['cache']
Performance," or a; :ref:`function attribute <fnattrs>` holds for a certain value at a certain; location. Operand bundles enable assumptions that are either hard or impossible; to represent as a boolean argument of an :ref:`llvm.assume <int_assume>`. An assume operand bundle has the form:. ::. ""<tag>""([ <arguments>] ]). In the case of function or parameter attributes, the operand bundle has the; restricted form:. ::. ""<tag>""([ <holds for value> [, <attribute argument>] ]). * The tag of the operand bundle is usually the name of attribute that can be; assumed to hold. It can also be `ignore`, this tag doesn't contain any; information and should be ignored.; * The first argument if present is the value for which the attribute hold.; * The second argument if present is an argument of the attribute. If there are no arguments the attribute is a property of the call location. For example:. .. code-block:: llvm. call void @llvm.assume(i1 true) [""align""(ptr %val, i32 8)]. allows the optimizer to assume that at location of call to; :ref:`llvm.assume <int_assume>` ``%val`` has an alignment of at least 8. .. code-block:: llvm. call void @llvm.assume(i1 %cond) [""cold""(), ""nonnull""(ptr %val)]. allows the optimizer to assume that the :ref:`llvm.assume <int_assume>`; call location is cold and that ``%val`` may not be null. Just like for the argument of :ref:`llvm.assume <int_assume>`, if any of the; provided guarantees are violated at runtime the behavior is undefined. While attributes expect constant arguments, assume operand bundles may be; provided a dynamic value, for example:. .. code-block:: llvm. call void @llvm.assume(i1 true) [""align""(ptr %val, i32 %align)]. If the operand bundle value violates any requirements on the attribute value,; the behavior is undefined, unless one of the following exceptions applies:. * ``""align""`` operand bundles may specify a non-power-of-two alignment; (including a zero alignment). If this is the case, then the pointer value; must be a null pointer, otherwi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:124256,optimiz,optimizer,124256,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance," or pointing to a different prebuilt module cache path. For example:. .. code-block:: sh. rm -rf prebuilt ; mkdir prebuilt ; rm -rf prebuilt_a ; mkdir prebuilt_a; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -fdisable-module-hash; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt_a -fdisable-module-hash -DENABLE_A; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt_a -DENABLE_A. Instead of managing the different module versions manually, we can build implicit modules in a given cache path (using ``-fmodules-cache-path``), and reuse them as prebuilt implicit modules by passing ``-fprebuilt-module-path`` and ``-fprebuilt-implicit-modules``. .. code-block:: sh. rm -rf prebuilt; mkdir prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -DENABLE_A; find prebuilt -name ""*.pcm""; # prebuilt/1AYBIGPM8R2GA/A-3L1K4LUA6O31.pcm; # prebuilt/1AYBIGPM8R2GA/B-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/A-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -DENABLE_A. Finally we want to allow implicit modules for configurations that were not prebuilt. When using the clang driver a module cache path is implicitly selected. Using ``-cc1``, we simply add use the ``-fmodules-cache-path`` option. .. code-block:: sh. clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:22542,cache,cache-path,22542,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['cache'],['cache-path']
Performance," ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; to local have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - loc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:223263,load,load,223263,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," other hand, benefit a lot. Every call to a Python member function results in a lookup of that; member function and an association of this method with `'self'`.; Furthermore, a temporary object is created during this process that is; discarded after the method call. In inner loops, it may be worth your; while (up to 30%), to short-cut this process by looking up and binding; the method before the loop, and discarding it afterwards. Here is an; example:. ``` {.cpp}; hpx = TH1F('hpx','px',100,-4,4); hpxFill = hpx.Fill # cache bound method; for i in xrange(25000):; px = gRandom.Gaus(); hpxFill(px) # use bound method: no lookup needed; del hpxFill # done with cached method; ```. Note that if you do not discard the bound method, a reference to the; histogram will remain outstanding, and it will not be deleted when it; should be. It is therefore important to delete the method when you're; done with it. ### Use of Python Functions. It is possible to mix Python functions with ROOT and perform such; operations as plotting and fitting of histograms with them. In all; cases, the procedure consists of instantiating a ROOT **`TF1`**,; **`TF2`**, or **`TF3`** with the Python function and working with that; ROOT object. There are some memory issues, so it is for example not yet; possible to delete a **`TF1`** instance and then create another one with; the same name. In addition, the Python function, once used for; instantiating the **`TF1`**, is never deleted. Instead of a Python function, you can also use callable instances (e.g.,; an instance of a class that has implemented the `__call__` member; function). The signature of the Python callable should provide for one; or two arrays. The first array, which must always be present, shall; contain the `x`, `y`, `z`, and t values for the call. The second array,; which is optional and its size depends on the number given to the; **`TF1`** constructor, contains the values that parameterize the; function. For more details, see the **`TF1`*",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:24983,perform,perform,24983,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['perform'],['perform']
Performance," other hand, changing the types of; local variables won't have such ABI implications. Hence, ``-fbounds-safety``; considers the outermost pointer types of local variables as non-ABI visible. The; rest of the pointers such as nested pointer types, pointer types of global; variables, struct fields, and function prototypes are considered ABI-visible. All ABI-visible pointers are treated as ``__single`` by default unless annotated; otherwise. This default both preserves ABI and makes these pointers safe by; default. This behavior can be controlled with macros, i.e.,; ``__ptrcheck_abi_assume_*ATTR*()``, to set the default annotation for; ABI-visible pointers to be either ``__single``, ``__bidi_indexable``,; ``__indexable``, or ``__unsafe_indexable``. For instance,; ``__ptrcheck_abi_assume_unsafe_indexable()`` will make all ABI-visible pointers; be ``__unsafe_indexable``. Non-ABI visible pointers — the outermost pointer; types of local variables — are ``__bidi_indexable`` by default, so that these; pointers have the bounds information necessary to perform bounds checks without; the need for a manual annotation. All ``const char`` pointers or any typedefs; equivalent to ``const char`` pointers are ``__null_terminated`` by default. This; means that ``char8_t`` is ``unsigned char`` so ``const char8_t *`` won't be; ``__null_terminated`` by default. Similarly, ``const wchar_t *`` won't be; ``__null_terminated`` by default unless the platform defines it as ``typedef; char wchar_t``. Please note, however, that the programmers can still explicitly; use ``__null_terminated`` in any other pointers, e.g., ``char8_t; *__null_terminated``, ``wchar_t *__null_terminated``, ``int; *__null_terminated``, etc. if they should be treated as ``__null_terminated``.; The same applies to other annotations.; In system headers, the default pointer attribute for ABI-visible pointers is set; to ``__unsafe_indexable`` by default. The ``__ptrcheck_abi_assume_*ATTR*()`` macros are defined as pragmas in ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst:21881,perform,perform,21881,interpreter/llvm-project/clang/docs/BoundsSafety.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst,1,['perform'],['perform']
Performance," other words,; convergence regions must be reasonably nested.). .. note::. For brevity, this document uses the term ""convergence region of a token; definition ``D``"" to actually refer to the convergence region of the token; ``T`` defined by ``D``. .. _inferring_noconvergent:. Inferring non-convergence; =========================. When the target or the environment guarantees that threads do not; communicate using convergent operations or that threads never diverge,; the dynamic instances in the program are irrelevant and an optimizer; may remove any occurrence of the ``convergent`` attribute on a; call-site or a function and any explicit ``convergencectrl`` operand; bundle at a call-site. An optimizer may remove the ``convergent`` attribute and any explicit; ``convergencectrl`` operand bundle from a call-site if it can prove; that the execution of this call-site always results in a call to a; non-convergent function. An optimizer may remove the ``convergent`` attribute on a function if it can; prove that the function does not contain a call to; :ref:`llvm.experimental.convergence.entry; <llvm.experimental.convergence.entry>`, or any uncontrolled convergent; operations. Memory Model Non-Interaction; ============================. The fact that an operation is convergent has no effect on how it is treated for; memory model purposes. In particular, an operation that is ``convergent`` and; ``readnone`` does not introduce additional ordering constraints as far as the; memory model is concerned. There is no implied barrier, neither in the memory; barrier sense nor in the control barrier sense of synchronizing the execution; of threads. Informational note: Threads that execute converged dynamic instances do not; necessarily do so at the same time. Other Interactions; ==================. A function can be both ``convergent`` and; ``speculatable``, indicating that the function does not have undefined; behavior and has no effects besides calculating its result, but is still; af",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ConvergentOperations.rst:33876,optimiz,optimizer,33876,interpreter/llvm-project/llvm/docs/ConvergentOperations.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ConvergentOperations.rst,1,['optimiz'],['optimizer']
Performance," otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``mayWriteToMemory()``: Existing predicate, but note; that they return true for any operation which is volatile or at least; Monotonic. * ``isStrongerThan`` / ``isAtLeastOrStrongerThan``: These are predicates on; orderings. They can be useful for passes that are aware of atomics, for; example to do DSE across a single atomic access, but not across a; release-acquire pair (see MemoryDependencyAnalysis for an example of this). * Alias analysis: Note that AA will return ModRef for anything Acquire or; Release, and for the address accessed by any Monotonic operation. To support optimizin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15785,optimiz,optimization,15785,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['optimiz'],['optimization']
Performance," out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``-noalias`` is the; only option). Essentially, there is no option to specify a different memory; type (e.g., write-back, write-combining, write-through; etc.) and consequently; to weaken, or strengthen, the memory model. Other limitations are:. * The LSUnit does not know when store-to-load forwarding may occur.; * The LSUnit does not know anything about cache hierarchy and memory types.; * The LSUnit does not know how to identify serializing operations and memory; fences. The LSUnit does not attempt to predict if a load or store hits or misses the L1; cache. It only knows if an instruction ""MayLoad"" and/or ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:40284,load,loads,40284,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['loads']
Performance," over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has beco",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10875,concurren,concurrency,10875,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['concurren'],['concurrency']
Performance," own vendor clones of LLVM and Clang (part of ROOT's trunk); on which it is based. This tool is the easiest way to build Cling for your favorite; platorm and bundle it into an installer. If you want to manually compile Cling; from source, go through the [README] of Cling or the build instructions [here]. [README]:https://github.com/root-project/cling/blob/master/README.md; [here]:https://root.cern/cling/cling_build_instructions/. Below is a list of platforms currently supported by CPT:; * Ubuntu and distros based on Debian - *DEB packages*; * Windows - *NSIS installers*; * Distros based on Red Hat Linux (Fedora/Scientific Linux CERN) - *RPM packages*; * Mac OS X - *Apple Disk Images*; * Virtually any UNIX-like platform which supports Bash - *Tarballs*. ### Requirements; Before using this tool, make sure you have the required packages installed on; your system. Detailed information on what and how to install is provided below,; but the recommended (and much easier) way is to use the following command which; performs the required checks automatically and displays useful suggestions too; specific to your platform.; ```sh; cd tools/packaging/; ./cpt.py --check-requirements; ```; or; ```sh; cd tools/packaging/; ./cpt.py -c; ```; Regardless of the platform and operating system, make sure to call the cpt script; with Python 3.; CPT uses some features and modules which are not a part of older versions of Python.; The same holds true for the versions of GCC/Clang you have on your machine. Older; compilers do not support c++11 features and thus you can expect a build error if you; choose not to update them. All pre-compiled binaries of Python ship with built-in support for SSL. However if; the Python on your system was compiled by you manually, chances are that it doesn't; have SSL support. This is very likely if you had performed a minimal installation; of Scientific Linux CERN which doesn't include OpenSSL development package. In such; a case, you should install ```openssl-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/README.md:1256,perform,performs,1256,interpreter/cling/tools/packaging/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/README.md,1,['perform'],['performs']
Performance," package](Common Vector package). In contrast to CLHEP or; the ROOT physics libraries, `GenVector` provides class templates for; modeling the vectors. The user can control how the vector is internally; represented. This is expressed by a choice of coordinate system, which; is supplied as a template parameter when the vector is constructed.; Furthermore, each coordinate system is itself a template, so that the; user can specify the underlying scalar type. The `GenVector` classes do not inherit from **`TObject`**, therefore; cannot be used as in the case of the physics vector classes in ROOT; collections. In addition, to optimize performances, no virtual destructors are; provided. In the following paragraphs, the main characteristics of; `GenVector` are described. A more detailed description of all the; `GenVector` classes is available also at; <http://seal.cern.ch/documents/mathlib/GenVector.pdf>. ### Main Characteristics. #### Optimal Runtime Performances. We try to minimize any overhead in the run-time performance. We have; deliberately avoided the use of any virtual function and even virtual; destructors in the classes. In addition, as much as possible functions; are defined as inline. For this reason, we have chosen to use template; classes to implement the `GenVector` concepts instead of abstract or; base classes and virtual functions. It is then recommended to avoid; using the `GenVector` classes polymorphically and developing classes; inheriting from them. #### Points and Vector Concept. Mathematically vectors and points are two distinct concepts. They have; different transformations, as vectors only rotate while points rotate; and translate. You can add two vectors but not two points and the; difference between two points is a vector. We then distinguish for the 3; dimensional case, between points and vectors, modeling them with; different classes:. - `ROOT::Math::`**`DisplacementVector2D`** and; `ROOT::Math::`**`DisplacementVector3D`** template classes descri",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:68968,perform,performance,68968,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['performance']
Performance," page is geared towards users of the LLVM CMake build. If you're looking for; information about modifying the LLVM CMake build system you may want to see the; :doc:`CMakePrimer` page. It has a basic overview of the CMake language. .. _Quick start:. Quick start; ===========. We use here the command-line, non-interactive CMake interface. #. `Download <http://www.cmake.org/cmake/resources/software.html>`_ and install; CMake. Version 3.20.0 is the minimum required. #. Open a shell. Your development tools must be reachable from this shell; through the PATH environment variable. #. Create a build directory. Building LLVM in the source; directory is not supported. cd to this directory:. .. code-block:: console. $ mkdir mybuilddir; $ cd mybuilddir. #. Execute this command in the shell replacing `path/to/llvm/source/root` with; the path to the root of your LLVM source tree:. .. code-block:: console. $ cmake path/to/llvm/source/root. CMake will detect your development environment, perform a series of tests, and; generate the files required for building LLVM. CMake will use default values; for all build parameters. See the `Options and variables`_ section for; a list of build parameters that you can modify. This can fail if CMake can't detect your toolset, or if it thinks that the; environment is not sane enough. In this case, make sure that the toolset that; you intend to use is the only one reachable from the shell, and that the shell; itself is the correct one for your development environment. CMake will refuse; to build MinGW makefiles if you have a POSIX shell reachable through the PATH; environment variable, for instance. You can force CMake to use a given build; tool; for instructions, see the `Usage`_ section, below. You may; also wish to control which targets LLVM enables, or which LLVM; components are built; see the `Frequently Used LLVM-related; variables`_ below. #. After CMake has finished running, proceed to use IDE project files, or start; the build from the buil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:1915,perform,perform,1915,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['perform'],['perform']
Performance," painful. Debuggers generally read debug information from object files on; disk, but for JITed code there is no such file to look for. In order to hand over the necessary debug info, `GDB established an; interface <https://sourceware.org/gdb/onlinedocs/gdb/JIT-Interface.html>`_; for registering JITed code with debuggers. LLDB implements it in the; JITLoaderGDB plugin. On the JIT side, LLVM MCJIT does implement the interface; for ELF object files. At a high level, whenever MCJIT generates new machine code, it does so in an; in-memory object file that contains the debug information in DWARF format.; MCJIT then adds this in-memory object file to a global list of dynamically; generated object files and calls a special function; ``__jit_debug_register_code`` that the debugger knows about. When the debugger; attaches to a process, it puts a breakpoint in this function and associates a; special handler with it. Once MCJIT calls the registration function, the; debugger catches the breakpoint signal, loads the new object file from the; inferior's memory and resumes execution. This way it can obtain debug; information for pure in-memory object files. GDB Version; ===========. In order to debug code JIT-ed by LLVM, you need GDB 7.0 or newer, which is; available on most modern distributions of Linux. The version of GDB that; Apple ships with Xcode has been frozen at 6.3 for a while. LLDB Version; ============. Due to a regression in release 6.0, LLDB didn't support JITed code debugging for; a while. The bug was fixed in mainline recently, so that debugging JITed ELF; objects should be possible again from the upcoming release 12.0 on. On macOS the; feature must be enabled explicitly using the ``plugin.jit-loader.gdb.enable``; setting. Debugging MCJIT-ed code; =======================. The emerging MCJIT component of LLVM allows full debugging of JIT-ed code with; GDB. This is due to MCJIT's ability to use the MC emitter to provide full; DWARF debugging information to GDB. Note th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DebuggingJITedCode.rst:1183,load,loads,1183,interpreter/llvm-project/llvm/docs/DebuggingJITedCode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DebuggingJITedCode.rst,1,['load'],['loads']
Performance," parameter; 7. Provide new code loader via JSROOT.require(); - introduces clean dependencies in JSROOT code; - by default uses plain script loading emulating require.js behavior; - can use require.js when available; - uses require() method when running inside node.js; - supports openui5 sap.ui.require loader if available before JSRoot.core.js; - deprecates old JSROOT.AssertPrerequisites() function; 8. Upgrade d3.js to v6.1.1, skip support of older versions; 9. Upgrade three.js to r121:; - SoftwareRenderer deprecated and removed; - let use WebGL for browser, batch and node.js (via headless-gl); - support r3d_gl, r3d_img, r3d_svg rendering options for TGeo and histograms; - keep support of SVGRendered as backup solution; 10. Upgrade MathJax.js to version 3.1.1; - reliably works in browser and node.js!; - all latex/mathjax related methods moved to special JSRoot.latex.js script, loaded on demand; 11. Update jquery to 3.5.1, openui5 to 1.82.2; 12. Use JS classes only in few places - performance is not good enough compared to Object.prototype; 13. Deprecate IE support; 14. Deprecate bower package manager; 15. Add support of ZSTD compression - works only on https://root.cern/js/ website; 16. Add support of log2 scale for axes drawing, v7 can have arbitrary log base; 17. Improve TH2 col drawings for large number of bins - up to factor 5 faster; 18. Allow to move axis title to opposite position; 19. Fix zooming in color palette; 20. Implement monitoring of object inspector. ## Changes in 5.9.1; 1. Fix zooming in color palette; 2. Fix interactive update of TGraph painting on time scale; 3. Fix I/O error in reading std::map (#204); 4. Fix functionality of ""open all"" / ""close all"" GUI buttons. ## Changes in 5.9.0; 1. Support RX and RY drawing option together with COL of TH2; 2. Add support of #overline, #underline, #strike into TLatex parsing (#196); 3. Add support of TGeoTessellated shape; 4. Major changes in v7 drawing: RFrame, RPalette, RColor, RStatBox, ...; 5. Fix in readi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:26385,perform,performance,26385,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['perform'],['performance']
Performance," parameters (if any). M will try to find; $\mbox{npoints}$ points on the contour (default 20). To calculate; more than one contour, the user needs to set the error definition; [howto:errordef] in its $\mbox{FCN}$ to the appropriate value for; the desired confidence level and call the method MnContours::operator(); for each contour. ### contour(...) ###. MnContours::contour(unsigned int parx, unsigned int pary, unsigned int; npoints = 20) causes a $\mbox{CONTOURS}$ error analysis and returns; the result in form of ContoursError. As a by-product ContoursError keeps; the MinosError information of parameters $\mbox{parx}$ and; $\mbox{pary}$. The result ContoursError can be easily printed using; std::cout. ## MnEigen ##. [api:eigen] MnEigen calculates and the eigenvalues of the user; covariance matrix MnUserCovariance. ### MnEigen() ###. MnEigen is instantiated via default constructor. ### operator() ###. operator()(const MnUserCovariance&) const will perform the calculation; of the eigenvalues of the covariance matrix and return the result in; form of a std::vector\<double\>. The eigenvalues are ordered from the; smallest first to the largest eigenvalue. ## MnHesse ##. [api:hesse]. With MnHesse the user can instructs M to calculate, by finite; differences, the Hessian or error matrix. That is, it calculates the; full matrix of second derivatives of the function with respect to the; currently variable parameters, and inverts it. ### MnHesse() ###. The default constructor of MnHesse() will use default settings of; MnStrategy. Other constructors with user specific MnStrategy settings; are provided as well. ### operator() ###. The MnHesse::operator() is overloaded both for internal (M ) and; external (user) parameters. External parameters can be specified as; std::vector$<$double$>$ or as MnUserParameters. The return value is; always a MnUserParameterState. The optional argument $\mbox{maxcalls}$ specifies the (approximate); maximum number of function calls after which the c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:40171,perform,perform,40171,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['perform'],['perform']
Performance," part is how to then incorporate ``DeclContext``: all that is needed; is in ``bool DeclContext::classof(const Decl *)``, which asks the question; ""Given a ``Decl``, how can I determine if it is-a ``DeclContext``?"".; It answers this with a simple switch over the set of ``Decl`` ""kinds"", and; returning true for ones that are known to be ``DeclContext``'s. .. TODO::. Touch on some of the more advanced features, like ``isa_impl`` and; ``simplify_type``. However, those two need reference documentation in; the form of doxygen comments as well. We need the doxygen so that we can; say ""for full details, see https://llvm.org/doxygen/..."". Rules of Thumb; ==============. #. The ``Kind`` enum should have one entry per concrete class, ordered; according to a preorder traversal of the inheritance tree.; #. The argument to ``classof`` should be a ``const Base *``, where ``Base``; is some ancestor in the inheritance hierarchy. The argument should; *never* be a derived class or the class itself: the template machinery; for ``isa<>`` already handles this case and optimizes it.; #. For each class in the hierarchy that has no children, implement a; ``classof`` that checks only against its ``Kind``.; #. For each class in the hierarchy that has children, implement a; ``classof`` that checks a range of the first child's ``Kind`` and the; last child's ``Kind``. RTTI for Open Class Hierarchies; ===============================. Sometimes it is not possible to know all types in a hierarchy ahead of time.; For example, in the shapes hierarchy described above the authors may have; wanted their code to work for user defined shapes too. To support use cases; that require open hierarchies LLVM provides the ``RTTIRoot`` and; ``RTTIExtends`` utilities. The ``RTTIRoot`` class describes an interface for performing RTTI checks. The; ``RTTIExtends`` class template provides an implementation of this interface; for classes derived from ``RTTIRoot``. ``RTTIExtends`` uses the ""`Curiously; Recurring Template",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst:12542,optimiz,optimizes,12542,interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,1,['optimiz'],['optimizes']
Performance," passing the appropriate option; to `Draw() `when attaching the drawn object(s) to a pad. For a fuller; explanation of pads, attaching objects with `Draw()` etc. refer to; ""Graphical Containers: Canvas and Pad"". ``` {.cpp}; root[] myShapes->Draw(""ogl"");; ```. Valid option strings are:. - ""`ogl`"" : external GL viewer. - ""`x3d`"": external X3D viewer. - ""`pad`"": pad viewer. If no option is passed to `Draw()` then the ""`pad`"" is used by default.; If you already have content in a pad, which you would like to display in; one of the external viewers you can select from the canvas View menu /; View With, and pick the viewer type. ![Invoking external 3D viewers from canvas menus](pictures/030000D9.png). Note: A current limitation means that when an external viewer is created; the pad is no longer redrawn. When the external viewer is closed,; clicking in the pad will refresh. ### The GL Viewer. The GL Viewer uses <OpenGL®> (or compliant libraries such as <Mesa3D>); to generate high quality, high-performance 3D renderings, with; sophisticated lighting, materials and rendering styles for 3D scenes.; Many users will be able to take advantage of hardware acceleration of; the underlying OpenGL commands by their computer's video card, resulting; is considerable performance gains - up to interactive manipulation of; 1000's of complex shapes in real-time. The GL Viewer is supported on all official ROOT platforms (assuming you; have suitable <OpenGL®> libraries), and is the main 3D viewer, which; development effort is concentrated upon. As OpenGL® is a trademark we; refer to our viewer built on this technology as the ‘GL Viewer'. The; code for it can be found under `$ROOTSYS/gl`. ![The GL 3D Viewer](pictures/020000DA.jpg). You can manipulate the viewer via the GUI or via the base; **`TGLViewer`** object behind the interface. These are detailed below -; see also `$ROOTSYS/tutorials/gl/glViewerExercise.C`. #### Projections Modes (Cameras). The GL Viewer supports two basic types of camer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:107265,perform,performance,107265,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['perform'],['performance']
Performance," pcon/pgon shape with rmin==rmax on top or bottom side. ## Changes in 4.4; 1. Fix faces orientation for all TGeo shapes.; 2. Improve TGeoTorus creation - handle all parameters combinations; 3. Implement TGeoCompositeShape, using ThreeCSG.js; 4. Fix problem with color palette when switch to 3D mode (#28); 5. Use nested CSS classes to avoid conflicts with other libraries (#29); 6. Let move and resize TFrame; 7. Improve TH1/TH2 drawings; - draw all histograms points in the range (no any skipped bins); - minimize SVG code for drawing (up to factor 100); - gives significant speedup in drawings; 8. SVG code improvement for TGraph, TF1, TAxis drawings; 9. Provide new tooltip kind; - created only when needed (minimizing SVG code); - tooltip can be drawn for every object in the frame; - touch devices are supported; 10. Fix - let draw same object on the canvas with different options; 11. Create cached list of known class methods. It can be extended by users.; 12. Use of cached methods improves binary I/O performance by 20%; 13. Support TGaxis; 14. Project now can be obtained via 'bower install jsroot'; 15. Support 'scat' and 'text' draw options for TH2; 16. Support in binary I/O zipped buffer bigger than 16M; 17. Correctly handle in binary I/O pointer on TArray object (like in THnSparseArrayChunk). ## Changes in 4.3; 1. Implement TGeoCtub, TGeoParaboloid and TGeoHype shapes; 2. Support TGeoTube with Rmin==0; 3. Exclude empty faces in TGeoArb8; 4. Improve TGeoSphere creation - handle all parameters combinations; 5. Introduce JSROOT.cleanup() function to safely clear all drawn objects; 6. Fix wrong resize method in 'tabs' and 'collapsible' layouts; 7. Fix canvas resize problem (issue #27); 8. Fix zero-height canvas when draw TGeo in collapsible layout; 9. Fix problem of simultaneous move TGeo drawings and canvas in flexible layout. ## Changes in 4.2; 1. Significant performance improvements in 3D drawings - TGeo/TH2/TH3; 2. Implement TGeoPara, TGeoGtra, TGeoXtru and TGeoEltu sha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:57325,cache,cached,57325,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,2,"['cache', 'perform']","['cached', 'performance']"
Performance," point and direction coordinates to the local frame of this; object and computes the distance to its shape. The node returned is the; one pointed by the input path in case the shape is crossed; otherwise; the returned value is NULL. In case the distance to next crossed; boundary is required, the current point has to be physically INSIDE the; shape pointed by the current volume. This is only insured in case a call; to TGeoManager::FindNode() was performed for the current point.; Therefore, the first step is to convert the global current point and; direction in the local reference frame of the current volume and to; compute the distance to exit its shape from inside. The returned value; is again compared to the maximum allowed step (the proposed one) and in; case the distance is safe no other action is performed and the proposed; step is approved. In case the boundary is closer, the computed distance; is taken as maximum allowed step. For optimization purposed, for; particles starting very close to the current volume boundary (less than; 0.01 microns) and exiting the algorithm stops here. After computing the distance to exit the current node, the distance to; the daughter of the current volume which is crossed next is computed by; TGeoManager::FindNextDaughterBoundary(). This computes the; distance to all daughter candidates that can be possibly crossed by; using volume voxelization. The algorithm is efficient in average only in; case the number of daughters is greater than 4. For fewer nodes, a; simple loop is performed and the minimum distance (from a point outside; each shape) is taken and compared to the maximum allowed step. The step; value is again updated if `step<stepmax` . A special case is when the current node is declared as possibly; overlapping with something else. If this is the case, the distance is; computed for all possibly overlapping candidates, taking into account; the overlapping priorities (see also: "" Overlapping volumes ""). The global matrix de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:123474,optimiz,optimization,123474,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['optimiz'],['optimization']
Performance," poor; performance. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. If unspecified, the alignment is assumed to be equal to the; size of the '<value>' type. Note that this default alignment assumption is; different from the alignment used for the load/store instructions when align; isn't specified. The pointer passed into cmpxchg must have alignment greater than or; equal to the size in memory of the operand. Semantics:; """""""""""""""""""". The contents of memory at the location specified by the '``<pointer>``' operand; is read and compared to '``<cmp>``'; if the values are equal, '``<new>``' is; written to the location. The original value at the location is returned,; together with a flag indicating success (true) or failure (false). If the cmpxchg operation is marked as ``weak`` then a spurious failure is; permitted: the operation may not write ``<new>`` even if the comparison; matched. If the cmpxchg operation is strong (the default), the i1 value is 1 if and only; if the value loaded equals ``cmp``. A successful ``cmpxchg`` is a read-modify-write instruction for the purpose of; identifying release sequences. A failed ``cmpxchg`` is equivalent to an atomic; load with an ordering parameter determined the second ordering parameter. Example:; """""""""""""""". .. code-block:: llvm. entry:; %orig = load atomic i32, ptr %ptr unordered, align 4 ; yields i32; br label %loop. loop:; %cmp = phi i32 [ %orig, %entry ], [%value_loaded, %loop]; %squared = mul i32 %cmp, %cmp; %val_success = cmpxchg ptr %ptr, i32 %cmp, i32 %squared acq_rel monotonic ; yields { i32, i1 }; %value_loaded = extractvalue { i32, i1 } %val_success, 0; %success = extractvalue { i32, i1 } %val_success, 1; br i1 %success, label %done, label %loop. done:; ... .. _i_atomicrmw:. '``atomicrmw``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. atomicrmw [volatile] <operation> ptr <pointer>, <ty> <value> [syncscope(""<target-scope>"")] <ordering>[, align <alignment",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:428259,load,loaded,428259,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance," powerful as; necessary while providing a layer of simple ready-made solutions on top of it.; Probably a few reusable components for assembling checkers. And this layer; should ideally be pleasant enough to work with, so that people would prefer to; extend it when something is lacking, instead of falling back to the complex; omnipotent API. I'm thinking of AST matchers vs. AST visitors as a roughly; similar situation: matchers are not omnipotent, but they're so nice. * Separation between core and checkers is usually quite strange. Once we have; shared state traits, i generally wouldn't mind having region store or range; constraint manager as checkers (though it's probably not worth it to transform; them - just a mood). The main thing to avoid here would be the situation when; the checker overwrites stuff written by the core because it thinks it has a; better idea what's going on, so the core should provide a good default behavior. * Yeah, i totally care about performance as well, and if i try to implement; approach, i'd make sure it's good. **Artem:**. > Approach (2): We could teach the Store to scan itself for bindings to; > metadata-symbolic-based regions during scanReachableSymbols() whenever; > a region turns out to be reachable. This requires no work on checker side,; > but it sounds performance-heavy. Nope, this approach is wrong. Metadata symbols may become out-of-date: when the; object changes, metadata symbols attached to it aren't changing (because symbols; simply don't change). The same metadata may have different symbols to denote its; value in different moments of time, but at most one of them represents the; actual metadata value. So we'd be escaping more stuff than necessary. If only we had ""ghost fields""; (https://lists.llvm.org/pipermail/cfe-dev/2016-May/049000.html), it would have; been much easier, because the ghost field would only contain the actual; metadata, and the Store would always know about it. This example adds to my; belief that ghost f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst:7261,perform,performance,7261,interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,1,['perform'],['performance']
Performance," precede all of the integer parameters. Specific target extension types are registered with LLVM as having specific; properties. These properties can be used to restrict the type from appearing in; certain contexts, such as being the type of a global variable or having a; ``zeroinitializer`` constant be valid. A complete list of type properties may be; found in the documentation for ``llvm::TargetExtType::Property`` (`doxygen; <https://llvm.org/doxygen/classllvm_1_1TargetExtType.html>`_). :Syntax:. .. code-block:: llvm. target(""label""); target(""label"", void); target(""label"", void, i32); target(""label"", 0, 1, 2); target(""label"", void, i32, 0, 1, 2). .. _t_vector:. Vector Type; """""""""""""""""""""". :Overview:. A vector type is a simple derived type that represents a vector of; elements. Vector types are used when multiple primitive data are; operated in parallel using a single instruction (SIMD). A vector type; requires a size (number of elements), an underlying primitive data type,; and a scalable property to represent vectors where the exact hardware; vector length is unknown at compile time. Vector types are considered; :ref:`first class <t_firstclass>`. :Memory Layout:. In general vector elements are laid out in memory in the same way as; :ref:`array types <t_array>`. Such an analogy works fine as long as the vector; elements are byte sized. However, when the elements of the vector aren't byte; sized it gets a bit more complicated. One way to describe the layout is by; describing what happens when a vector such as <N x iM> is bitcasted to an; integer type with N*M bits, and then following the rules for storing such an; integer to memory. A bitcast from a vector type to a scalar integer type will see the elements; being packed together (without padding). The order in which elements are; inserted in the integer depends on endianness. For little endian element zero; is put in the least significant bits of the integer, and for big endian; element zero is put in the most signi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:174598,scalab,scalable,174598,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance," preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the caches. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:373658,load,load,373658,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address spa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:227628,load,load,227628,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," preferred. Floating-point math operations are allowed to treat all NaNs as if they were; quiet NaNs. For example, ""pow(1.0, SNaN)"" may be simplified to 1.0. Code that requires different behavior than this should use the; :ref:`Constrained Floating-Point Intrinsics <constrainedfp>`.; In particular, constrained intrinsics rule out the ""Unchanged NaN propagation""; case; they are guaranteed to return a QNaN. Unfortunately, due to hard-or-impossible-to-fix issues, LLVM violates its own; specification on some architectures:. - x86-32 without SSE2 enabled may convert floating-point values to x86_fp80 and; back when performing floating-point math operations; this can lead to results; with different precision than expected and it can alter NaN values. Since; optimizations can make contradicting assumptions, this can lead to arbitrary; miscompilations. See `issue #44218; <https://github.com/llvm/llvm-project/issues/44218>`_.; - x86-32 (even with SSE2 enabled) may implicitly perform such a conversion on; values returned from a function for some calling conventions. See `issue; #66803 <https://github.com/llvm/llvm-project/issues/66803>`_.; - Older MIPS versions use the opposite polarity for the quiet/signaling bit, and; LLVM does not correctly represent this. See `issue #60796; <https://github.com/llvm/llvm-project/issues/60796>`_. .. _fastmath:. Fast-Math Flags; ---------------. LLVM IR floating-point operations (:ref:`fneg <i_fneg>`, :ref:`fadd <i_fadd>`,; :ref:`fsub <i_fsub>`, :ref:`fmul <i_fmul>`, :ref:`fdiv <i_fdiv>`,; :ref:`frem <i_frem>`, :ref:`fcmp <i_fcmp>`), :ref:`phi <i_phi>`,; :ref:`select <i_select>` and :ref:`call <i_call>`; may use the following flags to enable otherwise unsafe; floating-point transformations. ``nnan``; No NaNs - Allow optimizations to assume the arguments and result are not; NaN. If an argument is a nan, or the result would be a nan, it produces; a :ref:`poison value <poisonvalues>` instead. ``ninf``; No Infs - Allow optimizations to assume the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:161058,perform,perform,161058,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance," printed out by the tool in; the analysis report. .. option:: -print-imm-hex. Prefer hex format for numeric literals in the output assembly printed as part; of the report. .. option:: -dispatch=<width>. Specify a different dispatch width for the processor. The dispatch width; defaults to field 'IssueWidth' in the processor scheduling model. If width is; zero, then the default dispatch width is used. .. option:: -register-file-size=<size>. Specify the size of the register file. When specified, this flag limits how; many physical registers are available for register renaming purposes. A value; of zero for this flag means ""unlimited number of physical registers"". .. option:: -iterations=<number of iterations>. Specify the number of iterations to run. If this flag is set to 0, then the; tool sets the number of iterations to a default value (i.e. 100). .. option:: -noalias=<bool>. If set, the tool assumes that loads and stores don't alias. This is the; default behavior. .. option:: -lqueue=<load queue size>. Specify the size of the load queue in the load/store unit emulated by the tool.; By default, the tool assumes an unbound number of entries in the load queue.; A value of zero for this flag is ignored, and the default load queue size is; used instead. .. option:: -squeue=<store queue size>. Specify the size of the store queue in the load/store unit emulated by the; tool. By default, the tool assumes an unbound number of entries in the store; queue. A value of zero for this flag is ignored, and the default store queue; size is used instead. .. option:: -timeline. Enable the timeline view. .. option:: -timeline-max-iterations=<iterations>. Limit the number of iterations to print in the timeline view. By default, the; timeline view prints information for up to 10 iterations. .. option:: -timeline-max-cycles=<cycles>. Limit the number of cycles in the timeline view, or use 0 for no limit. By; default, the number of cycles is set to 80. .. option:: -resource-pressure. Enab",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:4128,load,load,4128,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,"['load', 'queue']","['load', 'queue']"
Performance," probabilities); .uleb128 2 # BB_1 successor 1 BB ID (only enabled with branch probabilities); .uleb128 0x11111111 # BB_1 successor 1 branch probability (only enabled with branch probabilities); .uleb128 3 # BB_1 successor 2 BB ID (only enabled with branch probabilities); .uleb128 0x11111111 # BB_1 successor 2 branch probability (only enabled with branch probabilities); # PGO data record for BB_2; .uleb128 18 # BB_2 basic block frequency (only when enabled); .uleb128 1 # BB_2 successors count (only enabled with branch probabilities); .uleb128 3 # BB_2 successor 1 BB ID (only enabled with branch probabilities); .uleb128 0xffffffff # BB_2 successor 1 branch probability (only enabled with branch probabilities); # PGO data record for BB_3; .uleb128 1000 # BB_3 basic block frequency (only when enabled); .uleb128 0 # BB_3 successors count (only enabled with branch probabilities). ``SHT_LLVM_OFFLOADING`` Section (offloading data); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; This section stores the binary data used to perform offloading device linking; and execution, creating a fat binary. This section is emitted during compilation; of offloading languages such as OpenMP or CUDA. If the data is intended to be; used by the device linker only, it should use the ``SHF_EXCLUDE`` flag so it is; automatically stripped from the final executable or shared library. The binary data stored in this section conforms to a custom binary format used; for storing offloading metadata. This format is effectively a string table; containing metadata accompanied by a device image. ``SHT_LLVM_LTO`` Section (LLVM bitcode for fat LTO); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; This section stores LLVM bitcode used to perform regular LTO or ThinLTO at link; time. This section is generated when the compiler enables fat LTO. This section; has the ``SHF_EXCLUDE`` flag so that it is stripped from the final executable; or shared library. CodeView-Dependent; ------------------. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Extensions.rst:17899,perform,perform,17899,interpreter/llvm-project/llvm/docs/Extensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Extensions.rst,1,['perform'],['perform']
Performance," problem with [Cross-Origin Request](https://developer.mozilla.org/en/http_access_control) can appear. If the web server configuration cannot be changed, just copy JSROOT to the web server itself. ### Binary file-based monitoring (not recommended). Theoretically, one could use binary ROOT files to implement monitoring.; With such approach, a ROOT-based application creates and regularly updates content of a ROOT file, which can be accessed via normal web server. From the browser side, JSROOT could regularly read the specified objects and update their drawings. But such solution has three major caveats. First of all, one need to store the data of all objects, which only potentially could be displayed in the browser. In case of 10 objects it does not matter, but for 1000 or 100000 objects this will be a major performance penalty. With such big amount of data one will never achieve higher update rate. The second problem is I/O. To read the first object from the ROOT file, one need to perform several (about 5) file-reading operations via http protocol.; There is no http file locking mechanism (at least not for standard web servers),; therefore there is no guarantee that the file content is not changed/replaced between consequent read operations. Therefore, one should expect frequent I/O failures while trying to monitor data from ROOT binary files. There is a workaround for the problem - one could load the file completely and exclude many partial I/O operations by this. To achieve this with JSROOT, one should add ""+"" sign at the end of the file name. Of course, it only could work for small files. If somebody still wants to use monitoring of data from ROOT files, could try link like:. - <https://root.cern/js/latest/?nobrowser&file=../files/hsimple.root+&item=hpx;1&monitoring=2000>. In this particular case, the histogram is not changing. ## JSROOT API. JSROOT can be used in arbitrary HTML pages to display data, produced with or without ROOT-based applications. Many differen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:32943,perform,perform,32943,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['perform'],['perform']
Performance," process. The default pipeline implements the following sequence of stages used to; process instructions. * Dispatch (Instruction is dispatched to the schedulers).; * Issue (Instruction is issued to the processor pipelines).; * Write Back (Instruction is executed, and results are written back).; * Retire (Instruction is retired; writes are architecturally committed). The in-order pipeline implements the following sequence of stages:; * InOrderIssue (Instruction is issued to the processor pipelines).; * Retire (Instruction is retired; writes are architecturally committed). :program:`llvm-mca` assumes that instructions have all been decoded and placed; into a queue before the simulation start. Therefore, the instruction fetch and; decode stages are not modeled. Performance bottlenecks in the frontend are not; diagnosed. Also, :program:`llvm-mca` does not model branch prediction. Instruction Dispatch; """"""""""""""""""""""""""""""""""""""""; During the dispatch stage, instructions are picked in program order from a; queue of already decoded instructions, and dispatched in groups to the; simulated hardware schedulers. The size of a dispatch group depends on the availability of the simulated; hardware resources. The processor dispatch width defaults to the value; of the ``IssueWidth`` in LLVM's scheduling model. An instruction can be dispatched if:. * The size of the dispatch group is smaller than processor's dispatch width.; * There are enough entries in the reorder buffer.; * There are enough physical registers to do register renaming.; * The schedulers are not full. Scheduling models can optionally specify which register files are available on; the processor. :program:`llvm-mca` uses that information to initialize register; file descriptors. Users can limit the number of physical registers that are; globally available for register renaming by using the command option; ``-register-file-size``. A value of zero for this option means *unbounded*. By; knowing how many registers are available ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:34997,queue,queue,34997,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance," processed twice or more times).; Fix problem with the transmission of non-default file; attributes (e.g. the number of entries) from TChainElement to; TDSetElement during TChain processing in PROOF; Fix problem in the default packetizer with validating the; exact number of needed files when the information about the entries is; already available.; Fix problem with 'xpd.putenv' and 'xpd.putrc' occuring when the variables themselves contain commas.; Avoid resolving the workers FQDN when running in PROOF-Lite,; creating unnecessary delays when running PROOF-Lite within virtual; machines.; Fix problem with the permissions of the user data directory.; Add files to the list of files to process only when finally validated.; Fix; problem with canvases when the feedback canvas and the final canvas are; the same (do not delete the feedback canvas at the end of processing); Make sure that TProof::Load, TProofPlayer::SendSelector and; TSelector::GetSelector treat consistently the extensions of the; implementation files.; Unlock the cache after failure to load a selector; prevents session freezing; Correctly update the number of submergers when workers die; Add missing protection causing a crash in submergers when the output list contained TProofOutputFile objects.; Move the creation and start of the idle timeout from the end; of SetupCommon to the end of CreateServer, so that the timeout is not; active during worker setup.; Make sure that the TProof instance on the client is invalidated after an idle timeout.; Fix an old issue with DeactivateWorker(""*"") (the session is; was terminated because no worker was active; this call coudl not be; used as intermediate step to select a small number of workers).; Consistently check both Proof.Sandbox and ProofLite.Sandbox for sandbox non-default location as done in TProofLite; Fix a problem with the registration of missing files in the; 'MissingFiles' list (files which could not be open on the workers were; not always added to the list). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v528/index.html:12491,cache,cache,12491,proof/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v528/index.html,2,"['cache', 'load']","['cache', 'load']"
Performance," provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require independent; protection *and* which require hardening the address of the load. However, this; is unlikely to be profitable in practice. The latency hit of the hardening; would need to exceed that of an `lfence` when *correctly* speculatively; executed. But in that case, the `lfence` cost is a complete loss of speculative; execution (at a minimum). So far, the evidence we have of the performance cost; of using `lfence` indicates few if any hot code patterns where this trade off; would make sense. ###### Tempting optimizations that break the security model. Several optimizations were considered which didn't pan out due to failure to; uphold the security model. One in particular is worth discussing as many others; will reduce to it. We wondered whether only the *first* load in a basic block could be checked. If; the check works as intended, it forms an invalid pointer that doesn't even; virtual-address translate in the hardware. It should fault very early on in its; processing. Maybe that would stop things in time for the misspeculated path to; fail to leak any secrets. This doesn't end up working because the processor is; fundamentally out-of-order, even in its speculative doma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:36512,latency,latency,36512,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['latency'],['latency']
Performance," provides overloaded builtins giving direct access to the three key ARM; instructions for implementing atomic operations. .. code-block:: c. T __builtin_arm_ldrex(const volatile T *addr);; T __builtin_arm_ldaex(const volatile T *addr);; int __builtin_arm_strex(T val, volatile T *addr);; int __builtin_arm_stlex(T val, volatile T *addr);; void __builtin_arm_clrex(void);. The types ``T`` currently supported are:. * Integer types with width at most 64 bits (or 128 bits on AArch64).; * Floating-point types; * Pointer types. Note that the compiler does not guarantee it will not insert stores which clear; the exclusive monitor in between an ``ldrex`` type operation and its paired; ``strex``. In practice this is only usually a risk when the extra store is on; the same cache line as the variable being modified and Clang will only insert; stack stores on its own, so it is best not to use these operations on variables; with automatic storage duration. Also, loads and stores may be implicit in code written between the ``ldrex`` and; ``strex``. Clang will not necessarily mitigate the effects of these either, so; care should be exercised. For these reasons the higher level atomic primitives should be preferred where; possible. Non-temporal load/store builtins; --------------------------------. Clang provides overloaded builtins allowing generation of non-temporal memory; accesses. .. code-block:: c. T __builtin_nontemporal_load(T *addr);; void __builtin_nontemporal_store(T value, T *addr);. The types ``T`` currently supported are:. * Integer types.; * Floating-point types.; * Vector types. Note that the compiler does not guarantee that non-temporal loads or stores; will be used. C++ Coroutines support builtins; --------------------------------. .. warning::; This is a work in progress. Compatibility across Clang/LLVM releases is not; guaranteed. Clang provides experimental builtins to support C++ Coroutines as defined by; https://wg21.link/P0057. The following four are intended to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:144459,load,loads,144459,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['load'],['loads']
Performance," put a bunch of stuff into; a container that automatically eliminates duplicates. Some set-like; containers support efficient iteration through the elements in sorted order.; Set-like containers are more expensive than sequential containers. * a :ref:`sequential <ds_sequential>` container provides the most efficient way; to add elements and keeps track of the order they are added to the collection.; They permit duplicates and support efficient iteration, but do not support; efficient look-up based on a key. * a :ref:`string <ds_string>` container is a specialized sequential container or; reference structure that is used for character or byte arrays. * a :ref:`bit <ds_bit>` container provides an efficient way to store and; perform set operations on sets of numeric id's, while automatically; eliminating duplicates. Bit containers require a maximum of 1 bit for each; identifier you want to store. Once the proper category of container is determined, you can fine tune the; memory use, constant factors, and cache behaviors of access by intelligently; picking a member of the category. Note that constant factors and cache behavior; can be a big deal. If you have a vector that usually only contains a few; elements (but could contain many), for example, it's much better to use; :ref:`SmallVector <dss_smallvector>` than :ref:`vector <dss_vector>`. Doing so; avoids (relatively) expensive malloc/free calls, which dwarf the cost of adding; the elements to the container. .. _ds_sequential:. Sequential Containers (std::vector, std::list, etc); ---------------------------------------------------. There are a variety of sequential containers available for you, based on your; needs. Pick the first in this section that will do what you want. .. _dss_arrayref:. llvm/ADT/ArrayRef.h; ^^^^^^^^^^^^^^^^^^^. The ``llvm::ArrayRef`` class is the preferred class to use in an interface that; accepts a sequential list of elements in memory and just reads from them. By; taking an ``ArrayRef``, the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:56891,tune,tune,56891,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,2,"['cache', 'tune']","['cache', 'tune']"
Performance," r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. W",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19410,load,loading,19410,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['load'],['loading']
Performance," r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is translated; into a number of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:7217,load,load,7217,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['load'],['load']
Performance," ratio comparing to ZLIB); - If two or more files have an identical streamer info record, this is only treated once therewith avoiding to take the global lock.; - Allow writing temporary objects (with same address) in the same TBuffer(s). A new flag to TBuffer*::WriteObject allows to skip the mechanism that prevent the 2nd streaming of an object. This allows the (re)use of temporary objects to store different data in the same buffer.; - Reuse branch proxies internally used by TTreeReader{Value,Array} therewith increasing performance when having multiple readers pointing to the same branch.; - Implement reading of objects data from JSON; - Provide TBufferJSON::ToJSON() and TBufferJSON::FromJSON() methods; - Provide TBufferXML::ToXML() and TBufferXML::FromXML() methods; - Converts NaN and Infinity values into null in JSON, there are no other direct equivalent. ## TTree Libraries; - Enable the TTreeCache by default of `TTree::Draw`, `TTreeReader` and `RDataFrame`; - Significant enhancement in the `TTreeCache` filling algorithm to increase robustness in case of oddly clustered `TTree` and under provisioned cache size. See the [merge request](https://github.com/root-project/root/pull/1960) for more details.; - Proxies are now properly re-used when multiple TTreeReader{Value,Array}s are associated to a single branch. Deserialisation is therefore performed once. This is an advantage for complex TDataFrame graphs.; - Add TBranch::BackFill to allow the addition of new branches to an existing tree and keep the new basket clustered in the same way as the rest of the TTree. Use with the following pattern,; make sure to to call BackFill for the same entry for all the branches consecutively:; ```; for(auto e = 0; e < tree->GetEntries(); ++e) { // loop over entries.; for(auto branch : branchCollection) {; ... Make change to the data associated with the branch ...; branch->BackFill();; }; }; ```; Since we loop over all the branches for each new entry all the baskets for a cluster ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:4700,cache,cache,4700,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,1,['cache'],['cache']
Performance," reasonably possible, easing; migration from GCC to Clang. In most cases, code ""just works"".; Clang also provides an alternative driver, :ref:`clang-cl`, that is designed; to be compatible with the Visual C++ compiler, cl.exe. In addition to language specific features, Clang has a variety of; features that depend on what CPU architecture or operating system is; being compiled for. Please see the :ref:`Target-Specific Features and; Limitations <target_features>` section for more details. The rest of the introduction introduces some basic :ref:`compiler; terminology <terminology>` that is used throughout this manual and; contains a basic :ref:`introduction to using Clang <basicusage>` as a; command line compiler. .. _terminology:. Terminology; -----------. Front end, parser, backend, preprocessor, undefined behavior,; diagnostic, optimizer. .. _basicusage:. Basic Usage; -----------. Intro to how to use a C compiler for newbies. compile + link compile then link debug info enabling optimizations; picking a language to use, defaults to C17 by default. Autosenses based; on extension. using a makefile. Command Line Options; ====================. This section is generally an index into other sections. It does not go; into depth on the ones that are covered by other sections. However, the; first part introduces the language selection and other high level; options like :option:`-c`, :option:`-g`, etc. Options to Control Error and Warning Messages; ---------------------------------------------. .. option:: -Werror. Turn warnings into errors. .. This is in plain monospaced font because it generates the same label as; .. -Werror, and Sphinx complains. ``-Werror=foo``. Turn warning ""foo"" into an error. .. option:: -Wno-error=foo. Turn warning ""foo"" into a warning even if :option:`-Werror` is specified. .. option:: -Wfoo. Enable warning ""foo"".; See the :doc:`diagnostics reference <DiagnosticsReference>` for a complete; list of the warning flags that can be specified in this way. ..",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:3323,optimiz,optimizations,3323,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance," recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3681,optimiz,optimized,3681,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,1,['optimiz'],['optimized']
Performance," reference in the object graph. **Ignoring** them would badly violate user expectations about their code.; While it *would* make it easier to develop code simultaneously for ARC and; non-ARC, there is very little reason to do so except for certain library; developers. ARC and non-ARC translation units share an execution model and; can seamlessly interoperate. Within a translation unit, a developer who; faithfully maintains their code in non-ARC mode is suffering all the; restrictions of ARC for zero benefit, while a developer who isn't testing the; non-ARC mode is likely to be unpleasantly surprised if they try to go back to; it. **Banning** them has the disadvantage of making it very awkward to migrate; existing code to ARC. The best answer to that, given a number of other; changes and restrictions in ARC, is to provide a specialized tool to assist; users in that migration. Implementing these methods was banned because they are too integral to the; semantics of ARC; many tricks which worked tolerably under manual reference; counting will misbehave if ARC performs an ephemeral extra retain or two. If; absolutely required, it is still possible to implement them in non-ARC code,; for example in a category; the implementations must obey the :ref:`semantics; <arc.objects.retains>` laid out elsewhere in this document. .. _arc.misc.special_methods.dealloc:. ``dealloc``; ^^^^^^^^^^^. A program is ill-formed if it contains a message send or ``@selector``; expression for the selector ``dealloc``. .. admonition:: Rationale. There are no legitimate reasons to call ``dealloc`` directly. A class may provide a method definition for an instance method named; ``dealloc``. This method will be called after the final ``release`` of the; object but before it is deallocated or any of its instance variables are; destroyed. The superclass's implementation of ``dealloc`` will be called; automatically when the method returns. .. admonition:: Rationale. Even though ARC destroys instance varia",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:87051,perform,performs,87051,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performs']
Performance," regions are invisible since the current track has not yet reached; its mother. This is not the case when going the other way since the; track has first to exit the extruding node before checking the mother.; In other words, an extrusion behavior is dependent on the track; parameters, which is a highly undesirable effect. *B)* We will call ""overlaps"" only the regions in space contained by; more than one node inside the same container. The owner of such regions; cannot be determined based on hierarchical considerations; therefore; they will be considered as belonging to the node from which the current; track is coming from. When coming from their container, the ownership is totally; unpredictable. Again, the ownership of overlapping regions highly; depends on the current track parameters. We must say that even the overlaps of type *A)* and *B)* are allowed in case; the corresponding nodes are created using; TGeoVolume::AddNodeOverlap() method. Navigation is performed in such; cases by giving priority to the non-overlapping nodes. The modeller has; to perform an additional search through the overlapping candidates.; These are detected automatically during the geometry closing procedure; in order to optimize the algorithm, but we will stress that extensive; usage of this feature leads to a drastic deterioration of performance.; In the following we will focus on the non-declared overlaps of type *A)*; and *B)* since this is the main source of errors during tracking. These; are generally non-intended overlaps due to coding mistakes or bad; geometry design. The checking package is loaded together with the; painter classes and contains an automated overlap checker. \image html geometry008.png ""Overlap checking"". This can be activated both at volume level (checking for illegal; overlaps only one level inside a given volume) and from the geometry; manager level (checking full geometry):. ~~~{.cpp}; myVolume->CheckOverlaps(precision, option);; gGeoManager->CheckOverlaps(preci",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:92128,perform,performed,92128,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance," relevant new methods are:. TEntryList::Scan(const char *fn); Shows the root common paths for the files of the TEntryLists in 'fn'; TEntryList::Relocate(const char *fn, const char *newroot,; const char *oldroot = 0, const char *enlnm = 0); Relocates all paths starting with 'oldroot' to 'newroot' for the; entry-list 'enlnm' in file 'fn'. Remove 'protocol+server' from file tagging and matching, i.e. use; only filepath+anchor; in this way a list is valid even after re-staging; of the dataset files, which typically changes the end-point data servers.; Entry-lists created with the full path should still be matched correctly. Miscellaneous. Repaired the behavior of TTreeCache when the TTree has a dramatic dynamic range with a lots of very small entriesat the beginning and very large entries at the end, the size in bytes of the cluster for the later entries will be very large (because of the cluster size in entries is large!). TTreeCache::FillBuffer was always attempting to load complete clusters not matter the; size (even with the size was larger than 2GB!). This patch resolves the issue by limiting the amount of memory used to:. The requested size if more than one cluster fits in the cache.; Twice the requested size if at least one basket per branch fits in the cache.; Four time the requested size in the case where the cache can not even hold one basket per branch. The filling will restart at the next cluster boundary in the case a) and will; restart at the maximum of entry number read in the cache in the case b) and c).; Baskets that are below this boundary and did not fit in the cache will be read; individually.; Repaired the basket flushing frequency when the TTree has already more than one cluster size.; Repaired binning of string histogram generated by TTree::Draw.; Many bug fixes and fix for issues discovery by Coverity, see change log for more details.; In TTree::MakeProxy add proper support for top level stl collection of objects and for stl collection of objects ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html:3039,load,load,3039,tree/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html,1,['load'],['load']
Performance," repaint on the attached **`TPad`** object - hence you should attach you; master geometry object to the pad (via `TObject::Draw()`), and perform; the publishing to the viewer in response to **`TObject::Paint()`**. #### Physical IDs. TVirtualViewer3D provides for two methods of object addition:. ``` {.cpp}; virtual Int_t AddObject(const TBuffer3D &buffer,; Bool_t * addChildren = 0); virtual Int_t AddObject(UInt_t physicalID,; const TBuffer3D & buffer,; Bool_t *addChildren = 0); ```. If you use the first (simple) case a viewer using logical/physical pairs; will generate sequential IDs for each physical object internally. Scene; rebuilds will require destruction and recreation of all physical; objects. For the second you can specify an identifier from the client; side, which must be unique and stable - i.e. the IDs of a published; object is consistent, regardless of changes in termination of contained; child geometry branches. In this case the viewer can safely cache the; physical objects across scene rebuilds, discarding those no longer of; interest. #### Child Objects. In many geometries there is a rigid containment hierarchy, and so if the; viewer is not interested in a certain object due to limits/size then it; will also not be interest in any of the contained branch of siblings.; Both `TBuffer3D::AddObject()` methods have an `addChildren` return; parameter. The viewer will complete this (if passed) indicating if; children of the object just sent are worth sending. #### Recycling TBuffer3D. Once add `TBuffer3D::AddObject()` has been called, the contents are; copied to the viewer's internal data structures. You are free to destroy; this **`TBuffer3D`**, or recycle it for the next object if suitable. #### Examples. For an example of a simple geometry, working in master reference frame; examine the code under `$ROOTSYS/g3d`. For a more complex example, which; works in both master and local frames, and uses logical`/`physical; division of shape geometry and placement, e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:138295,cache,cache,138295,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['cache'],['cache']
Performance," resource pressure. Instruction Flow; ^^^^^^^^^^^^^^^^; This section describes the instruction flow through the default pipeline of; :program:`llvm-mca`, as well as the functional units involved in the process. The default pipeline implements the following sequence of stages used to; process instructions. * Dispatch (Instruction is dispatched to the schedulers).; * Issue (Instruction is issued to the processor pipelines).; * Write Back (Instruction is executed, and results are written back).; * Retire (Instruction is retired; writes are architecturally committed). The in-order pipeline implements the following sequence of stages:; * InOrderIssue (Instruction is issued to the processor pipelines).; * Retire (Instruction is retired; writes are architecturally committed). :program:`llvm-mca` assumes that instructions have all been decoded and placed; into a queue before the simulation start. Therefore, the instruction fetch and; decode stages are not modeled. Performance bottlenecks in the frontend are not; diagnosed. Also, :program:`llvm-mca` does not model branch prediction. Instruction Dispatch; """"""""""""""""""""""""""""""""""""""""; During the dispatch stage, instructions are picked in program order from a; queue of already decoded instructions, and dispatched in groups to the; simulated hardware schedulers. The size of a dispatch group depends on the availability of the simulated; hardware resources. The processor dispatch width defaults to the value; of the ``IssueWidth`` in LLVM's scheduling model. An instruction can be dispatched if:. * The size of the dispatch group is smaller than processor's dispatch width.; * There are enough entries in the reorder buffer.; * There are enough physical registers to do register renaming.; * The schedulers are not full. Scheduling models can optionally specify which register files are available on; the processor. :program:`llvm-mca` uses that information to initialize register; file descriptors. Users can limit the number of physical registers",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:34769,bottleneck,bottlenecks,34769,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['bottleneck'],['bottlenecks']
Performance," result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the same as any other symbol defined within the; execution session (i.e. they are findable via `ExecutionSession::lookup`, and; so visible to the JIT linker during linking). One way to reflect external symbols is to add them manually using the; absoluteSymbols function:. .. code-block:: c++. const DataLayout &DL = g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:31178,load,loading,31178,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['load'],['loading']
Performance," results than a; sampling profiler. It also provides reproducible results, at least to the; extent that the code behaves consistently across runs. Clang supports two types of instrumentation: frontend-based and IR-based.; Frontend-based instrumentation can be enabled with the option ``-fprofile-instr-generate``,; and IR-based instrumentation can be enabled with the option ``-fprofile-generate``.; For best performance with PGO, IR-based instrumentation should be used. It has; the benefits of lower instrumentation overhead, smaller raw profile size, and; better runtime performance. Frontend-based instrumentation, on the other hand,; has better source correlation, so it should be used with source line-based; coverage testing. The flag ``-fcs-profile-generate`` also instruments programs using the same; instrumentation method as ``-fprofile-generate``. However, it performs a; post-inline late instrumentation and can produce context-sensitive profiles. Here are the steps for using profile guided optimization with; instrumentation:. 1. Build an instrumented version of the code by compiling and linking with the; ``-fprofile-generate`` or ``-fprofile-instr-generate`` option. .. code-block:: console. $ clang++ -O2 -fprofile-instr-generate code.cc -o code. 2. Run the instrumented executable with inputs that reflect the typical usage.; By default, the profile data will be written to a ``default.profraw`` file; in the current directory. You can override that default by using option; ``-fprofile-instr-generate=`` or by setting the ``LLVM_PROFILE_FILE``; environment variable to specify an alternate file. If non-default file name; is specified by both the environment variable and the command line option,; the environment variable takes precedence. The file name pattern specified; can include different modifiers: ``%p``, ``%h``, ``%m``, ``%t``, and ``%c``. Any instance of ``%p`` in that file name will be replaced by the process; ID, so that you can easily distinguish the profile outp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:103369,optimiz,optimization,103369,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance," ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21985,optimiz,optimized,21985,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimized']
Performance," return type. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.inttoptr``' intrinsic takes a value to cast as its first operand; , which must be a vector of :ref:`integer <t_integer>` type, and a type to cast; it to return type, which must be a vector of pointers type.; The second operand is the vector mask. The return type, the value to cast, and; the vector mask have the same number of elements.; The third operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.inttoptr``' intrinsic converts ``value`` to return type by; applying either a zero extension or a truncation depending on the size of the; integer ``value``. If ``value`` is larger than the size of a pointer, then a; truncation is done. If ``value`` is smaller than the size of a pointer, then a; zero extension is done. If they are the same size, nothing is done (*no-op cast*).; The conversion is performed on lane positions below the explicit vector length; and where the vector mask is true. Masked-off lanes are ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x ptr> @llvm.vp.inttoptr.v4p0i32.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = inttoptr <4 x i32> %a to <4 x ptr>; %also.r = select <4 x i1> %mask, <4 x ptr> %t, <4 x ptr> poison. .. _int_vp_fcmp:. '``llvm.vp.fcmp.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i1> @llvm.vp.fcmp.v16f32(<16 x float> <left_op>, <16 x float> <right_op>, metadata <condition code>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i1> @llvm.vp.fcmp.nxv4f32(<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, metadata <condition code>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i1> @llvm.vp.fcmp.v256f64(<256 x double> <left_op>, <256 x double> <right_op>, metadata <conditi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:816400,perform,performed,816400,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance," return values.; This attribute applies only to the particular copy of the pointer passed in; this argument. A caller could pass two copies of the same pointer with one; being annotated nocapture and the other not, and the callee could validly; capture through the non annotated parameter. .. code-block:: llvm. define void @f(ptr nocapture %a, ptr %b) {; ; (capture %b); }. call void @f(ptr @glb, ptr @glb) ; well-defined. ``nofree``; This indicates that callee does not free the pointer argument. This is not; a valid attribute for return values. .. _nest:. ``nest``; This indicates that the pointer parameter can be excised using the; :ref:`trampoline intrinsics <int_trampoline>`. This is not a valid; attribute for return values and can only be applied to one parameter. ``returned``; This indicates that the function always returns the argument as its return; value. This is a hint to the optimizer and code generator used when; generating the caller, allowing value propagation, tail call optimization,; and omission of register saves and restores in some cases; it is not; checked or enforced when generating the callee. The parameter and the; function return type must be valid operands for the; :ref:`bitcast instruction <i_bitcast>`. This is not a valid attribute for; return values and can only be applied to one parameter. ``nonnull``; This indicates that the parameter or return pointer is not null. This; attribute may only be applied to pointer typed parameters. This is not; checked or enforced by LLVM; if the parameter or return pointer is null,; :ref:`poison value <poisonvalues>` is returned or passed instead.; The ``nonnull`` attribute should be combined with the ``noundef`` attribute; to ensure a pointer is not null or otherwise the behavior is undefined. ``dereferenceable(<n>)``; This indicates that the parameter or return pointer is dereferenceable. This; attribute may only be applied to pointer typed parameters. A pointer that; is dereferenceable can be loaded from s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:60216,optimiz,optimizer,60216,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizer']"
Performance," root session (like,; for instance, the axis titles).; When a THStack is drawn with the option ""pads"", the number of lines is; now optimized to make sure there is no empty line. . TUnfold. Introduces this new class for solving inverse problems:. data histograms with Gaussian errors are decomposed into; several template distributions (""generator level"" bins). The result are new normalisation constants for the template; distributions (the unfolded ""generator level"" distribution). The solution can be tuned by properly adjusting the; regularisation parameter tau. A standard method, the L-curve scan is; implemented to help finding a good choice of this parameter. Two example tutorials are included to show the usage of this class: tutorials/math/testUnfiold1.C and tutorials/math/testUnfiold2.C. FitPanel; Add a new revised version of the Fit Panel with the following functionality:. Add support now for fitting, in addition to the TH1 and TGraph; also for TH2, TH3, TMultiGraph and TGraph2D and TTree (with un-binned; fits); Add possibility to select the data object directly from the Fit; panel. The Fit Panel can also be open directly from the TCanvas menu; (under Tools); Improve the function selection by having the possibility to; support user defined function, predefined functions and functions; used before for fitting. ; Allow the opening of the parameter dialog in case of linear; fitter. This is needed for example for fixing some of the; parameters; Improve minimization panel by adding some extra methods, like; combined for a combined migrad-simplex minimization (option; ""MINIMIZE"" in Minuit).; Improve the slider by adding a numeric entry. ; Add the Advanced Graphics dialog, that allows the user to perform; Contour and Scan operation on the last fit.; Fix various bugs in setting the fit model function and in; setting the parameters (values, limits, etc..). Here is how the fit panel is now:; ; These are the currently support methods for the new Advance Graphics dialog:;  ; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v522/index.html:8890,perform,perform,8890,hist/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v522/index.html,1,['perform'],['perform']
Performance," rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; prece",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:330409,load,load,330409,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," rules; used by the ``DW_OP_call*`` operations. .. note::. Delete the description of how the ``DW_OP_call*`` operations evaluate a; ``DW_AT_location`` attribute as that is now described in the operations. .. note::. See the discussion about the ``DW_AT_location`` attribute in the; ``DW_OP_call*`` operation. Having each attribute only have a single; purpose and single execution semantics seems desirable. It makes it easier; for the consumer that no longer have to track the context. It makes it; easier for the producer as it can rely on a single semantics for each; attribute. For that reason, limiting the ``DW_AT_location`` attribute to only; supporting evaluating the location description of an object, and using a; different attribute and encoding class for the evaluation of DWARF; expression *procedures* on the same operation expression stack seems; desirable. 2. ``DW_AT_const_value``. .. note::. Could deprecate using the ``DW_AT_const_value`` attribute for; ``DW_TAG_variable`` or ``DW_TAG_formal_parameter`` debugger information; entries that have been optimized to a constant. Instead,; ``DW_AT_location`` could be used with a DWARF expression that produces an; implicit location description now that any location description can be; used within a DWARF expression. This allows the ``DW_OP_call*`` operations; to be used to push the location description of any variable regardless of; how it is optimized. 3. ``DW_AT_LLVM_memory_space``. A ``DW_AT_memory_space`` attribute with a constant value representing a source; language specific DWARF memory space (see 2.14 ""Memory Spaces""). If omitted,; defaults to ``DW_MSPACE_none``. A.4.2 Common Block Entries; ~~~~~~~~~~~~~~~~~~~~~~~~~~. A common block entry also has a ``DW_AT_location`` attribute whose value is a; DWARF expression E that describes the location of the common block at run-time.; The result of the attribute is obtained by evaluating E with a context that has; a result kind of a location description, an unspecified obj",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:174464,optimiz,optimized,174464,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimized']
Performance," run against each modules after the build finished.; Use `--intercept-first` and `--override-compiler` flags together to get; this model. The 1. and 3. are using compiler wrappers, which works only if the build; process respects the `CC` and `CXX` environment variables. (Some build; process can override these variable as command line parameter only. This case; you need to pass the compiler wrappers manually. eg.: `intercept-build; --override-compiler make CC=intercept-cc CXX=intercept-c++ all` where the; original build command would have been `make all` only.). The 1. runs the analyzer right after the real compilation. So, if the build; process removes removes intermediate modules (generated sources) the analyzer; output still kept. The 2. and 3. generate the compilation database first, and filters out those; modules which are not exists. So, it's suitable for incremental analysis during; the development. The 2. mode is available only on FreeBSD and Linux. Where library preload; is available from the dynamic loader. Not supported on OS X (unless System; Integrity Protection feature is turned off). `intercept-build` command uses only the 2. and 3. mode to generate the; compilation database. `analyze-build` does only run the analyzer against the; captured compiler calls. Known problems; --------------. Because it uses `LD_PRELOAD` or `DYLD_INSERT_LIBRARIES` environment variables,; it does not append to it, but overrides it. So builds which are using these; variables might not work. (I don't know any build tool which does that, but; please let me know if you do.). Problem reports; ---------------. If you find a bug in this documentation or elsewhere in the program or would; like to propose an improvement, please use the project's [issue tracker][3].; Please describing the bug and where you found it. If you have a suggestion; how to fix it, include that as well. Patches are also welcome. License; -------. The project is licensed under Apache-2.0 with LLVM exceptions.; Se",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/README.md:3786,load,loader,3786,interpreter/llvm-project/clang/tools/scan-build-py/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/README.md,1,['load'],['loader']
Performance," run; optimizations or code generator components against every program in the; tree, collecting statistics or running custom checks for correctness. At; base, this is how the nightly tester works, it's just one example of a; general framework. Lets say that you have an LLVM optimization pass, and you want to see; how many times it triggers. First thing you should do is add an LLVM; `statistic <ProgrammersManual.html#Statistic>`_ to your pass, which will; tally counts of things you care about. Following this, you can set up a test and a report that collects these; and formats them for easy viewing. This consists of two files, a; ""``test-suite/TEST.XXX.Makefile``"" fragment (where XXX is the name of; your test) and a ""``test-suite/TEST.XXX.report``"" file that indicates; how to format the output into a table. There are many example reports of; various levels of sophistication included with the test suite, and the; framework is very general. If you are interested in testing an optimization pass, check out the; ""libcalls"" test as an example. It can be run like this:. .. code-block:: bash. % cd llvm/projects/test-suite/MultiSource/Benchmarks # or some other level; % make TEST=libcalls report. This will do a bunch of stuff, then eventually print a table like this:. ::. Name | total | #exit |; ...; FreeBench/analyzer/analyzer | 51 | 6 |; FreeBench/fourinarow/fourinarow | 1 | 1 |; FreeBench/neural/neural | 19 | 9 |; FreeBench/pifft/pifft | 5 | 3 |; MallocBench/cfrac/cfrac | 1 | * |; MallocBench/espresso/espresso | 52 | 12 |; MallocBench/gs/gs | 4 | * |; Prolangs-C/TimberWolfMC/timberwolfmc | 302 | * |; Prolangs-C/agrep/agrep | 33 | 12 |; Prolangs-C/allroots/allroots | * | * |; Prolangs-C/assembler/assembler | 47 | * |; Prolangs-C/bison/mybison | 74 | * |; ... This basically is grepping the -stats output and displaying it in a; table. You can also use the ""TEST=libcalls report.html"" target to get; the table in HTML form, similarly for report.csv and report.tex. The source for t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst:6154,optimiz,optimization,6154,interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst,1,['optimiz'],['optimization']
Performance," rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1251,optimiz,optimizations,1251,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,1,['optimiz'],['optimizations']
Performance," running other heavy applications - the number of your own threads; running at any time may be lower than the limit due to demand on the CPU.; - The 'Real Time' is similar to 'CPU Time / number of threads' AND 'Compressed Throughput' is lower than expected; for your storage medium: this would imply that your CPU threads aren't decompressing data as fast as your storage; medium can provide it, and so decompression is the bottleneck.; The best way to decrease your runtime would be to utilise a system with a faster CPU, or make use; use of more threads when running, or use a compression algorithm with a higher decompression rate such as LZ4,; possibly at the cost of some extra file size. ### A note on caching. If your data is stored on a local disk, the system may cache some/all of the file in memory after it is; first read. If this is realistic of how your analysis will run - then there is no concern. However, if; you expect to only read files once in a while - and as such the files are unlikely to be in the cache -; consider clearing the cache before running rootreadspeed.; On Linux this can be done by running 'echo 3 > /proc/sys/vm/drop_caches' as a superuser,; or a specific file can be dropped from the cache with; `dd of=<FILENAME> oflag=nocache conv=notrunc,fdatasync count=0 > /dev/null 2>&1`. ### Known overhead of TTreeReader, RDataFrame. `rootreadspeed` is designed to read all data present in the specified branches, trees and files at the highest; possible speed. When the application bottleneck is not in the computations performed by analysis logic,; higher-level interfaces built on top of TTree such as TTreeReader and RDataFrame are known to add a significant; runtime overhead with respect to the runtimes reported by `rootreadspeed` (up to a factor 2). In realistic analysis; applications it has been observed that a large part of that overhead is compensated by the ability of TTreeReader and; RDataFrame to read branch values selectively, based on event cuts, and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md:3720,cache,cache,3720,tree/readspeed/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md,2,['cache'],['cache']
Performance," safe. The next stage is to check if computation of the distance to a give; physical object specified by a path was required. If this is the case,; the modeller changes the state to point to the required object, converts; the current point and direction coordinates to the local frame of this; object and computes the distance to its shape. The node returned is the; one pointed by the input path in case the shape is crossed; otherwise; the returned value is NULL. In case the distance to next crossed; boundary is required, the current point has to be physically INSIDE the; shape pointed by the current volume. This is only insured in case a call; to `TGeoManager::FindNode()` was performed for the current point.; Therefore, the first step is to convert the global current point and; direction in the local reference frame of the current volume and to; compute the distance to exit its shape from inside. The returned value; is again compared to the maximum allowed step (the proposed one) and in; case the distance is safe no other action is performed and the proposed; step is approved. In case the boundary is closer, the computed distance; is taken as maximum allowed step. For optimization purposed, for; particles starting very close to the current volume boundary (less than; 0.01 microns) and exiting the algorithm stops here. After computing the distance to exit the current node, the distance to; the daughter of the current volume which is crossed next is computed by; **`TGeoManager`**`::FindNextDaughterBoundary().` This computes the; distance to all daughter candidates that can be possibly crossed by; using volume voxelization. The algorithm is efficient in average only in; case the number of daughters is greater than 4. For fewer nodes, a; simple loop is performed and the minimum distance (from a point outside; each shape) is taken and compared to the maximum allowed step. The step; value is again updated if `step<stepmax` . A special case is when the current node is decla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:162801,perform,performed,162801,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance," scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12922,optimiz,optimizations,12922,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['optimiz'],['optimizations']
Performance," scoping rules; * Same enum in transparent scope refers to same type; * More detailed enum ``repr()`` printing, where possible; * Fix for (extern) explicit template instantiations in namespaces; * Throw objects from an std::tuple a life line; * Global pythonizors now always run on all classes; * Simplified iteraton over STL-like containers defining ``begin()``/``end()``. 2020-09-08: 1.8.2; -----------------. * Add ``cppyy.set_debug()`` to enable debug output for fixing template errors; * Cover more partial template instantiation use cases; * Force template instantiation if necessary for type deduction (i.e. ``auto``). 2020-09-01: 1.8.1; -----------------. * Setup build dependencies with pyproject.toml; * Simplified flow of pointer types for callbacks and cross-derivation; * Pointer-comparing objects performs auto-cast as needed; * Add main dimension for ptr-ptr to builtin returns; * Transparant handling of ptr-ptr to instance returns; * Stricter handling of bool type in overload with int types; * Fix uint64_t template instantiation regression; * Do not filter out enum data for ``__dir__``; * Fix lookup of interpreter-only explicit instantiations; * Fix inconsistent naming of std types with char_traits; * Further hiding of upstream code/dependencies; * Extended documentation. 2020-07-12: 1.8.0; -----------------. * Support mixing of Python and C++ types in global operators; * Capture Cling error messages from cppdef and include in the Python exception; * Add a cppexec method to evalutate statements in Cling's global scope; * Support initialization of ``std::array<>`` from sequences; * Support C++17 style initialization of common STL containers; * Allow base classes with no virtual destructor (with warning); * Support const by-value returns in Python-side method overrides; * Support for cross-language multiple inheritance of C++ bases; * Allow for pass-by-value of ``std::unique_ptr`` through move; * Reduced dependencies on upstream code; * Put remaining upstream code i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst:11280,perform,performs,11280,bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,1,['perform'],['performs']
Performance," second argument ``%Ptr`` is a; pointer to the vector type of ``%In``, and is the start address of the matrix; in memory. The third argument ``%Stride`` is a positive, constant integer with; ``%Stride >= <Rows>``. ``%Stride`` is used to compute the column memory; addresses. I.e., for a column ``C``, its start memory addresses is calculated; with ``%Ptr + C * %Stride``. The fourth argument ``<IsVolatile>`` is a boolean; value. The arguments ``<Rows>`` and ``<Cols>`` correspond to the number of rows; and columns, respectively, and must be positive, constant integers. The :ref:`align <attr_align>` parameter attribute can be provided; for the ``%Ptr`` arguments. Half Precision Floating-Point Intrinsics; ----------------------------------------. For most target platforms, half precision floating-point is a; storage-only format. This means that it is a dense encoding (in memory); but does not support computation in the format. This means that code must first load the half-precision floating-point; value as an i16, then convert it to float with; :ref:`llvm.convert.from.fp16 <int_convert_from_fp16>`. Computation can; then be performed on the float value (including extending to double; etc). To store the value back to memory, it is first converted to float; if needed, then converted to i16 with; :ref:`llvm.convert.to.fp16 <int_convert_to_fp16>`, then storing as an; i16 value. .. _int_convert_to_fp16:. '``llvm.convert.to.fp16``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i16 @llvm.convert.to.fp16.f32(float %a); declare i16 @llvm.convert.to.fp16.f64(double %a). Overview:; """""""""""""""""". The '``llvm.convert.to.fp16``' intrinsic function performs a conversion from a; conventional floating-point type to half precision floating-point format. Arguments:; """""""""""""""""""". The intrinsic function contains single argument - the value to be; converted. Semantics:; """""""""""""""""""". The '``llvm.convert.to.fp16``' intrinsic function performs a conversion from a; conve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:681026,load,load,681026,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance," second list entry. The metadata identifying each scope is also itself a list containing two or; three entries. The first entry is the name of the scope. Note that if the name; is a string then it can be combined across functions and translation units. A; self-reference can be used to create globally unique scope names. A metadata; reference to the scope's domain is the second entry. A descriptive string may; optionally be provided as a third list entry. For example,. .. code-block:: llvm. ; Two scope domains:; !0 = !{!0}; !1 = !{!1}. ; Some scopes in these domains:; !2 = !{!2, !0}; !3 = !{!3, !0}; !4 = !{!4, !1}. ; Some scope lists:; !5 = !{!4} ; A list containing only scope !4; !6 = !{!4, !3, !2}; !7 = !{!3}. ; These two instructions don't alias:; %0 = load float, ptr %c, align 4, !alias.scope !5; store float %0, ptr %arrayidx.i, align 4, !noalias !5. ; These two instructions also don't alias (for domain !1, the set of scopes; ; in the !alias.scope equals that in the !noalias list):; %2 = load float, ptr %c, align 4, !alias.scope !5; store float %2, ptr %arrayidx.i2, align 4, !noalias !6. ; These two instructions may alias (for domain !0, the set of scopes in; ; the !noalias list is not a superset of, or equal to, the scopes in the; ; !alias.scope list):; %2 = load float, ptr %c, align 4, !alias.scope !6; store float %0, ptr %arrayidx.i, align 4, !noalias !7. '``fpmath``' Metadata; ^^^^^^^^^^^^^^^^^^^^^. ``fpmath`` metadata may be attached to any instruction of floating-point; type. It can be used to express the maximum acceptable error in the; result of that instruction, in ULPs, thus potentially allowing the; compiler to use a more efficient but less accurate method of computing; it. ULP is defined as follows:. If ``x`` is a real number that lies between two finite consecutive; floating-point numbers ``a`` and ``b``, without being equal to one; of them, then ``ulp(x) = |b - a|``, otherwise ``ulp(x)`` is the; distance between the two non-equal finite floating-poin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:283443,load,load,283443,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance," sections. Further, section clusters can also be specified using the ``list=<arg>``; option. For example, ``list=spec.txt`` where ``spec.txt`` contains:. ::. !foo; !!1 !!3 !!5; !!2 !!4 !!6. will create two unique sections for function ``foo`` with the first; containing the odd numbered basic blocks and the second containing the; even numbered basic blocks. Basic block sections allow the linker to reorder basic blocks and enables; link-time optimizations like whole program inter-procedural basic block; reordering. Profile Guided Optimization; ---------------------------. Profile information enables better optimization. For example, knowing that a; branch is taken very frequently helps the compiler make better decisions when; ordering basic blocks. Knowing that a function ``foo`` is called more; frequently than another function ``bar`` helps the inliner. Optimization; levels ``-O2`` and above are recommended for use of profile guided optimization. Clang supports profile guided optimization with two different kinds of; profiling. A sampling profiler can generate a profile with very low runtime; overhead, or you can build an instrumented version of the code that collects; more detailed profile information. Both kinds of profiles can provide execution; counts for instructions in the code and information on branches taken and; function invocation. Regardless of which kind of profiling you use, be careful to collect profiles; by running your code with inputs that are representative of the typical; behavior. Code that is not exercised in the profile will be optimized as if it; is unimportant, and the compiler may make poor optimization choices for code; that is disproportionately used while profiling. Differences Between Sampling and Instrumentation; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Although both techniques are used for similar purposes, there are important; differences between the two:. 1. Profile data generated with one cannot be used by the other, and the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:90029,optimiz,optimization,90029,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance," serial calculation of a `RooNLLVar`.; 2. `LikelihoodGradientJob` calculates the partial derivatives or the gradient in parallel on multiple CPUs/cores, based on `RooFit::MultiProcess`, which is a fork-based multi-processing task execution framework with dynamic load balancing. Other possible implementations could use the GPU or external tools like TensorFlow. The coupling of all these classes to `RooMinimizer` is made via the `MinuitFcnGrad` class, which owns the `Wrappers` that calculate the likelihood components. Note: a second `LikelihoodWrapper` class called `LikelihoodJob` is also available.; This class emulates the existing `NumCPU(>1)` functionality of the `RooAbsTestStatistic` tree, which is implemented based on `RooRealMPFE`.; This class is not yet thoroughly tested and should not be considered production ready. ### Usage example: `MultiProcess` enabled parallel gradient calculator. The main selling point of using `RooFit::TestStatistics` from a performance point of view is the implementation of the `RooFit::MultiProcess` based `LikelihoodGradientJob` calculator class.; To use it, one should create a `RooMinimizer` using the new constructor that takes a `RooAbsL`-based likelihood instead of a `RooAbsReal`. Taking any of the above created `likelihood` objects (as long as they are in a `std::shared_ptr`), we can create a `RooMinimizer` with parallel gradient calculation using:; ``` {.cpp}; std::shared_ptr<RooAbsL> likelihood = /* see examples above */;; RooMinimizer m(likelihood);; ```. By default, `RooFit::MultiProcess` spins up as many workers as there are cores in the system (as detected by `std::thread::hardware_concurrency()`).; To change the number of workers, call `RooFit::MultiProcess::Config::setDefaultNWorkers(desired_N_workers)` **before** creating the `RooMinimizer`. As noted above, offsetting is purely a function of the `RooMinimizer` when using `TestStatistics` classes.; Whereas with `fitTo` we can pass in a `RooFit::Offset(true)` optional `Roo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md:8125,perform,performance,8125,roofit/doc/developers/test_statistics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md,1,['perform'],['performance']
Performance," set of condition flags we need to capture; prior to entering each basic block and reuse a common `cmovCC` sequence for; those.; * We could further reuse suffixes when there are multiple `cmovCC`; instructions required to capture the set of flags. Currently this is; believed to not be worth the cost as paired flags are relatively rare and; suffixes of them are exceedingly rare.; * A common pattern in x86 is to have multiple conditional jump instructions; that use the same flags but handle different conditions. Naively, we could; consider each fallthrough between them an ""edge"" but this causes a much more; complex control flow graph. Instead, we accumulate the set of conditions; necessary for fallthrough and use a sequence of `cmovCC` instructions in a; single fallthrough edge to track it. Second, we trade register pressure for simpler `cmovCC` instructions by; allocating a register for the ""bad"" state. We could read that value from memory; as part of the conditional move instruction, however, this creates more; micro-ops and requires the load-store unit to be involved. Currently, we place; the value into a virtual register and allow the register allocator to decide; when the register pressure is sufficient to make it worth spilling to memory; and reloading. #### Hardening Loads. Once we have the predicate accumulated into a special value for correct vs.; misspeculated, we need to apply this to loads in a way that ensures they do not; leak secret data. There are two primary techniques for this: we can either; harden the loaded value to prevent observation, or we can harden the address; itself to prevent the load from occurring. These have significantly different; performance tradeoffs. ##### Hardening loaded values. The most appealing way to harden loads is to mask out all of the bits loaded.; The key requirement is that for each bit loaded, along the misspeculated path; that bit is always fixed at either 0 or 1 regardless of the value of the bit; loaded. The most ob",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:22050,load,load-store,22050,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load-store']
Performance," setting called `ROOT.ZipMode` is now unused and ignored.; Instead, use `Root.CompressionAlgorithm` which sets the compression algorithm according to the values of [ECompression](https://root.cern/doc/master/Compression_8h.html#a0a7df9754a3b7be2b437f357254a771c):. * 0: use the default value of `R__ZipMode` (currently selecting ZLIB); * 1: use ZLIB (the default until 6.12 and from 6.16); * 2: use LZMA; * 3: legacy, please don't use; * 4: LZ4. ### TRef. * Improve thread scalability of `TRef`. Creating and looking up a lot of `TRef` from the same `processID` now has practically perfect weak scaling. ### Parallelism; * Upgrade the built-in TBB version to 2019_U1. ### Type System; * Upgrade the `TClass::GetMissingDictionaries` method to support `std::unique_ptr`, `std::array` and `std::tuple` without getting trapped in the internal STL implementation details. ## I/O Libraries. * To allow for increase run-time performance and increase thread scalability the override ability of `TFile::GetStreamerInfoList` is replaced by an override of `TFile::GetStreamerInfoListImp` with updated return type and arguments. If a class override `TFile::GetStreamerInfoList` you will now see a compilation error like:. ```; /opt/build/root_builds/rootcling.cmake/include/TSQLFile.h:225:19: error: declaration of 'GetStreamerInfoList' overrides a 'final' function; virtual TList *GetStreamerInfoList();; ^; /opt/build/root_builds/rootcling.cmake/include/TFile.h:231:24: note: overridden virtual function is here; virtual TList *GetStreamerInfoList() final; // Note: to override behavior, please override GetStreamerInfoListImpl; ^; ```. Instead you need to override the protected method:. ```; InfoListRet GetStreamerInfoListImpl(bool lookupSICache);; ```. which can be implemented as. ```; InfoListRet DerivedClass::GetStreamerInfoListImpl(bool /*lookupSICache*/) {; ROOT::Internal::RConcurrentHashColl::HashValue hash;; TList *infolist = nullptr;; //; // Body of the former Derived::GetStreamerInfoList with ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md:3969,perform,performance,3969,README/ReleaseNotes/v616/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance," should be treated as a memory-barrier. This was; inaccurate in general and was changed so that now each instruction has an; IsAStoreBarrier and IsALoadBarrier flag. These flags are mca specific and; default to false for every instruction. If any instruction should have either of; these flags set, it should be done within the target's InstrPostProcess class.; For an example, look at the `X86InstrPostProcess::postProcessInstruction` method; within `llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp`. A load/store barrier consumes one entry of the load/store queue. A load/store; barrier enforces ordering of loads/stores. A younger load cannot pass a load; barrier. Also, a younger store cannot pass a store barrier. A younger load; has to wait for the memory/load barrier to execute. A load/store barrier is; ""executed"" when it becomes the oldest entry in the load/store queue(s). That; also means, by construction, all of the older loads/stores have been executed. In conclusion, the full set of load/store consistency rules are:. #. A store may not pass a previous store.; #. A store may not pass a previous load (regardless of ``-noalias``).; #. A store has to wait until an older store barrier is fully executed.; #. A load may pass a previous load.; #. A load may not pass a previous store unless ``-noalias`` is set.; #. A load has to wait until an older load barrier is fully executed. In-order Issue and Execute; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; In-order processors are modelled as a single ``InOrderIssueStage`` stage. It; bypasses Dispatch, Scheduler and Load/Store unit. Instructions are issued as; soon as their operand registers are available and resource requirements are; met. Multiple instructions can be issued in one cycle according to the value of; the ``IssueWidth`` parameter in LLVM's scheduling model. Once issued, an instruction is moved to ``IssuedInst`` set until it is ready to; retire. :program:`llvm-mca` ensures that writes are committed in-order. However,; an in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:42728,load,load,42728,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['load']
Performance," show that; the target can lower correctly with extensive LIT tests (IR to MIR, MIR to; ASM, etc).; 5. Some patches may be approved before others, but only after *all* patches are; approved that the whole set can be merged in one go. This is to guarantee; that all changes are good as a single block.; 6. After the initial merge, the target community can stop numbering patches and; start working asynchronously on the target to complete support. They should; still seek review from those who helped them in the initial phase, to make; sure the progress is still consistent.; 7. Once all official requirements have been fulfilled (as above), the code owner; should request the target to be enabled by default by sending another RFC to; the `LLVM Discourse forums`_. Adding an Established Project To the LLVM Monorepo; --------------------------------------------------. The `LLVM monorepo <https://github.com/llvm/llvm-project>`_ is the centerpoint; of development in the LLVM world, and has all of the primary LLVM components,; including the LLVM optimizer and code generators, Clang, LLDB, etc. `Monorepos; in general <https://en.wikipedia.org/wiki/Monorepo>`_ are great because they; allow atomic commits to the project, simplify CI, and make it easier for; subcommunities to collaborate. Like new targets, most projects already in the monorepo are considered to be in; the *core tier* of our :doc:`support policy<SupportPolicy>`. The burden to add; things to the LLVM monorepo needs to be very high - code that is added to this; repository is checked out by everyone in the community. As such, we hold; components to a high bar similar to ""official targets"", they:. * Must be generally aligned with the mission of the LLVM project to advance; compilers, languages, tools, runtimes, etc.; * Must conform to all of the policies laid out in this developer policy; document, including license, patent, coding standards, and code of conduct.; * Must have an active community that maintains the code, in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst:45175,optimiz,optimizer,45175,interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst,1,['optimiz'],['optimizer']
Performance," single leading ``/`` removed.; ======================= ==============. Other substitutions are provided that are variations on this base set and; further substitution patterns can be defined by each test module. See the; modules :ref:`local-configuration-files`. More detailed information on substitutions can be found in the; :doc:`../TestingGuide`. TEST RUN OUTPUT FORMAT; ~~~~~~~~~~~~~~~~~~~~~~. The :program:`lit` output for a test run conforms to the following schema, in; both short and verbose modes (although in short mode no PASS lines will be; shown). This schema has been chosen to be relatively easy to reliably parse by; a machine (for example in buildbot log scraping), and for other tools to; generate. Each test result is expected to appear on a line that matches:. .. code-block:: none. <result code>: <test name> (<progress info>). where ``<result-code>`` is a standard test result such as PASS, FAIL, XFAIL,; XPASS, UNRESOLVED, or UNSUPPORTED. The performance result codes of IMPROVED and; REGRESSED are also allowed. The ``<test name>`` field can consist of an arbitrary string containing no; newline. The ``<progress info>`` field can be used to report progress information such; as (1/300) or can be empty, but even when empty the parentheses are required. Each test result may include additional (multiline) log information in the; following format:. .. code-block:: none. <log delineator> TEST '(<test name>)' <trailing delineator>; ... log message ...; <log delineator>. where ``<test name>`` should be the name of a preceding reported test, ``<log; delineator>`` is a string of ""*"" characters *at least* four characters long; (the recommended length is 20), and ``<trailing delineator>`` is an arbitrary; (unparsed) string. The following is an example of a test run output which consists of four tests A,; B, C, and D, and a log message for the failing test C:. .. code-block:: none. PASS: A (1 of 4); PASS: B (2 of 4); FAIL: C (3 of 4); ******************** TEST 'C' FAILE",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst:22773,perform,performance,22773,interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,1,['perform'],['performance']
Performance," single-threaded mode, and; using the resultant compiler to build a copy of LLVM with multithreading; support. .. _shutdown:. Ending Execution with ``llvm_shutdown()``; -----------------------------------------. When you are done using the LLVM APIs, you should call ``llvm_shutdown()`` to; deallocate memory used for internal structures. .. _managedstatic:. Lazy Initialization with ``ManagedStatic``; ------------------------------------------. ``ManagedStatic`` is a utility class in LLVM used to implement static; initialization of static resources, such as the global type tables. In a; single-threaded environment, it implements a simple lazy initialization scheme.; When LLVM is compiled with support for multi-threading, however, it uses; double-checked locking to implement thread-safe lazy initialization. .. _llvmcontext:. Achieving Isolation with ``LLVMContext``; ----------------------------------------. ``LLVMContext`` is an opaque class in the LLVM API which clients can use to; operate multiple, isolated instances of LLVM concurrently within the same; address space. For instance, in a hypothetical compile-server, the compilation; of an individual translation unit is conceptually independent from all the; others, and it would be desirable to be able to compile incoming translation; units concurrently on independent server threads. Fortunately, ``LLVMContext``; exists to enable just this kind of scenario!. Conceptually, ``LLVMContext`` provides isolation. Every LLVM entity; (``Module``\ s, ``Value``\ s, ``Type``\ s, ``Constant``\ s, etc.) in LLVM's; in-memory IR belongs to an ``LLVMContext``. Entities in different contexts; *cannot* interact with each other: ``Module``\ s in different contexts cannot be; linked together, ``Function``\ s cannot be added to ``Module``\ s in different; contexts, etc. What this means is that is safe to compile on multiple; threads simultaneously, as long as no two threads operate on entities within the; same context. In practice, very fe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:122708,concurren,concurrently,122708,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['concurren'],['concurrently']
Performance," singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:221349,load,load,221349,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeMod",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:4582,optimiz,optimizations,4582,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimizations']
Performance," so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/ind",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1449,optimiz,optimizing,1449,interpreter/llvm-project/llvm/docs/Packaging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst,1,['optimiz'],['optimizing']
Performance," software), please do read the section below on; 'benchmark' selection. Please note that this script is only tested on a few Linux distros. Patches to; add support for other platforms, as always, are highly appreciated. :). This script also supports a ``--dry-run`` option, which causes it to print; important commands instead of running them. Selecting 'benchmarks'; ======================. PGO does best when the profiles gathered represent how the user plans to use the; compiler. Notably, highly accurate profiles of llc building x86_64 code aren't; incredibly helpful if you're going to be targeting ARM. By default, the script above does two things to get solid coverage. It:. - runs all of Clang and LLVM's lit tests, and; - uses the instrumented Clang to build Clang, LLVM, and all of the other; LLVM subprojects available to it. Together, these should give you:. - solid coverage of building C++,; - good coverage of building C,; - great coverage of running optimizations,; - great coverage of the backend for your host's architecture, and; - some coverage of other architectures (if other arches are supported backends). Altogether, this should cover a diverse set of uses for Clang and LLVM. If you; have very specific needs (e.g. your compiler is meant to compile a large browser; for four different platforms, or similar), you may want to do something else.; This is configurable in the script itself. Building Clang with PGO; =======================. If you prefer to not use the script or the cmake cache, this briefly goes over; how to build Clang/LLVM with PGO. First, you should have at least LLVM, Clang, and compiler-rt checked out; locally. Next, at a high level, you're going to need to do the following:. 1. Build a standard Release Clang and the relevant libclang_rt.profile library; 2. Build Clang using the Clang you built above, but with instrumentation; 3. Use the instrumented Clang to generate profiles, which consists of two steps:. - Running the instrumented Clang/LLVM",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst:2329,optimiz,optimizations,2329,interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,1,['optimiz'],['optimizations']
Performance," specific address space identifier AS. The operation is equivalent to performing ``DW_OP_swap;; DW_OP_LLVM_form_aspace_address; DW_OP_deref_size S``. The zero-extended; value V retrieved is left on the stack with the generic type. *This operation is deprecated as the* ``DW_OP_LLVM_form_aspace_address``; *operation can be used and provides greater expressiveness.*. 7. ``DW_OP_xderef_type`` *Deprecated*. ``DW_OP_xderef_type`` has two operands. The first is a 1-byte unsigned; integral constant S. The second operand is an unsigned LEB128 integer DR; that represents the byte offset of a debugging information entry D relative; to the beginning of the current compilation unit, that provides the type T; of the result value. It pops two stack entries. The first must be an integral type value that; represents an address A. The second must be an integral type value that; represents a target architecture specific address space identifier AS. The operation is equivalent to performing ``DW_OP_swap;; DW_OP_LLVM_form_aspace_address; DW_OP_deref_type S DR``. The value V; retrieved is left on the stack with the type T. *This operation is deprecated as the* ``DW_OP_LLVM_form_aspace_address``; *operation can be used and provides greater expressiveness.*. 8. ``DW_OP_entry_value`` *Deprecated*. ``DW_OP_entry_value`` pushes the value of an expression that is evaluated in; the context of the calling frame. *It may be used to determine the value of arguments on entry to the current; call frame provided they are not clobbered.*. It has two operands. The first is an unsigned LEB128 integer S. The second; is a block of bytes, with a length equal S, interpreted as a DWARF; operation expression E. E is evaluated with the current context, except the result kind is; unspecified, the call frame is the one that called the current frame, the; program location is the call site in the calling frame, the object is; unspecified, and the initial stack is empty. The calling frame information; is obtained b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:95053,perform,performing,95053,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance," specific; non-dynamic rounding mode which does not match the actual rounding mode at; runtime results in undefined behavior. The exception behavior argument is a metadata string describing the floating; point exception semantics that required for the intrinsic. This argument; must be one of the following strings:. ::. ""fpexcept.ignore""; ""fpexcept.maytrap""; ""fpexcept.strict"". If this argument is ""fpexcept.ignore"" optimization passes may assume that the; exception status flags will not be read and that floating-point exceptions will; be masked. This allows transformations to be performed that may change the; exception semantics of the original code. For example, FP operations may be; speculatively executed in this case whereas they must not be for either of the; other possible values of this argument. If the exception behavior argument is ""fpexcept.maytrap"" optimization passes; must avoid transformations that may raise exceptions that would not have been; raised by the original code (such as speculatively executing FP operations), but; passes are not required to preserve all exceptions that are implied by the; original code. For example, exceptions may be potentially hidden by constant; folding. If the exception behavior argument is ""fpexcept.strict"" all transformations must; strictly preserve the floating-point exception semantics of the original code.; Any FP exception that would have been raised by the original code must be raised; by the transformed code, and the transformed code must not raise any FP; exceptions that would not have been raised by the original code. This is the; exception behavior argument that will be used if the code being compiled reads; the FP exception status flags, but this mode can also be used with code that; unmasks FP exceptions. The number and order of floating-point exceptions is NOT guaranteed. For; example, a series of FP operations that each may raise exceptions may be; vectorized into a single instruction that raises each unique ex",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:870526,optimiz,optimization,870526,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance," stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:275075,perform,performing,275075,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance," stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit exe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:248117,load,loads,248117,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance," store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:359090,perform,performing,359090,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance," stray ``#pragma clang optimize on`` does not selectively enable; additional optimizations when compiling at low optimization levels. This feature; can only be used to selectively disable optimizations. The pragma has an effect on functions only at the point of their definition; for; function templates, this means that the state of the pragma at the point of an; instantiation is not necessarily relevant. Consider the following example:. .. code-block:: c++. template<typename T> T twice(T t) {; return 2 * t;; }. #pragma clang optimize off; template<typename T> T thrice(T t) {; return 3 * t;; }. int container(int a, int b) {; return twice(a) + thrice(b);; }; #pragma clang optimize on. In this example, the definition of the template function ``twice`` is outside; the pragma region, whereas the definition of ``thrice`` is inside the region.; The ``container`` function is also in the region and will not be optimized, but; it causes the instantiation of ``twice`` and ``thrice`` with an ``int`` type; of; these two instantiations, ``twice`` will be optimized (because its definition; was outside the region) and ``thrice`` will not be optimized. Clang also implements MSVC's range-based pragma,; ``#pragma optimize(""[optimization-list]"", on | off)``. At the moment, Clang only; supports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:160766,optimiz,optimized,160766,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,3,['optimiz'],['optimized']
Performance," structured in three main sections. The first section collects a; few performance numbers; the goal of this section is to give a very quick; overview of the performance throughput. Important performance indicators are; **IPC**, **uOps Per Cycle**, and **Block RThroughput** (Block Reciprocal; Throughput). Field *DispatchWidth* is the maximum number of micro opcodes that are dispatched; to the out-of-order backend every simulated cycle. For processors with an; in-order backend, *DispatchWidth* is the maximum number of micro opcodes issued; to the backend every simulated cycle. IPC is computed dividing the total number of simulated instructions by the total; number of cycles. Field *Block RThroughput* is the reciprocal of the block throughput. Block; throughput is a theoretical quantity computed as the maximum number of blocks; (i.e. iterations) that can be executed per simulated clock cycle in the absence; of loop carried dependencies. Block throughput is superiorly limited by the; dispatch rate, and the availability of hardware resources. In the absence of loop-carried data dependencies, the observed IPC tends to a; theoretical maximum which can be computed by dividing the number of instructions; of a single iteration by the `Block RThroughput`. Field 'uOps Per Cycle' is computed dividing the total number of simulated micro; opcodes by the total number of cycles. A delta between Dispatch Width and this; field is an indicator of a performance issue. In the absence of loop-carried; data dependencies, the observed 'uOps Per Cycle' should tend to a theoretical; maximum throughput which can be computed by dividing the number of uOps of a; single iteration by the `Block RThroughput`. Field *uOps Per Cycle* is bounded from above by the dispatch width. That is; because the dispatch width limits the maximum size of a dispatch group. Both IPC; and 'uOps Per Cycle' are limited by the amount of hardware parallelism. The; availability of hardware resources affects the resource pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:16549,throughput,throughput,16549,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['throughput'],['throughput']
Performance," sub %eax, %edx; # LLVM-MCA-END foo; add %eax, %edx; # LLVM-MCA-END bar. Note that multiple anonymous regions cannot overlap. Also, overlapping regions; cannot have the same name. There is no support for marking regions from high-level source code, like C or; C++. As a workaround, inline assembly directives may be used:. .. code-block:: c++. int foo(int a, int b) {; __asm volatile(""# LLVM-MCA-BEGIN foo"":::""memory"");; a += 42;; __asm volatile(""# LLVM-MCA-END"":::""memory"");; a *= b;; return a;; }. However, this interferes with optimizations like loop vectorization and may have; an impact on the code generated. This is because the ``__asm`` statements are; seen as real code having important side effects, which limits how the code; around them can be transformed. If users want to make use of inline assembly; to emit markers, then the recommendation is to always verify that the output; assembly is equivalent to the assembly generated in the absence of markers.; The `Clang options to emit optimization reports <https://clang.llvm.org/docs/UsersManual.html#options-to-emit-optimization-reports>`_; can also help in detecting missed optimizations. INSTRUMENT REGIONS; ------------------. An InstrumentRegion describes a region of assembly code guarded by; special LLVM-MCA comment directives. .. code-block:: none. # LLVM-MCA-<INSTRUMENT_TYPE> <data>; ... ## asm. where `INSTRUMENT_TYPE` is a type defined by the target and expects; to use `data`. A comment starting with substring `LLVM-MCA-<INSTRUMENT_TYPE>`; brings data into scope for llvm-mca to use in its analysis for; all following instructions. If a comment with the same `INSTRUMENT_TYPE` is found later in the; instruction list, then the original InstrumentRegion will be; automatically ended, and a new InstrumentRegion will begin. If there are comments containing the different `INSTRUMENT_TYPE`,; then both data sets remain available. In contrast with an AnalysisRegion,; an InstrumentRegion does not need a comment to end the regi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:10590,optimiz,optimization,10590,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['optimiz'],['optimization']
Performance," subtraction, and the second element of; which is a bit specifying if the signed subtraction resulted in an; overflow. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.ssub.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %overflow, label %normal. '``llvm.usub.with.overflow.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.usub.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.usub.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.usub.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.usub.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.usub.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.usub.with.overflow``' family of intrinsic functions perform; an unsigned subtraction of the two arguments, and indicate whether an; overflow occurred during the unsigned subtraction. Arguments:; """""""""""""""""""". The arguments (%a and %b) and the first element of the result structure; may be of integer types of any bit width, but they must have the same; bit width. The second element of the result structure must be of type; ``i1``. ``%a`` and ``%b`` are the two values that will undergo unsigned; subtraction. Semantics:; """""""""""""""""""". The '``llvm.usub.with.overflow``' family of intrinsic functions perform; an unsigned subtraction of the two arguments. They return a structure ---; the first element of which is the subtraction, and the second element of; which is a bit specifying if the unsigned subtraction resulted in an; overflow. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.usub.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %overflow, label %normal. '``llvm.smul.with.overflow",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:606287,perform,perform,606287,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance," temporary destination to temp; followed by a TFile::Cp to the final destination); this allows to avoid; reported problems with small temp partitions (see Forum).; In XrdProofConn, enable cycling through the; authentication protocol presented by the server. This only holds for; the choice of the protocol, because the server currently supports only; one full handshake.; In test/stressProof.cxx, avoid interferences between the; settings used for the PROOF tutorial and possible local settings; (daemon, dataset manager).; Add possibility to control the automatic re-loading of; the <proof.conf> file via the keyword; 'reload:1'/'reload:0'; in the xpd.resource directive.; Move the validation of <proof.conf> at the; moment of use; this allows to specify a file path and to dynamically; create/modify/destroy the file; used by PoD.; Improve displaying speed of large log files. Fixes. Fix two severe; bugs in the way TTreeCache; was used in PROOF: one bug was de facto disactivating the cache; the; other was causing a std::bad_alloc exception to be thrown on workers; when opening a remote file after a local one.    ; Fix several problems in TChain::Draw including. drawing into an existing histogram, i.e.; chain->Draw(""var>>myhist"");. treatment of histogram merging in case of small; statistics, i.e. when; the autobinning is not or only partially active;. usage of existing canvases when different histogram; names are specified;. Fix a problem causing a duplication of the final feedback; object. Fix problem with determining the subdir name in; TFileMerger::MergeRecursive on Windows; Make sure that the default sandbox is under $HOME/.proof; Fix a problem with dataset validation in multi-level; master setups; Fix a problem with ordinal numbers in multi-master setups; Fix a problem with defining the internal paths for; executables when configuring with '--prefix'; Fix backward-incompatibility issue giving the error; message  ""unknown action code: 5112""; Fix a few problems with file ret",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html:8749,cache,cache,8749,proof/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html,1,['cache'],['cache']
Performance, test programs that are only a single source file in size. A; subdirectory may contain several programs. - `MultiSource/`. Contains subdirectories which entire programs with multiple source files.; Large benchmarks and whole applications go here. - `MicroBenchmarks/`. Programs using the [google-benchmark](https://github.com/google/benchmark); library. The programs define functions that are run multiple times until the; measurement results are statistically significant. - `External/`. Contains descriptions and test data for code that cannot be directly; distributed with the test-suite. The most prominent members of this; directory are the SPEC CPU benchmark suites.; See [External Suites](#external-suites). - `Bitcode/`. These tests are mostly written in LLVM bitcode. - `CTMark/`. Contains symbolic links to other benchmarks forming a representative sample; for compilation performance measurements. ### Benchmarks. Every program can work as a correctness test. Some programs are unsuitable for; performance measurements. Setting the `TEST_SUITE_BENCHMARKING_ONLY` CMake; option to `ON` will disable them. Configuration; -------------. The test-suite has configuration options to customize building and running the; benchmarks. CMake can print a list of them:. ```bash; % cd test-suite-build; # Print basic options:; % cmake -LH; # Print all options:; % cmake -LAH; ```. ### Common Configuration Options. - `CMAKE_C_FLAGS`. Specify extra flags to be passed to C compiler invocations. The flags are; also passed to the C++ compiler and linker invocations. See; [https://cmake.org/cmake/help/latest/variable/CMAKE_LANG_FLAGS.html](https://cmake.org/cmake/help/latest/variable/CMAKE_LANG_FLAGS.html). - `CMAKE_C_COMPILER`. Select the C compiler executable to be used. Note that the C++ compiler is; inferred automatically i.e. when specifying `path/to/clang` CMake will; automatically use `path/to/clang++` as the C++ compiler. See; [https://cmake.org/cmake/help/latest/variable/CMAKE_LANG_COMP,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:3606,perform,performance,3606,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['perform'],['performance']
Performance," textures. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== ================================================; tfe Set tfe bit to 1.; ======================================== ================================================. .. _amdgpu_synid_sc0:. sc0; ~~~. For atomic opcodes, this modifier indicates that the instruction returns the value from memory; before the operation. For other opcodes, it is used together with :ref:`sc1<amdgpu_synid_sc1>`; to specify cache policy. ======================================== ================================================; Syntax Description; ======================================== ================================================; sc0 Set sc0 bit to 1.; ======================================== ================================================. .. _amdgpu_synid_sc1:. sc1; ~~~. This modifier is used together with :ref:`sc0<amdgpu_synid_sc0>` to specify cache; policy. ======================================== ================================================; Syntax Description; ======================================== ================================================; sc1 Set sc1 bit to 1.; ======================================== ================================================. .. _amdgpu_synid_nt:. nt; ~~. Indicates an operation with non-temporal data. ======================================== ================================================; Syntax Description; ======================================== ================================================; nt Set nt bit to 1.; ======================================== ================================================. MUBUF/MTBUF Modifiers; ---------------------. .. _amdgpu_synid_idxen:. idxen; ~~~~~. Specifies whether address components include an index. By default, the index is not used. May be used together with :ref:`offen<amdgpu_synid_offen>`. Cannot be us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:20564,cache,cache,20564,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['cache'],['cache']
Performance," than; reading each requested branch individually, the TTreeCache will read all the; branches thus trading off the latencies inherent to multiple small reads for; the potential of requesting more data than needed by read from the disk or; server the baskets for too many branches. The default behavior can be changed by either updating one of the rootrc files; or by setting environment variables. The rootrc files, both the global and the; local ones, now support the following the resource variable TTreeCache.Size; which set the default size factor for auto sizing TTreeCache for TTrees. The; estimated cluster size for the TTree and this factor is used to give the cache; size. If option is set to zero auto cache creation is disabled and the default; cache size is the historical one (equivalent to factor 1.0). If set to; non zero auto cache creation is enabled and both auto created and; default sized caches will use the configured factor: 0.0 no automatic cache; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:13769,cache,cache,13769,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cache']
Performance," that a newer compiler cannot read an older; precompiled header (and vice-versa). Original file name; The full path of the header that was used to generate the AST file. Predefines buffer; Although not explicitly stored as part of the metadata, the predefines buffer; is used in the validation of the AST file. The predefines buffer itself; contains code generated by the compiler to initialize the preprocessor state; according to the current target, platform, and command-line options. For; example, the predefines buffer will contain ""``#define __STDC__ 1``"" when we; are compiling C without Microsoft extensions. The predefines buffer itself; is stored within the :ref:`pchinternals-sourcemgr`, but its contents are; verified along with the rest of the metadata. A chained PCH file (that is, one that references another PCH) and a module; (which may import other modules) have additional metadata containing the list; of all AST files that this AST file depends on. Each of those files will be; loaded along with this AST file. For chained precompiled headers, the language options, target architecture and; predefines buffer data is taken from the end of the chain, since they have to; match anyway. .. _pchinternals-sourcemgr:. Source Manager Block; ^^^^^^^^^^^^^^^^^^^^. The source manager block contains the serialized representation of Clang's; :ref:`SourceManager <SourceManager>` class, which handles the mapping from; source locations (as represented in Clang's abstract syntax tree) into actual; column/line positions within a source file or macro instantiation. The AST; file's representation of the source manager also includes information about all; of the headers that were (transitively) included when building the AST file. The bulk of the source manager block is dedicated to information about the; various files, buffers, and macro instantiations into which a source location; can refer. Each of these is referenced by a numeric ""file ID"", which is a; unique number (allocated st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:9444,load,loaded,9444,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loaded']
Performance," that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:322679,load,load,322679,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," that might have; an exception thrown through them). For most targets, this is enabled by; default for C++. .. option:: -ftrapv. Generate code to catch integer overflow errors. Signed integer overflow is; undefined in C. With this flag, extra code is generated to detect this and; abort when it happens. .. option:: -fvisibility. This flag sets the default visibility level. .. option:: -fcommon, -fno-common. This flag specifies that variables without initializers get common linkage.; It can be disabled with :option:`-fno-common`. .. option:: -ftls-model=<model>. Set the default thread-local storage (TLS) model to use for thread-local; variables. Valid values are: ""global-dynamic"", ""local-dynamic"",; ""initial-exec"" and ""local-exec"". The default is ""global-dynamic"". The default; model can be overridden with the tls_model attribute. The compiler will try; to choose a more efficient model if possible. .. option:: -flto, -flto=full, -flto=thin, -emit-llvm. Generate output files in LLVM formats, suitable for link time optimization.; When used with :option:`-S` this generates LLVM intermediate language; assembly files, otherwise this generates LLVM bitcode format object files; (which may be passed to the linker depending on the stage selection options). The default for :option:`-flto` is ""full"", in which the; LLVM bitcode is suitable for monolithic Link Time Optimization (LTO), where; the linker merges all such modules into a single combined module for; optimization. With ""thin"", :doc:`ThinLTO <../ThinLTO>`; compilation is invoked instead. .. note::. On Darwin, when using :option:`-flto` along with :option:`-g` and; compiling and linking in separate steps, you also need to pass; ``-Wl,-object_path_lto,<lto-filename>.o`` at the linking step to instruct the; ld64 linker not to delete the temporary object file generated during Link; Time Optimization (this flag is automatically passed to the linker by Clang; if compilation and linking are done in a single step). This allows debu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:15089,optimiz,optimization,15089,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,1,['optimiz'],['optimization']
Performance," that there were a total of 35,504 samples; collected in main. All of those were at line 1 (the call to ``foo``).; Of those, 31,977 were spent inside the body of ``bar``. The last line; of the profile (``2: 0``) corresponds to line 2 inside ``main``. No; samples were collected there. .. _prof_instr:. Profiling with Instrumentation; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Clang also supports profiling via instrumentation. This requires building a; special instrumented version of the code and has some runtime; overhead during the profiling, but it provides more detailed results than a; sampling profiler. It also provides reproducible results, at least to the; extent that the code behaves consistently across runs. Clang supports two types of instrumentation: frontend-based and IR-based.; Frontend-based instrumentation can be enabled with the option ``-fprofile-instr-generate``,; and IR-based instrumentation can be enabled with the option ``-fprofile-generate``.; For best performance with PGO, IR-based instrumentation should be used. It has; the benefits of lower instrumentation overhead, smaller raw profile size, and; better runtime performance. Frontend-based instrumentation, on the other hand,; has better source correlation, so it should be used with source line-based; coverage testing. The flag ``-fcs-profile-generate`` also instruments programs using the same; instrumentation method as ``-fprofile-generate``. However, it performs a; post-inline late instrumentation and can produce context-sensitive profiles. Here are the steps for using profile guided optimization with; instrumentation:. 1. Build an instrumented version of the code by compiling and linking with the; ``-fprofile-generate`` or ``-fprofile-instr-generate`` option. .. code-block:: console. $ clang++ -O2 -fprofile-instr-generate code.cc -o code. 2. Run the instrumented executable with inputs that reflect the typical usage.; By default, the profile data will be written to a ``default.profraw`` file; in the curren",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:102773,perform,performance,102773,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performance']
Performance," that; exactly matches the operation-by-operation behavior of native support,; but that can require many extra truncations and extensions. By default,; when emulating ``_Float16`` and ``__bf16`` arithmetic using ``float``, Clang; does not truncate intermediate operands back to their true type unless the; operand is the result of an explicit cast or assignment. This is generally; much faster but can generate different results from strict operation-by-operation; emulation. Usually the results are more precise. This is permitted by the; C and C++ standards under the rules for excess precision in intermediate operands;; see the discussion of evaluation formats in the C standard and [expr.pre] in; the C++ standard. The use of excess precision can be independently controlled for these two; types with the ``-ffloat16-excess-precision=`` and; ``-fbfloat16-excess-precision=`` options. Valid values include:. * ``none``: meaning to perform strict operation-by-operation emulation; * ``standard``: meaning that excess precision is permitted under the rules; described in the standard, i.e. never across explicit casts or statements; * ``fast``: meaning that excess precision is permitted whenever the; optimizer sees an opportunity to avoid truncations; currently this has no; effect beyond ``standard``. The ``_Float16`` type is an interchange floating type specified in; ISO/IEC TS 18661-3:2015 (""Floating-point extensions for C""). It will; be supported on more targets as they define ABIs for it. The ``__bf16`` type is a non-standard extension, but it generally follows; the rules for arithmetic interchange floating types from ISO/IEC TS; 18661-3:2015. In previous versions of Clang, it was a storage-only type; that forbade arithmetic operations. It will be supported on more targets; as they define ABIs for it. The ``__fp16`` type was originally an ARM extension and is specified; by the `ARM C Language Extensions <https://github.com/ARM-software/acle/releases>`_.; Clang uses the ``binary",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:33366,perform,perform,33366,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['perform']
Performance," that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1. buffer_wbl2 sc0=1 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:329610,load,load,329610,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," the ""``=``"" in case of an output). This indicates that the asm; will write to or read from the contents of an *address* provided as an input; argument. (Note that in this way, indirect outputs act more like an *input* than; an output: just like an input, they consume an argument of the call expression,; rather than producing a return value. An indirect output constraint is an; ""output"" only in that the asm is expected to write to the contents of the input; memory location, instead of just read from it). This is most typically used for memory constraint, e.g. ""``=*m``"", to pass the; address of a variable as a value. It is also possible to use an indirect *register* constraint, but only on output; (e.g. ""``=*r``""). This will cause LLVM to allocate a register for an output; value normally, and then, separately emit a store to the address provided as; input, after the provided inline asm. (It's not clear what value this; functionality provides, compared to writing the store explicitly after the asm; statement, and it can only produce worse code, since it bypasses many; optimization passes. I would recommend not using it.). Call arguments for indirect constraints must have pointer type and must specify; the :ref:`elementtype <attr_elementtype>` attribute to indicate the pointer; element type. Clobber constraints; """""""""""""""""""""""""""""""""""""". A clobber constraint is indicated by a ""``~``"" prefix. A clobber does not; consume an input operand, nor generate an output. Clobbers cannot use any of the; general constraint code letters -- they may use only explicit register; constraints, e.g. ""``~{eax}``"". The one exception is that a clobber string of; ""``~{memory}``"" indicates that the assembly writes to arbitrary undeclared; memory locations -- not only the memory pointed to by a declared indirect; output. Note that clobbering named registers that are also present in output; constraints is not legal. Label constraints; """""""""""""""""""""""""""""""""". A label constraint is indicated by a ""``!``"" pre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:216774,optimiz,optimization,216774,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance," the ''align'' attribute on parameters and return values.; This metadata can only be applied to loads of a pointer type. If the returned; value is not appropriately aligned at runtime, a poison value is returned; instead. The optional ``!noundef`` metadata must reference a single metadata name; ``<empty_node>`` corresponding to a node with no entries. The existence of; ``!noundef`` metadata on the instruction tells the optimizer that the value; loaded is known to be :ref:`well defined <welldefinedvalues>`.; If the value isn't well defined, the behavior is undefined. If the ``!noundef``; metadata is combined with poison-generating metadata like ``!nonnull``,; violation of that metadata constraint will also result in undefined behavior. Semantics:; """""""""""""""""""". The location of memory pointed to is loaded. If the value being loaded; is of scalar type then the number of bytes read does not exceed the; minimum number of bytes needed to hold all bits of the type. For; example, loading an ``i24`` reads at most three bytes. When loading a; value of a type like ``i20`` with a size that is not an integral number; of bytes, the result is undefined if the value was not originally; written using a store of the same type.; If the value being loaded is of aggregate type, the bytes that correspond to; padding may be accessed but are ignored, because it is impossible to observe; padding from the loaded aggregate value.; If ``<pointer>`` is not a well-defined value, the behavior is undefined. Examples:; """""""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; store i32 3, ptr %ptr ; yields void; %val = load i32, ptr %ptr ; yields i32:val = i32 3. .. _i_store:. '``store``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. store [volatile] <ty> <value>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.group !<empty_node>] ; yields void; store atomic [volatile] <ty> <value>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <al",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:418331,load,loading,418331,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance," the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *value*. ``!substr(``\ *string*\ ``,`` *start*\ [``,`` *length*]\ ``)``; This operator extracts a substring of the given *string*. The starting; position of the substring is specified by *start*, which can range; between 0 and the length of the string. The length of the substring; is specified by *length*; if not specified, the rest of the string is; extracted. The *start* and *length* arguments must be integers. ``!tail(``\ *a*\ ``)``; This operator produces a new list with all the elements; of the list *a* except for the zeroth one. (See also ``!head``.). ``!tolower(``\ *a*\ ``)``; This operator converts a string input *a* to lower case. ``!toupper(``\ *a*\ ``)``; This operator converts a string input *a* to upper case. ``!xor(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise EXCLUSIVE OR on *a*, *b*, etc., and produces; the result. A logical XOR can be performed if all the arguments are either; 0 or 1. Appendix B: Paste Operator Examples; ===================================. Here is an example illustrating the use of the paste operator in record names. .. code-block:: text. defvar suffix = ""_suffstring"";; defvar some_ints = [0, 1, 2, 3];. def name # suffix {; }. foreach i = [1, 2] in {; def rec # i {; }; }. The first ``def`` does not use the value of the ``suffix`` variable. The; second def does use the value of the ``i`` iterator variable, because it is not a; global name. The following records are produced. .. code-block:: text. def namesuffix {; }; def rec1 {; }; def rec2 {; }. Here is a second example illustrating the paste operator in field value expressions. .. code-block:: text. def test {; string strings = suffix # suffix;; list<int> integers = some_ints # [4, 5, 6];; }. The ``strings`` field expression uses ``suffix`` on both sides of the paste; operator. It is evaluated normally on the left hand side, but taken verbatim; on the right han",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:73758,perform,performed,73758,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performed']
Performance," the 0'th; constraint). It is permitted to tie an input to an ""early-clobber"" output. In that case, no; *other* input may share the same register as the input tied to the early-clobber; (even when the other input has the same value). You may only tie an input to an output which has a register constraint, not a; memory constraint. Only a single input may be tied to an output. There is also an ""interesting"" feature which deserves a bit of explanation: if a; register class constraint allocates a register which is too small for the value; type operand provided as input, the input value will be split into multiple; registers, and all of them passed to the inline asm. However, this feature is often not as useful as you might think. Firstly, the registers are *not* guaranteed to be consecutive. So, on those; architectures that have instructions which operate on multiple consecutive; instructions, this is not an appropriate way to support them. (e.g. the 32-bit; SparcV8 has a 64-bit load, which instruction takes a single 32-bit register. The; hardware then loads into both the named register, and the next register. This; feature of inline asm would not be useful to support that.). A few of the targets provide a template string modifier allowing explicit access; to the second register of a two-register operand (e.g. MIPS ``L``, ``M``, and; ``D``). On such an architecture, you can actually access the second allocated; register (yet, still, not any subsequent ones). But, in that case, you're still; probably better off simply splitting the value into two separate operands, for; clarity. (e.g. see the description of the ``A`` constraint on X86, which,; despite existing only for use with this feature, is not really a good idea to; use). Indirect inputs and outputs; """""""""""""""""""""""""""""""""""""""""""""""""""""". Indirect output or input constraints can be specified by the ""``*``"" modifier; (which goes after the ""``=``"" in case of an output). This indicates that the asm; will write to or read from the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:214775,load,load,214775,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance," the addressing mode to be less than or equal to 9. This means the full; address can only be guaranteed to be less than `(1 << 31) + 9`. The OS may wish; to protect an extra page of the low address space to account for this. ##### Optimizations. A very large portion of the cost for this approach comes from checking loads in; this way, so it is important to work to optimize this. However, beyond making; the instruction sequences to *apply* the checks efficient (for example by; avoiding `pushfq` and `popfq` sequences), the only significant optimization is; to check fewer loads without introducing a vulnerability. We apply several; techniques to accomplish that. ###### Don't check loads from compile-time constant stack offsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a si",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:35265,perform,performance,35265,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance," the apply button is; pressed, the changes are applied to the edited shape and drawn. The; ""Undo"" button becomes active after the first modification has been; applied. It allows restoring the initial parameters of the shape. NOTE: In this version the ""Undo"" does not allow restoring an; intermediate state of the parameters that was applied - it will always; restore the parameters at the moment the shape was edited. All material properties changes are undoable. The mixture editor; currently allows adding elements one by one in the mixture composition.; This can be done either by element weight fraction or by number of; atoms. Once an element was added using one method the other method is not; selectable anymore. Summing component fractions up to 1 in the final; mixture is the user responsibility. Adding materials as components of a; mixture is not supported in this version. The elements that were added to the mixture appear in the bottom of the; mixture editor. The operations performed on mixture are not undoable. \anchor GP08d; ### Creation of New Objects. As described above, all geometry object creators are accessible within; the geometry manager editor frame. Generally, if the new object that; needs to be created does not depend on other objects, it will be built; with a set of default parameters. This is the case for all shapes; (except composite shapes) and matrices. For all the other objects the; interface forces the selection of components before creating the object. \anchor GP08e; ### Editing Volumes. Volumes are hierarchical components in the geometry, therefore their; editor is more complex. It provides the following functionalities:. - *General*. This category allows changing the name of the volume and; selecting other shape or medium among existing ones. - *Daughters*. The category allows removing existing daughter nodes or; adding new ones. The button ""Position"" allows editing the; positioning matrix of a given node. \image html geometry022.jpg width=600p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:131660,perform,performed,131660,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance," the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:30428,load,load,30428,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['load'],['load']
Performance," the computation; (see ``-fsanitize=implicit-conversion``). Both of these two issues are; handled by ``-fsanitize=implicit-conversion`` group of checks.; - ``-fsanitize=unreachable``: If control flow reaches an unreachable; program point.; - ``-fsanitize=unsigned-integer-overflow``: Unsigned integer overflow, where; the result of an unsigned integer computation cannot be represented in its; type. Unlike signed integer overflow, this is not undefined behavior, but; it is often unintentional. This sanitizer does not check for lossy implicit; conversions performed before such a computation; (see ``-fsanitize=implicit-conversion``).; - ``-fsanitize=vla-bound``: A variable-length array whose bound; does not evaluate to a positive value.; - ``-fsanitize=vptr``: Use of an object whose vptr indicates that it is of; the wrong dynamic type, or that its lifetime has not begun or has ended.; Incompatible with ``-fno-rtti``. Link must be performed by ``clang++``, not; ``clang``, to make sure C++-specific parts of the runtime library and C++; standard libraries are present. You can also use the following check groups:; - ``-fsanitize=undefined``: All of the checks listed above other than; ``float-divide-by-zero``, ``unsigned-integer-overflow``,; ``implicit-conversion``, ``local-bounds`` and the ``nullability-*`` group; of checks.; - ``-fsanitize=undefined-trap``: Deprecated alias of; ``-fsanitize=undefined``.; - ``-fsanitize=implicit-integer-truncation``: Catches lossy integral; conversions. Enables ``implicit-signed-integer-truncation`` and; ``implicit-unsigned-integer-truncation``.; - ``-fsanitize=implicit-integer-arithmetic-value-change``: Catches implicit; conversions that change the arithmetic value of the integer. Enables; ``implicit-signed-integer-truncation`` and ``implicit-integer-sign-change``.; - ``-fsanitize=implicit-conversion``: Checks for suspicious; behavior of implicit conversions. Enables; ``implicit-unsigned-integer-truncation``,; ``implicit-signed-integer-trunc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst:9909,perform,performed,9909,interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UndefinedBehaviorSanitizer.rst,1,['perform'],['performed']
Performance," the corresponding coroutine. In other words, if two coroutines have the; same `promise_type`, they should behave in the same way.; To print a `promise_type` in a debugger when stopped at a breakpoint inside a; coroutine, printing the `promise_type` can be done by:. .. parsed-literal::. print __promise. It is also possible to print the `promise_type` of a coroutine from the address; of the coroutine frame. For example, if the address of a coroutine frame is; 0x416eb0, and the type of the `promise_type` is `task::promise_type`, printing; the `promise_type` can be done by:. .. parsed-literal::. print (task::promise_type)*(0x416eb0+0x10). This is possible because the `promise_type` is guaranteed by the ABI to be at a; 16 bit offset from the coroutine frame. Note that there is also an ABI independent method:. .. parsed-literal::. print std::coroutine_handle<task::promise_type>::from_address((void*)0x416eb0).promise(). The functions `from_address(void*)` and `promise()` are often small enough to; be removed during optimization, so this method may not be possible. Print coroutine frames; ======================. LLVM generates the debug information for the coroutine frame in the LLVM middle; end, which permits printing of the coroutine frame in the debugger. Much like; the `promise_type`, when stopped at a breakpoint inside a coroutine we can; print the coroutine frame by:. .. parsed-literal::. print __coro_frame. Just as printing the `promise_type` is possible from the coroutine address,; printing the details of the coroutine frame from an address is also possible:. ::. (gdb) # Get the address of coroutine frame; (gdb) print/x *0x418eb0; $1 = 0x4019e0; (gdb) # Get the linkage name for the coroutine; (gdb) x 0x4019e0; 0x4019e0 <_ZL9coro_taski>: 0xe5894855; (gdb) # Turn off the demangler temporarily to avoid the debugger misunderstanding the name.; (gdb) set demangle-style none; (gdb) # The coroutine frame type is 'linkage_name.coro_frame_ty'; (gdb) print ('_ZL9coro_taski.co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:4431,optimiz,optimization,4431,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,1,['optimiz'],['optimization']
Performance," the current compilation unit for a base type of the; generic type. The operation is equivalent to performing ``DW_OP_deref_type S, DR``. 3. ``DW_OP_deref_size``. ``DW_OP_deref_size`` has a single 1-byte unsigned integral constant that; represents a byte result size S. TS is the smaller of the generic type bit size and S scaled by 8 (the byte; size). If TS is smaller than the generic type bit size then T is an unsigned; integral type of bit size TS, otherwise T is the generic type. DR is the; offset of a hypothetical debug information entry D in the current; compilation unit for a base type T. .. note::. Truncating the value when S is larger than the generic type matches what; GDB does. This allows the generic type size to not be an integral byte; size. It does allow S to be arbitrarily large. Should S be restricted to; the size of the generic type rounded up to a multiple of 8?. The operation is equivalent to performing ``DW_OP_deref_type S, DR``, except; if T is not the generic type, the value V pushed is zero-extended to the; generic type bit size and its type changed to the generic type. 4. ``DW_OP_deref_type``. ``DW_OP_deref_type`` has two operands. The first is a 1-byte unsigned; integral constant S. The second is an unsigned LEB128 integer DR that; represents the byte offset of a debugging information entry D relative to; the beginning of the current compilation unit, that provides the type T of; the result value. TS is the bit size of the type T. *While the size of the pushed value V can be inferred from the type T, it is; encoded explicitly as the operand S so that the operation can be parsed; easily without reference to the* ``.debug_info`` *section.*. .. note::. It is unclear why the operand S is needed. Unlike ``DW_OP_const_type``,; the size is not needed for parsing. Any evaluation needs to get the base; type T to push with the value to know its encoding and bit size. It pops one stack entry that must be a location description L. A value V of TS bits is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:89777,perform,performing,89777,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance," the file; ``yyy/zzz/default_xxxx.profraw``. To generate the profile data file with the compiler readable format, the; ``llvm-profdata`` tool can be used with the profile directory as the input:. .. code-block:: console. $ llvm-profdata merge -output=code.profdata yyy/zzz/. If the user wants to turn off the auto-merging feature, or simply override the; the profile dumping path specified at command line, the environment variable; ``LLVM_PROFILE_FILE`` can still be used to override; the directory and filename for the profile file at runtime.; To override the path and filename at compile time, use; ``-Xclang -fprofile-instrument-path=/path/to/file_pattern.profraw``. .. option:: -fcs-profile-generate[=<dirname>]. The ``-fcs-profile-generate`` and ``-fcs-profile-generate=`` flags will use; the same instrumentation method, and generate the same profile as in the; ``-fprofile-generate`` and ``-fprofile-generate=`` flags. The difference is; that the instrumentation is performed after inlining so that the resulted; profile has a better context sensitive information. They cannot be used; together with ``-fprofile-generate`` and ``-fprofile-generate=`` flags.; They are typically used in conjunction with ``-fprofile-use`` flag.; The profile generated by ``-fcs-profile-generate`` and ``-fprofile-generate``; can be merged by llvm-profdata. A use example:. .. code-block:: console. $ clang++ -O2 -fprofile-generate=yyy/zzz code.cc -o code; $ ./code; $ llvm-profdata merge -output=code.profdata yyy/zzz/. The first few steps are the same as that in ``-fprofile-generate``; compilation. Then perform a second round of instrumentation. .. code-block:: console. $ clang++ -O2 -fprofile-use=code.profdata -fcs-profile-generate=sss/ttt \; -o cs_code; $ ./cs_code; $ llvm-profdata merge -output=cs_code.profdata sss/ttt code.profdata. The resulted ``cs_code.prodata`` combines ``code.profdata`` and the profile; generated from binary ``cs_code``. Profile ``cs_code.profata`` can be used by; ``-fprofi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:109076,perform,performed,109076,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performed']
Performance," the final vectorization decisions, and to execute them:; the Hierarchical CFG models the planned control-flow, and Recipes capture; decisions taken inside basic-blocks. Next, VPlan will be used also as the basis; for taking these decisions, effectively turning them into a series of; VPlan-to-VPlan algorithms. Finally, VPlan will support the planning process; itself including cost-based analyses for making these decisions, to fully; support compositional and iterative decision making. Some decisions are local to an instruction in the loop, such as whether to widen; it into a vector instruction or replicate it, keeping the generated instructions; in place. Other decisions, however, involve moving instructions, replacing them; with other instructions, and/or introducing new instructions. For example, a; cast may sink past a later instruction and be widened to handle first-order; recurrence; an interleave group of strided gathers or scatters may effectively; move to one place where they are replaced with shuffles and a common wide vector; load or store; new instructions may be introduced to compute masks, shuffle the; elements of vectors, and pack scalar values into vectors or vice-versa. In order for VPlan to support making instruction-level decisions and analyses,; it needs to model the relevant instructions along with their def/use relations.; This too follows a staged approach: first, the new instructions that compute; masks are modeled as VPInstructions, along with their induced def/use subgraph.; This effectively models masks in VPlan, facilitating VPlan-based predication.; Next, the logic embedded within each Recipe for generating its instructions at; VPlan execution time, will instead take part in the planning process by modeling; them as VPInstructions. Finally, only logic that applies to instructions as a; group will remain in Recipes, such as interleave groups and potentially other; idiom groups having synergistic cost. Related LLVM components; -------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst:9314,load,load,9314,interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,1,['load'],['load']
Performance," the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the momen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4508,load,load,4508,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst,1,['load'],['load']
Performance," the interface between; the two modules. The ``ASTReader`` class, which handles the loading of an AST; file, inherits from all of these abstract classes to provide lazy; deserialization of Clang's data structures. ``ASTReader`` implements the; following abstract classes:. ``ExternalSLocEntrySource``; This abstract interface is associated with the ``SourceManager`` class, and; is used whenever the :ref:`source manager <pchinternals-sourcemgr>` needs to; load the details of a file, buffer, or macro instantiation. ``IdentifierInfoLookup``; This abstract interface is associated with the ``IdentifierTable`` class, and; is used whenever the program source refers to an identifier that has not yet; been seen. In this case, the AST reader searches for this identifier within; its :ref:`identifier table <pchinternals-ident-table>` to load any top-level; declarations or macros associated with that identifier. ``ExternalASTSource``; This abstract interface is associated with the ``ASTContext`` class, and is; used whenever the abstract syntax tree nodes need to loaded from the AST; file. It provides the ability to de-serialize declarations and types; identified by their numeric values, read the bodies of functions when; required, and read the declarations stored within a declaration context; (either for iteration or for name lookup). ``ExternalSemaSource``; This abstract interface is associated with the ``Sema`` class, and is used; whenever semantic analysis needs to read information from the :ref:`global; method pool <pchinternals-method-pool>`. .. _pchinternals-chained:. Chained precompiled headers; ---------------------------. Chained precompiled headers were initially intended to improve the performance; of IDE-centric operations such as syntax highlighting and code completion while; a particular source file is being edited by the user. To minimize the amount; of reparsing required after a change to the file, a form of precompiled header; --- called a precompiled *preamble* -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:23001,load,loaded,23001,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loaded']
Performance," the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Programmer's Language; Reference* [HSA]_. For AMDGPU the kernel execution directly accesses the; kernel argument memory in the same way constant memory is accessed. (Note; that the HSA specification allows an implementation to copy the kernel; argument contents to another location that is accessed by the kernel.); 5. An AQL kernel dispatch packet is created on the AQL queue. The HSA compatible; runtime api uses 64-bit atomic operations to reserve space in the AQL queue; for the packet. The packet must be set up, and the final write must use an; atomic store release to set the packet kind to ensure the packet contents are; visible to the kernel agent. AQL defines a doorbell signal mechanism to; notify the kernel agent that the AQL queue has been updated. These rules, and; the layout of the AQL queue and kernel dispatch packet is defined in the *HSA; System Architecture Specification* [HSA]_.; 6. A kernel dispatch packet includes information about the actual dispatch,; such as grid and work-group size, together with information from the code; object about the kernel, such as segment sizes. The HSA compatible runtime; queries on the kernel symbol can be used to obtain the code object values; which are recorded in the :ref:`amdgpu-amdhsa-code-object-metadata`.; 7. CP executes micro-code and is responsible for detecting and setting up the; GPU to execute the wavefronts of a kernel dispatch.; 8. CP ensures that when the a wavefront starts executing the kernel machine; code, the scalar general purpose registers (SGPR) and vector general purpose; registers (VGPR) are set up as required by the machine code. The required; setup is defined in the :ref:`amdgpu-amdhsa-kernel-descriptor`. The initial; register state is defined in; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`.; 9. The prolog of the kernel mach",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:151457,queue,queue,151457,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance," the latter are defined statically for each; instruction, whereas the former may vary depending on the program being; compiled. For example, an instruction that represents a function call will; always implicitly define or use the same set of physical registers. To read the; registers implicitly used by an instruction, use; ``TargetInstrInfo::get(opcode)::ImplicitUses``. Pre-colored registers impose; constraints on any register allocation algorithm. The register allocator must; make sure that none of them are overwritten by the values of virtual registers; while still alive. Mapping virtual registers to physical registers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. There are two ways to map virtual registers to physical registers (or to memory; slots). The first way, that we will call *direct mapping*, is based on the use; of methods of the classes ``TargetRegisterInfo``, and ``MachineOperand``. The; second way, that we will call *indirect mapping*, relies on the ``VirtRegMap``; class in order to insert loads and stores sending and getting values to and from; memory. The direct mapping provides more flexibility to the developer of the register; allocator; however, it is more error prone, and demands more implementation; work. Basically, the programmer will have to specify where load and store; instructions should be inserted in the target function being compiled in order; to get and store values in memory. To assign a physical register to a virtual; register present in a given operand, use ``MachineOperand::setReg(p_reg)``. To; insert a store instruction, use ``TargetInstrInfo::storeRegToStackSlot(...)``,; and to insert a load instruction, use ``TargetInstrInfo::loadRegFromStackSlot``. The indirect mapping shields the application developer from the complexities of; inserting load and store instructions. In order to map a virtual register to a; physical one, use ``VirtRegMap::assignVirt2Phys(vreg, preg)``. In order to map; a certain virtual register to memory, us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:64236,load,loads,64236,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['loads']
Performance," the left. You can change it; to be centered or right aligned if you use **`TGGroupFrame::kCenter`** or; `TGGroupFrame::kRight` as a parameter. ![](pictures/02000208.jpg). Be conservative in the use of borders because of the potential for; clutter. Do not place them around single entry fields, single combo; boxes, list boxes and groups of command buttons. The design of these; widgets provides them with a border. The picture above provides kind of; borders to avoid. ## Layout Management. The layout process is an integral part of any GUI. When you create a; simple message window, laying out its few buttons and text widgets is; quite simple. However, this process becomes increasingly difficult if; you have to implement large GUI's with many widgets that should behave; properly when the GUI is resized or uses a different font type or size.; Layout management is the process of determining the size and position of; every widget in a container. A layout manager is an object that performs layout management for the; widgets within a container. You already know that when adding a; component (child widget) to a container (parent widget) you can provide; alignment hints (or rely on the default ones). These hints are used by; the layout manager to correctly position the widgets in the container.; The **`TGLayoutManager`** is an abstract class providing the basic; layout functionality. ![The layout classes hierarchy](pictures/02000209.jpg). The base ""container"" class is **`TGCmpositeFrame`**. You can easily; change the layout manager using the; `SetLayoutManager(TGLayoutManager *l)` method. Setting the proper layout; manager for each container is the first step you have to do. The; container uses that layout manager to position and size the components; before they are painted. ROOT currently provides the layout managers; shown on the picture above. The next important step is to provide hints about every widget in the; container, i.e. to provide positions and right amount of space ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:34099,perform,performs,34099,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['perform'],['performs']
Performance," the mode is consistent. User or platform code is expected to set; the floating point mode appropriately before function entry. If the input mode is ``""preserve-sign""``, or ``""positive-zero""``,; a floating-point operation must treat any input denormal value as; zero. In some situations, if an instruction does not respect this; mode, the input may need to be converted to 0 as if by; ``@llvm.canonicalize`` during lowering for correctness. ``""denormal-fp-math-f32""``; Same as ``""denormal-fp-math""``, but only controls the behavior of; the 32-bit float type (or vectors of 32-bit floats). If both are; are present, this overrides ``""denormal-fp-math""``. Not all targets; support separately setting the denormal mode per type, and no; attempt is made to diagnose unsupported uses. Currently this; attribute is respected by the AMDGPU and NVPTX backends. ``""thunk""``; This attribute indicates that the function will delegate to some other; function with a tail call. The prototype of a thunk should not be used for; optimization purposes. The caller is expected to cast the thunk prototype to; match the thunk target prototype. ``""tls-load-hoist""``; This attribute indicates that the function will try to reduce redundant; tls address calculation by hoisting tls variable. ``uwtable[(sync|async)]``; This attribute indicates that the ABI being targeted requires that; an unwind table entry be produced for this function even if we can; show that no exceptions passes by it. This is normally the case for; the ELF x86-64 abi, but it can be disabled for some compilation; units. The optional parameter describes what kind of unwind tables; to generate: ``sync`` for normal unwind tables, ``async`` for asynchronous; (instruction precise) unwind tables. Without the parameter, the attribute; ``uwtable`` is equivalent to ``uwtable(async)``.; ``nocf_check``; This attribute indicates that no control-flow check will be performed on; the attributed entity. It disables -fcf-protection=<> for a specific; ent",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:108360,optimiz,optimization,108360,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance," the native functions.; For example, given the ABI list example provided in the user manual, the; following wrappers will be generated under the args ABI:. .. code-block:: llvm. define linkonce_odr { i8*, i16 } @""dfsw$malloc""(i64 %0, i16 %1) {; entry:; %2 = call i8* @malloc(i64 %0); %3 = insertvalue { i8*, i16 } undef, i8* %2, 0; %4 = insertvalue { i8*, i16 } %3, i16 0, 1; ret { i8*, i16 } %4; }. define linkonce_odr { i32, i16 } @""dfsw$tolower""(i32 %0, i16 %1) {; entry:; %2 = call i32 @tolower(i32 %0); %3 = insertvalue { i32, i16 } undef, i32 %2, 0; %4 = insertvalue { i32, i16 } %3, i16 %1, 1; ret { i32, i16 } %4; }. define linkonce_odr { i8*, i16 } @""dfsw$memcpy""(i8* %0, i8* %1, i64 %2, i16 %3, i16 %4, i16 %5) {; entry:; %labelreturn = alloca i16; %6 = call i8* @__dfsw_memcpy(i8* %0, i8* %1, i64 %2, i16 %3, i16 %4, i16 %5, i16* %labelreturn); %7 = load i16* %labelreturn; %8 = insertvalue { i8*, i16 } undef, i8* %6, 0; %9 = insertvalue { i8*, i16 } %8, i16 %7, 1; ret { i8*, i16 } %9; }. As an optimization, direct calls to native ABI functions will call the; native ABI function directly and the pass will compute the appropriate label; internally. This has the advantage of reducing the number of union operations; required when the return value label is known to be zero (i.e. ``discard``; functions, or ``functional`` functions with known unlabelled arguments). Checking ABI Consistency; ------------------------. DFSan changes the ABI of each function in the module. This makes it possible; for a function with the native ABI to be called with the instrumented ABI,; or vice versa, thus possibly invoking undefined behavior. A simple way; of statically detecting instances of this problem is to append the suffix; "".dfsan"" to the name of each instrumented-ABI function. This will not catch every such problem; in particular function pointers passed; across the instrumented-native barrier cannot be used on the other side.; These problems could potentially be caught dynamically.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst:12248,optimiz,optimization,12248,interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst,1,['optimiz'],['optimization']
Performance," the node for ``A`` (the constructor; needs a reference to ``B``). And the same could be true for the import of ``B``; (``A`` is requested to be imported before we could create the node for ``B``).; In case of the :ref:`templated-described swing <templated>` we take; extra attention to break the cyclical dependency: we import and set the; described template only after the ``CXXRecordDecl`` is created. As a best; practice, before creating the node in the ""to"" context, avoid importing of; other nodes which are not needed for the constructor of node ``A``. Error Handling; ^^^^^^^^^^^^^^. Every import function returns with either an ``llvm::Error`` or an; ``llvm::Expected<T>`` object. This enforces to check the return value of the; import functions. If there was an error during one import then we return with; that error. (Exception: when we import the members of a class, we collect the; individual errors with each member and we concatenate them in one Error; object.) We cache these errors in cases of declarations. During the next import; call if there is an existing error we just return with that. So, clients of the; library receive an Error object, which they must check. During import of a specific declaration, it may happen that some AST nodes had; already been created before we recognize an error. In this case, we signal back; the error to the caller, but the ""to"" context remains polluted with those nodes; which had been created. Ideally, those nodes should not had been created, but; that time we did not know about the error, the error happened later. Since the; AST is immutable (most of the cases we can't remove existing nodes) we choose; to mark these nodes as erroneous. We cache the errors associated with declarations in the ""from"" context in; ``ASTImporter::ImportDeclErrors`` and the ones which are associated with the; ""to"" context in ``ASTImporterSharedState::ImportErrors``. Note that, there may; be several ASTImporter objects which import into the same ""to"" cont",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:100657,cache,cache,100657,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['cache'],['cache']
Performance," the object; editor. Then, it scans all object base classes searching the; corresponding object editors. When it finds one, it makes an instance of; the base class editor too. Once the object editor is in place, it sets the user interface elements; according to the object's status. After that, it is ready to interact; with the object following the user actions. The graphics editor gives an intuitive way to edit objects in a canvas; with immediate feedback. Complexity of some object editors is reduced by; hiding GUI elements and revealing them only on users' requests. An object in the canvas is selected by clicking on it with the left; mouse button. Its name is displayed on the top of the editor frame in; red color. If the editor frame needs more space than the canvas window,; a vertical scroll bar appears for easy navigation. ![Histogram, pad and axis editors](pictures/03000222.png). ### Editor Design Elements. The next rules describe the path to follow when creating your own object; editor that will be recognized and loaded by the graphics editor in; ROOT, i.e. it will be included as a part of it. (a) Derive the code of your object editor from the base editor class; **`TGedFrame`**. (b) Keep the correct naming convention: the name of the object editor; should be the object class name concatenated with the word `‘Editor'`. (c) Provide a default constructor. (d) Use the signals/slots communication mechanism for event processing. (e) Implement the virtual method `SetModel(TObject *obj)` where all; widgets are set with the current object's attributes. This method is; called when the editor receives a signal from the canvas saying that an; object is the selected. (f) Implement all necessary slots and connect them to appropriate; signals that GUI widgets send out. The GUI classes in ROOT are developed; to emit signals whenever they change a state that others might be; interested. As we noted already, the signals/slots communication; mechanism allows total independence of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:104431,load,loaded,104431,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['load'],['loaded']
Performance," the one found in the executable name. The following canonical driver names are used:. - ``clang`` for the ``gcc`` driver (used to compile C programs); - ``clang++`` for the ``gxx`` driver (used to compile C++ programs); - ``clang-cpp`` for the ``cpp`` driver (pure preprocessor); - ``clang-cl`` for the ``cl`` driver; - ``flang`` for the ``flang`` driver; - ``clang-dxc`` for the ``dxc`` driver. For example, when calling ``x86_64-pc-linux-gnu-clang-g++``,; the driver will first attempt to use the configuration file named::. x86_64-pc-linux-gnu-clang++.cfg. If this file is not found, it will attempt to use the name found; in the executable instead::. x86_64-pc-linux-gnu-clang-g++.cfg. Note that options such as ``--driver-mode=``, ``--target=``, ``-m32`` affect; the search algorithm. For example, the aforementioned executable called with; ``-m32`` argument will instead search for::. i386-pc-linux-gnu-clang++.cfg. If none of the aforementioned files are found, the driver will instead search; for separate driver and target configuration files and attempt to load both.; The former is named ``<driver>.cfg`` while the latter is named; ``<triple>.cfg``. Similarly to the previous variants, the canonical driver name; will be preferred, and the compiler will fall back to the actual name. For example, ``x86_64-pc-linux-gnu-clang-g++`` will attempt to load two; configuration files named respectively::. clang++.cfg; x86_64-pc-linux-gnu.cfg. with fallback to trying::. clang-g++.cfg; x86_64-pc-linux-gnu.cfg. It is not an error if either of these files is not found. The configuration file consists of command-line options specified on one or; more lines. Lines composed of whitespace characters only are ignored as well as; lines in which the first non-blank character is ``#``. Long options may be split; between several lines by a trailing backslash. Here is example of a; configuration file:. ::. # Several options on line; -c --target=x86_64-unknown-linux-gnu. # Long option split between",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:33549,load,load,33549,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['load'],['load']
Performance," the output NaN is definitely quiet. Signaling NaN outputs can only occur if they; are provided as an input value. For example, ""fmul SNaN, 1.0"" may be simplified; to SNaN rather than QNaN. Similarly, if all input NaNs are preferred (or if; there are no input NaNs) and the target does not have any ""extra"" NaN payloads,; then the output NaN is guaranteed to be preferred. Floating-point math operations are allowed to treat all NaNs as if they were; quiet NaNs. For example, ""pow(1.0, SNaN)"" may be simplified to 1.0. Code that requires different behavior than this should use the; :ref:`Constrained Floating-Point Intrinsics <constrainedfp>`.; In particular, constrained intrinsics rule out the ""Unchanged NaN propagation""; case; they are guaranteed to return a QNaN. Unfortunately, due to hard-or-impossible-to-fix issues, LLVM violates its own; specification on some architectures:. - x86-32 without SSE2 enabled may convert floating-point values to x86_fp80 and; back when performing floating-point math operations; this can lead to results; with different precision than expected and it can alter NaN values. Since; optimizations can make contradicting assumptions, this can lead to arbitrary; miscompilations. See `issue #44218; <https://github.com/llvm/llvm-project/issues/44218>`_.; - x86-32 (even with SSE2 enabled) may implicitly perform such a conversion on; values returned from a function for some calling conventions. See `issue; #66803 <https://github.com/llvm/llvm-project/issues/66803>`_.; - Older MIPS versions use the opposite polarity for the quiet/signaling bit, and; LLVM does not correctly represent this. See `issue #60796; <https://github.com/llvm/llvm-project/issues/60796>`_. .. _fastmath:. Fast-Math Flags; ---------------. LLVM IR floating-point operations (:ref:`fneg <i_fneg>`, :ref:`fadd <i_fadd>`,; :ref:`fsub <i_fsub>`, :ref:`fmul <i_fmul>`, :ref:`fdiv <i_fdiv>`,; :ref:`frem <i_frem>`, :ref:`fcmp <i_fcmp>`), :ref:`phi <i_phi>`,; :ref:`select <i_select>` and :ref:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:160695,perform,performing,160695,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performing']
Performance," the program; state (if any). This should be minimized as much as possible. More detail about; implementing custom program state is given in section Custom Program States.; ; Checker Registration; All checker implementation files are located in; clang/lib/StaticAnalyzer/Checkers folder. The steps below describe; how the checker SimpleStreamChecker, which checks for misuses of; stream APIs, was registered with the analyzer.; Similar steps should be followed for a new checker. A new checker implementation file, SimpleStreamChecker.cpp, was; created in the directory lib/StaticAnalyzer/Checkers.; The following registration code was added to the implementation file:. void ento::registerSimpleStreamChecker(CheckerManager &mgr) {; mgr.registerChecker<SimpleStreamChecker>();; }. A package was selected for the checker and the checker was defined in the; table of checkers at include/clang/StaticAnalyzer/Checkers/Checkers.td.; Since all checkers should first be developed as ""alpha"", and the SimpleStreamChecker; performs UNIX API checks, the correct package is ""alpha.unix"", and the following; was added to the corresponding UnixAlpha section of Checkers.td:. let ParentPackage = UnixAlpha in {; ...; def SimpleStreamChecker : Checker<""SimpleStream"">,; HelpText<""Check for misuses of stream APIs"">,; DescFile<""SimpleStreamChecker.cpp"">;; ...; } // end ""alpha.unix"". The source code file was made visible to CMake by adding it to; lib/StaticAnalyzer/Checkers/CMakeLists.txt. After adding a new checker to the analyzer, one can verify that the new checker; was successfully added by seeing if it appears in the list of available checkers:; $clang -cc1 -analyzer-checker-help; Events, Callbacks, and Checker Class Structure; All checkers inherit from the ; Checker template class; the template parameter(s) describe the type of; events that the checker is interested in processing. The various types of events; that are available are described in the file ; CheckerDocumentation.cpp; For each event ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html:9404,perform,performs,9404,interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html,1,['perform'],['performs']
Performance," the proposed step is; negative. In this case, one can subsequently call; TGeoManager::ComputeNormalFast() to get the normal vector to the; crossed surface, after propagating the current point with the; TGeoManager::GetStep() value. This propagation can be done like:. ~~~{.cpp}; Double_t *current_point = gGeoManager->GetCurrentPoint();; Double_t *current_dir = gGeoManager->GetCurrentDirection();; for (Int_t i=0; i<3; i++); current_point[i] += step * current_dir[I];; ~~~. Note: The method TGeoManager::FindNextBoundary() does not modify the; current point/direction nor the current volume/state. The returned node; is the next crossed one, but the physical path (state) AFTER crossing; the boundary is not determined. In order to find out this new state, one; has to propagate the point with a distance slightly bigger that the; computed step value (which is accurate within numerical precision). A; method that performs this task finding the next location is; TGeoManager::Step(), described in ""Making a Step"", but users may; implement more precise methods to insure post-step boundary crossing. \anchor GP08; ## Geometry Graphical User Interface. The geombuilder package allows you to create and edit geometries. The; package provides a library of all GUI classes related to geometry. Each; editable geometry class `TGeoXXX` have a correspondent editor; `TGeoXXXEditor` that provides a graphics user interface allowing to; edit some (or all) parameters of a geometry object. The editable objects; are geometry manager, volumes, nodes, shapes, media, materials and; matrices. The interfaces provide also access to specific functionality; of geometry objects. The editing mechanism is based on ROOT GED; (Graphics Editors) functionality and the library is loaded using the; plug-in mechanism. \anchor GP08a; ### Editing a Geometry. There are two different use cases having different ways of invoking the; geometry editors. The first one applies when starting with geometry from; scratch and using",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:125527,perform,performs,125527,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performs']
Performance," the same underlying type as the element of the returned vector. The second operand, mask, is a vector of boolean values with the same number of elements as the return type. The third is a pass-through value that is used to fill the masked-off lanes of the result. The return type and the type of the '``passthru``' operand have the same vector type. Semantics:; """""""""""""""""""". The '``llvm.masked.expandload``' intrinsic is designed for reading multiple scalar values from adjacent memory addresses into possibly non-adjacent vector lanes. It is useful for targets that support vector expanding loads and allows vectorizing loop with cross-iteration dependency like in the following example:. .. code-block:: c. // In this loop we load from B and spread the elements into array A.; double *A, B; int *C;; for (int i = 0; i < size; ++i) {; if (C[i] != 0); A[i] = B[j++];; }. .. code-block:: llvm. ; Load several elements from array B and expand them in a vector.; ; The number of loaded elements is equal to the number of '1' elements in the Mask.; %Tmp = call <8 x double> @llvm.masked.expandload.v8f64(ptr %Bptr, <8 x i1> %Mask, <8 x double> poison); ; Store the result in A; call void @llvm.masked.store.v8f64.p0(<8 x double> %Tmp, ptr %Aptr, i32 8, <8 x i1> %Mask). ; %Bptr should be increased on each iteration according to the number of '1' elements in the Mask.; %MaskI = bitcast <8 x i1> %Mask to i8; %MaskIPopcnt = call i8 @llvm.ctpop.i8(i8 %MaskI); %MaskI64 = zext i8 %MaskIPopcnt to i64; %BNextInd = add i64 %BInd, %MaskI64. Other targets may support this intrinsic differently, for example, by lowering it into a sequence of conditional scalar load operations and shuffles.; If all mask elements are '1', the intrinsic behavior is equivalent to the regular unmasked vector load. .. _int_compressstore:. '``llvm.masked.compressstore.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. A number of scalar values of integer, floating ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:857346,load,loaded,857346,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance," the scalar epilogue loop into the; main loop. The first operand is the string; ``llvm.loop.vectorize.predicate.enable`` and the second operand is a bit. If; the bit operand value is 1 vectorization is enabled. A value of 0 disables; vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.predicate.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.predicate.enable"", i1 1}. '``llvm.loop.vectorize.scalable.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata selectively enables or disables scalable vectorization for the; loop, and only has any effect if vectorization for the loop is already enabled.; The first operand is the string ``llvm.loop.vectorize.scalable.enable``; and the second operand is a bit. If the bit operand value is 1 scalable; vectorization is enabled, whereas a value of 0 reverts to the default fixed; width vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.scalable.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.scalable.enable"", i1 1}. '``llvm.loop.vectorize.width``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata sets the target width of the vectorizer. The first; operand is the string ``llvm.loop.vectorize.width`` and the second; operand is an integer specifying the width. For example:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.width"", i32 4}. Note that setting ``llvm.loop.vectorize.width`` to 1 disables; vectorization of the loop. If ``llvm.loop.vectorize.width`` is set to; 0 or if the loop does not have this metadata the width will be; determined automatically. '``llvm.loop.vectorize.followup_vectorized``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata defines which loop attributes the vectorized loop will; have. See :ref:`transformation-metadata` for details. '``llvm.loop.vectorize.followup_epilogue``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata defines which loop attributes the epilogue will have. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:299058,scalab,scalable,299058,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance," the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return value's second; element is true, the following rules apply to the first element:. - If the given pointer is associated with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function pointer that would have been loaded from an arbitrarily chosen; (through an unspecified mechanism) pointer associated with the type; metadata. 2. If the function has a non-void return type, a pointer to a function that; returns an unspecified value without causing side effects. If the function's return value's second element is false, the value of the; first element is undefined. .. _type.checked.load.relative:. '``llvm.type.checked.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load.relative(ptr %ptr, i32 %offset, metadata %type) argmemonly nounwind readonly. Overview:; """""""""""""""""". The ``llvm.type.checked.load.relative`` intrinsic loads a relative pointer to a; function from a virtual table pointer using metadata. Otherwise, its semantic is; identical to the ``llvm.type.checked.load`` intrinsic. A relative pointer is a pointer to an offset to the pointed to value. The; address of the underlying pointer of the relative pointer is obtained by adding; the offset to the address of the offset value. '``llvm.arithmetic.fence``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.arithmetic.fence(<type> <op>). Overview:; """""""""""""""""". The purpose of the ``llvm.arithmetic.fence`` intrinsic; is to prevent the optimizer from performing fast-math optimizations,; par",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:939768,load,load,939768,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance," the simulation:. Instruction Dependency Information; +----< 2. vhaddps %xmm3, %xmm3, %xmm4; |; | < loop carried >; |; | 0. vmulps %xmm0, %xmm1, %xmm2; +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ]; +----> 2. vhaddps %xmm3, %xmm3, %xmm4 ## REGISTER dependency: %xmm3; |; | < loop carried >; |; +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RESOURCE interference: JFPA [ probability: 74% ]. According to the analysis, throughput is limited by resource pressure and not by; data dependencies. The analysis observed increases in backend pressure during; 48.07% of the simulated run. Almost all those pressure increase events were; caused by contention on processor resources JFPA/JFPU0. The `critical sequence` is the most expensive sequence of instructions according; to the simulation. It is annotated to provide extra information about critical; register dependencies and resource interferences between instructions. Instructions from the critical sequence are expected to significantly impact; performance. By construction, the accuracy of this analysis is strongly; dependent on the simulation and (as always) by the quality of the processor; model in llvm. Bottleneck analysis is currently not supported for processors with an in-order; backend. Extra Statistics to Further Diagnose Performance Issues; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; The ``-all-stats`` command line option enables extra statistics and performance; counters for the dispatch logic, the reorder buffer, the retire control unit,; and the register file. Below is an example of ``-all-stats`` output generated by :program:`llvm-mca`; for 300 iterations of the dot-product example discussed in the previous; sections. .. code-block:: none. Dynamic Dispatch Stall Cycles:; RAT - Register unavailable: 0; RCU - Retire tokens unavailable: 0; SCHEDQ - Scheduler full: 272 (44.6%); LQ - Load queue full: 0; SQ - Store queue full: 0; GROUP - Static restrictions on the dispatch ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:28287,perform,performance,28287,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance," the sort order to use the 'entry number' when the seek position are equal.; Consequently the default sort order for an older in-memory TTree is now; essentially kSortBasketsByEntry rather than kSortBasketsByBranch (old 'correct' sort; order) or 'random' (the 'broken' sort order prior to this release). IMPORTANT enhancement in TTree::Fill:; Slides from a recent seminar describing the main features of ROOT IO and Trees and the recent; improvements described below are available at; http://root.cern/files/brun_lcgapp09.pptx ; or; http://root.cern/files/brun_lcgapp09.pdf .; The baskets are flushed and the Tree header saved at regular intervals (See AutoFlush and OptimizeBaskets); When the amount of data written so far (fTotBytes) is greater than fAutoFlush (see SetAutoFlush) all the baskets are flushed to disk.; This makes future reading faster as it guarantees that baskets belonging to nearby entries will be on the same disk region.; When the first call to flush the baskets happens, we also take this opportunity to optimize the baskets buffers.; We also check if the number of bytes written is greater than fAutoSave (see SetAutoSave).; In this case we also write the Tree header. This makes the Tree recoverable up to this point in case the program writing the Tree crashes.; Note that the user can also decide to call FlushBaskets and AutoSave in her event loop on the base of the number of events written instead of the number of bytes written.; New function TTree::OptimizeBaskets. void TTree::OptimizeBaskets(Int_t maxMemory, Float_t minComp, Option_t *option). This function may be called after having filled some entries in a Tree; using the information in the existing branch buffers, it will reassign; new branch buffer sizes to optimize time and memory.; The function computes the best values for branch buffer sizes such that; the total buffer sizes is less than maxMemory and nearby entries written; at the same time.; In case the branch compression factor for the data writt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:3865,optimiz,optimize,3865,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,1,['optimiz'],['optimize']
Performance," the symbol address assignments made for this graph.; In ORC this is used to notify any pending queries for *resolved* symbols,; including pending queries from concurrently running JITLink instances that; have reached the next step and are waiting on the address of a symbol in; this graph to proceed with their link. #. Identify external symbols and resolve their addresses asynchronously. Calls the ``JITLinkContext`` to resolve the target address of any external; symbols in the graph. #. Phase 3. #. Apply external symbol resolution results. This updates the addresses of all external symbols. At this point all; nodes in the graph have their final target addresses, however node; content still points back to the original data in the object file. #. Run pre-fixup passes. These passes are called on the graph after all nodes have been assigned; their final target addresses, but before node content is copied into; working memory and fixed up. Passes run at this stage can make late; optimizations to the graph and content based on address layout. Notable use cases: GOT and PLT relaxation, where GOT and PLT accesses are; bypassed for fixup targets that are directly accessible under the assigned; memory layout. #. Copy block content to working memory and apply fixups. Copies all block content into allocated working memory (following the; target layout) and applies fixups. Graph blocks are updated to point at; the fixed up content. #. Run post-fixup passes. These passes are called on the graph after fixups have been applied and; blocks updated to point to the fixed up content. Post-fixup passes can inspect blocks contents to see the exact bytes that; will be copied to the assigned target addresses. #. Finalize memory asynchronously. Calls the ``JITLinkMemoryManager`` to copy working memory to the executor; process and apply the requested permissions. #. Phase 3. #. Notify the context that the graph has been emitted. Calls ``JITLinkContext::notifyFinalized`` and hands off the; ``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:21362,optimiz,optimizations,21362,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['optimiz'],['optimizations']
Performance," the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - H",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:301705,load,load,301705,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10964,optimiz,optimization,10964,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['optimiz'],['optimization']
Performance," the; execution known as 'safepoints' For most collectors, it is sufficient; to track at least one copy of each unique pointer value. However, for; a collector which wishes to relocate objects directly reachable from; running code, a higher standard is required. One additional challenge is that the compiler may compute intermediate; results (""derived pointers"") which point outside of the allocation or; even into the middle of another allocation. The eventual use of this; intermediate value must yield an address within the bounds of the; allocation, but such ""exterior derived pointers"" may be visible to the; collector. Given this, a garbage collector can not safely rely on the; runtime value of an address to indicate the object it is associated; with. If the garbage collector wishes to move any object, the; compiler must provide a mapping, for each pointer, to an indication of; its allocation. To simplify the interaction between a collector and the compiled code,; most garbage collectors are organized in terms of three abstractions:; load barriers, store barriers, and safepoints. #. A load barrier is a bit of code executed immediately after the; machine load instruction, but before any use of the value loaded.; Depending on the collector, such a barrier may be needed for all; loads, merely loads of a particular type (in the original source; language), or none at all. #. Analogously, a store barrier is a code fragment that runs; immediately before the machine store instruction, but after the; computation of the value stored. The most common use of a store; barrier is to update a 'card table' in a generational garbage; collector. #. A safepoint is a location at which pointers visible to the compiled; code (i.e. currently in registers or on the stack) are allowed to; change. After the safepoint completes, the actual pointer value; may differ, but the 'object' (as seen by the source language); pointed to will not. Note that the term 'safepoint' is somewhat overloaded. It",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:2527,load,load,2527,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['load'],['load']
Performance," the; linker via object files. The list is encoded in the IR using named metadata with the name; ``!llvm.dependent-libraries``. Each operand is expected to be a metadata node; which should contain a single string operand. For example, the following metadata section contains two library specifiers::. !0 = !{!""a library specifier""}; !1 = !{!""another library specifier""}; !llvm.dependent-libraries = !{ !0, !1 }. Each library specifier will be handled independently by the consuming linker.; The effect of the library specifiers are defined by the consuming linker. .. _summary:. ThinLTO Summary; ===============. Compiling with `ThinLTO <https://clang.llvm.org/docs/ThinLTO.html>`_; causes the building of a compact summary of the module that is emitted into; the bitcode. The summary is emitted into the LLVM assembly and identified; in syntax by a caret ('``^``'). The summary is parsed into a bitcode output, along with the Module; IR, via the ""``llvm-as``"" tool. Tools that parse the Module IR for the purposes; of optimization (e.g. ""``clang -x ir``"" and ""``opt``""), will ignore the; summary entries (just as they currently ignore summary entries in a bitcode; input file). Eventually, the summary will be parsed into a ModuleSummaryIndex object under; the same conditions where summary index is currently built from bitcode.; Specifically, tools that test the Thin Link portion of a ThinLTO compile; (i.e. llvm-lto and llvm-lto2), or when parsing a combined index; for a distributed ThinLTO backend via clang's ""``-fthinlto-index=<>``"" flag; (this part is not yet implemented, use llvm-as to create a bitcode object; before feeding into thin link tools for now). There are currently 3 types of summary entries in the LLVM assembly:; :ref:`module paths<module_path_summary>`,; :ref:`global values<gv_summary>`, and; :ref:`type identifiers<typeid_summary>`. .. _module_path_summary:. Module Path Summary Entry; -------------------------. Each module path summary entry lists a module containing gl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:339066,optimiz,optimization,339066,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance," the; transition between local and remote files fixed in 5.24/00 . Enable mass storage domain settings when working with; TChain's; in multi-master mode. The Mass Storage Domain must be specified as; option in the URL.              ; chain.AddFile(""root:// .....?msd=CERN"").  and the string must match the value specified in defining the; submaster node.; Improved performance monitoring: the 'Rate plot' button; in the dialog box has been renamed 'Performance Plot' and now shows up; to 4 plots as a function of the processing time:. Instantaneous processing rate, which is now better; estimated by a better estimation of the normalizing times; Average read chunck size, defined as; TFile::GetFileBytesRead() / TFile::GetFileReadCalls() during the last; unit of time; this allows to monitor the usage of the cache; this plot; is present only if some I/O is done, i.e. not for pure CPU tasks.; The number of active workers; The number of total and effecive sessions running; concurrently on the cluster (started by the same daemon); this plot is; present only is the number is at least onec different from 1. If enabled, send monitoring information from the master; at each GetNextPacket (at each call of TPerfStat::PacketEvent) to allow; extrnal real-time progress monitoring.; Save the status of a 'proofserv' session into a new file; in the 'activesessions' area. The full path of the new file is;          ; <admin_path>/.xproofd.<port>/activesessions/<user>.<group>.<pid>.status. The status indicates whether the session is idle, running or queued.; The status is updated every 'checkfq' secs (see xpd.proofservmgr;; default 30 s). The status is dumped by the reader thread of TXProofServ; and therefore its r/w access is protected. Enable the use of the tree cache also for local files,; adapting the default settings for the cache to the recent changes; In the XrdProofd plug-in. Improve synchronization between parent and child during; fork; Optimize loops over directory entries; Improve err",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:5224,concurren,concurrently,5224,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,1,['concurren'],['concurrently']
Performance," their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:281397,load,load,281397,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25981,perform,performs,25981,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performs']
Performance," these compilers requires a lot of options. To simplify the; configuration the Apple Clang build settings are contained in CMake Cache files.; You can build an Apple Clang compiler using the following commands:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/Apple-stage1.cmake <path to source>/llvm; $ ninja stage2-distribution. This CMake invocation configures the stage1 host compiler, and sets; CLANG_BOOTSTRAP_CMAKE_ARGS to pass the Apple-stage2.cmake cache script to the; stage2 configuration step. When you build the stage2-distribution target it builds the minimal stage1; compiler and required tools, then configures and builds the stage2 compiler; based on the settings in Apple-stage2.cmake. This pattern of using cache scripts to set complex settings, and specifically to; make later stage builds include cache scripts is common in our more advanced; build configurations. Multi-stage PGO; ===============. Profile-Guided Optimizations (PGO) is a really great way to optimize the code; clang generates. Our multi-stage PGO builds are a workflow for generating PGO; profiles that can be used to optimize clang. At a high level, the way PGO works is that you build an instrumented compiler,; then you run the instrumented compiler against sample source files. While the; instrumented compiler runs it will output a bunch of files containing; performance counters (.profraw files). After generating all the profraw files; you use llvm-profdata to merge the files into a single profdata file that you; can feed into the LLVM_PROFDATA_FILE option. Our PGO.cmake cache automates that whole process. You can use it for; configuration with CMake with the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; <path to source>/llvm. There are several additional options that the cache file also accepts to modify; the build, particularly the PGO_INSTRUMENT_LTO option. Setting this option to; Thin or Fu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:4798,optimiz,optimize,4798,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimize']
Performance," this ID through directly to the stack map; record without checking uniqueness. LLVM guarantees a shadow of instructions following the stack map's; instruction offset during which neither the end of the basic block nor; another call to ``llvm.experimental.stackmap`` or; ``llvm.experimental.patchpoint`` may occur. This allows the runtime to; patch the code at this point in response to an event triggered from; outside the code. The code for instructions following the stack map; may be emitted in the stack map's shadow, and these instructions may; be overwritten by destructive patching. Without shadow bytes, this; destructive patching could overwrite program text or data outside the; current function. We disallow overlapping stack map shadows so that; the runtime does not need to consider this corner case. For example, a stack map with 8 byte shadow:. .. code-block:: llvm. call void @runtime(); call void (i64, i32, ...) @llvm.experimental.stackmap(i64 77, i32 8,; ptr %ptr); %val = load i64, ptr %ptr; %add = add i64 %val, 3; ret i64 %add. May require one byte of nop-padding:. .. code-block:: none. 0x00 callq _runtime; 0x05 nop <--- stack map address; 0x06 movq (%rdi), %rax; 0x07 addq $3, %rax; 0x0a popq %rdx; 0x0b ret <---- end of 8-byte shadow. Now, if the runtime needs to invalidate the compiled code, it may; patch 8 bytes of code at the stack map's address at follows:. .. code-block:: none. 0x00 callq _runtime; 0x05 movl $0xffff, %rax <--- patched code at stack map address; 0x0a callq *%rax <---- end of 8-byte shadow. This way, after the normal call to the runtime returns, the code will; execute a patched call to a special entry point that can rebuild a; stack frame from the values located by the stack map. '``llvm.experimental.patchpoint.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void; @llvm.experimental.patchpoint.void(i64 <id>, i32 <numBytes>,; ptr <target>, i32 <numArgs>, ...); declare i64; @llvm.experimental.pat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:6581,load,load,6581,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['load'],['load']
Performance," this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:24240,load,loader,24240,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['load'],['loader']
Performance," this is a very straight forward and literal translation: exactly; what we want for zero cost (when unused) exception handling. Especially on; platforms with many registers (ie, the IA64) setjmp/longjmp style exception; handling is *very* impractical. Also, the ""with"" clauses describe the ; control flow paths explicitly so that analysis is not adversly effected. The foo/barCleanup labels are implemented as:. TryCleanup: // Executed if an exception escapes the try block ; c->~C(); barCleanup: // Executed if an exception escapes from bar(); // fall through; fooCleanup: // Executed if an exception escapes from foo(); b->~B(); a->~A(); Exception *E = getThreadLocalException(); call throw(E) // Implemented by the C++ runtime, described below. Which does the work one would expect. getThreadLocalException is a function; implemented by the C++ support library. It returns the current exception ; object for the current thread. Note that we do not attempt to recycle the ; shutdown code from before, because performance of the mainline code is ; critically important. Also, obviously fooCleanup and barCleanup may be ; merged and one of them eliminated. This just shows how the code generator ; would most likely emit code. The bazCleanup label is more interesting. Because the exception may be caught; by the try block, we must dispatch to its handler... but it does not exist; on the call stack (it does not have a VM Call->Label mapping installed), so ; we must dispatch statically with a goto. The bazHandler thus appears as:. bazHandler:; d->~D(); // destruct D as it goes out of scope when entering catch clauses; goto TryHandler. In general, TryHandler is not the same as bazHandler, because multiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:4407,perform,performance,4407,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,1,['perform'],['performance']
Performance," three arguments are the same as they are in the :ref:`@llvm.memcpy <int_memcpy>`; intrinsic, with the added constraint that ``len`` is required to be a positive integer; multiple of the ``element_size``. If ``len`` is not a positive integer multiple of; ``element_size``, then the behaviour of the intrinsic is undefined. ``element_size`` must be a compile-time constant positive power of two no greater than; target-specific atomic access size limit. For each of the input pointers ``align`` parameter attribute must be specified. It; must be a power of two no less than the ``element_size``. Caller guarantees that; both the source and destination pointers are aligned to that boundary. Semantics:; """""""""""""""""""". The '``llvm.memcpy.element.unordered.atomic.*``' intrinsic copies ``len`` bytes of; memory from the source location to the destination location. These locations are not; allowed to overlap. The memory copy is performed as a sequence of load/store operations; where each access is guaranteed to be a multiple of ``element_size`` bytes wide and; aligned at an ``element_size`` boundary. The order of the copy is unspecified. The same value may be read from the source; buffer many times, but only one write is issued to the destination buffer per; element. It is well defined to have concurrent reads and writes to both source and; destination provided those reads and writes are unordered atomic when specified. This intrinsic does not provide any additional ordering guarantees over those; provided by a set of unordered loads from the source location and stores to the; destination. Lowering:; """""""""""""""""". In the most general case call to the '``llvm.memcpy.element.unordered.atomic.*``' is; lowered to a call to the symbol ``__llvm_memcpy_element_unordered_atomic_*``. Where '*'; is replaced with an actual element size. See :ref:`RewriteStatepointsForGC intrinsic; lowering <RewriteStatepointsForGC_intrinsic_lowering>` for details on GC specific; lowering. Optimizer is allowed to inl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:958996,perform,performed,958996,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['load', 'perform']","['load', 'performed']"
Performance," thus loaded at start-up. Plugins are defined by a base class (e.g.; **`TFile`**) that will be implemented in a plugin, a tag used to; identify the plugin (e.g. `^rfio:` as part of the protocol string),; the plugin class of which an object will be created; (e.g. **`TRFIOFile`**), the library to be loaded (in short; `libRFIO.so` to RFIO), and the constructor to be called (e.g.; ""`TRFIOFile()`""). This can be specified in the `.rootrc` which already; contains many plugin definitions, or by calls to; `gROOT->GetPluginManager()->AddHandler()`. #### Library AutoLoading. When using a class in Cling, e.g. in an interpreted source file, ROOT; will automatically load the library that defines this class. On; start-up, ROOT parses all files ending on `.rootmap` rootmap that are; in one of the `$LD_LIBRARY_PATH` (or `$DYLD_LIBRARY_PATH` for `MacOS`,; or `$PATH` for `Windows`). They contain class names and the library; names that the class depends on. After reading them, ROOT knows which; classes are available, and which libraries to load for them. When `TSystem::Load(""ALib"")` is called, ROOT uses this information to; determine which libraries `libALib.so` depends on. It will load these; libraries first. Otherwise, loading the requested library could cause; a system (dynamic loader) error due to unresolved symbols. ### \$ROOTSYS/tutorials. tutorials The tutorials directory contains many example example; scripts. They assume some basic knowledge of ROOT, and for the new; user we recommend reading the chapters: ""Histograms"" and; ""Input/Output"" before trying the examples. The more experienced user; can jump to chapter ""The Tutorials and Tests"" to find more explicit; and specific information about how to build and run the examples. The `$ROOTSYS/tutorials/` directory include the following; sub-directories:. `fft`: Fast Fourier Transform with the fftw package `fit`: Several; examples illustrating minimization/fitting `foam`: Random generator in; multidimensional space `geom`: Examples ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md:20218,load,load,20218,documentation/users-guide/Introduction.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md,1,['load'],['load']
Performance," to 1.; ======================================== ================================================. .. _amdgpu_synid_lds:. lds; ~~~. Specifies where to store the result: VGPRs or LDS (VGPRs by default). ======================================== ===========================; Syntax Description; ======================================== ===========================; lds Store the result in LDS.; ======================================== ===========================. .. _amdgpu_synid_nv:. nv; ~~. Specifies if the instruction is operating on non-volatile memory.; By default, memory is volatile. ======================================== ================================================; Syntax Description; ======================================== ================================================; nv Indicates that the instruction operates on; non-volatile memory.; ======================================== ================================================. .. _amdgpu_synid_slc:. slc; ~~~. Controls behavior of L2 cache. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== ================================================; slc Set slc bit to 1.; ======================================== ================================================. .. _amdgpu_synid_tfe:. tfe; ~~~. Controls access to partially resident textures. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== ================================================; tfe Set tfe bit to 1.; ======================================== ================================================. .. _amdgpu_synid_sc0:. sc0; ~~~. For atomic opcodes, this modifier indicates that the instruction returns the value from memory; before the operation. For other opcodes, it is used together with :ref:`sc1<amdgpu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:19109,cache,cache,19109,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['cache'],['cache']
Performance," to a `RooFit` class are passed as; constructor arguments, it can be imported using a `factory; expression`. For the importer, an entry in the; [factory expressions](https://github.com/root-project/root/blob/master/etc/RooFitHS3_wsfactoryexpressions.json); needs to be added as follows:. ``` {.json}; ""<json-key>"": {; ""class"": ""<C++ class name>"",; ""arguments"": [; ""<json-key of constructor argument #1>"",; ""<json-key of constructor argument #2>"",; ...; ]; }; ```. Similarly, for the exporter, an entry in the; [export keys](https://github.com/root-project/root/blob/master/etc/RooFitHS3_wsexportkeys.json); needs to be added as follows:. ``` {.json}; ""<C++ class name>"": {; ""type"": ""<json-key>"",; ""proxies"": {; ""<name of proxy>"": ""<json-key of this element>"",; ""<name of proxy>"": ""<json-key of this element>"",; ...; }; }; ```. If you don't want to edit the central `json` files containing the; factory expressions or export keys, you can also put your custom; export keys or factory expressions into a different json file and load; that using `RooFit::JSONIO::loadExportKeys(const std::string; &fname)` and `RooFit::JSONIO::loadFactoryExpressions(const; std::string &fname)`. If either the importer or the exporter cannot be created with factory; expressions and export keys, you can instead write a custom `C++`; class to perform the import and export for you. ### Writing your own importers and exporters: Custom `C++` code. In order to implement your own importer or exporter, you can inherit; from the corresponding base classes `RooFit::JSONIO::Importer`; or `RooFit::JSONIO::Exporter`, respectively. You can find; [simple examples](https://github.com/root-project/root/blob/master/roofit/hs3/src/JSONFactories_RooFitCore.cxx); as well as; [more complicated ones](https://github.com/root-project/root/blob/master/roofit/hs3/src/JSONFactories_HistFactory.cxx); in `ROOT`. Any importer should take the following form:. ``` {.cpp}; class MyClassFactory : public RooFit::JSONIO::Importer {; public:; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_hs3.md:4424,load,load,4424,roofit/doc/developers/roofit_hs3.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_hs3.md,3,['load'],"['load', 'loadExportKeys', 'loadFactoryExpressions']"
Performance," to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; to local have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:222092,load,load,222092,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance," to connect signals to slots methods. Therefore, all necessary elements for an object-oriented editor design; are in place. The editor complexity can be reduced by splitting it into; discrete units of so-called *`object`* *`editors`*. Any object editor; provides an object specific GUI. The main purpose of the ROOT graphics; editor is the organization of the object editors' appearance and the; task sequence between them. ### Object Editors. Every object editor follows a simple naming convention: to have as a; name the object class name concatenated with ‘*`Editor`*' (e.g. for; **`TGraph`** objects the object editor is **`TGraphEditor`**). Thanks to; the signals/slots communication mechanism and to the method; `DistancetoPrimitive()` that computes a ‘‘distance'' to an object from; the mouse position, it was possible to implement a signal method of the; canvas that says which is the selected object and to which pad it; belongs. Having this information the graphics editor loads the; corresponding object editor and the user interface is ready for use.; This way after a click on ‘axis'—the axis editor is active; a click on a; ‘pad' activates the pad editor, etc. The algorithm in use is simple and is based on the object-oriented; relationship and communication. When the user activates the editor,; according to the selected object **`<obj>`** in the canvas it looks for; a class name **`<obj>Editor`**. For that reason, the correct naming is; very important. If a class with this name is found, the editor verifies; that this class derives from the base editor class **`TGedFrame`**. If; all checks are satisfied, the editor makes an instance of the object; editor. Then, it scans all object base classes searching the; corresponding object editors. When it finds one, it makes an instance of; the base class editor too. Once the object editor is in place, it sets the user interface elements; according to the object's status. After that, it is ready to interact; with the object follo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:102721,load,loads,102721,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,2,['load'],['loads']
Performance," to execute multiple iterations using; vector registers. Note that although this is similar to SIMT execution, the way a client debugger; uses the information is fundamentally different. In SIMT execution the debugger; needs to present the concurrent execution as distinct source language threads; that the user can list and switch focus between. With iteration concurrency; optimizations, such as software pipelining and vectorized SIMD, the debugger; must not present the concurrency as distinct source language threads. Instead,; it must inform the user that multiple loop iterations are executing in parallel; and allow the user to select between them. In general, SIMT execution fixes the number of concurrent executions per target; architecture thread. However, both software pipelining and SIMD vectorization; may vary the number of concurrent iterations for different loops executed by a; single source language thread. It is possible for the compiler to use both SIMT concurrency and iteration; concurrency techniques in the code of a single source language thread. Therefore, a DWARF operation is required to denote the current concurrent; iteration instance, much like ``DW_OP_push_object_address`` denotes the current; object. See ``DW_OP_LLVM_push_iteration`` in; :ref:`amdgpu-dwarf-literal-operations`. In addition, a way is needed for the compiler to communicate how many source; language loop iterations are executing concurrently. See; ``DW_AT_LLVM_iterations`` in :ref:`amdgpu-dwarf-low-level-information`. 2.20 DWARF Operation to Create Runtime Overlay Composite Location Description; -----------------------------------------------------------------------------. It is common in SIMD vectorization for the compiler to generate code that; promotes portions of an array into vector registers. For example, if the; hardware has vector registers with 8 elements, and 8 wide SIMD instructions, the; compiler may vectorize a loop so that is executes 8 iterations concurrently for; each v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:34233,concurren,concurrency,34233,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,2,['concurren'],['concurrency']
Performance," to leak information. While in some cases static; analysis is effective at doing this at scale, in many cases it still relies on; human judgement to evaluate whether code might be vulnerable. Especially for; software systems which receive less detailed scrutiny but remain sensitive to; these attacks, this seems like an impractical security model. We need an; automatic and systematic mitigation strategy. ### Automatic `lfence` on Conditional Edges. A natural way to scale up the existing hand-coded mitigations is simply to; inject an `lfence` instruction into both the target and fallthrough; destinations of every conditional branch. This ensures that no predicate or; bounds check can be bypassed speculatively. However, the performance overhead; of this approach is, simply put, catastrophic. Yet it remains the only truly; ""secure by default"" approach known prior to this effort and serves as the; baseline for performance. One attempt to address the performance overhead of this and make it more; realistic to deploy is [MSVC's /Qspectre; switch](https://blogs.msdn.microsoft.com/vcblog/2018/01/15/spectre-mitigations-in-msvc/).; Their technique is to use static analysis within the compiler to only insert; `lfence` instructions into conditional edges at risk of attack. However,; [initial](https://arstechnica.com/gadgets/2018/02/microsofts-compiler-level-spectre-fix-shows-how-hard-this-problem-will-be-to-solve/); [analysis](https://www.paulkocher.com/doc/MicrosoftCompilerSpectreMitigation.html); has shown that this approach is incomplete and only catches a small and limited; subset of attackable patterns which happen to resemble very closely the initial; proofs of concept. As such, while its performance is acceptable, it does not; appear to be an adequate systematic mitigation. ## Performance Overhead. The performance overhead of this style of comprehensive mitigation is very; high. However, it compares very favorably with previously recommended; approaches such as the `lfence",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:45612,perform,performance,45612,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance," to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4365,perform,performs,4365,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst,2,['perform'],['performs']
Performance," to perform the check; according to the kind. Paired assignment check; =======================. ``-fbounds-safety`` enforces that variables or fields related with the same; external bounds annotation (e.g., ``buf`` and ``count`` related with; ``__counted_by`` in the example below) must be updated side by side within the; same basic block and without side effect in between. .. code-block:: c. typedef struct {; int *__counted_by(count) buf; size_t count;; } sized_buf_t;. void alloc_buf(sized_buf_t *sbuf, sized_t nelems) {; sbuf->buf = (int *)malloc(sizeof(int) * nelems);; sbuf->count = nelems;; }. To implement this rule, the compiler requires a linear representation of; statements to understand the ordering and the adjacency between the two or more; assignments. The Clang CFG is used to implement this analysis as Clang CFG; provides a linear view of statements within each ``CFGBlock`` (Clang; ``CFGBlock`` represents a single basic block in a source-level CFG). Bounds check optimizations; ==========================. In ``-fbounds-safety``, the Clang frontend emits run-time checks for every; memory dereference if the type system or analyses in the frontend couldn’t; verify its bounds safety. The implementation relies on LLVM optimizations to; remove redundant run-time checks. Using this optimization strategy, if the; original source code already has bounds checks, the fewer additional checks; ``-fbounds-safety`` will introduce. The LLVM ``ConstraintElimination`` pass is; design to remove provable redundant checks (please check Florian Hahn’s; presentation in 2021 LLVM Dev Meeting and the implementation to learn more). In; the following example, ``-fbounds-safety`` implicitly adds the redundant bounds; checks that the optimizer can remove:. .. code-block:: c. void fill_array_with_indices(int *__counted_by(count) p, size_t count) {; for (size_t i = 0; i < count; ++i) {; // implicit bounds checks:; // if (p + i < p || p + i + 1 > p + count) trap();; p[i] = i;; }; }. ``Cons",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst:5970,optimiz,optimizations,5970,interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,1,['optimiz'],['optimizations']
Performance," to save a canvas into `file.xml` file; format instead of `file.root`. XML files do not have any advantages; compared to the normal ROOT files, except that the information in these; files can be edited via a normal editor. The main motivation for this; new format is to facilitate the communication with other non ROOT; applications. Currently writing and reading XML files is limited to ROOT; applications. It is our intention to develop a simple reader independent; of the ROOT libraries that could be used as an example for real; applications. The XML format should be used only for small data volumes, typically; histogram files, pictures, geometries, calibrations. The XML file is; built in memory before being dumped to disk. Like for normal ROOT files,; XML files use the same I/O mechanism exploiting the ROOT/Cling; dictionary. Any class having a dictionary can be saved in XML format.; This first implementation does not support subdirectories or trees. The shared library `libRXML.so` may be loaded dynamically via; `gSystem->Load(""libRXML"")`. This library is also automatically loaded by; the plug-in manager as soon a XML file is created. To create an XTM; file, simply specify a filename with an .xml extension when calling; **`TFile`**`::Open`. **`TFile`**`::Open` will recognize that you are trying to; open an XML file and return a **`TXMLFile`** object. When a XML file is; open in write mode, one can use the normal `TObject::Write` to write an; object in the file. ``` {.cpp}; // example of a session saving a histogram to a XML file; TFile *f = TFile::Open(""Example.xml"",""recreate"");; TH1F *h = new TH1F(""h"",""test"",1000,-2,2); h->FillRandom(""gaus"");; h->Write();; delete f;; // example of a session saving a histogram to a XML file; TFile *f = TFile::Open(""Example.xml"");; TH1F *h = (TH1F*)f->Get(""h"");; h->Draw();; ```. The canvas can be saved as a XML file format via File menu / Save or; Save As menu entries. One can do also:. ``` {.cpp}; canvas->Print(""Example.xml"");; ```; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:98094,load,loaded,98094,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,2,['load'],['loaded']
Performance," to simple alias analysis; information, this class exposes Mod/Ref information from those implementations; which can provide it, allowing for powerful analyses and transformations to work; well together. This document contains information necessary to successfully implement this; interface, use it, and to test both sides. It also explains some of the finer; points about what exactly results mean. ``AliasAnalysis`` Class Overview; ================================. The `AliasAnalysis <https://llvm.org/doxygen/classllvm_1_1AliasAnalysis.html>`__; class defines the interface that the various alias analysis implementations; should support. This class exports two important enums: ``AliasResult`` and; ``ModRefResult`` which represent the result of an alias query or a mod/ref; query, respectively. The ``AliasAnalysis`` interface exposes information about memory, represented in; several different ways. In particular, memory objects are represented as a; starting address and size, and function calls are represented as the actual; ``call`` or ``invoke`` instructions that performs the call. The; ``AliasAnalysis`` interface also exposes some helper methods which allow you to; get mod/ref information for arbitrary instructions. All ``AliasAnalysis`` interfaces require that in queries involving multiple; values, values which are not :ref:`constants <constants>` are all; defined within the same function. Representation of Pointers; --------------------------. Most importantly, the ``AliasAnalysis`` class provides several methods which are; used to query whether or not two memory objects alias, whether function calls; can modify or read a memory object, etc. For all of these queries, memory; objects are represented as a pair of their starting address (a symbolic LLVM; ``Value*``) and a static size. Representing memory objects as a starting address and a size is critically; important for correct Alias Analyses. For example, consider this (silly, but; possible) C code:. .. code-block::",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:2364,perform,performs,2364,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,1,['perform'],['performs']
Performance," to:. ```cpp; // ROOT prompt; root [] import Foo.*; // Preload Foo if it is not in the GMI.; root [] S *s; // #1: does not require a definition.; root [] foo::bar *baz1; // #2: does not require a definition.; root [] foo::bar baz2; // #3: requires a definition.; root [] TCanvas* c = new TCanvas(); // #4 requires a definition; ```. Line #4 forces cling to send ROOT a callback that TCanvas in unknown but; the GMI resolves it to module Gpad, loads it and returns the control to cling. ### Performance; This section compares ROOT PCH technology with C++ Modules which is important but; unfair comparison. As we noted earlier, PCH is very efficient, it cannot be; extended to the experiments’ software stacks because of its design constraints.; On the contrary, the C++ Modules can be used in third-party code where the PCH; is not available. The comparisons are to give a good metric when we are ready to switch ROOT to use; C++ Modules by default. However, since it is essentially the same technology,; optimizations of C++ Modules also affect the PCH. We have a few tricks up in; the sleeves to but they come with given trade-offs. #### Preloading of C++ Modules. The main focus for the technology preview was not in performance until recently.; We have invested some resources in optimizations and we would like to show you; (probably outdated) performance results:. * Memory footprint -- mostly due to importing all C++ Modules at startup; we see overhead which depends on the number of preloaded modules. For; ROOT it is between 40-60 MB depending on the concrete configuration.; When the workload increases we notice that the overall memory performance; decreases in number of cases.; * Execution times -- likewise we have an execution overhead. For ; workflows which take ms the slowdown can be 2x. Increasing of the work; to seconds shows 50-60% slowdowns. The performance is dependent on many factors such as configuration of ROOT and; workflow. You can read more at our Intel IPCC-ROOT Showc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:17810,optimiz,optimizations,17810,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['optimiz'],['optimizations']
Performance," to; metadata-symbolic-based regions during scanReachableSymbols() whenever a region; turns out to be reachable. This requires no work on checker side, but it sounds; performance-heavy. Approach (3): We could let checkers maintain the set of active metadata symbols; in the program state (ideally somewhere in the Store, which sounds weird but; causes the smallest amount of layering violations), so that the core knew what; to escape. This puts a stress on the checkers, but with a smart data map it; wouldn't be a problem. Approach (4): We could allow checkers to trigger pointer escapes in arbitrary; moments. If we allow doing this within ``checkPointerEscape`` callback itself, we; would be able to express facts like ""when this region escapes, that metadata; symbol attached to it should also escape"". This sounds like an ultimate freedom,; with maximum stress on the checkers - still not too much stress when we have; smart data maps. I'm personally liking the approach (2) - it should be possible to avoid; performance overhead, and clarity seems nice. **Gabor:**. At this point, I am a bit wondering about two questions. * When should something belong to a checker and when should something belong to the engine?; Sometimes we model library aspects in the engine and model language constructs in checkers. * What is the checker programming model that we are aiming for? Maximum freedom or more easy checker development?. I think if we aim for maximum freedom, we do not need to worry about the; potential stress on checkers, and we can introduce abstractions to mitigate that; later on.; If we want to simplify the API, then maybe it makes more sense to move language; construct modeling to the engine when the checker API is not sufficient instead; of complicating the API. Right now I have no preference or objections between the alternatives but there; are some random thoughts:. * Maybe it would be great to have a guideline how to evolve the analyzer and; follow it, so it can help us to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst:4586,perform,performance,4586,interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,1,['perform'],['performance']
Performance," together, for different possible reasons. One; of these is that the structure has to be replicated in several parts of; the geometry, or it may simply happen that they really represent a; single object, too complex to be described by a primitive shape. Usually handling structures like these can be easily done by positioning; all components in the same container volume, then positioning the; container itself. However, there are many practical cases when defining; such a container is not straightforward or even possible without; generating overlaps with the rest of the geometry. There are few ways; out of this:. - Defining the container for the structure as ""overlapping"" (see also; "" Overlapping Volumes **""**); - Representing the container as a composite shape - the Boolean union; of all components (see also "" Composite Shapes ""); - Using an assembly volume - this will be described in the following. The first two approaches have the disadvantage of penalizing the; navigation performance with a factor increasing more than linear of the; number of components in the structure. The best solution is the third; one because it uses all volume-related navigation optimizations. The; class **`TGeoVolumeAssembly`** represents an assembly volume. Its shape; is represented by **`TGeoShapeAssembly`** class that is the union of all; components. It uses volume voxelization to perform navigation tasks. An assembly volume creates a hierarchical level and it geometrically; insulates the structure from the rest (as a normal volume). Physically,; a point that is INSIDE a **`TGeoShapeAssembly`** is always inside one of; the components, so a **`TGeoVolumeAssembly`** does not need to have a; medium. Due to the self-containment of assemblies, they are very; practical to use when a container is hard to define due to possible; overlaps during positioning. For instance, it is very easy creating; honeycomb structures. A very useful example for creating and using; assemblies can be found at:; <ht",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:89043,perform,performance,89043,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performance']
Performance," tool; <ClangFormat>` with the goal of automatically reformatting C++ sources files; according to configurable style guides. To do so, clang-format uses Clang's; ``Lexer`` to transform an input file into a token stream and then changes all; the whitespace around those tokens. The goal is for clang-format to serve both; as a user tool (ideally with powerful IDE integrations) and as part of other; refactoring tools, e.g. to do a reformatting of all the lines changed during a; renaming. Extra Clang Tools; =================. As various categories of Clang Tools are added to the extra repository,; they'll be tracked here. The focus of this documentation is on the scope; and features of the tools for other tool developers; each tool should; provide its own user-focused documentation. ``clang-tidy``; --------------. `clang-tidy <https://clang.llvm.org/extra/clang-tidy/>`_ is a clang-based C++; linter tool. It provides an extensible framework for building compiler-based; static analyses detecting and fixing bug-prone patterns, performance,; portability and maintainability issues. Ideas for new Tools; ===================. * C++ cast conversion tool. Will convert C-style casts (``(type) value``) to; appropriate C++ cast (``static_cast``, ``const_cast`` or; ``reinterpret_cast``).; * Non-member ``begin()`` and ``end()`` conversion tool. Will convert; ``foo.begin()`` into ``begin(foo)`` and similarly for ``end()``, where; ``foo`` is a standard container. We could also detect similar patterns for; arrays.; * ``tr1`` removal tool. Will migrate source code from using TR1 library; features to C++11 library. For example:. .. code-block:: c++. #include <tr1/unordered_map>; int main(); {; std::tr1::unordered_map <int, int> ma;; std::cout << ma.size () << std::endl;; return 0;; }. should be rewritten to:. .. code-block:: c++. #include <unordered_map>; int main(); {; std::unordered_map <int, int> ma;; std::cout << ma.size () << std::endl;; return 0;; }. * A tool to remove ``auto``. Will ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangTools.rst:4358,perform,performance,4358,interpreter/llvm-project/clang/docs/ClangTools.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangTools.rst,1,['perform'],['performance']
Performance," trees converted; 75 raise - Number of other getelementptr's formed; 138 raise - Number of load/store peepholes; 42 deadtypeelim - Number of unused typenames removed from symtab; 392 funcresolve - Number of varargs functions resolved; 27 globaldce - Number of global variables removed; 2 adce - Number of basic blocks removed; 134 cee - Number of branches revectored; 49 cee - Number of setcc instruction eliminated; 532 gcse - Number of loads removed; 2919 gcse - Number of instructions removed; 86 indvars - Number of canonical indvars added; 87 indvars - Number of aux indvars removed; 25 instcombine - Number of dead inst eliminate; 434 instcombine - Number of insts combined; 248 licm - Number of load insts hoisted; 1298 licm - Number of insts hoisted to a loop pre-header; 3 licm - Number of insts hoisted to multiple loop preds (bad, no loop pre-header); 75 mem2reg - Number of alloca's promoted; 1444 cfgsimplify - Number of blocks simplified. Obviously, with so many optimizations, having a unified framework for this stuff; is very nice. Making your pass fit well into the framework makes it more; maintainable and useful. .. _DebugCounters:. Adding debug counters to aid in debugging your code; ---------------------------------------------------. Sometimes, when writing new passes, or trying to track down bugs, it; is useful to be able to control whether certain things in your pass; happen or not. For example, there are times the minimization tooling; can only easily give you large testcases. You would like to narrow; your bug down to a specific transformation happening or not happening,; automatically, using bisection. This is where debug counters help.; They provide a framework for making parts of your code only execute a; certain number of times. The ``llvm/Support/DebugCounter.h`` (`doxygen; <https://llvm.org/doxygen/DebugCounter_8h_source.html>`__) file; provides a class named ``DebugCounter`` that can be used to create; command line counter options that control execu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:49501,optimiz,optimizations,49501,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['optimiz'],['optimizations']
Performance," trees with and without compression for the following cases:; `vector<THit>`, `vector<THit*>`, `TClonesArray(`**`TObjHit`**`)`; not split `TClonesArray(`**`TObjHit`**`)` split. The next graphs show the two columns on the right which represent the split and; non-split **`TClonesArray`**, are significantly lower than the vectors. The most; significant difference is in reading a file without compression. The file size with compression, write times with and without compression; and the read times with and without compression all favor the; **`TClonesArray`**. ## Impact of Compression on I/O. This benchmark illustrates the pros and cons of the compression option.; We recommend using compression when the time spent in I/O is small; compared to the total processing time. In this case, if the I/O; operation is increased by a factor of 5 it is still a small percentage; of the total time and it may very well save a factor of 10 on disk; space. On the other hand if the time spend on I/O is large, compression; may slow down the program's performance. The standard test program; `$ROOTSYS/test/Event` was used in various configurations with 400; events. The data file contains a **`TTree`**. The program was invoked; with:. ``` {.cpp}; Event 400 comp split; ```. - comp = 0 means: no compression at all.; - comp = 1 means: compress everything if split = 0.; - comp = 1 means: compress only the tree branches with integers if; split = 1.; - comp = 2 means: compress everything if split=1.; - split = 0 : the full event is serialized into one single buffer.; - split = 1 : the event is split into branches. One branch for each; data member of the Event class. The list of tracks (a; **`TClonesArray`**) has the data members of the Track class also; split into individual buffers. These tests were run on Pentium III CPU with 650 MHz. +------------+--------+---------------+---------------+----------------+----------------+; | Event | File | Total Time to | Effective | Total Time to | Total Time to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:136342,perform,performance,136342,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['perform'],['performance']
Performance," trivial copy and move constructors. Better diagnostics for loops that execute 0 times; Fixes a linking issue that prevented the checker from running on OS X v10.6 and earlier; Fixes for misc. crashes and false positives. checker-271; built: February 8, 2013; highlights:. Faster analysis for scan-build xcodebuild when using Xcode 4.6 and higher:; ; scan-build now uses Xcode's built-in interposition mechanism for the static analyzer to provide faster builds while doing static analysis (PCH files are now built).; This change also allows scan-build to have better support for iOS project analysis without having to specifying weird SDK settings to scan-build. Better diagnostics for implicitly-defined member functions in C++.; New warning for malloc/free checker when passing malloc'ed pointer with non-zero offset to free().; Fixes for misc. parser crashes.; Newer than the static analyzer version in Xcode 4.6. checker-270; built: January 4, 2013; highlights:. Major performance enhancements to speed up interprocedural analysis.; Misc. bug fixes. checker-269; built: September 25, 2012; highlights:. Significantly improves interprocedural analysis for Objective-C.; Numerous bug fixes and heuristics to reduce false positives reported; 			over checker-268. checker-268; built: September 11, 2012; highlights:. Adds initial interprocedural analysis support for C++ and Objective-C. This will greatly improve analysis coverage and find deeper bugs in Objective-C and C++ code.; Contains a static analyzer newer than Xcode 4.4. NOTE: this checker build includes a huge number of changes. It has the potential to find many more bugs, but may report new kinds of false positives. We'd like to know about; these, and any other problems you encounter. When you encounter an issue, please file a bug report.; checker-267; built: June 1, 2012; highlights:; Adds basic interprocedural analysis support for blocks.; checker-266; built: May 23, 2012; highlights:; Contains numerous stability fixes over che",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html:6295,perform,performance,6295,interpreter/llvm-project/clang/www/analyzer/release_notes.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html,1,['perform'],['performance']
Performance," two maps. Some map-like containers also support efficient; iteration through the keys in sorted order. Map-like containers are the most; expensive sort, only use them if you need one of these capabilities. * a :ref:`set-like <ds_set>` container if you need to put a bunch of stuff into; a container that automatically eliminates duplicates. Some set-like; containers support efficient iteration through the elements in sorted order.; Set-like containers are more expensive than sequential containers. * a :ref:`sequential <ds_sequential>` container provides the most efficient way; to add elements and keeps track of the order they are added to the collection.; They permit duplicates and support efficient iteration, but do not support; efficient look-up based on a key. * a :ref:`string <ds_string>` container is a specialized sequential container or; reference structure that is used for character or byte arrays. * a :ref:`bit <ds_bit>` container provides an efficient way to store and; perform set operations on sets of numeric id's, while automatically; eliminating duplicates. Bit containers require a maximum of 1 bit for each; identifier you want to store. Once the proper category of container is determined, you can fine tune the; memory use, constant factors, and cache behaviors of access by intelligently; picking a member of the category. Note that constant factors and cache behavior; can be a big deal. If you have a vector that usually only contains a few; elements (but could contain many), for example, it's much better to use; :ref:`SmallVector <dss_smallvector>` than :ref:`vector <dss_vector>`. Doing so; avoids (relatively) expensive malloc/free calls, which dwarf the cost of adding; the elements to the container. .. _ds_sequential:. Sequential Containers (std::vector, std::list, etc); ---------------------------------------------------. There are a variety of sequential containers available for you, based on your; needs. Pick the first in this section that will do what",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:56650,perform,perform,56650,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['perform']
Performance," two; pads on it. Both pads keep histograms updated and filled by three; different threads. With the `CalcPi` example, you should be able to see; two threads calculating Pi with the given number of intervals as; precision. ### TThread in More Details. Cling is not thread safe yet, and it will block the execution of the; threads until it has finished executing. #### Asynchronous Actions. Different threads can work simultaneously with the same object. Some; actions can be dangerous. For example, when two threads create a; histogram object, ROOT allocates memory and puts them to the same; collection. If it happens at the same time, the results are; undetermined. To avoid this problem, the user has to synchronize these; actions with:. ``` {.cpp}; TThread::Lock() // Locking the following part of code; ... // Create an object, etc...; TThread::UnLock() // Unlocking; ```. The code between `Lock()` and `UnLock()` will be performed; uninterrupted. No other threads can perform actions or access; objects/collections while it is being executed. The methods; `TThread::Lock() `and **`TThread::UnLock()`** internally use a global; `TMutex` instance for locking. The user may also define their own **`TMutex`** `MyMutex` instance and may; locally protect their asynchronous actions by calling `MyMutex.Lock()` and; `MyMutex.UnLock().`. #### Synchronous Actions: TCondition. To synchronize the actions of different threads you can use the; **`TCondition`** class, which provides a signaling mechanism. The; **`TCondition`** instance must be accessible by all threads that need to; use it, i.e. it should be a global object (or a member of the class; which owns the threaded methods, see below). To create a; **`TCondition`** object, a **`TMutex`** instance is required for the; Wait and `TimedWait` locking methods. One can pass the address of an; external mutex to the **`TCondition`** constructor:. ``` {.cpp}; TMutex MyMutex;; TCondition MyCondition(&MyMutex);; ```. If zero is passed, **`TConditi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:7117,perform,perform,7117,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['perform'],['perform']
Performance," two; values that will undergo unsigned addition. Semantics:; """""""""""""""""""". The maximum value this operation can clamp to is the largest unsigned value; representable by the bit width of the arguments. Because this is an unsigned; operation, the result will never saturate towards zero. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.uadd.sat.i4(i4 1, i4 2) ; %res = 3; %res = call i4 @llvm.uadd.sat.i4(i4 5, i4 6) ; %res = 11; %res = call i4 @llvm.uadd.sat.i4(i4 8, i4 8) ; %res = 15. '``llvm.ssub.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.ssub.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.ssub.sat.i16(i16 %a, i16 %b); declare i32 @llvm.ssub.sat.i32(i32 %a, i32 %b); declare i64 @llvm.ssub.sat.i64(i64 %a, i64 %b); declare <4 x i32> @llvm.ssub.sat.v4i32(<4 x i32> %a, <4 x i32> %b). Overview; """""""""""""""""". The '``llvm.ssub.sat``' family of intrinsic functions perform signed; saturating subtraction on the 2 arguments. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo signed subtraction. Semantics:; """""""""""""""""""". The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the arguments. The minimum value is the; smallest signed value representable by this bit width. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.ssub.sat.i4(i4 2, i4 1) ; %res = 1; %res = call i4 @llvm.ssub.sat.i4(i4 2, i4 6) ; %res = -4; %res = call i4 @llvm.ssub.sat.i4(i4 -4, i4 5) ; %res = -8; %res = call i4 @llvm.ssub.sat.i4(i4 4, i4 -5) ; %res = 7. '``llvm.usub.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.usub.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.usub.s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:613925,perform,perform,613925,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance," type value that represents the overlay bit size value BS. The; second must be an integral type value that represents the overlay bit offset; value BO. The third must be a location description that represents the; overlay location description OL. The fourth must be a location description; that represents the base location description BL. The DWARF expression is ill-formed if BS or BO are negative values. *rbss(L)* is the minimum remaining bit storage size of L which is defined as; follows. LS is the location storage and LO is the location bit offset; specified by a single location description SL of L. The remaining bit; storage size RBSS of SL is the bit size of LS minus LO. *rbss(L)* is the; minimum RBSS of each single location description SL of L. The DWARF expression is ill-formed if *rbss(BL)* is less than BO plus BS. If BS is 0, then the operation pushes BL. If BO is 0 and BS equals *rbss(BL)*, then the operation pushes OL. Otherwise, the operation is equivalent to performing the following steps to; push a composite location description. *The composite location description is conceptually the base location; description BL with the overlay location description OL positioned as an; overlay starting at the overlay offset BO and covering overlay bit size BS.*. 1. If BO is not 0 then push BL followed by performing the ``DW_OP_bit_piece; BO, 0`` operation.; 2. Push OL followed by performing the ``DW_OP_bit_piece BS, 0`` operation.; 3. If *rbss(BL)* is greater than BO plus BS, push BL followed by performing; the ``DW_OP_bit_piece (rbss(BL) - BO - BS), (BO + BS)`` operation.; 4. Perform the ``DW_OP_LLVM_piece_end`` operation. .. _amdgpu-dwarf-location-list-expressions:. A.2.5.5 DWARF Location List Expressions; +++++++++++++++++++++++++++++++++++++++. .. note::. This section replaces DWARF Version 5 section 2.6.2. *To meet the needs of recent computer architectures and optimization techniques,; debugging information must be able to describe the location of an object who",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:141639,perform,performing,141639,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance," type. The function will be placed such that the; beginning of the prefix data is aligned. This means that if the size; of the prefix data is not a multiple of the alignment size, the; function's entrypoint will not be aligned. If alignment of the; function's entrypoint is desired, padding must be added to the prefix; data. A function may have prefix data but no body. This has similar semantics; to the ``available_externally`` linkage in that the data may be used by the; optimizers but will not be emitted in the object file. .. _prologuedata:. Prologue Data; -------------. The ``prologue`` attribute allows arbitrary code (encoded as bytes) to; be inserted prior to the function body. This can be used for enabling; function hot-patching and instrumentation. To maintain the semantics of ordinary function calls, the prologue data must; have a particular format. Specifically, it must begin with a sequence of; bytes which decode to a sequence of machine instructions, valid for the; module's target, which transfer control to the point immediately succeeding; the prologue data, without performing any other visible action. This allows; the inliner and other passes to reason about the semantics of the function; definition without needing to reason about the prologue data. Obviously this; makes the format of the prologue data highly target dependent. A trivial example of valid prologue data for the x86 architecture is ``i8 144``,; which encodes the ``nop`` instruction:. .. code-block:: text. define void @f() prologue i8 144 { ... }. Generally prologue data can be formed by encoding a relative branch instruction; which skips the metadata, as in this example of valid prologue data for the; x86_64 architecture, where the first two bytes encode ``jmp .+10``:. .. code-block:: text. %0 = type <{ i8, i8, ptr }>. define void @f() prologue %0 <{ i8 235, i8 8, ptr @md}> { ... }. A function may have prologue data but no body. This has similar semantics; to the ``available_externally`` li",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:74499,perform,performing,74499,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performing']
Performance," undefined behavior (e.g.; reading a variable before it is defined). In particular, check to see if the; program is clean under various `sanitizers; <https://github.com/google/sanitizers>`_ (e.g. ``clang; -fsanitize=undefined,address``) and `valgrind <http://valgrind.org/>`_. Many; ""LLVM bugs"" that we have chased down ended up being bugs in the program being; compiled, not LLVM. Once you determine that the program itself is not buggy, you should choose; which code generator you wish to compile the program with (e.g. LLC or the JIT); and optionally a series of LLVM passes to run. For example:. .. code-block:: bash. bugpoint -run-llc [... optzn passes ...] file-to-test.bc --args -- [program arguments]. bugpoint will try to narrow down your list of passes to the one pass that; causes an error, and simplify the bitcode file as much as it can to assist; you. It will print a message letting you know how to reproduce the; resulting error. The :doc:`OptBisect <OptBisect>` page shows an alternative method for finding; incorrect optimization passes. Incorrect code generation; =========================. Similarly to debugging incorrect compilation by mis-behaving passes, you; can debug incorrect code generation by either LLC or the JIT, using; ``bugpoint``. The process ``bugpoint`` follows in this case is to try to; narrow the code down to a function that is miscompiled by one or the other; method, but since for correctness, the entire program must be run,; ``bugpoint`` will compile the code it deems to not be affected with the C; Backend, and then link in the shared object it generates. To debug the JIT:. .. code-block:: bash. bugpoint -run-jit -output=[correct output file] [bitcode file] \; --tool-args -- [arguments to pass to lli] \; --args -- [program arguments]. Similarly, to debug the LLC, one would run:. .. code-block:: bash. bugpoint -run-llc -output=[correct output file] [bitcode file] \; --tool-args -- [arguments to pass to llc] \; --args -- [program arguments]. **Sp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst:9214,optimiz,optimization,9214,interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,1,['optimiz'],['optimization']
Performance," underlying pipelines. The algorithm prioritizes older instructions over younger; instructions. Write-Back and Retire Stage; """"""""""""""""""""""""""""""""""""""""""""""""""""""; Issued instructions are moved from the ReadySet to the IssuedSet. There,; instructions wait until they reach the write-back stage. At that point, they; get removed from the queue and the retire control unit is notified. When instructions are executed, the retire control unit flags the instruction as; ""ready to retire."". Instructions are retired in program order. The register file is notified of the; retirement so that it can free the physical registers that were allocated for; the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; To simulate an out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:39469,load,load,39469,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,3,"['load', 'queue']","['load', 'queue']"
Performance," use of the target hook does not cause any test failures in common; architectures. If the compiler for a target architecture did want some; form of conversion, including a larger result type, it could always; explicitly use the ``DW_OP_convert`` operation. If T is a larger type than the register size, then the default GDB; register hook reads bytes from the next register (or reads out of bounds; for the last register!). Removing use of the target hook does not cause; any test failures in common architectures (except an illegal hand written; assembly test). If a target architecture requires this behavior, these; extensions allow a composite location description to be used to combine; multiple registers. 2. ``DW_OP_deref``. S is the bit size of the generic type divided by 8 (the byte size) and; rounded up to a whole number. DR is the offset of a hypothetical debug; information entry D in the current compilation unit for a base type of the; generic type. The operation is equivalent to performing ``DW_OP_deref_type S, DR``. 3. ``DW_OP_deref_size``. ``DW_OP_deref_size`` has a single 1-byte unsigned integral constant that; represents a byte result size S. TS is the smaller of the generic type bit size and S scaled by 8 (the byte; size). If TS is smaller than the generic type bit size then T is an unsigned; integral type of bit size TS, otherwise T is the generic type. DR is the; offset of a hypothetical debug information entry D in the current; compilation unit for a base type T. .. note::. Truncating the value when S is larger than the generic type matches what; GDB does. This allows the generic type size to not be an integral byte; size. It does allow S to be arbitrarily large. Should S be restricted to; the size of the generic type rounded up to a multiple of 8?. The operation is equivalent to performing ``DW_OP_deref_type S, DR``, except; if T is not the generic type, the value V pushed is zero-extended to the; generic type bit size and its type changed to the generic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:88952,perform,performing,88952,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance," use the command line option `-cachesize SIZE`. `SIZE` shouyld be given in number bytes and can be expressed in 'human readable form' (number followed by size unit like MB, MiB, GB or GiB, etc. or SIZE can be set zero to disable the cache. ### Other Changes. * Update `TChain::LoadTree` so that the user call back routine is actually called for each input file even those containing `TTree` objects with no entries.; * Repair setting the branch address of a leaflist style branch taking directly the address of the struct. (Note that leaflist is nonetheless still deprecated and declaring the struct to the interpreter and passing the object directly to create the branch is much better).; * Provide an implicitly parallel implementation of `TTree::GetEntry`. The approach is based on creating a task per top-level branch in order to do the reading, unzipping and deserialisation in parallel. In addition, a getter and a setter methods are provided to check the status and enable/disable implicit multi-threading for that tree (see Parallelisation section for more information about implicit multi-threading).; * Properly support `std::cin` (and other stream that can not be rewound) in `TTree::ReadStream`. This fixes [ROOT-7588].; * Prevent `TTreeCloner::CopyStreamerInfos()` from causing an autoparse on an abstract base class. ## Histogram Libraries. * TH2Poly has a functional Merge method.; * Implemented the `TGraphAsymmErrors` constructor directly from an ASCII file. ## Math Libraries. * New template class `TRandomGen<Engine>` which derives from `TRandom` and integrate new random generator engines as TRandom classes.; * New TRandom specific types have been defined for these following generators:; * `TRandomMixMax` - recommended MIXMAX generator with N=240; * `TRandomMixMax17` - MIXMAX generator with smaller state (N=17) and faster seeding time; * `TRandomMixMax256` - old version of MIXMAX generator (N=256); * `TRandomMT64` - 64 bit Mersenenne Twister generator from the standard libr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:11006,multi-thread,multi-threading,11006,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,2,['multi-thread'],['multi-threading']
Performance," used in randomised (and bagged) trees. NNodesMax No 0 − deprecated: Use MaxDepth instead to limit the tree size. Configuration options for MVA method :. Configuration options reference for MVA method: Boost. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Boost_Num No 100 − Number of times the classifier is boosted. Boost_MonitorMethod No True − Write monitoring histograms for each boosted classifier. Boost_DetailedMonitoring No False − Produce histograms for detailed boost-wise monitoring. Boost_Type No AdaBoost AdaBoost, Bagging, HighEdgeGauss, HighEdgeCoPara Boosting type for the classifiers. Boost_BaggedSampleFraction No 0.6 − Relative size of bagged event sample to original size of the data sample (used whenever bagging is used). Boost_MethodWeightType No ByError ByError, Average, ByROC, ByOverlap, LastMethod How to set the final weight of the boosted classifiers. Boost_RecalculateMVACut No True − Recalculate the classifier MVA Signallike cut at every boost iteration. Boost_AdaBoostBeta No 1 − The ADA boost parameter that sets the effect of every boost step on the events' weights. Boost_Transform No step step, linear, log, gauss Type of transform applied ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:16535,perform,performance,16535,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,1,['perform'],['performance']
Performance," user must only provide; an implementation of the FCNBase class to M and parameters and; uncertainties in form of std::vector containers. ### Memory allocation and thread safety ###. Differently to the version of M , the version has its own memory manager; (StackAllocator. The user can select between the standard dynamic memory; allocation and deallocation (default) and performance-optimized; stack–like allocation (optional). However, the library is not thread; save using stack–allocation. ### M parameters ###. Differently to the version of M there is no limit on the number of; parameters, variable or non-variable. Memory allocation is done; dynamically according to the actual needs and ""on demand"". There is no; protection against an upper limit on the number of parameters, however; the ""technological"" limitations of M can be seen around a maximum of 15; free parameters at a time. ## Interference with other packages ##. The new M has been designed to interfere as little as possible with; other programs or packages which may be loaded at the same time. M is; thread safe by default. Optionally the user can select a different way; of dynamically allocating memory in the class StackAllacator for M , in; which case (and after an entire recompilation of the whole library) the; thread safety is lost. ## Floating-point precision ##. [install:epsmac]. M is entirely based on double precision. The actual floating point; precision of double precision (32–bit or 64–bit) is platform dependent; and can even vary on the same platform, depending on whether a floating; point number is read from memory a CPU register. The argument of the user's implementation of FCNBase::operator() is; therefore a std:vector$<$double$>$. M expects that the calculations; inside $\mbox{FCN}$ will be performed approximately to the same; accuracy. The accuracy M expects is called *machine precision*; (MnMachinePrecision, see [api:epsmac]) and can be printed on demand; using std::cout. If the user fools M ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:26175,load,loaded,26175,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['load'],['loaded']
Performance," user sets this variable and the; directory contains executables with the expected names, no separate; native versions of those executables will be built. **LLVM_NO_INSTALL_NAME_DIR_FOR_BUILD_TREE**:BOOL; Defaults to ``OFF``. If set to ``ON``, CMake's default logic for library IDs; on Darwin in the build tree will be used. Otherwise the install-time library; IDs will be used in the build tree as well. Mainly useful when other CMake; library ID control variables (e.g., ``CMAKE_INSTALL_NAME_DIR``) are being; set to non-standard values. **LLVM_OPTIMIZED_TABLEGEN**:BOOL; If enabled and building a debug or asserts build the CMake build system will; generate a Release build tree to build a fully optimized tablegen for use; during the build. Enabling this option can significantly speed up build times; especially when building LLVM in Debug configurations. **LLVM_PARALLEL_COMPILE_JOBS**:STRING; Define the maximum number of concurrent compilation jobs. **LLVM_PARALLEL_LINK_JOBS**:STRING; Define the maximum number of concurrent link jobs. **LLVM_RAM_PER_COMPILE_JOB**:STRING; Calculates the amount of Ninja compile jobs according to available resources.; Value has to be in MB, overwrites LLVM_PARALLEL_COMPILE_JOBS. Compile jobs ; will be between one and amount of logical cores. **LLVM_RAM_PER_LINK_JOB**:STRING; Calculates the amount of Ninja link jobs according to available resources.; Value has to be in MB, overwrites LLVM_PARALLEL_LINK_JOBS. Link jobs will ; be between one and amount of logical cores. Link jobs will not run ; exclusively therefore you should add an offset of one or two compile jobs ; to be sure its not terminated in your memory restricted environment. On ELF; platforms also consider ``LLVM_USE_SPLIT_DWARF`` in Debug build. **LLVM_PROFDATA_FILE**:PATH; Path to a profdata file to pass into clang's -fprofile-instr-use flag. This; can only be specified if you're building with clang. **LLVM_REVERSE_ITERATION**:BOOL; If enabled, all supported unordered llvm containe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:32557,concurren,concurrent,32557,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['concurren'],['concurrent']
Performance," uses ROOT's task arena to compress and decompress pages.; That requires writes to be buffered and reads uses the cluster pool resp.; The RNTuple data source for RDataFrame lets RDataFrame full control of the thread pool.; That means that RDataFrame uses a separate data source for every thread, each of the data sources runs in sequential mode. ### Concurrent Readers; Multiple readers can read the same RNTuple concurrently as long as access to every individual reader is sequential. ### Parallel REntry Preparation; Multiple `REntry` object can be concurrently prepared by multiple threads.; I.e., construction and binding of the objects can happen in parallel.; The actual reading and writing of entries (`RNTupleReader::LoadEntry()`, `RNTupleWriter::Fill()`) needs to be protected by a mutex.; This is considered ""mild scalability parallelization"" in RNTuple. ### RNTupleParallelWriter; The parallel writer offers the most scalable parallel writing interface.; Multiple _fill contexts_ can concurrently serialize and compress data.; Every fill context prepares a set of entire clusters in the final on-disk layout.; When a fill context flushes data,; a brief serialization point handles the RNTuple meta-data updates and the reservation of disk space to write into. Low precision float types; --------------------------; RNTuple supports encoding floating point types with a lower precision when writing them to disk. This encoding is specified by the; user per field and it is independent on the in-memory type used for that field (meaning both a `RField<double>` or `RField<float>` can; be mapped to e.g. a low-precision 16 bit float). RNTuple supports the following encodings (all mutually exclusive):. - **Real16**/**SplitReal16**: IEEE-754 half precision float. Set by calling `RField::SetHalfPrecision()`;; - **Real32Trunc**: floating point with less than 32 bits of precision (truncated mantissa).; Set by calling `RField::SetTruncated(n)`, with $10 <= n <= 31$ equal to the total number ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:25551,concurren,concurrently,25551,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['concurren'],['concurrently']
Performance," using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6639,perform,performance,6639,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['perform'],['performance']
Performance," using a retpoline, these edges will need independent protection from; variant #1 style attacks. The analogous approach to that used for conditional; control flow should work:; ```; uintptr_t all_ones_mask = std::numerical_limits<uintptr_t>::max();; uintptr_t all_zeros_mask = 0;; void leak(int data);; void example(int* pointer1, int* pointer2) {; uintptr_t predicate_state = all_ones_mask;; switch (condition) {; case 0:; // Assuming ?: is implemented using branchless logic...; predicate_state = (condition != 0) ? all_zeros_mask : predicate_state;; // ... lots of code ...; //; // Harden the pointer so it can't be loaded; pointer1 &= predicate_state;; leak(*pointer1);; break;. case 1:; predicate_state = (condition != 1) ? all_zeros_mask : predicate_state;; // ... more code ...; //; // Alternative: Harden the loaded value; int value2 = *pointer2 & predicate_state;; leak(value2);; break;. // ...; }; }; ```. The core idea remains the same: validate the control flow using data-flow and; use that validation to check that loads cannot leak information along; misspeculated paths. Typically this involves passing the desired target of such; control flow across the edge and checking that it is correct afterwards. Note; that while it is tempting to think that this mitigates variant #2 attacks, it; does not. Those attacks go to arbitrary gadgets that don't include the checks. ### Variant #1.1 and #1.2 attacks: ""Bounds Check Bypass Store"". Beyond the core variant #1 attack, there are techniques to extend this attack.; The primary technique is known as ""Bounds Check Bypass Store"" and is discussed; in this research paper: https://people.csail.mit.edu/vlk/spectre11.pdf. We will analyze these two variants independently. First, variant #1.1 works by; speculatively storing over the return address after a bounds check bypass. This; speculative store then ends up being used by the CPU during speculative; execution of the return, potentially directing speculative execution to; arbitrary gadg",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:10591,load,loads,10591,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance," usually autodetected, but it can be; configured manually to explicitly control the generation of those targets. **LLVM_ENABLE_LIBCXX**:BOOL; If the host compiler and linker supports the stdlib flag, -stdlib=libc++ is; passed to invocations of both so that the project is built using libc++; instead of stdlibc++. Defaults to OFF. **LLVM_ENABLE_LLVM_LIBC**: BOOL; If the LLVM libc overlay is installed in a location where the host linker; can access it, all built executables will be linked against the LLVM libc; overlay before linking against the system libc. Defaults to OFF. **LLVM_ENABLE_LIBPFM**:BOOL; Enable building with libpfm to support hardware counter measurements in LLVM; tools.; Defaults to ON. **LLVM_ENABLE_LLD**:BOOL; This option is equivalent to `-DLLVM_USE_LINKER=lld`, except during a 2-stage; build where a dependency is added from the first stage to the second ensuring; that lld is built before stage2 begins. **LLVM_ENABLE_LTO**:STRING; Add ``-flto`` or ``-flto=`` flags to the compile and link command; lines, enabling link-time optimization. Possible values are ``Off``,; ``On``, ``Thin`` and ``Full``. Defaults to OFF. **LLVM_ENABLE_MODULES**:BOOL; Compile with `Clang Header Modules; <https://clang.llvm.org/docs/Modules.html>`_. **LLVM_ENABLE_PEDANTIC**:BOOL; Enable pedantic mode. This disables compiler-specific extensions, if; possible. Defaults to ON. **LLVM_ENABLE_PIC**:BOOL; Add the ``-fPIC`` flag to the compiler command-line, if the compiler supports; this flag. Some systems, like Windows, do not need this flag. Defaults to ON. **LLVM_ENABLE_PROJECTS**:STRING; Semicolon-separated list of projects to build, or *all* for building all; (clang, lldb, lld, polly, etc) projects. This flag assumes that projects; are checked out side-by-side and not nested, i.e. clang needs to be in; parallel of llvm instead of nested in `llvm/tools`. This feature allows; to have one build for only LLVM and another for clang+llvm using the same; source checkout.; The full list",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:23434,optimiz,optimization,23434,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['optimiz'],['optimization']
Performance," value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:40763,optimiz,optimization,40763,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimization']
Performance," values.; It is possible to print the list of default control parameters using the `ROOT::Math::IntegratorMultiDimOptions::Print` function.; Example:; ```{.cpp}; ROOT::Math::IntegratorMultiDimOptions opt;; opt.Print();; Integrator Type : ADAPTIVE; Absolute tolerance : 1e-09; Relative tolerance : 1e-09; Workspace size : 100000; (max) function calls : 100000; ```; Depending on the algorithm, some of the control parameters might have no effect. #### `ROOT::Math::AdaptiveIntegratorMultiDim`. This class implements an adaptive quadrature integration method for multi dimensional functions. It is described in this paper; *Genz, A.A. Malik, An adaptive algorithm for numerical integration over an N-dimensional rectangular region, J. Comput. Appl. Math. 6 (1980) 295-302*.; It is part of the *MathCore* library.; The user can control the relative and absolute tolerance and the maximum allowed number of function evaluation. #### `ROOT::Math::GSLMCIntegrator`. It is a class for performing numerical integration of a multidimensional function. It uses the numerical integration algorithms of GSL, which reimplements the algorithms used; in the QUADPACK, a numerical integration package written in Fortran. Plain MC, MISER and VEGAS integration algorithms are supported for integration over finite (hypercubic) ranges.; For a detail description of the GSL methods visit the GSL users guide.; Specific configuration options (documented in the GSL user guide) for the `ROOT::Math::GSLMCIntegration` can be set directly in the class, or when using it via the `ROOT::Math::IntegratorMultiDim`; interface, can be defined using the `ROOT::Math::IntegratorMultiDimOptions`. ## Function Derivation. There are in ROOT only two classes to perform numerical derivation. One of them is in the MathCore library while the other is in the MathMore wrapping an integration function from the GSL library.; * RichardsonDerivator: Implements the Richardson method for numerical integration. It can calculate up to the thir",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:61668,perform,performing,61668,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['performing']
Performance," var); -fprofile-instr-generate=<file_name_pattern>; Generate instrumented code to collect execution counts into the file whose name pattern is specified as the argument; (overridden by LLVM_PROFILE_FILE env var); -fprofile-instr-generate; Generate instrumented code to collect execution counts into default.profraw file; (overridden by '=' form of option or LLVM_PROFILE_FILE env var); -fprofile-instr-use=<value>; Use instrumentation data for coverage testing or profile-guided optimization; -fprofile-use=<value>; Use instrumentation data for profile-guided optimization; -fprofile-remapping-file=<file>; Use the remappings described in <file> to match the profile data against names in the program; -fprofile-list=<file>; Filename defining the list of functions/files to instrument; -fsanitize-address-field-padding=<value>; Level of field padding for AddressSanitizer; -fsanitize-address-globals-dead-stripping; Enable linker dead stripping of globals in AddressSanitizer; -fsanitize-address-poison-custom-array-cookie; Enable poisoning array cookies when using custom operator new[] in AddressSanitizer; -fsanitize-address-use-after-return=<mode>; Select the mode of detecting stack use-after-return in AddressSanitizer: never | runtime (default) | always; -fsanitize-address-use-after-scope; Enable use-after-scope detection in AddressSanitizer; -fsanitize-address-use-odr-indicator; Enable ODR indicator globals to avoid false ODR violation reports in partially sanitized programs at the cost of an increase in binary size; -fsanitize-ignorelist=<value>; Path to ignorelist file for sanitizers; -fsanitize-cfi-cross-dso; Enable control flow integrity (CFI) checks for cross-DSO calls.; -fsanitize-cfi-icall-generalize-pointers; Generalize pointers in CFI indirect call type signature checks; -fsanitize-coverage=<value>; Specify the type of coverage instrumentation for Sanitizers; -fsanitize-hwaddress-abi=<value>; Select the HWAddressSanitizer ABI to target (interceptor or platform, defaul",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:182583,optimiz,optimization,182583,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,['optimiz'],['optimization']
Performance," variable; definitions:; - CMAKE_C_COMPILER=clang; - CMAKE_CXX_COMPILER=clang++; - LLVM_USE_SANITIZE_COVERAGE=YES; - LLVM_USE_SANITIZER=Address; - CLANG_ENABLE_PROTO_FUZZER=ON. Then build the clang-proto-fuzzer and clang-proto-to-cxx targets. Optionally,; you may also build clang-fuzzer with this setup. Example:; cd $LLVM_SOURCE_DIR; mkdir build && cd build; cmake .. -GNinja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ \; -DLLVM_USE_SANITIZE_COVERAGE=YES -DLLVM_USE_SANITIZER=Address \; -DCLANG_ENABLE_PROTO_FUZZER=ON; ninja clang-proto-fuzzer clang-proto-to-cxx. This directory also contains a Dockerfile which sets up all required; dependencies and builds the fuzzers. ============================; Running clang-proto-fuzzer; ============================; bin/clang-proto-fuzzer CORPUS_DIR. Arguments can be specified after -ignore_remaining_args=1 to modify the compiler; invocation. For example, the following command line will fuzz LLVM with a; custom optimization level and target triple:; bin/clang-proto-fuzzer CORPUS_DIR -ignore_remaining_args=1 -O3 -triple \; arm64apple-ios9. To translate a clang-proto-fuzzer corpus output to C++:; bin/clang-proto-to-cxx CORPUS_OUTPUT_FILE. ===================; llvm-proto-fuzzer; ===================; Like, clang-proto-fuzzer, llvm-proto-fuzzer is also a protobuf-mutator based; fuzzer. It receives as input a cxx_loop_proto which it then converts into a; string of valid LLVM IR: a function with either a single loop or two nested; loops. It then creates a new string of IR by running optimization passes over; the original IR. Currently, it only runs a loop-vectorize pass but more passes; can easily be added to the fuzzer. Once there are two versions of the input; function (optimized and not), llvm-proto-fuzzer uses LLVM's JIT Engine to; compile both functions. Lastly, it runs both functions on a suite of inputs and; checks that both functions behave the same on all inputs. In this way,; llvm-proto-fuzzer can find not only compil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt:3866,optimiz,optimization,3866,interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,1,['optimiz'],['optimization']
Performance," variant contains undefined; behavior and will probably crash:. .. code-block:: c++. void foo(const Twine &T);; ...; StringRef X = ...; unsigned i = ...; const Twine &Tmp = X + ""."" + Twine(i);; foo(Tmp);. ... because the temporaries are destroyed before the call. That said, Twine's; are much more efficient than intermediate std::string temporaries, and they work; really well with StringRef. Just be aware of their limitations. .. _dss_smallstring:. llvm/ADT/SmallString.h; ^^^^^^^^^^^^^^^^^^^^^^. SmallString is a subclass of :ref:`SmallVector <dss_smallvector>` that adds some; convenience APIs like += that takes StringRef's. SmallString avoids allocating; memory in the case when the preallocated space is enough to hold its data, and; it calls back to general heap allocation when required. Since it owns its data,; it is very safe to use and supports full mutation of the string. Like SmallVector's, the big downside to SmallString is their sizeof. While they; are optimized for small strings, they themselves are not particularly small.; This means that they work great for temporary scratch buffers on the stack, but; should not generally be put into the heap: it is very rare to see a SmallString; as the member of a frequently-allocated heap data structure or returned; by-value. .. _dss_stdstring:. std::string; ^^^^^^^^^^^. The standard C++ std::string class is a very general class that (like; SmallString) owns its underlying data. sizeof(std::string) is very reasonable; so it can be embedded into heap data structures and returned by-value. On the; other hand, std::string is highly inefficient for inline editing (e.g.; concatenating a bunch of stuff together) and because it is provided by the; standard library, its performance characteristics depend a lot of the host; standard library (e.g. libc++ and MSVC provide a highly optimized string class,; GCC contains a really slow implementation). The major disadvantage of std::string is that almost every operation that makes; them",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:75505,optimiz,optimized,75505,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['optimiz'],['optimized']
Performance," vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = A + B; %valC = fadd float %valA, %valB. ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:. .. code-block:: text. # llc -mcpu=sm_20 kernel.ll -o kernel.ptx. .. note::. If you want to generate 32-bit code, change ``p:64:64:64`` to ``p:32:32:32``; in the module data layout string and use ``nvptx-nvidia-cuda`` as the; target triple. The output we get from ``llc`` (as of LLVM 3.4):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .f32 %f<4>;; .reg .s32 %r<2>;;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:13626,load,load,13626,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,2,['load'],['load']
Performance," vectorization passes; /Qvec Enable the loop vectorization passes; /showFilenames- Don't print the name of each compiled file (default); /showFilenames Print the name of each compiled file; /showIncludes Print info about included files to stderr; /source-charset:<value> Source encoding, supports only UTF-8; /std:<value> Language standard to compile for; /TC Treat all source files as C; /Tc <filename> Specify a C source file; /TP Treat all source files as C++; /Tp <filename> Specify a C++ source file; /utf-8 Set source and runtime encoding to UTF-8 (default); /U <macro> Undefine macro; /vd<value> Control vtordisp placement; /vmb Use a best-case representation method for member pointers; /vmg Use a most-general representation for member pointers; /vmm Set the default most-general representation to multiple inheritance; /vms Set the default most-general representation to single inheritance; /vmv Set the default most-general representation to virtual inheritance; /volatile:iso Volatile loads and stores have standard semantics; /volatile:ms Volatile loads and stores have acquire and release semantics; /W0 Disable all warnings; /W1 Enable -Wall; /W2 Enable -Wall; /W3 Enable -Wall; /W4 Enable -Wall and -Wextra; /Wall Enable -Weverything; /WX- Do not treat warnings as errors; /WX Treat warnings as errors; /w Disable all warnings; /X Don't add %INCLUDE% to the include search path; /Y- Disable precompiled headers, overrides /Yc and /Yu; /Yc<filename> Generate a pch file for all code up to and including <filename>; /Yu<filename> Load a pch file and use it instead of all code up to and including <filename>; /Z7 Enable CodeView debug information in object files; /Zc:char8_t Enable C++20 char8_t type; /Zc:char8_t- Disable C++20 char8_t type; /Zc:dllexportInlines- Don't dllexport/dllimport inline member functions of dllexport/import classes; /Zc:dllexportInlines dllexport/dllimport inline member functions of dllexport/import classes (default); /Zc:sizedDealloc- Disable C++14 sized",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:175839,load,loads,175839,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['load'],['loads']
Performance," volumes) models, only most significant volumes are shown; - one could activate several clip planes (only with WebGL); - interaction with object browser to change visibility flags or focus on selected volume; - support of floating browser for TGeo objects; - intensive use of HTML Worker to offload computation tasks and keep interactivity; - enable more details when changing camera position/zoom; - Improvements in histograms 3D drawing; - all lego options: lego1..lego4, combined with 'fb', 'bb', '0' or 'z'; - support axis labels on lego plots; - support lego plots for TH1; - Significant (up to factor 10) performance improvement in 3D-graphics; - Implement ROOT6-like color palettes; - Support non-equidistant bins for TH1/TH2 objects.; - Improve TF1 drawing - support exp function in TFormula, fix errors with logx scale, enable zoom-in, (re)calculate function points when zooming; - Introduce many context menus for improving interactivity; - Implement col0 and col0z draw option for TH2 histograms, similar to ROOT6; - Implement box and hbox draw options for TH1 class; - Significant (factor 4) I/O performance improvement; - New 'flex' layout:; - create frames like in Multi Document Interface; - one could move/resize/minimize/maximize such frames. For more details, like the complete change log, the documentation, and very detailed examples, see the [JSROOT home page](https://root.cern.ch/js) and the [JSROOT project github page](https://github.com/linev/jsroot) . ## Tutorials; * New tutorial `treegetval.C` illustrating how to retrieve `TTree` variables in arrays.; * Add script to automatically translate tutorials into notebooks; * Embed it into the documentation generation; * Make the notebooks available in the [tutorials section of the class documentation](https://root.cern/doc/master/group__Tutorials.html). ## Build, Configuration and Testing Infrastructure; - `root-config` does not suppress deprecation warnings (-Wno-deprecated-declarations) anymore. This means compilers ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:27443,perform,performance,27443,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['perform'],['performance']
Performance," we can normally apply the `Alignment`_ and `Eliminating Bit Vector Checks; for All-Ones Bit Vectors`_ optimizations thus simplifying the check at each; call site to a range and alignment check. Shared library support; ======================. **EXPERIMENTAL**. The basic CFI mode described above assumes that the application is a; monolithic binary; at least that all possible virtual/indirect call; targets and the entire class hierarchy are known at link time. The; cross-DSO mode, enabled with **-f[no-]sanitize-cfi-cross-dso** relaxes; this requirement by allowing virtual and indirect calls to cross the; DSO boundary. Assuming the following setup: the binary consists of several; instrumented and several uninstrumented DSOs. Some of them may be; dlopen-ed/dlclose-d periodically, even frequently. - Calls made from uninstrumented DSOs are not checked and just work.; - Calls inside any instrumented DSO are fully protected.; - Calls between different instrumented DSOs are also protected, with; a performance penalty (in addition to the monolithic CFI; overhead).; - Calls from an instrumented DSO to an uninstrumented one are; unchecked and just work, with performance penalty.; - Calls from an instrumented DSO outside of any known DSO are; detected as CFI violations. In the monolithic scheme a call site is instrumented as. .. code-block:: none. if (!InlinedFastCheck(f)); abort();; call *f. In the cross-DSO scheme it becomes. .. code-block:: none. if (!InlinedFastCheck(f)); __cfi_slowpath(CallSiteTypeId, f);; call *f. CallSiteTypeId; --------------. ``CallSiteTypeId`` is a stable process-wide identifier of the; call-site type. For a virtual call site, the type in question is the class; type; for an indirect function call it is the function signature. The; mapping from a type to an identifier is an ABI detail. In the current,; experimental, implementation the identifier of type T is calculated as; follows:. - Obtain the mangled name for ""typeinfo name for T"".; - Calculate MD5 h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:19420,perform,performance,19420,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['perform'],['performance']
Performance," we look at the *Dynamic Dispatch Stall Cycles* table, we see the counter for; SCHEDQ reports 272 cycles. This counter is incremented every time the dispatch; logic is unable to dispatch a full group because the scheduler's queue is full. Looking at the *Dispatch Logic* table, we see that the pipeline was only able to; dispatch two micro opcodes 51.5% of the time. The dispatch group was limited to; one micro opcode 44.6% of the cycles, which corresponds to 272 cycles. The; dispatch statistics are displayed by either using the command option; ``-all-stats`` or ``-dispatch-stats``. The next table, *Schedulers*, presents a histogram displaying a count,; representing the number of micro opcodes issued on some number of cycles. In; this case, of the 610 simulated cycles, single opcodes were issued 306 times; (50.2%) and there were 7 cycles where no opcodes were issued. The *Scheduler's queue usage* table shows that the average and maximum number of; buffer entries (i.e., scheduler queue entries) used at runtime. Resource JFPU01; reached its maximum (18 of 18 queue entries). Note that AMD Jaguar implements; three schedulers:. * JALU01 - A scheduler for ALU instructions.; * JFPU01 - A scheduler floating point operations.; * JLSAGU - A scheduler for address generation. The dot-product is a kernel of three floating point instructions (a vector; multiply followed by two horizontal adds). That explains why only the floating; point scheduler appears to be used. A full scheduler queue is either caused by data dependency chains or by a; sub-optimal usage of hardware resources. Sometimes, resource pressure can be; mitigated by rewriting the kernel using different instructions that consume; different scheduler resources. Schedulers with a small queue are less resilient; to bottlenecks caused by the presence of long data dependencies. The scheduler; statistics are displayed by using the command option ``-all-stats`` or; ``-scheduler-stats``. The next table, *Retire Control Unit*, pre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:31410,queue,queue,31410,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance," we would like to add four optimization levels to our optimizer,; using the standard flags ""``-g``"", ""``-O0``"", ""``-O1``"", and ""``-O2``"". We; could easily implement this with boolean options like above, but there are; several problems with this strategy:. #. A user could specify more than one of the options at a time, for example,; ""``compiler -O3 -O2``"". The CommandLine library would not be able to catch; this erroneous input for us. #. We would have to test 4 different variables to see which ones are set. #. This doesn't map to the numeric levels that we want... so we cannot easily; see if some level >= ""``-O1``"" is enabled. To cope with these problems, we can use an enum value, and have the CommandLine; library fill it in with the appropriate level directly, which is used like this:. .. code-block:: c++. enum OptLevel {; g, O1, O2, O3; };. cl::opt<OptLevel> OptimizationLevel(cl::desc(""Choose optimization level:""),; cl::values(; clEnumVal(g , ""No optimizations, enable debugging""),; clEnumVal(O1, ""Enable trivial optimizations""),; clEnumVal(O2, ""Enable default optimizations""),; clEnumVal(O3, ""Enable expensive optimizations"")));. ...; if (OptimizationLevel >= O2) doPartialRedundancyElimination(...);; ... This declaration defines a variable ""``OptimizationLevel``"" of the; ""``OptLevel``"" enum type. This variable can be assigned any of the values that; are listed in the declaration. The CommandLine library enforces that; the user can only specify one of the options, and it ensure that only valid enum; values can be specified. The ""``clEnumVal``"" macros ensure that the command; line arguments matched the enum values. With this option added, our help output; now is:. ::. USAGE: compiler [options] <input file>. OPTIONS:; Choose optimization level:; -g - No optimizations, enable debugging; -O1 - Enable trivial optimizations; -O2 - Enable default optimizations; -O3 - Enable expensive optimizations; -f - Enable binary output on terminals; -help - display available options (-h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:14728,optimiz,optimization,14728,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,5,['optimiz'],"['optimization', 'optimizations']"
Performance," were issued 306 times; (50.2%) and there were 7 cycles where no opcodes were issued. The *Scheduler's queue usage* table shows that the average and maximum number of; buffer entries (i.e., scheduler queue entries) used at runtime. Resource JFPU01; reached its maximum (18 of 18 queue entries). Note that AMD Jaguar implements; three schedulers:. * JALU01 - A scheduler for ALU instructions.; * JFPU01 - A scheduler floating point operations.; * JLSAGU - A scheduler for address generation. The dot-product is a kernel of three floating point instructions (a vector; multiply followed by two horizontal adds). That explains why only the floating; point scheduler appears to be used. A full scheduler queue is either caused by data dependency chains or by a; sub-optimal usage of hardware resources. Sometimes, resource pressure can be; mitigated by rewriting the kernel using different instructions that consume; different scheduler resources. Schedulers with a small queue are less resilient; to bottlenecks caused by the presence of long data dependencies. The scheduler; statistics are displayed by using the command option ``-all-stats`` or; ``-scheduler-stats``. The next table, *Retire Control Unit*, presents a histogram displaying a count,; representing the number of instructions retired on some number of cycles. In; this case, of the 610 simulated cycles, two instructions were retired during the; same cycle 399 times (65.4%) and there were 109 cycles where no instructions; were retired. The retire statistics are displayed by using the command option; ``-all-stats`` or ``-retire-stats``. The last table presented is *Register File statistics*. Each physical register; file (PRF) used by the pipeline is presented in this table. In the case of AMD; Jaguar, there are two register files, one for floating-point registers (JFpuPRF); and one for integer registers (JIntegerPRF). The table shows that of the 900; instructions processed, there were 900 mappings created. Since this dot-produc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:32178,queue,queue,32178,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,"['bottleneck', 'queue']","['bottlenecks', 'queue']"
Performance," when in learning phase rather than; reading each requested branch individually, the TTreeCache will read all the; branches thus trading off the latencies inherent to multiple small reads for; the potential of requesting more data than needed by read from the disk or; server the baskets for too many branches. The default behavior can be changed by either updating one of the rootrc files; or by setting environment variables. The rootrc files, both the global and the; local ones, now support the following the resource variable TTreeCache.Size; which set the default size factor for auto sizing TTreeCache for TTrees. The; estimated cluster size for the TTree and this factor is used to give the cache; size. If option is set to zero auto cache creation is disabled and the default; cache size is the historical one (equivalent to factor 1.0). If set to; non zero auto cache creation is enabled and both auto created and; default sized caches will use the configured factor: 0.0 no automatic cache; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:13731,cache,cache,13731,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cache']
Performance," when used for generating a linked binary from a source file; (through an intermediate object file), the driver will invoke `cc1` to; generate a temporary object file. The temporary remark file will be emitted; next to the object file, which will then be picked up by `dsymutil` and; emitted in the .dSYM bundle. This is available for all formats except YAML. For example:. ``clang -fsave-optimization-record=bitstream in.c -o out`` will generate. * ``/var/folders/43/9y164hh52tv_2nrdxrj31nyw0000gn/T/a-9be59b.o``. * ``/var/folders/43/9y164hh52tv_2nrdxrj31nyw0000gn/T/a-9be59b.opt.bitstream``. * ``out``. * ``out.dSYM/Contents/Resources/Remarks/out``. Darwin-only: compiling for multiple architectures will use the following; scheme:. ``<base>-<arch>.opt.<format>``. Note that this is incompatible with passing the; :option:`-foptimization-record-file` option. .. option:: -foptimization-record-file. Control the file to which optimization reports are written. This implies; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>`. On Darwin platforms, this is incompatible with passing multiple; ``-arch <arch>`` options. .. option:: -foptimization-record-passes. Only include passes which match a specified regular expression. When optimization reports are being output (see; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>`), this; option controls the passes that will be included in the final report. If this option is not used, all the passes are included in the optimization; record. .. _opt_fdiagnostics-show-hotness:. .. option:: -f[no-]diagnostics-show-hotness. Enable profile hotness information in diagnostic line. This option controls whether Clang prints the profile hotness associated; with diagnostics in the presence of profile-guided optimization information.; This is currently supported with optimization remarks (see; :ref:`Options to Emit Optimization Reports <rpass>`). The hotness information; allows users to focus on the hot optimization remarks tha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:13202,optimiz,optimization-record,13202,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,['optimiz'],['optimization-record']
Performance," where this hasn't been practical to apply. In; situations where you absolutely must emit a non-programmatic error and; the ``Error`` model isn't workable you can call ``report_fatal_error``,; which will call installed error handlers, print a message, and abort the; program. The use of `report_fatal_error` in this case is discouraged. Recoverable errors are modeled using LLVM's ``Error`` scheme. This scheme; represents errors using function return values, similar to classic C integer; error codes, or C++'s ``std::error_code``. However, the ``Error`` class is; actually a lightweight wrapper for user-defined error types, allowing arbitrary; information to be attached to describe the error. This is similar to the way C++; exceptions allow throwing of user-defined types. Success values are created by calling ``Error::success()``, E.g.:. .. code-block:: c++. Error foo() {; // Do something.; // Return success.; return Error::success();; }. Success values are very cheap to construct and return - they have minimal; impact on program performance. Failure values are constructed using ``make_error<T>``, where ``T`` is any class; that inherits from the ErrorInfo utility, E.g.:. .. code-block:: c++. class BadFileFormat : public ErrorInfo<BadFileFormat> {; public:; static char ID;; std::string Path;. BadFileFormat(StringRef Path) : Path(Path.str()) {}. void log(raw_ostream &OS) const override {; OS << Path << "" is malformed"";; }. std::error_code convertToErrorCode() const override {; return make_error_code(object_error::parse_failed);; }; };. char BadFileFormat::ID; // This should be declared in the C++ file. Error printFormattedFile(StringRef Path) {; if (<check for valid format>); return make_error<BadFileFormat>(Path);; // print file contents.; return Error::success();; }. Error values can be implicitly converted to bool: true for error, false for; success, enabling the following idiom:. .. code-block:: c++. Error mayFail();. Error foo() {; if (auto Err = mayFail()); return Er",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:19162,perform,performance,19162,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performance']
Performance," which might cause an object to be deallocated; before it would otherwise be. Without this, it would be almost; impossible to eliminate any ``retain``/``release`` pairs. For; example, consider the following code:. .. code-block:: objc. id x = _ivar;; [x foo];. If we were not permitted in any event to shorten the lifetime of the; object in ``x``, then we would not be able to eliminate this retain; and release unless we could prove that the message send could not; modify ``_ivar`` (or deallocate ``self``). Since message sends are; opaque to the optimizer, this is not possible, and so ARC's hands; would be almost completely tied. ARC makes no guarantees about the execution of a computation history; which contains undefined behavior. In particular, ARC makes no; guarantees in the presence of race conditions. ARC may assume that any retainable object pointers it receives or; generates are instantaneously valid from that point until a point; which, by the concurrency model of the host language, happens-after; the generation of the pointer and happens-before a release of that; object (possibly via an aliasing pointer or indirectly due to; destruction of a different object). .. admonition:: Rationale. There is very little point in trying to guarantee correctness in the; presence of race conditions. ARC does not have a stack-scanning; garbage collector, and guaranteeing the atomicity of every load and; store operation would be prohibitive and preclude a vast amount of; optimization. ARC may assume that non-ARC code engages in sensible balancing; behavior and does not rely on exact or minimum retain count values; except as guaranteed by ``__strong`` object invariants or +1 transfer; conventions. For example, if an object is provably double-retained; and double-released, ARC may eliminate the inner retain and release;; it does not need to guard against code which performs an unbalanced; release followed by a ""balancing"" retain. .. _arc.optimization.liveness:. Object liveness; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:77646,concurren,concurrency,77646,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['concurren'],['concurrency']
Performance," which reproduces opened page with layout and drawn items. ### August 2014; 1. All communication between server and browser done with JSON format.; 2. Fix small error in dtree.js - one should always set; last sibling (_ls) property while tree can be dynamically changed.; 3. In JSRootCore.js provide central function, which handles different kinds; of XMLHttpRequest. Use only async requests, also when getting file header.; 4. Fully reorganize data management in file/tree/directory/collection hierarchical; display. Now complete description collected in HPainter class and decoupled from; visualization, performed with dTree.js.; 5. Remove all global variables from the code.; 6. Automatic scripts/style loading handled via JSROOT.loadScript() function.; One can specify arbitrary scripts list, which asynchronously loaded by browser.; 7. Method to build simple GUI changed and more simplified :). The example in index.htm.; While loadScript and AssertPrerequisites functions moved to JSROOT, one; can easily build many different kinds of GUIs, reusing provided JSRootCore.js functions.; 8. In example.htm also use AssertPrerequisites to load necessary scripts.; This helps to keep code up-to-date even by big changes in JavaScript code.; 9. Provide monitoring of online THttpServer with similar interface as for ROOT files.; 10. Fix several errors in TKey Streamer, use member names as in ROOT itself.; 11. Keep the only version identifier JSROOT.version for JS code; 12. One can specify in JSROOT.AssertPrerequisites functionality which is required.; One could specify '2d', 'io' (default) or '3d'.; 13. Use new AssertPrerequisites functionality to load only required functionality.; 14. When displaying single element, one could specify draw options and monitor property like:; <http://localhost:8080/Files/job1.root/hpxpy/draw.htm?opt=col&monitor=2000>; Such link is best possibility to integrate display into different HTML pages,; using `<iframe/>` tag like:; `<iframe src=""http://localhost:8",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:74625,load,loadScript,74625,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loadScript']
Performance," whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4421,perform,performed,4421,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst,1,['perform'],['performed']
Performance," will exit with a; non-zero value. TUTORIAL; --------. FileCheck is typically used from LLVM regression tests, being invoked on the RUN; line of the test. A simple example of using FileCheck from a RUN line looks; like this:. .. code-block:: llvm. ; RUN: llvm-as < %s | llc -march=x86-64 | FileCheck %s. This syntax says to pipe the current file (""``%s``"") into ``llvm-as``, pipe; that into ``llc``, then pipe the output of ``llc`` into ``FileCheck``. This; means that FileCheck will be verifying its standard input (the llc output); against the filename argument specified (the original ``.ll`` file specified by; ""``%s``""). To see how this works, let's look at the rest of the ``.ll`` file; (after the RUN line):. .. code-block:: llvm. define void @sub1(i32* %p, i32 %v) {; entry:; ; CHECK: sub1:; ; CHECK: subl; %0 = tail call i32 @llvm.atomic.load.sub.i32.p0i32(i32* %p, i32 %v); ret void; }. define void @inc4(i64* %p) {; entry:; ; CHECK: inc4:; ; CHECK: incq; %0 = tail call i64 @llvm.atomic.load.add.i64.p0i64(i64* %p, i64 1); ret void; }. Here you can see some ""``CHECK:``"" lines specified in comments. Now you can; see how the file is piped into ``llvm-as``, then ``llc``, and the machine code; output is what we are verifying. FileCheck checks the machine code output to; verify that it matches what the ""``CHECK:``"" lines specify. The syntax of the ""``CHECK:``"" lines is very simple: they are fixed strings that; must occur in order. FileCheck defaults to ignoring horizontal whitespace; differences (e.g. a space is allowed to match a tab) but otherwise, the contents; of the ""``CHECK:``"" line is required to match some thing in the test file exactly. One nice thing about FileCheck (compared to grep) is that it allows merging; test cases together into logical groups. For example, because the test above; is checking for the ""``sub1:``"" and ""``inc4:``"" labels, it will not match; unless there is a ""``subl``"" in between those labels. If it existed somewhere; else in the file, that would",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:8748,load,load,8748,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['load'],['load']
Performance," will not be optimized, but; it causes the instantiation of ``twice`` and ``thrice`` with an ``int`` type; of; these two instantiations, ``twice`` will be optimized (because its definition; was outside the region) and ``thrice`` will not be optimized. Clang also implements MSVC's range-based pragma,; ``#pragma optimize(""[optimization-list]"", on | off)``. At the moment, Clang only; supports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ignored if; the optimization is not safe to apply. There are loop hints that control transformations (e.g. vectorization, loop; unrolling) and there a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:161742,optimiz,optimize,161742,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimize']
Performance," with ROOT under the `$ROOTSYS/tutorials` directory. ``` {.cpp}; # Example: displaying a ROOT histogram from Python; from ROOT import gRandom,TCanvas,TH1F. c1 = TCanvas('c1','Example',200,10,700,500); hpx = TH1F('hpx','px',100,-4,4). for i in xrange(25000):; px = gRandom.Gaus(); hpx.Fill(px). hpx.Draw(); c1.Update(); ```. ### Access to Python from ROOT. Access to Python objects from Cling is not completely fleshed out.; Currently, ROOT objects and built-in types can cross the boundary; between the two interpreters, but other objects are much more; restricted. For example, for a Python object to cross, it has to be a; class instance, and its class has to be known to Cling first (i.e. the; class has to cross first, before the instance can). All other; cross-coding is based on strings that are run on the Python interpreter; and vise-versa. With the ROOT v4.00/06 and later, the **`TPython`** class will be loaded; automatically on use, for older editions, the `libPyROOT.so` needs to be; loaded first before use. It is possible to switch between interpreters; by calling **`TPython::Prompt()`** on the ROOT side, while returning with; `^D` (`EOF`). State is preserved between successive switches, and string; based cross calls can nest as long as shared resources are properly; handled. ``` {.cpp}; // Example: accessing the Python interpreter from ROOT; // either load PyROOT explicitly or rely on auto-loading; root[] gSystem->Load( ""libPyROOT"" );; root[] TPython::Exec(""print1+1"");; 2. // create a TBrowser on the Python side, and transfer it back and forth; root[] TBrowser* b = (void*)TPython::Eval(""ROOT.TBrowser()"");; (class TObject*)0x8d1daa0; root[] TPython::Bind(b,""b"");. // builtin variables can cross-over (after the call i==2); root[] int i = TPython::Eval( ""1+1"" );; root[] i; (int)2; ```. ### Installation. There are several ways of obtaining `PyROOT`, and which is best depends; on your specific situation. If you work at CERN, you can use the; installation available on `afs`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:4599,load,loaded,4599,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['loaded']
Performance," with a given shape in one step; TGeoVolume *vol = gGeoManager->MakeBox(""VNAME"",ptrMed,dx,dy,dz);; TGeoVolume *vol = gGeoManager->MakeTubs(""VNAME"",ptrMed,rmin,rmax,; dz,phi1,phi2);. // See class TGeoManager for the rest of shapes.; // Making a volume with a given shape with a unique prototype; TGeoVolume *vol = gGeoManager->Volume(""VNAME"",""XXXX"",nmed,upar,; npar);. // Where XXXX stands for the first 4 letters of the specific shape; // classes, nmed is the medium number, upar is an Double_t * array; // of the shape parameters and npar is the number of parameters.; // This prototype allows (npar = 0) to define volumes with shape; // defined only at positioning time (volumes defined in this way; // need to be positioned using TGeoManager::Node() method); ~~~. \anchor GP01bc; #### Positioned Volumes (Nodes). Geometrical modeling is a difficult task when the number of different; geometrical objects is 106-108. This is more or less the case for; detector geometries of complex experiments, where a ‘flat' CSG model; description cannot scale with the current CPU performances. This is the; reason why models like GEANT [1] introduced an additional dimension; (depth) in order to reduce the complexity of the problem. This concept; is also preserved by the ROOT modeller and introduces a pure geometrical; constraint between objects (volumes in our case) - containment. This; means in fact that any positioned volume has to be contained by another.; Now what means contained and positioned?. - We will say that a volume `contains` a point if this is inside the; shape associated to the volume. For instance, a volume having a box; shape will contain all points `P=(X,Y,Z)` verifying the conditions:; `Abs(Pi)dXi`. The points on the shape boundaries are considered as; inside the volume. The volume contains a daughter if it contains all; the points contained by the daughter.; - The definition of containment works of course only with points; defined in the local coordinate system of the consid",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:26276,perform,performances,26276,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performances']
Performance," with a single hardening instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; addl 4(%rsi), %edi # Continue without leaking.; addl 8(%rsi), %edi; orl %eax, %edi # Mask out bits from all three loads.; ```. ###### Preserving the flags while hardening loaded values on Haswell, Zen, and newer processors. Sadly, there are no useful instructions on x86 that apply a mask to all 64 bits; without touching the flag registers. However, we can harden loaded values that; are narrower than a word (fewer than 32-bits on 32-bit systems and fewer than; 64-bits on 64-bit systems) by zero-extending the value to the full word size; and then shifting right by at least the number of original bits using the BMI2; `shrx` instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate 32 bits of data.; shrxq %rax, %rdi, %rdi # Shift out all 32 bits loaded.; ```. Because on x86 the zero-extend is free, this can efficiently harden the loaded; value. ##### Hardening the address of the load. When hardening the loaded value is inapplicable, most often because the; instruction directly leaks information (like `cmp` or `jmpq`), we switch to; hardening the _address_ of the load instead of the loaded value. This avoids; increasing register pressure by unfolding the load or paying some other high; cost. To understand how this works in practice, we need to examine the exact; semantics of the x86 addressing modes which, in its fully general form, looks; like `(%base,%index,scale)offset`. Here `%base` and `%index` are 64-bit; registers that can potentially be any value, and may be attacker controlled,; and `scale` and `offset` are fixed immediate values. `scale` must be `1`, `2`,; `4`, or `8`, and `offset` can be any 32-bit sign extended value. The exact; computation performed to find the address is then: `%base + (scale ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:27390,load,loaded,27390,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance," with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; atomicrmw-no-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_gl*_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; caches. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must hap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:354618,load,loads,354618,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance," with branch probabilities); # PGO data record for BB_3; .uleb128 1000 # BB_3 basic block frequency (only when enabled); .uleb128 0 # BB_3 successors count (only enabled with branch probabilities). ``SHT_LLVM_OFFLOADING`` Section (offloading data); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; This section stores the binary data used to perform offloading device linking; and execution, creating a fat binary. This section is emitted during compilation; of offloading languages such as OpenMP or CUDA. If the data is intended to be; used by the device linker only, it should use the ``SHF_EXCLUDE`` flag so it is; automatically stripped from the final executable or shared library. The binary data stored in this section conforms to a custom binary format used; for storing offloading metadata. This format is effectively a string table; containing metadata accompanied by a device image. ``SHT_LLVM_LTO`` Section (LLVM bitcode for fat LTO); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; This section stores LLVM bitcode used to perform regular LTO or ThinLTO at link; time. This section is generated when the compiler enables fat LTO. This section; has the ``SHF_EXCLUDE`` flag so that it is stripped from the final executable; or shared library. CodeView-Dependent; ------------------. ``.cv_file`` Directive; ^^^^^^^^^^^^^^^^^^^^^^; Syntax:; ``.cv_file`` *FileNumber FileName* [ *checksum* ] [ *checksumkind* ]. ``.cv_func_id`` Directive; ^^^^^^^^^^^^^^^^^^^^^^^^^; Introduces a function ID that can be used with ``.cv_loc``. Syntax:; ``.cv_func_id`` *FunctionId*. ``.cv_inline_site_id`` Directive; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; Introduces a function ID that can be used with ``.cv_loc``. Includes; ``inlined at`` source location information for use in the line table of the; caller, whether the caller is a real function or another inlined call site. Syntax:; ``.cv_inline_site_id`` *FunctionId* ``within`` *Function* ``inlined_at`` *FileNumber Line* [ *Column* ]. ``.cv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Extensions.rst:18598,perform,perform,18598,interpreter/llvm-project/llvm/docs/Extensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Extensions.rst,1,['perform'],['perform']
Performance," with the Library, with the complete machine-readable ""work that; uses the Library"", as object code and/or source code, so that the; user can modify the Library and then relink to produce a modified; executable containing the modified Library. (It is understood; that the user who changes the contents of definitions files in the; Library will not necessarily be able to recompile the application; to use the modified definitions.). b) Use a suitable shared library mechanism for linking with the; Library. A suitable mechanism is one that (1) uses at run time a; copy of the library already present on the user's computer system,; rather than copying library functions into the executable, and (2); will operate properly with a modified version of the library, if; the user installs one, as long as the modified version is; interface-compatible with the version that the work was made with. c) Accompany the work with a written offer, valid for at; least three years, to give the same user the materials; specified in Subsection 6a, above, for a charge no more; than the cost of performing this distribution. d) If distribution of the work is made by offering access to copy; from a designated place, offer equivalent access to copy the above; specified materials from the same place. e) Verify that the user has already received a copy of these; materials or that you have already sent this user a copy. For an executable, the required form of the ""work that uses the; Library"" must include any data and utility programs needed for; reproducing the executable from it. However, as a special exception,; the materials to be distributed need not include anything that is; normally distributed (in either source or binary form) with the major; components (compiler, kernel, and so on) of the operating system on; which the executable runs, unless that component itself accompanies; the executable. It may happen that this requirement contradicts the license; restrictions of other proprietary librarie",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/LGPL2_1.txt:16204,perform,performing,16204,LGPL2_1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/LGPL2_1.txt,2,['perform'],['performing']
Performance," workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:246759,load,load,246759,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance," would take the previous two and add the Parser library and; some actions for indexing. If you want a refactoring, static analysis, or; source-to-source compiler tool, you would then add the AST building and; semantic analyzer libraries.; For more information about the low-level implementation details of the; various clang libraries, please see the ; clang Internals Manual. Support Diverse Clients. Clang is designed and built with many grand plans for how we can use it. The; driving force is the fact that we use C and C++ daily, and have to suffer due to; a lack of good tools available for it. We believe that the C and C++ tools; ecosystem has been significantly limited by how difficult it is to parse and; represent the source code for these languages, and we aim to rectify this; problem in clang.; The problem with this goal is that different clients have very different; requirements. Consider code generation, for example: a simple front-end that; parses for code generation must analyze the code for validity and emit code; in some intermediate form to pass off to a optimizer or backend. Because; validity analysis and code generation can largely be done on the fly, there is; not hard requirement that the front-end actually build up a full AST for all; the expressions and statements in the code. TCC and GCC are examples of; compilers that either build no real AST (in the former case) or build a stripped; down and simplified AST (in the later case) because they focus primarily on; codegen.; On the opposite side of the spectrum, some clients (like refactoring) want; highly detailed information about the original source code and want a complete; AST to describe it with. Refactoring wants to have information about macro; expansions, the location of every paren expression '(((x)))' vs 'x', full; position information, and much more. Further, refactoring wants to look; across the whole program to ensure that it is making transformations; that are safe. Making this efficient ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html:7110,optimiz,optimizer,7110,interpreter/llvm-project/clang/www/features.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html,1,['optimiz'],['optimizer']
Performance," writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensur",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:323270,cache,cache,323270,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance," writeback; the CPU cache due to the L2 probe filter and and the PTE C-bit being set.; * Since all work-groups on the same agent share the same L2, no L2; invalidation or writeback is required for coherence.; * To ensure coherence of local and remote memory writes of work-groups in; different agents a ``buffer_wbl2`` is required. It will writeback dirty L2; cache lines of MTYPE RW (used for local coarse grain memory) and MTYPE NC; ()used for remote coarse grain memory). Note that MTYPE CC (used for local; fine grain memory) causes write through to DRAM, and MTYPE UC (used for; remote fine grain memory) bypasses the L2, so both will never result in; dirty L2 cache lines.; * To ensure coherence of local and remote memory reads of work-groups in; different agents a ``buffer_invl2`` is required. It will invalidate L2; cache lines with MTYPE NC (used for remote coarse grain memory). Note that; MTYPE CC (used for local fine grain memory) and MTYPE RW (used for local; coarse memory) cause local reads to be invalidated by remote writes with; with the PTE C-bit so these cache lines are not invalidated. Note that; MTYPE UC (used for remote fine grain memory) bypasses the L2, so will; never result in L2 cache lines that need to be invalidated. * PCIe access from the GPU to the CPU memory is kept coherent by using the; MTYPE UC (uncached) which bypasses the L2. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector caches are; invalidated between kernel dispatches by CP since constant address space data; may change between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR regi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:239480,cache,cache,239480,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance," x float> @llvm.vp.maximum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.maximum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.maximum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point maximum of two vectors of floating-point values,; propagating NaNs and treating -0.0 as less than +0.0. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.maximum``' intrinsic performs floating-point maximum (:ref:`maximum <i_maximum>`); of the first and second vector operand on each enabled lane, the result being ; NaN if either operand is a NaN. -0.0 is considered to be less than +0.0 for this; intrinsic. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. ; The operation is performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.maximum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.maximum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fadd:. '``llvm.vp.fadd.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fadd.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:731465,perform,performs,731465,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance," x float> @llvm.vp.minimum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.minimum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.minimum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point minimum of two vectors of floating-point values,; propagating NaNs and treating -0.0 as less than +0.0. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.minimum``' intrinsic performs floating-point minimum (:ref:`minimum <i_minimum>`); of the first and second vector operand on each enabled lane, the result being ; NaN if either operand is a NaN. -0.0 is considered to be less than +0.0 for this; intrinsic. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. ; The operation is performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.minimum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.minimum.v4f32(<4 x float> %a, <4 x float> %b); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_maximum:. '``llvm.vp.maximum.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.maximum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.maximum.nxv4f32 (",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:729644,perform,performs,729644,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance," x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_or:. '``llvm.vp.or.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.or.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.or.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.or.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Vector-predicated or. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of integer type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.or``' intrinsic performs a bitwise or (:ref:`or <i_or>`) of the; first two operands on each enabled lane. The result on disabled lanes is; a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.or.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = or <4 x i32> %a, %b; %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_and:. '``llvm.vp.and.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.and.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.and.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.and.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Vect",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:712690,perform,performs,712690,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance," x30, x20, [sp], #16; ret. [...]; __hwasan_check_x0_2_short_v2:; sbfx x16, x0, #4, #52 // shadow offset; ldrb w16, [x20, x16] // load shadow tag; cmp x16, x0, lsr #56 // extract address tag, compare with shadow tag; b.ne .Ltmp0 // jump to short tag handler on mismatch; .Ltmp1:; ret; .Ltmp0:; cmp w16, #15 // is this a short tag?; b.hi .Ltmp2 // if not, error; and x17, x0, #0xf // find the address's position in the short granule; add x17, x17, #3 // adjust to the position of the last byte loaded; cmp w16, w17 // check that position is in bounds; b.ls .Ltmp2 // if not, error; orr x16, x0, #0xf // compute address of last byte of granule; ldrb w16, [x16] // load tag from it; cmp x16, x0, lsr #56 // compare with pointer tag; b.eq .Ltmp1 // if matches, continue; .Ltmp2:; stp x0, x1, [sp, #-256]! // save original x0, x1 on stack (they will be overwritten); stp x29, x30, [sp, #232] // create frame record; mov x1, #2 // set x1 to a constant indicating the type of failure; adrp x16, :got:__hwasan_tag_mismatch_v2 // call runtime function to save remaining registers and report error; ldr x16, [x16, :got_lo12:__hwasan_tag_mismatch_v2] // (load address from GOT to avoid potential register clobbers in delay load handler); br x16. Heap; ----. Tagging the heap memory/pointers is done by `malloc`.; This can be based on any malloc that forces all objects to be TG-aligned.; `free` tags the memory with a different tag. Stack; -----. Stack frames are instrumented by aligning all non-promotable allocas; by `TG` and tagging stack memory in function prologue and epilogue. Tags for different allocas in one function are **not** generated; independently; doing that in a function with `M` allocas would require; maintaining `M` live stack pointers, significantly increasing register; pressure. Instead we generate a single base tag value in the prologue,; and build the tag for alloca number `M` as `ReTag(BaseTag, M)`, where; ReTag can be as simple as exclusive-or with constant `M`. Stack instrument",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst:5261,load,load,5261,interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,2,['load'],['load']
Performance," you; can build and load a shared library containing your script. To load it; use the command `.L` and append the file name with a `+`. ``` {.cpp}; root[] .L MyScript.C+; ```. The + option generates the shared library and names it by taking; the name of the file ""filename"" but replacing the dot before the; extension by an underscore and by adding the shared library extension; for the current platform. For example on most platforms, `hsimple.cxx`; will generate `hsimple_cxx.so`. The + command rebuild the library only if the script or any of the; files it includes are newer than the library. When checking the; timestamp, ACLiC generates a dependency file which name is the same as; the library name, just replacing the 'so' extension by the extension; 'd'. For example on most platforms, `hsimple.cxx` will generate; `hsimple_cxx.d`. To ensure that the shared library is rebuilt you can use the ++; syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. To build, load, and execute the function with the same name as the; file you can use the `.x` command. This is the same as executing a; named script; you can also provide parameters. The only; difference is you need to append a + or a ++. ``` {.cpp}; root[] .x MyScript.C+(4000); Creating shared library /home/./MyScript_C.so; ```. You can select whether the script in compiled with debug symbol or; with optimization by appending the letter 'g' or 'O' after the '+' or; '++'. Without the specification, the script is compiled with the same; level of debugging symbol and optimization as the currently running; ROOT executable. For example:. ``` {.cpp}; root[] .L MyScript.C++g; ```. will compile `MyScript.C` with debug symbols; usually this means; giving the `-g` option to compiler. ``` {.cpp}; root[] .L MyScript.C++O; ```. will compile `MyScript.C` with optimizations; usually this means; giving the `-O` option to compiler. The syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. is using the default optimization level. The initial default ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:14538,load,load,14538,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['load']
Performance," your results. \label{f61}][f61]. ## Toy Monte Carlo Experiments ##. Let us look at a simple example of a toy experiment comparing two; methods to fit a function to a histogram, the $\chi^{2}$. method and a method called ""binned log-likelihood fit"", both available in ROOT. As a very simple yet powerful quantity to check the quality of the fit; results, we construct for each pseudo-data set the so-called ""pull"", the; difference of the estimated and the true value of a parameter,; normalised to the estimated error on the parameter,; $\frac{(p_{estim} - p_{true})}{\sigma_{p}}$. If everything is OK, the; distribution of the pull values is a standard normal distribution, i.e.; a Gaussian distribution centred around zero with a standard deviation of one. The macro performs a rather big number of toy experiments, where a; histogram is repeatedly filled with Gaussian distributed numbers,; representing the pseudo-data in this example. Each time, a fit is; performed according to the selected method, and the pull is calculated; and filled into a histogram. Here is the code:. ``` {.cpp .numberLines}; @ROOT_INCLUDE_FILE macros/macro9.C; ```. Your present knowledge of ROOT should be enough to understand all the; technicalities behind the macro. Note that the variable `pull` in line; *61* is different from the definition above: instead of the parameter; error on `mean`, the fitted standard deviation of the distribution; divided by the square root of the number of entries,; `sig/sqrt(n_tot_entries)`, is used. - What method exhibits the better performance with the default; parameters ?. - What happens if you increase the number of entries per histogram by; a factor of ten ? Why ?. The answers to these questions are well beyond the scope of this guide.; Basically all books about statistical methods provide a complete; treatment of the aforementioned topics. [^5]: ""Monte Carlo"" simulation means that random numbers play a role here; which is as crucial as in games of pure chance in the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/functions_and_parameter_estimation.md:4940,perform,performed,4940,documentation/primer/functions_and_parameter_estimation.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/functions_and_parameter_estimation.md,1,['perform'],['performed']
Performance,"!0 ; Can assume that value under %ptr didn't change; call void @foo(ptr %ptr). %newPtr = call ptr @getPointer(ptr %ptr); %c = load i8, ptr %newPtr, !invariant.group !0 ; Can't assume anything, because we only have information about %ptr. %unknownValue = load i8, ptr @unknownPtr; store i8 %unknownValue, ptr %ptr, !invariant.group !0 ; Can assume that %unknownValue == 42. call void @foo(ptr %ptr); %newPtr2 = call ptr @llvm.launder.invariant.group.p0(ptr %ptr); %d = load i8, ptr %newPtr2, !invariant.group !0 ; Can't step through launder.invariant.group to get value of %ptr. ...; declare void @foo(ptr); declare ptr @getPointer(ptr); declare ptr @llvm.launder.invariant.group.p0(ptr). !0 = !{}. The invariant.group metadata must be dropped when replacing one pointer by; another based on aliasing information. This is because invariant.group is tied; to the SSA value of the pointer operand. .. code-block:: llvm. %v = load i8, ptr %x, !invariant.group !0; ; if %x mustalias %y then we can replace the above instruction with; %v = load i8, ptr %y. Note that this is an experimental feature, which means that its semantics might; change in the future. '``type``' Metadata; ^^^^^^^^^^^^^^^^^^^. See :doc:`TypeMetadata`. '``associated``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^. The ``associated`` metadata may be attached to a global variable definition with; a single argument that references a global object (optionally through an alias). This metadata lowers to the ELF section flag ``SHF_LINK_ORDER`` which prevents; discarding of the global variable in linker GC unless the referenced object is; also discarded. The linker support for this feature is spotty. For best; compatibility, globals carrying this metadata should:. - Be in ``@llvm.compiler.used``.; - If the referenced global variable is in a comdat, be in the same comdat. ``!associated`` can not express many-to-one relationship. A global variable with; the metadata should generally not be referenced by a function: the function may; be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:317701,load,load,317701,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"!nontemporal !<nontemp_node>][, !invariant.load !<empty_node>][, !invariant.group !<empty_node>][, !nonnull !<empty_node>][, !dereferenceable !<deref_bytes_node>][, !dereferenceable_or_null !<deref_bytes_node>][, !align !<align_node>][, !noundef !<empty_node>]; <result> = load atomic [volatile] <ty>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>]; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}; !<deref_bytes_node> = !{ i64 <dereferenceable_bytes> }; !<align_node> = !{ i64 <value_alignment> }. Overview:; """""""""""""""""". The '``load``' instruction is used to read from memory. Arguments:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :ref:`volatile operations <volatile>`. If the ``load`` is marked as ``atomic``, it takes an extra :ref:`ordering; <ordering>` and optional ``syncscope(""<target-scope>"")`` argument. The; ``release`` and ``acq_rel`` orderings are not valid on ``load`` instructions.; Atomic loads produce :ref:`defined <memmodel>` results when they may see; multiple atomic stores. The type of the pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size limit. ``align`` must be; explicitly specified on atomic loads. Note: if the alignment is not greater or; equal to the size of the `<value>` type, the atomic operation is likely to; require a lock and have poor performance. ``!nontemporal`` does not have any; defined semantics for atomic loads. The optional constant ``align`` argument specifies the alignment of the; operation (that is,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:412879,load,load,412879,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,"['load', 'optimiz']","['load', 'optimizer']"
Performance,""""""""""""""""""". The '``llvm.experimental.constrained.fcmp``' and; '``llvm.experimental.constrained.fcmps``' intrinsics return a boolean; value or vector of boolean values based on comparison of its operands. If the operands are floating-point scalars, then the result type is a; boolean (:ref:`i1 <t_integer>`). If the operands are floating-point vectors, then the result type is a; vector of boolean with the same number of elements as the operands being; compared. The '``llvm.experimental.constrained.fcmp``' intrinsic performs a quiet; comparison operation while the '``llvm.experimental.constrained.fcmps``'; intrinsic performs a signaling comparison operation. Arguments:; """""""""""""""""""". The first two arguments to the '``llvm.experimental.constrained.fcmp``'; and '``llvm.experimental.constrained.fcmps``' intrinsics must be; :ref:`floating-point <t_floating>` or :ref:`vector <t_vector>`; of floating-point values. Both arguments must have identical types. The third argument is the condition code indicating the kind of comparison; to perform. It must be a metadata string with one of the following values:. .. _fcmp_md_cc:. - ""``oeq``"": ordered and equal; - ""``ogt``"": ordered and greater than; - ""``oge``"": ordered and greater than or equal; - ""``olt``"": ordered and less than; - ""``ole``"": ordered and less than or equal; - ""``one``"": ordered and not equal; - ""``ord``"": ordered (no nans); - ""``ueq``"": unordered or equal; - ""``ugt``"": unordered or greater than; - ""``uge``"": unordered or greater than or equal; - ""``ult``"": unordered or less than; - ""``ule``"": unordered or less than or equal; - ""``une``"": unordered or not equal; - ""``uno``"": unordered (either nans). *Ordered* means that neither operand is a NAN while *unordered* means; that either operand may be a NAN. The fourth argument specifies the exception behavior as described above. Semantics:; """""""""""""""""""". ``op1`` and ``op2`` are compared according to the condition code given; as the third argument. If the operands are vectors, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:884507,perform,perform,884507,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,""""""""""""""""""". The '``llvm.experimental.vp.reverse.*``' intrinsic is the vector length; predicated version of the '``llvm.experimental.vector.reverse.*``' intrinsic. Arguments:; """""""""""""""""""". The result and the first argument ``vec`` are vectors with the same type.; The second argument ``mask`` is a vector mask and has the same number of; elements as the result. The third argument is the explicit vector length of; the operation. Semantics:; """""""""""""""""""". This intrinsic reverses the order of the first ``evl`` elements in a vector.; The lanes in the result vector disabled by ``mask`` are ``poison``. The; elements past ``evl`` are poison. .. _int_vp_load:. '``llvm.vp.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.vp.load.v4f32.p0(ptr %ptr, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.vp.load.nxv2i16.p0(ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare <8 x float> @llvm.vp.load.v8f32.p1(ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p6(ptr addrspace(6) %ptr, <vscale x 1 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.load.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.load <int_mload>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is a; vector of boolean values with the same number of elements as the return type.; The third is the explicit vector length of the operation. The return type and; underlying type of the base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.vp.load``' intrinsic reads a vector from memory in the same way as; the '``llvm.masked.load``' intrinsic, where the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way.; Certain '``ll",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:783235,load,load,783235,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,""""""""""""""""""". The integer operand is the loop trip count of the hardware-loop, and thus; not e.g. the loop back-edge taken count. Semantics:; """""""""""""""""""". The '``llvm.start.loop.iterations.*``' intrinsics do not perform any arithmetic; on their operand. It's a hint to the backend that can use this to set up the; hardware-loop count with a target specific instruction, usually a move of this; value to a special register or a hardware-loop instruction. '``llvm.test.set.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare i1 @llvm.test.set.loop.iterations.i32(i32); declare i1 @llvm.test.set.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.test.set.loop.iterations.*``' intrinsics are used to specify the; the loop trip count, and also test that the given count is not zero, allowing; it to control entry to a while-loop. They are placed in the loop preheader's; predecessor basic block, and are marked as ``IntrNoDuplicate`` to avoid; optimizers duplicating these instructions. Arguments:; """""""""""""""""""". The integer operand is the loop trip count of the hardware-loop, and thus; not e.g. the loop back-edge taken count. Semantics:; """""""""""""""""""". The '``llvm.test.set.loop.iterations.*``' intrinsics do not perform any; arithmetic on their operand. It's a hint to the backend that can use this to; set up the hardware-loop count with a target specific instruction, usually a; move of this value to a special register or a hardware-loop instruction.; The result is the conditional value of whether the given count is not zero. '``llvm.test.start.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare {i32, i1} @llvm.test.start.loop.iterations.i32(i32); declare {i64, i1} @llvm.test.start.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.test.start.loop.iterations.*``' intrinsics are similar to the; '`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:645823,optimiz,optimizers,645823,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,""""""""""""""""". The first operand is the base pointer for the load. The second operand is the alignment of the source location. It must be a power of two constant integer value. The third operand, mask, is a vector of boolean values with the same number of elements as the return type. The fourth is a pass-through value that is used to fill the masked-off lanes of the result. The return type, underlying type of the base pointer and the type of the '``passthru``' operand are the same vector types. Semantics:; """""""""""""""""""". The '``llvm.masked.load``' intrinsic is designed for conditional reading of selected vector elements in a single IR operation. It is useful for targets that support vector masked loads and allows vectorizing predicated basic blocks on these targets. Other targets may support this intrinsic differently, for example by lowering it into a sequence of branches that guard scalar load operations.; The result of this operation is equivalent to a regular vector load instruction followed by a 'select' between the loaded and the passthru values, predicated on the same mask. However, using this intrinsic prevents exceptions on memory access to masked-off lanes. ::. %res = call <16 x float> @llvm.masked.load.v16f32.p0(ptr %ptr, i32 4, <16 x i1>%mask, <16 x float> %passthru). ;; The result of the two following instructions is identical aside from potential memory access exception; %loadlal = load <16 x float>, ptr %ptr, align 4; %res = select <16 x i1> %mask, <16 x float> %loadlal, <16 x float> %passthru. .. _int_mstore:. '``llvm.masked.store.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The data stored in memory is a vector of any integer, floating-point or pointer data type. ::. declare void @llvm.masked.store.v8i32.p0 (<8 x i32> <value>, ptr <ptr>, i32 <alignment>, <8 x i1> <mask>); declare void @llvm.masked.store.v16f32.p0(<16 x float> <value>, ptr <ptr>, i32 <alignment>, <16 x i1> <mask>); ;; The data is a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:845020,load,load,845020,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],"['load', 'loaded']"
Performance,""""""""""""""". ::. <result> = frem [fast-math flags]* <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``frem``' instruction returns the remainder from the division of; its two operands. .. note::. 	The instruction is implemented as a call to libm's '``fmod``'; 	for some targets, and using the instruction may thus require linking libm. Arguments:; """""""""""""""""""". The two arguments to the '``frem``' instruction must be; :ref:`floating-point <t_floating>` or :ref:`vector <t_vector>` of; floating-point values. Both arguments must have identical types. Semantics:; """""""""""""""""""". The value produced is the floating-point remainder of the two operands.; This is the same output as a libm '``fmod``' function, but without any; possibility of setting ``errno``. The remainder has the same sign as the; dividend.; This instruction is assumed to execute in the default :ref:`floating-point; environment <floatenv>`.; This instruction can also take any number of :ref:`fast-math; flags <fastmath>`, which are optimization hints to enable otherwise; unsafe floating-point optimizations:. Example:; """""""""""""""". .. code-block:: text. <result> = frem float 4.0, %var ; yields float:result = 4.0 % %var. .. _bitwiseops:. Bitwise Binary Operations; -------------------------. Bitwise binary operators are used to do various forms of bit-twiddling; in a program. They are generally very efficient instructions and can; commonly be strength reduced from other instructions. They require two; operands of the same type, execute an operation on them, and produce a; single value. The resulting value is the same type as its operands. .. _i_shl:. '``shl``' Instruction; ^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = shl <ty> <op1>, <op2> ; yields ty:result; <result> = shl nuw <ty> <op1>, <op2> ; yields ty:result; <result> = shl nsw <ty> <op1>, <op2> ; yields ty:result; <result> = shl nuw nsw <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``shl``' instruction returns the first opera",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:391736,optimiz,optimization,391736,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"""""""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.add.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.add.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``ADD`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.add``' intrinsic performs the integer ``ADD`` reduction; (:ref:`llvm.vector.reduce.add <int_vector_reduce_add>`) of the vector operand; ``val`` on each enabled lane, adding it to the scalar ``start_value``. Disabled; lanes are treated as containing the neutral value ``0`` (i.e. having no effect; on the reduction operation). If the vector length is zero, the result is equal; to ``start_value``. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.add.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> zeroinitializer; %reduction = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %masked.a); %also.r = add i32 %reduction, %start. .. _int_vp_reduce_fadd:. '``llvm.vp.reduce.fadd.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:749333,perform,performs,749333,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"""""""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.and.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.and.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``AND`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.and``' intrinsic performs the integer ``AND`` reduction; (:ref:`llvm.vector.reduce.and <int_vector_reduce_and>`) of the vector operand; ``val`` on each enabled lane, performing an '``and``' of that with with the; scalar ``start_value``. Disabled lanes are treated as containing the neutral; value ``UINT_MAX``, or ``-1`` (i.e. having no effect on the reduction; operation). If the vector length is zero, the result is the start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.and.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> <i32 -1, i32 -1, i32 -1, i32 -1>; %reduction = call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %masked.a); %also.r = and i32 %reduction, %start. .. _int_vp_reduce_or:. '``llvm.vp.reduce.or.*``' Intrinsics; ^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:757733,perform,performs,757733,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"""""""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.mul.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.mul.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``MUL`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.mul``' intrinsic performs the integer ``MUL`` reduction; (:ref:`llvm.vector.reduce.mul <int_vector_reduce_mul>`) of the vector operand ``val``; on each enabled lane, multiplying it by the scalar ``start_value``. Disabled; lanes are treated as containing the neutral value ``1`` (i.e. having no effect; on the reduction operation). If the vector length is zero, the result is the; start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.mul.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> <i32 1, i32 1, i32 1, i32 1>; %reduction = call i32 @llvm.vector.reduce.mul.v4i32(<4 x i32> %masked.a); %also.r = mul i32 %reduction, %start. .. _int_vp_reduce_fmul:. '``llvm.vp.reduce.fmul.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:753536,perform,performs,753536,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"""""""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.xor.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.xor.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``XOR`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.xor``' intrinsic performs the integer ``XOR`` reduction; (:ref:`llvm.vector.reduce.xor <int_vector_reduce_xor>`) of the vector operand; ``val`` on each enabled lane, performing an '``xor``' of that with the scalar; ``start_value``. Disabled lanes are treated as containing the neutral value; ``0`` (i.e. having no effect on the reduction operation). If the vector length; is zero, the result is the start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.xor.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> <i32 0, i32 0, i32 0, i32 0>; %reduction = call i32 @llvm.vector.reduce.xor.v4i32(<4 x i32> %masked.a); %also.r = xor i32 %reduction, %start. .. _int_vp_reduce_smax:. '``llvm.vp.reduce.smax.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:761781,perform,performs,761781,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,""""""""". The '``llvm.convert.to.fp16``' intrinsic function performs a conversion from a; conventional floating-point format to half precision floating-point format. The; return value is an ``i16`` which contains the converted number. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call i16 @llvm.convert.to.fp16.f32(float %a); store i16 %res, i16* @x, align 2. .. _int_convert_from_fp16:. '``llvm.convert.from.fp16``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare float @llvm.convert.from.fp16.f32(i16 %a); declare double @llvm.convert.from.fp16.f64(i16 %a). Overview:; """""""""""""""""". The '``llvm.convert.from.fp16``' intrinsic function performs a; conversion from half precision floating-point format to single precision; floating-point format. Arguments:; """""""""""""""""""". The intrinsic function contains single argument - the value to be; converted. Semantics:; """""""""""""""""""". The '``llvm.convert.from.fp16``' intrinsic function performs a; conversion from half single precision floating-point format to single; precision floating-point format. The input half-float value is; represented by an ``i16`` value. Examples:; """""""""""""""""". .. code-block:: llvm. %a = load i16, ptr @x, align 2; %res = call float @llvm.convert.from.fp16(i16 %a). Saturating floating-point to integer conversions; ------------------------------------------------. The ``fptoui`` and ``fptosi`` instructions return a; :ref:`poison value <poisonvalues>` if the rounded-towards-zero value is not; representable by the result type. These intrinsics provide an alternative; conversion, which will saturate towards the smallest and largest representable; integer values instead. '``llvm.fptoui.sat.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.fptoui.sat`` on any; floating-point argument type and any integer result type, or vectors thereof.; Not all targets may support all types, however. ::. declare i32 @llvm.fptoui.sat.i32.f32",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:682924,perform,performs,682924,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.merge.v16i32 (<16 x i1> <condition>, <16 x i32> <on_true>, <16 x i32> <on_false>, i32 <pivot>); declare <vscale x 4 x i64> @llvm.vp.merge.nxv4i64 (<vscale x 4 x i1> <condition>, <vscale x 4 x i64> <on_true>, <vscale x 4 x i64> <on_false>, i32 <pivot>). Overview:; """""""""""""""""". The '``llvm.vp.merge``' intrinsic is used to choose one value based on a; condition vector and an index operand, without IR-level branching. Arguments:; """""""""""""""""""". The first operand is a vector of ``i1`` and indicates the condition. The; second operand is the value that is merged where the condition vector is true.; The third operand is the value that is selected where the condition vector is; false or the lane position is greater equal than the pivot. The fourth operand; is the pivot. #. The optional ``fast-math flags`` marker indicates that the merge has one or; more :ref:`fast-math flags <fastmath>`. These are optimization hints to; enable otherwise unsafe floating-point optimizations. Fast-math flags are; only valid for merges that return a floating-point scalar or vector type,; or an array (nested to any depth) of floating-point scalar or vector types. Semantics:; """""""""""""""""""". The intrinsic selects lanes from the second and third operand depending on a; condition vector and pivot value. For all lanes where the condition vector is true and the lane position is less; than ``%pivot`` the lane is taken from the second operand. Otherwise, the lane; is taken from the third operand. Example:; """""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.merge.v4i32(<4 x i1> %cond, <4 x i32> %on_true, <4 x i32> %on_false, i32 %pivot). ;;; Expansion.; ;; Lanes at and above %pivot are taken from %on_false; %atfirst = insertelement <4 x i32> undef, i32 %pivot, i32 0; %splat = shufflevector <4 x i32> %atfirst, <4 x i32> poison, <4 x i32> zeroinitializer; %pivotmask = icmp ult <4 x i32> <i32 0, i32 1, i32 2, i32 3>, <4 x i32> %splat; %m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:696123,optimiz,optimization,696123,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,""""""". .. code-block:: text. call void @llvm.vp.store.v8i8.p0(<8 x i8> %val, ptr align 4 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, the call above is lane-wise equivalent to the call below. call void @llvm.masked.store.v8i8.p0(<8 x i8> %val, ptr %ptr, i32 4, <8 x i1> %mask). .. _int_experimental_vp_strided_load:. '``llvm.experimental.vp.strided.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.experimental.vp.strided.load.v4f32.i64(ptr %ptr, i64 %stride, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.experimental.vp.strided.load.nxv2i16.i64(ptr %ptr, i64 %stride, <vscale x 2 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, scalar values from; memory locations evenly spaced apart by '``stride``' number of bytes, starting from '``ptr``'. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the stride; value expressed in bytes. The third operand is a vector of boolean values; with the same number of elements as the return type. The fourth is the explicit; vector length of the operation. The base pointer underlying type matches the type of the scalar; elements of the return operand. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, multiple scalar; values from memory in the same way as the :ref:`llvm.vp.gather <int_vp_gather>` intrinsic,; where the vector of pointers is in the form:. ``%ptrs = <%ptr, %ptr + %stride, %ptr + 2 * %stride, ... >``,. with '``ptr``' previously casted to a pointer '``i8``', '``stride``' always interpreted as a signed; integer and all arithmetic occurring in the pointer type. Examples:; """""""""""""""""". .. code-block:: text. 	 %r = call <8 x i64> @llvm.experimental.vp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:787725,load,load,787725,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,""""". This function multiplies the first argument by 2 raised to the second; argument's power. If the first argument is NaN or infinite, the same; value is returned. If the result underflows a zero with the same sign; is returned. If the result overflows, the result is an infinity with; the same sign. .. _int_frexp:. '``llvm.frexp.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.frexp`` on any; floating point or vector of floating point type. Not all targets support; all types however. ::. declare { float, i32 } @llvm.frexp.f32.i32(float %Val); declare { double, i32 } @llvm.frexp.f64.i32(double %Val); declare { x86_fp80, i32 } @llvm.frexp.f80.i32(x86_fp80 %Val); declare { fp128, i32 } @llvm.frexp.f128.i32(fp128 %Val); declare { ppc_fp128, i32 } @llvm.frexp.ppcf128.i32(ppc_fp128 %Val); declare { <2 x float>, <2 x i32> } @llvm.frexp.v2f32.v2i32(<2 x float> %Val). Overview:; """""""""""""""""". The '``llvm.frexp.*``' intrinsics perform the frexp function. Arguments:; """""""""""""""""""". The argument is a :ref:`floating-point <t_floating>` or; :ref:`vector <t_vector>` of floating-point values. Returns two values; in a struct. The first struct field matches the argument type, and the; second field is an integer or a vector of integer values with the same; number of elements as the argument. Semantics:; """""""""""""""""""". This intrinsic splits a floating point value into a normalized; fractional component and integral exponent. For a non-zero argument, returns the argument multiplied by some power; of two such that the absolute value of the returned value is in the; range [0.5, 1.0), with the same sign as the argument. The second; result is an integer such that the first result raised to the power of; the second result is the input argument. If the argument is a zero, returns a zero with the same sign and a 0; exponent. If the argument is a NaN, a NaN is returned and the returned exponent; is unspecified. If the argument is an infin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:566431,perform,perform,566431,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,""""". This is an overloaded intrinsic. You can use ``llvm.sdiv.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.sdiv.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.sdiv.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.sdiv.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.sdiv.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.sdiv.fix``' family of intrinsic functions perform signed; fixed point division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type, or if the second argument is zero. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.sdiv.fix.i4(i4 6, i4 2, i32 0) ; %res = 3 (6 / 2 = 3); %res = call i4 @llvm.sdiv.fix.i4(i4 6, i4 4, i32 1) ; %res = 3 (3 / 2 = 1.5); %res = call i4 @llvm.sdiv.fix.i4(i4 3, i4 -2, i32 1) ; %res = -3 (1.5 / -1 = -1.5). ; The result in the following could be rounded up to 1 or down to 0.5; %res = call i4 @llvm.sdiv.fix.i4(i4 3, i4 4, i32 1) ; %res = 2 (or 1) (1.5 / 2 = 0.75). '``llvm.udiv.fix.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrins",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:630959,perform,performs,630959,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"""${CMAKE_CURRENT_SOURCE_DIR}/../${proj}""); if(NOT EXISTS ""${PROJ_DIR}"" OR NOT IS_DIRECTORY ""${PROJ_DIR}""); message(FATAL_ERROR ""LLVM_ENABLE_PROJECTS requests ${proj} but directory not found: ${PROJ_DIR}""); endif(); if( LLVM_EXTERNAL_${upper_proj}_SOURCE_DIR STREQUAL """" ); set(LLVM_EXTERNAL_${upper_proj}_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../${proj}"" CACHE PATH """" FORCE); else(); set(LLVM_EXTERNAL_${upper_proj}_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../${proj}"" CACHE PATH """"); endif(); elseif (""${proj}"" IN_LIST LLVM_EXTERNAL_PROJECTS); message(STATUS ""${proj} project is enabled""); set(SHOULD_ENABLE_PROJECT TRUE); else(); message(STATUS ""${proj} project is disabled""); set(SHOULD_ENABLE_PROJECT FALSE); endif(); # Force `LLVM_TOOL_${upper_proj}_BUILD` variables to have values that; # corresponds with `LLVM_ENABLE_PROJECTS`. This prevents the user setting; # `LLVM_TOOL_${upper_proj}_BUILD` variables externally. At some point; # we should deprecate allowing users to set these variables by turning them; # into normal CMake variables rather than cache variables.; set(LLVM_TOOL_${upper_proj}_BUILD; ${SHOULD_ENABLE_PROJECT}; CACHE; BOOL ""Whether to build ${upper_proj} as part of LLVM"" FORCE; ); endforeach(); endif(); unset(SHOULD_ENABLE_PROJECT). # Build llvm with ccache if the package is present; set(LLVM_CCACHE_BUILD OFF CACHE BOOL ""Set to ON for a ccache enabled build""); if(LLVM_CCACHE_BUILD); find_program(CCACHE_PROGRAM ccache); if(CCACHE_PROGRAM); set(LLVM_CCACHE_MAXSIZE """" CACHE STRING ""Size of ccache""); set(LLVM_CCACHE_DIR """" CACHE STRING ""Directory to keep ccached data""); set(LLVM_CCACHE_PARAMS ""CCACHE_CPP2=yes CCACHE_HASHDIR=yes""; CACHE STRING ""Parameters to pass through to ccache""). if(NOT CMAKE_SYSTEM_NAME MATCHES ""Windows""); set(CCACHE_PROGRAM ""${LLVM_CCACHE_PARAMS} ${CCACHE_PROGRAM}""); if (LLVM_CCACHE_MAXSIZE); set(CCACHE_PROGRAM ""CCACHE_MAXSIZE=${LLVM_CCACHE_MAXSIZE} ${CCACHE_PROGRAM}""); endif(); if (LLVM_CCACHE_DIR); set(CCACHE_PROGRAM ""CCACHE_DIR=${LLVM_C",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:10190,cache,cache,10190,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['cache'],['cache']
Performance,""",-20,20) ;; x.setBins(1000,""cache"") ;. // End point shapes : a gaussian on one end, a polynomial on the other; RooGaussian f1(""f1"",""f1"",x,RooConst(-10),RooConst(2)) ;; RooPolynomial f2(""f2"",""f2"",x,RooArgSet(RooConst(-0.03),RooConst(-0.001))) ;. // Interpolation parameter: rlm=f1 at alpha=0, rlm=f2 at alpha=1; RooRealVar alpha(""alpha"",""alpha"",0,1.0) ;; RooLinearMorph rlm(""rlm"",""rlm"",g1,g2,x,alpha) ;. // Plot halfway shape; alpha=0.5; RooPlot* frame = x.frame() ;; rlm.plotOn(frame) ;. In short the algorithm works as follows: for both f1(x) and f2(x), the cumulative distribution; functions F1(x) and F2(x) are calculated. One finds takes a value 'y' of both c.d.fs and ; determines the corresponding x values x1,x2 at which F1(x1)=F2(x2)=y. The value of the interpolated ; p.d.f fbar(x) is then calculated as fbar(alpha*x1+(1-alpha)*x2) = f1(x1)*f2(x2) / ( alpha*f2(x2) + ; (1-alpha)*f1(x1) ). Given that it is not easily possible to calculate the value of RooLinearMorph; at a given value of x, the value for all values of x are calculated in one by (through a scan over y); and stored in a cache. NB: The range of the interpolation parameter does not need to be [0,1], it can; be anything. New workspace tool RooSimWSTool. A new tool to clone and customize p.d.f.s into a RooSimultaneous p.d.f has been added. This new; tool succeeds the original RooSimPdfBuilder tool which had a similar functionality but; has a much cleaner interface, partly thanks to its use of the RooWorkspace class for both input; of prototype p.d.fs and output of built p.d.f.s. The simplest use case to to take a workspace p.d.f as prototype and 'split' a parameter of that p.d.f ; into two specialized parameters depending on a category in the dataset. ; For example, given a Gaussian p.d.f G(x,m,s) we want to construct a G_a(x,m_a,s) and a G_b(x,m_b,s); with different mean parameters to be fit to a dataset with observables; (x,c) where c is a category with states 'a' and 'b'.; Using RooSimWSTool one can create a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:11305,cache,cache,11305,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,1,['cache'],['cache']
Performance,"""IEEE""; ""support_math_errno"", ""on"", ""on"", ""off""; ""no_honor_nans"", ""off"", ""off"", ""on""; ""no_honor_infinities"", ""off"", ""off"", ""on""; ""no_signed_zeros"", ""off"", ""off"", ""on""; ""allow_reciprocal"", ""off"", ""off"", ""on""; ""allow_approximate_fns"", ""off"", ""off"", ""on""; ""allow_reassociation"", ""off"", ""off"", ""on"". .. option:: -ffast-math. Enable fast-math mode. This option lets the; compiler make aggressive, potentially-lossy assumptions about; floating-point math. These include:. * Floating-point math obeys regular algebraic rules for real numbers (e.g.; ``+`` and ``*`` are associative, ``x/y == x * (1/y)``, and; ``(a + b) * c == a * c + b * c``),; * Operands to floating-point operations are not equal to ``NaN`` and; ``Inf``, and; * ``+0`` and ``-0`` are interchangeable. ``-ffast-math`` also defines the ``__FAST_MATH__`` preprocessor; macro. Some math libraries recognize this macro and change their behavior.; With the exception of ``-ffp-contract=fast``, using any of the options; below to disable any of the individual optimizations in ``-ffast-math``; will cause ``__FAST_MATH__`` to no longer be set.; ``-ffast-math`` enables ``-fcx-limited-range``. This option implies:. * ``-fno-honor-infinities``. * ``-fno-honor-nans``. * ``-fapprox-func``. * ``-fno-math-errno``. * ``-ffinite-math-only``. * ``-fassociative-math``. * ``-freciprocal-math``. * ``-fno-signed-zeros``. * ``-fno-trapping-math``. * ``-fno-rounding-math``. * ``-ffp-contract=fast``. Note: ``-ffast-math`` causes ``crtfastmath.o`` to be linked with code. See; :ref:`crtfastmath.o` for more details. .. option:: -fno-fast-math. Disable fast-math mode. This options disables unsafe floating-point; optimizations by preventing the compiler from making any transformations that; could affect the results. This option implies:. * ``-fhonor-infinities``. * ``-fhonor-nans``. * ``-fno-approx-func``. * ``-fno-finite-math-only``. * ``-fno-associative-math``. * ``-fno-reciprocal-math``. * ``-fsigned-zeros``. * ``-ffp-contract=on``. Also, this op",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:52490,optimiz,optimizations,52490,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"""Test builder"");; root[] geom->Edit(Option_t *option="""");; ```. The lines above will create a new **`TGeoManager`** class, create an; empty canvas and start the editor in the left-sided editor frame; attached to the canvas. To open the editor in a separate frame one; should provide a non-empty string as option to the `Edit()` method. ![The geometry manager editor](pictures/030001E9.png). ### The Geometry Manager Editor. ![Accessing/creating different categories of editable; objects](pictures/020001EA.jpg) ![Accessing/creating different; categories of editable objects](pictures/020001EB.jpg); ![Accessing/creating different categories of editable; objects](pictures/020001EC.jpg) ![Accessing/creating different; categories of editable objects](pictures/020001ED.jpg); ![Accessing/creating different categories of editable; objects](pictures/020001EE.jpg). The second use case applies when starting to edit an existing geometry.; Supposing the geometry was loaded into memory, besides the first method; that still applies one can also edit drawn geometry objects. For this,; the menu entry View/Editor of the canvas containing for instance a drawn; volume must be activated. For starting the volume editor one can click; on a volume. The GUI of the **`TGeoManager`** class can be started by; clicking on the top-right `40x40` pixels corner of the pad with a drawn; geometry. This is the main entry point for editing the geometry or creating new; objects. Once the interface is created (using one of the methods; described above), several categories can be accessed via a shutter GUI; widget:. - *General.* This allows changing the name/title of the geometry,; setting the top volume, closing the geometry and saving the geometry; in a file. The file name is formed by `geometry_name.C` or `.root`; depending if the geometry need to be saved as a `C` macro or a; `.root` file.; - *Shapes.* The category provides buttons for creation of all; supported shapes. The new shape name is chosen by the in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:167207,load,loaded,167207,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loaded']
Performance,"""Verdana"", fontsize=""12""];; node [fontname=""Verdana"", fontsize=""12""];; edge [fontname=""Sans"", fontsize=""9""];. manual [label="" Manual PrintF "", shape=""box""];; int1 [label="" int ( &) 42 "", shape=""box""]; auto [label="" Automatic PrintF "", shape=""box""];; int2 [label="" int ( &) 42 "", shape=""box""]. auto -> int2 [label=""int x = 42; \n x""];; manual -> int1 [label=""int x = 42; \n printf(&quot;(int &) %d \\n&quot;, x);""];; }. Significance of this feature; ----------------------------. Inspired by a similar implementation in `Cling <https://github.com/root-project/cling>`_,; this feature added to upstream Clang repo has essentially extended the syntax of; C++, so that it can be more helpful for people that are writing code for data; science applications. This is useful, for example, when you want to experiment with a set of values; against a set of functions, and you'd like to know the results right away.; This is similar to how Python works (hence its popularity in data science; research), but the superior performance of C++, along with this flexibility; makes it a more attractive option. Implementation Details; ======================. Parsing mechanism:; ------------------. The Interpreter in Clang-Repl (``Interpreter.cpp``) includes the function; ``ParseAndExecute()`` that can accept a 'Value' parameter to capture the result.; But if the value parameter is made optional and it is omitted (i.e., that the; user does not want to utilize it elsewhere), then the last value can be; validated and pushed into the ``dump()`` function. .. graphviz::; :name: parsing; :caption: Parsing Mechanism; :alt: Shows the Parsing Mechanism for Pretty Printing; :align: center. digraph ""prettyprint"" {; rankdir=""LR"";; graph [fontname=""Verdana"", fontsize=""12""];; node [fontname=""Verdana"", fontsize=""12""];; edge [fontname=""Verdana"", fontsize=""9""];. parse [label="" ParseAndExecute() \n in Clang "", shape=""box""];; capture [label="" Capture 'Value' parameter \n for processing? "", shape=""diamond""];; use [label",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangRepl.rst:15599,perform,performance,15599,interpreter/llvm-project/clang/docs/ClangRepl.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangRepl.rst,1,['perform'],['performance']
Performance,"""`+`""- add function to the list; without deleting the previous one. When fitting a histogram, the; function is attached to the histogram's list of functions. By default,; the previously fitted function is deleted and replaced with the most; recent one, so the list only contains one function. Setting this; option to On will add the newly fitted function to the existing list; of functions for the histogram. Note that the fitted functions are; saved with the histogram when it is written to a ROOT file. By; default, the function is drawn on the pad displaying the histogram. ### Draw Options. *‘SAME'* sets On/Off function drawing on the same pad. When a fit is; executed, the image of the function is drawn on the current pad. *‘No drawing'* sets On/Off the option ""`0`""- do not draw the fit; results. *‘Do not store/draw'* sets On/Off option ""`N`""- do not store the; function and do not draw it. ### Advances Options. The advance option button is enabled only after having performed the fit and provides; additional drawing options that can be used after having done the fit. These new drawing tools,; which can be selected by the ""Advanced Drawing Tool"" panel that pops up when clicking the ""Advanced"" button, are:. * *Contour*: to plot the confidence contour of two chosen parameters. One can select the number of points to draw the contour; (more points might require more time to compute it), the parameters and the desired confidence level . * *Scan* : to plot a scan of the minimization function (likelihood or chi-squared) around the minimum as function of the chosen parameter. * *Conf Interval* : to plot the confidence interval of the fitted function as a filled coloured band around its central value.; One can select the desired confidence level for the band to be plotted. ### Print Options. This set of options specifies the amount of feedback printed on the; root command line after performed fits. *‘Verbose'* - prints fit results after each iteration. *‘Quiet'* - no fit informat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:24551,perform,performed,24551,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,2,['perform'],['performed']
Performance,"""cpu""));. // fit using the CUDA library along with the most efficient library that the computer's CPU can support; RooMyPDF.fitTo(data, BatchMode(""cuda""));; ```; **Note: In case the system does not support vector instructions, the `RooBatchCompute::Cpu` option is guaranteed to work properly by using a generic CPU library. In contrast, users must first make sure that their system supports CUDA in order to use the `RooBatchCompute::Cuda` option. If this is not the case, an exception will be thrown.**. If `""cuda""` is selected, RooFit will launch CUDA kernels for computing likelihoods and potentially other intense computations. At the same time, the most efficient CPU library loaded will also handle parts of the computations in parallel with the GPU (or potentially, if it's faster, all of them), thus gaining full advantage of the available hardware. For this purpose `RooFitDriver`, a newly created RooFit class (in roofitcore) takes over the task of analyzing the computations and assigning each to the correct piece of hardware, taking into consideration the performance boost or penalty that may arise with every method of computing. #### Multithread computations; The CPU instance of the computing library can furthermore execute multithread computations. This also applies for computations handled by the CPU in the `""cuda""` mode. To use them, one needs to set the desired number of parallel tasks before calling `fitTo()` as shown below:; ``` {.cpp}; ROOT::EnableImplicitMT(nThreads);; RooMyPDF.fitTo(data, BatchMode(""cuda"")); // can also use ""cuda""; ```. ### User-made PDFs; The easiest and most efficient way of accelerating your PDFs is to request their addition to the official RooFit by submitting a ticket [here](https://github.com/root-project/root/issues/new). The ROOT team will gladly assist you and take care of the details. While your code is integrated, you are able to significantly improve the speed of fitting (but not take full advantage of the RooBatchCompute library),",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/batchcompute.md:3058,perform,performance,3058,roofit/doc/developers/batchcompute.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/batchcompute.md,1,['perform'],['performance']
Performance,"""global""; ID number space for the current translation unit, providing a 1-1 mapping; between entities (in whatever AST file they inhabit) and global ID numbers.; If that translation unit is then serialized into an AST file, this mapping; will be stored for use when the AST file is imported. Declaration merging; It is possible for a given entity (from the language's perspective) to be; declared multiple times in different places. For example, two different; headers can have the declaration of ``printf`` or could forward-declare; ``struct stat``. If each of those headers is included in a module, and some; third party imports both of those modules, there is a potentially serious; problem: name lookup for ``printf`` or ``struct stat`` will find both; declarations, but the AST nodes are unrelated. This would result in a; compilation error, due to an ambiguity in name lookup. Therefore, the AST; reader performs declaration merging according to the appropriate language; semantics, ensuring that the two disjoint declarations are merged into a; single redeclaration chain (with a common canonical declaration), so that it; is as if one of the headers had been included before the other. Name Visibility; Modules allow certain names that occur during module creation to be ""hidden"",; so that they are not part of the public interface of the module and are not; visible to its clients. The AST reader maintains a ""visible"" bit on various; AST nodes (declarations, macros, etc.) to indicate whether that particular; AST node is currently visible; the various name lookup mechanisms in Clang; inspect the visible bit to determine whether that entity, which is still in; the AST (because other, visible AST nodes may depend on it), can actually be; found by name lookup. When a new (sub)module is imported, it may make; existing, non-visible, already-deserialized AST nodes visible; it is the; responsibility of the AST reader to find and update these AST nodes when it; is notified of the import. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:28975,perform,performs,28975,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['performs']
Performance,"""stage 1""; compiler. This is done so that the compiler you distribute benefits from all the; bug fixes, performance optimizations and general improvements provided by the; new compiler. In deciding how to build your distribution there are a few trade-offs that you; will need to evaluate. The big two are:. #. Compile time of the distribution against performance of the built compiler. #. Binary size of the distribution against performance of the built compiler. The guidance for maximizing performance of the generated compiler is to use LTO,; PGO, and statically link everything. This will result in an overall larger; distribution, and it will take longer to generate, but it provides the most; opportunity for the compiler to optimize. The guidance for minimizing distribution size is to dynamically link LLVM and; Clang libraries into the tools to reduce code duplication. This will come at a; substantial performance penalty to the generated binary both because it reduces; optimization opportunity, and because dynamic linking requires resolving symbols; at process launch time, which can be very slow for C++ code. .. _shared_libs:. .. warning::; One very important note: Distributions should never be built using the; *BUILD_SHARED_LIBS* CMake option. That option exists for optimizing developer; workflow only. Due to design and implementation decisions, LLVM relies on; global data which can end up being duplicated across shared libraries; resulting in bugs. As such this is not a safe way to distribute LLVM or; LLVM-based tools. The simplest example of building a distribution with reasonable performance is; captured in the DistributionExample CMake cache file located at; clang/cmake/caches/DistributionExample.cmake. The following command will perform; and install the distribution build:. .. code-block:: console. $ cmake -G Ninja -C <path to clang>/cmake/caches/DistributionExample.cmake <path to LLVM source>; $ ninja stage2-distribution; $ ninja stage2-install-distribution. Diff",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:1858,perform,performance,1858,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"# Architecture of the VecOps library. > This document is meant for ROOT developers, to quickly get their bearings around the VecOps library. The main type in the library is `RVec`. Besides `RVec`, the library only contains helper types and functions. `RVec` is a vector type that tries to be as `std::vector`-like as possible while adding a; few important features, namely:; - the ability to act as a view over an existing memory buffer (see ""Memory adoption"" below); - a small-buffer optimization; - vectorized operator overloads; - a vectorized `operator[](mask)` to allow quick element selection together with vectorized operators; (e.g. `etas[etas > k]` returns a new `RVec` with all elements greater than `k`); - helper functions such as `InvariantMass`, `DeltaR`, `Argsort` are also provided. The current implementation of `RVec` is based on LLVM's SmallVector, extracted; from the head of LLVM's repo around December 2020.; We are not tracking the upstream implementation. Compared to LLVM's SmallVectors:. - memory adoption capabilities have been added; - patches have been applied to make RVec work with (ROOT's version of) cppyy (notably `using` declarations had to be; lowered in the inheritance hierarchy for cppyy to pick them up); - `operator[](mask)` has been added, as well as several other ""numpy-like"" helper; functions (these latter ones are free functions); - logical operators `==`, `<`, `>` etc. return vectors rather than booleans; - the type of fSize and fCapacity is signed rather than unsigned, and fixed to 32 bits; - a number of minor patches have been applied for backward compatibility with the previous; implementation of RVec (which did not have a small buffer optimization and was implemented; in terms of `std::vector` with a custom allocator) and to make the code more consistent; with ROOT's coding conventions. ## RVec design. `SmallVectorBase`; - `fBeginX`; - `fSize`; - `fCapacity`. Basically the same as the corresponding LLVM class, with the template parameter",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/vecops/ARCHITECTURE.md:485,optimiz,optimization,485,math/vecops/ARCHITECTURE.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/vecops/ARCHITECTURE.md,1,['optimiz'],['optimization']
Performance,"# C++ Modules in ROOT. Technology Overview. *Vassil Vassilev, Oksana Shadura, Yuka Takahashi and Raphael Isemann*. ## Overview. ROOT has several features which interact with libraries and require implicit; header inclusion. This can be triggered by reading or writing data on disk,; or user actions at the prompt. Often, the headers are immutable and reparsing is; redundant. C++ Modules are designed to minimize the reparsing of the same; header content by providing an efficient on-disk representation of C++ Code. The ROOT v6.16 release came with a preview of the module technology;; dedicated binaries have been built and can be reproduced by passing; `-Druntime_cxxmodules=On` as configure flag. The goals of this technology are:; * Gain feedback from early adoption -- the technology is being long anticipated; by some of the users of ROOT. It improves correctness of ROOT and improves; performance when carefully adopted.; * Study performance bottlenecks -- the feature is designed with performance; considerations in mind. In this document we describe the current performance; bottlenecks and trade-offs.; * Understand if the gradual migration policy is sufficient -- C++ Modules in; ROOT support gradual migration. In particular, ROOT can enable C++ Modules for; itself and still run in legacy mode for the third-party code (generating; rootmap files and other scaffolding). C++ Modules are here and we would like to give a brief introduction of how the; feature works, what are its pros and cons, what's the current state of the; implementation and how third-party code can use it. Read more [[1]]. C++ Modules in ROOT are default since v6.20 (Unix) and v6.22 (OSX). ## Design Goals. * Coherence with standard C++ -- C++ Modules TS is advancing and will be; likely part the upcoming C++20 standard;; * Performance -- provide performance that is competitive to ROOT with PCH and; advance further the implementation of the C++ Modules in clang to optimize; memory footprint and execution time;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:893,perform,performance,893,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,4,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"# Copyright (C) 1995-2019, Rene Brun and Fons Rademakers.; # All rights reserved.; #; # For the licensing terms see $ROOTSYS/LICENSE.; # For the list of contributors see $ROOTSYS/README/CREDITS. # Test library loads during importing ROOT; # Testing only the Linux systems is sufficient to detect unwanted links to libraries at import time.; # Mac (and potentially Windows) pull in many system libraries which makes this test very complex.; if (NOT APPLE AND NOT WIN32); ROOT_ADD_PYUNITTEST(pyroot_import_load_libs import_load_libs.py); endif(). # Test ROOT module; ROOT_ADD_PYUNITTEST(pyroot_root_module root_module.py). # @pythonization decorator; ROOT_ADD_PYUNITTEST(pyroot_pyz_decorator pythonization_decorator.py). # General pythonizations; ROOT_ADD_PYUNITTEST(pyroot_pyz_pretty_printing pretty_printing.py); ROOT_ADD_PYUNITTEST(pyroot_pyz_array_interface array_interface.py PYTHON_DEPS numpy). # STL containers pythonizations; ROOT_ADD_PYUNITTEST(pyroot_pyz_stl_vector stl_vector.py); ROOT_ADD_PYUNITTEST(pyroot_pyz_stl_set stl_set.py). # TObject and subclasses pythonisations; ROOT_ADD_PYUNITTEST(pyroot_pyz_tobject_contains tobject_contains.py); ROOT_ADD_PYUNITTEST(pyroot_pyz_tobject_comparisonops tobject_comparisonops.py). # TClass pythonisations; ROOT_ADD_PYUNITTEST(pyroot_pyz_tclass_dynamiccast tclass_dynamiccast.py). # TContext pythonizations; ROOT_ADD_PYUNITTEST(pyroot_pyz_tcontext_contextmanager tcontext_contextmanager.py). # TDirectory and subclasses pythonizations; ROOT_ADD_PYUNITTEST(pyroot_pyz_tdirectory_attrsyntax tdirectory_attrsyntax.py); ROOT_ADD_PYUNITTEST(pyroot_pyz_tdirectoryfile_attrsyntax_get tdirectoryfile_attrsyntax_get.py); ROOT_ADD_PYUNITTEST(pyroot_pyz_tfile_attrsyntax_get_writeobject_open tfile_attrsyntax_get_writeobject_open.py); ROOT_ADD_PYUNITTEST(pyroot_pyz_tfile_constructor tfile_constructor.py); ROOT_ADD_PYUNITTEST(pyroot_pyz_tfile_context_manager tfile_context_manager.py). # TTree and subclasses pythonizations; file(COPY TreeHelper.h DESTINATION ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/CMakeLists.txt:210,load,loads,210,bindings/pyroot/pythonizations/test/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/CMakeLists.txt,1,['load'],['loads']
Performance,"# Copyright (C) 1995-2019, Rene Brun and Fons Rademakers.; # All rights reserved.; #; # For the licensing terms see $ROOTSYS/LICENSE.; # For the list of contributors see $ROOTSYS/README/CREDITS. ############################################################################; # CMakeLists.txt file for building ROOT core/base package; ############################################################################. if(MSVC AND MSVC_VERSION GREATER_EQUAL 1925 AND MSVC_VERSION LESS 1929); # FIXME: since Visual Studio v16.5.0 the /O2 optimization flag makes most of the roofit/roostats tests failing; # Try to re-enable /O2 after the upgrade of llvm & clang, or when upgrading Visual Studio; string(REPLACE ""-O2"" ""-O1 -Oi"" CMAKE_CXX_FLAGS_RELEASE ""${CMAKE_CXX_FLAGS_RELEASE}""); string(REPLACE ""-O2"" ""-O1 -Oi"" CMAKE_CXX_FLAGS_RELWITHDEBINFO ""${CMAKE_CXX_FLAGS_RELWITHDEBINFO}""); endif(). set(BASE_HEADERS; ROOT/TErrorDefaultHandler.hxx; ROOT/TExecutorCRTP.hxx; ROOT/TSequentialExecutor.hxx; ROOT/StringConv.hxx; Buttons.h; Bytes.h; Byteswap.h; KeySymbols.h; MessageTypes.h; Riostream.h; Rtypes.h; TApplication.h; TAtt3D.h; TAttAxis.h; TAttBBox2D.h; TAttBBox.h; TAttFill.h; TAttLine.h; TAttMarker.h; TAttPad.h; TAttText.h; TBase64.h; TBenchmark.h; TBuffer3D.h; TBuffer3DTypes.h; TBuffer.h; TColor.h; TColorGradient.h; TDatime.h; TDirectory.h; TEnv.h; TException.h; TExec.h; TFileCollection.h; TFileInfo.h; TFolder.h; TInetAddress.h; TMacro.h; TMathBase.h; TMD5.h; TMemberInspector.h; TMessageHandler.h; TNamed.h; TNotifyLink.h; TObject.h; TObjString.h; TParameter.h; TPluginManager.h; TPoint.h; TPRegexp.h; TProcessID.h; TProcessUUID.h; TQClass.h; TQCommand.h; TQConnection.h; TQObject.h; TRedirectOutputGuard.h; TRefCnt.h; TRef.h; TRegexp.h; TRemoteObject.h; TROOT.h; TRootIOCtor.h; TStopwatch.h; TStorage.h; TString.h; TStringLong.h; TStyle.h; TSysEvtHandler.h; TSystemDirectory.h; TSystemFile.h; TSystem.h; TTask.h; TThreadSlots.h; TTime.h; TTimer.h; TTimeStamp.h; TUri.h; TUrl.h; TUUID.h; TVersionCheck.h;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/base/CMakeLists.txt:528,optimiz,optimization,528,core/base/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/base/CMakeLists.txt,1,['optimiz'],['optimization']
Performance,"# Copyright (C) 1995-2019, Rene Brun and Fons Rademakers.; # All rights reserved.; #; # For the licensing terms see $ROOTSYS/LICENSE.; # For the list of contributors see $ROOTSYS/README/CREDITS. ############################################################################; # CMakeLists.txt file for building ROOT misc/minicern package; ############################################################################. ROOT_LINKER_LIBRARY(minicern *.c *.f TYPE STATIC); set_property(TARGET minicern PROPERTY POSITION_INDEPENDENT_CODE ON); target_link_libraries(minicern ${CMAKE_Fortran_IMPLICIT_LINK_LIBRARIES}). # Disable optimization since it some cases was causing crashes.; # Disable warnings, since what has worked for 40 years...; # (see https://sft.its.cern.ch/jira/browse/ROOT-9179 for the warnings); set_target_properties(minicern PROPERTIES COMPILE_FLAGS ""-O0 -w""); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/misc/minicern/CMakeLists.txt:618,optimiz,optimization,618,misc/minicern/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/minicern/CMakeLists.txt,1,['optimiz'],['optimization']
Performance,"# Copyright (C) 1995-2019, Rene Brun and Fons Rademakers.; # All rights reserved.; #; # For the licensing terms see $ROOTSYS/LICENSE.; # For the list of contributors see $ROOTSYS/README/CREDITS. include(ExternalProject). # Clear cache variables set by find_package(PCRE); # to ensure that we use the builtin version; foreach(var PCRE_FOUND PCRE_VERSION PCRE_INCLUDE_DIR PCRE_PCRE_LIBRARY PCRE_LIBRARIES); unset(${var} CACHE); endforeach(). if(WIN32); if(CMAKE_GENERATOR MATCHES Ninja); if (CMAKE_BUILD_TYPE MATCHES Debug); set(PCRE_POSTFIX d); endif(); else(); if(winrtdebug); set(PCRE_POSTFIX $<$<CONFIG:Debug>:d>); set(pcre_config_kind ""Debug""); else(); set(pcre_config_kind ""Release""); endif(); set(pcre_config ""--config ${pcre_config_kind}""); endif(); endif(). set(PCRE_VERSION ""8.43"" CACHE INTERNAL """" FORCE); set(PCRE_LIBNAME ${CMAKE_STATIC_LIBRARY_PREFIX}pcre${PCRE_POSTFIX}${CMAKE_STATIC_LIBRARY_SUFFIX}). # build byproducts only needed by Ninja; if(""${CMAKE_GENERATOR}"" STREQUAL ""Ninja""); set(PCRE_BYPRODUCTS; <BINARY_DIR>/pcre.h; <BINARY_DIR>/${PCRE_LIBNAME}; ); endif(). ExternalProject_Add(PCRE; URL ${CMAKE_CURRENT_SOURCE_DIR}/pcre-${PCRE_VERSION}.tar.bz2; URL_HASH SHA256=91e762520003013834ac1adb4a938d53b22a216341c061b0cf05603b290faf6b. LOG_DOWNLOAD TRUE; LOG_CONFIGURE TRUE; LOG_BUILD TRUE; LOG_INSTALL TRUE. CMAKE_CACHE_ARGS; -DCMAKE_INSTALL_PREFIX:PATH=<INSTALL_DIR>; -DCMAKE_GENERATOR:STRING=${CMAKE_GENERATOR}; -DCMAKE_BUILD_TYPE:STRING=${CMAKE_BUILD_TYPE}; -DCMAKE_C_COMPILER:STRING=${CMAKE_C_COMPILER}; -DCMAKE_CXX_COMPILER:STRING=${CMAKE_CXX_COMPILER}; -DCMAKE_BUILD_SHARED_LIBS:BOOL=FALSE; -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=TRUE. BUILD_COMMAND; ${CMAKE_COMMAND} --build <BINARY_DIR> ${pcre_config} --target pcre. BUILD_BYPRODUCTS; ${PCRE_BYPRODUCTS}. INSTALL_COMMAND """"; TIMEOUT 600; ). ExternalProject_Get_Property(PCRE BINARY_DIR). set(PCRE_FOUND TRUE CACHE INTERNAL """" FORCE); set(PCRE_INCLUDE_DIR ""${BINARY_DIR}"" CACHE INTERNAL """" FORCE); if(WIN32); set(PCRE_PCRE_LIBR",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/builtins/pcre/CMakeLists.txt:229,cache,cache,229,builtins/pcre/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/builtins/pcre/CMakeLists.txt,1,['cache'],['cache']
Performance,"# Copyright (C) 1995-2019, Rene Brun and Fons Rademakers.; # All rights reserved.; #; # For the licensing terms see $ROOTSYS/LICENSE.; # For the list of contributors see $ROOTSYS/README/CREDITS. set(py_sources; cppyy_backend/__init__.py; cppyy_backend/_cling_config.py; cppyy_backend/_cppyy_generator.py; cppyy_backend/_genreflex.py; cppyy_backend/_rootcling.py; cppyy_backend/bindings_utils.py; cppyy_backend/loader.py; cppyy_backend/_get_cppflags.py; ). set(cppyy_backendPySrcDir cling/python/cppyy_backend); file(COPY ${cppyy_backendPySrcDir}; DESTINATION ${localruntimedir}; PATTERN ""cmake"" EXCLUDE; PATTERN ""pkg_templates"" EXCLUDE). file(RELATIVE_PATH PYTHONDIR_TO_LIBDIR ""${CMAKE_INSTALL_FULL_PYTHONDIR}"" ""${CMAKE_INSTALL_FULL_LIBDIR}""). set(libname cppyy_backend). add_library(${libname} SHARED clingwrapper/src/clingwrapper.cxx); if(MSVC); set_target_properties(${libname} PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS TRUE); endif(); # Set the suffix to '.so' and the prefix to 'lib'; set_target_properties(${libname} PROPERTIES ${ROOT_LIBRARY_PROPERTIES}); target_link_libraries(${libname} Core ${CMAKE_DL_LIBS}). # cppyy uses ROOT headers from binary directory; add_dependencies(${libname} move_headers). set_property(GLOBAL APPEND PROPERTY ROOT_EXPORTED_TARGETS ${libname}). # Install library; install(TARGETS ${libname} EXPORT ${CMAKE_PROJECT_NAME}Exports; RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT libraries; LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} COMPONENT libraries; ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} COMPONENT libraries); if (NOT MSVC AND NOT CMAKE_INSTALL_LIBDIR STREQUAL CMAKE_INSTALL_PYTHONDIR); # add a symlink to ${libname} in CMAKE_INSTALL_PYTHONDIR; set(LIB_FILE_NAME ${CMAKE_SHARED_LIBRARY_PREFIX}${libname}.so); install(CODE ""file(CREATE_LINK ${PYTHONDIR_TO_LIBDIR}/${LIB_FILE_NAME}; \$ENV{DESTDIR}${CMAKE_INSTALL_FULL_PYTHONDIR}/${LIB_FILE_NAME} SYMBOLIC)""); endif(). # Compile .py files; foreach(py_source ${py_sources}); install(CODE ""execute_process(CO",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/CMakeLists.txt:410,load,loader,410,bindings/pyroot/cppyy/cppyy-backend/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/CMakeLists.txt,1,['load'],['loader']
Performance,"# Copyright (C) 1995-2023, Rene Brun and Fons Rademakers.; # All rights reserved.; #; # For the licensing terms see $ROOTSYS/LICENSE.; # For the list of contributors see $ROOTSYS/README/CREDITS. #####################################################################################################################. # Details about integrating ROOT into CMake projects:; # https://root.cern/manual/integrate_root_into_my_cmake_project/. #####################################################################################################################. # CMakeLists.txt that creates a library with dictionary and a main program; cmake_minimum_required(VERSION 3.10 FATAL_ERROR). project(treeUsingCustomClass). #---Locate the ROOT package and defines a number of variables (e.g. ROOT_INCLUDE_DIRS); find_package(ROOT REQUIRED COMPONENTS Tree TreePlayer ROOTDataFrame). #---Include a CMake module which makes use of the previous variables and loads modules ; # with useful macros or functions such as ROOT_GENERATE_DICTIONARY; # For further details: https://root-forum.cern.ch/t/how-to-integrate-root-into-my-project-with-cmake/37175; include(${ROOT_USE_FILE}). #---Add include directory of ROOT to the build; include_directories(${CMAKE_SOURCE_DIR}). # CMake function provided by ROOT, used to generate the dictionary file, G__data2Tree.cxx; # See this link for further details:; # https://root.cern/manual/io_custom_classes/#using-cmake; ROOT_GENERATE_DICTIONARY(G__data2Tree data2Tree.hxx LINKDEF data2TreeLinkDef.hxx). #---Create a shared library from; # * the previously generated dictionary, G__data2Tree.cxx; # * the class implementation; add_library(data2TreeLib SHARED data2Tree.cxx G__data2Tree.cxx); target_link_libraries(data2TreeLib ${ROOT_LIBRARIES} ) ; add_dependencies(data2TreeLib G__data2Tree ). #--- This is needed on Windows in order to export the symbols and create the data2TreeLib.lib file; if(MSVC); set_target_properties(data2TreeLib PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS TRUE)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/tree/dictionary/CMakeLists.txt:942,load,loads,942,tutorials/tree/dictionary/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tree/dictionary/CMakeLists.txt,1,['load'],['loads']
Performance,"# Debug Info Assignment Tracking. Assignment Tracking is an alternative technique for tracking variable location; debug info through optimisations in LLVM. It provides accurate variable; locations for assignments where a local variable (or a field of one) is the; LHS. In rare and complicated circumstances indirect assignments might be; optimized away without being tracked, but otherwise we make our best effort to; track all variable locations. The core idea is to track more information about source assignments in order; and preserve enough information to be able to defer decisions about whether to; use non-memory locations (register, constant) or memory locations until after; middle end optimisations have run. This is in opposition to using; `llvm.dbg.declare` and `llvm.dbg.value`, which is to make the decision for most; variables early on, which can result in suboptimal variable locations that may; be either incorrect or incomplete. A secondary goal of assignment tracking is to cause minimal additional work for; LLVM pass writers, and minimal disruption to LLVM in general. ## Status and usage. **Status**: Experimental work in progress. Enabling is strongly advised against; except for development and testing. **Enable in Clang**: `-Xclang -fexperimental-assignment-tracking`. That causes Clang to get LLVM to run the pass `declare-to-assign`. The pass; converts conventional debug intrinsics to assignment tracking metadata and sets; the module flag `debug-info-assignment-tracking` to the value `i1 true`. To; check whether assignment tracking is enabled for a module call; `isAssignmentTrackingEnabled(const Module &M)` (from `llvm/IR/DebugInfo.h`). ## Design and implementation. ### Assignment markers: `llvm.dbg.assign`. `llvm.dbg.value`, a conventional debug intrinsic, marks out a position in the; IR where a variable takes a particular value. Similarly, Assignment Tracking; marks out the position of assignments with a new intrinsic called; `llvm.dbg.assign`. In order to k",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AssignmentTracking.md:338,optimiz,optimized,338,interpreter/llvm-project/llvm/docs/AssignmentTracking.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AssignmentTracking.md,1,['optimiz'],['optimized']
Performance,"# File I/O and Parallel Analysis #. ## Storing ROOT Objects ##. ROOT offers the possibility to write instances of classes on; disk, into a *ROOT-file* (see the `TFile` class for more details).; One says that the object is made ""persistent"" by storing; it on disk. When reading the file back, the object is reconstructed; in memory. The requirement to be satisfied to perform I/O of instances; of a certain class is that the ROOT type system is aware of the layout; in memory of that class.; This topic is beyond the scope of this document: it is worth to mention; that I/O can be performed out of the box for the almost complete set; of ROOT classes. We can explore this functionality with histograms and two simple macros. ``` {.cpp}; @ROOT_INCLUDE_FILE macros/write_to_file.C; ```. Not bad, eh ? Especially for a language that does not foresees; persistency natively like C++. The *RECREATE* option forces ROOT to; create a new file even if a file with the same name exists on disk. Now, you may use the Cling command line to access information in the file; and draw the previously written histogram:. ``` {.cpp}; > root my_rootfile.root; root [0]; Attaching file my_rootfile.root as _file0...; root [1] _file0->ls(); TFile** my_rootfile.root; TFile* my_rootfile.root; KEY: TH1F	my_histogram;1 My Title; root [2] my_histogram->Draw(); ```; \newpage; Alternatively, you can use a simple macro to carry out the job:. ``` {.cpp}; @ROOT_INCLUDE_FILE macros/read_from_file.C; ```. ## N-tuples in ROOT ##. ### Storing simple N-tuples ###. Up to now we have seen how to manipulate input read from ASCII files.; ROOT offers the possibility to do much better than that, with its own; n-tuple classes. Among the many advantages provided by these classes one; could cite. - Optimised disk I/O. - Possibility to store many n-tuple rows. - Write the n-tuples in ROOT files. - Interactive inspection with `TBrowser`. - Store not only numbers, but also *objects* in the columns. In this section we will discuss bri",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md:367,perform,perform,367,documentation/primer/filio.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md,2,['perform'],"['perform', 'performed']"
Performance,"# Functions and Parameter Estimation #. After going through the previous chapters, you already know how to use; analytical functions (class `TF1`), and you got some insight into the; graph (`TGraphErrors`) and histogram classes (`TH1F`) for data; visualisation. In this chapter we will add more detail to the previous; approximate explanations to face the fundamental topic of parameter; estimation by fitting functions to data. For graphs and histograms, ROOT; offers an easy-to-use interface to perform fits - either the fit panel; of the graphical interface, or the `Fit` method. The class `TFitResult`; allows access to the detailed results. Very often it is necessary to study the statistical properties of; analysis procedures. This is most easily achieved by applying the; analysis to many sets of simulated data (or ""pseudo data""), each; representing one possible version of the true experiment. If the; simulation only deals with the final distributions observed in data, and; does not perform a full simulation of the underlying physics and the; experimental apparatus, the name ""Toy Monte Carlo"" is frequently used; [^5]. Since the true values of all parameters are known in the; pseudo-data, the differences between the parameter estimates from the; analysis procedure w.r.t. the true values can be determined, and it is; also possible to check that the analysis procedure provides correct; error estimates. ## Fitting Functions to Pseudo Data ##. In the example below, a pseudo-data set is produced and a model fitted; to it. ROOT offers various minimisation algorithms to minimise a chi2 or a; negative log-likelihood function. The default minimiser is MINUIT, a; package originally implemented in the FORTRAN programming language. A; C++ version is also available, MINUIT2, as well as Fumili [@Fumili] an; algorithm optimised for fitting. The; minimisation algorithms can be selected using the static functions of; the `ROOT::Math::MinimizerOptions` class. Steering options for the; min",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/functions_and_parameter_estimation.md:497,perform,perform,497,documentation/primer/functions_and_parameter_estimation.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/functions_and_parameter_estimation.md,1,['perform'],['perform']
Performance,"# JSROOT changelog. ## Changes in dev; 1. Let use custom time zone for time display, support '&utc' and '&cet' in URL parameters; 2. Support gStyle.fLegendFillStyle; 3. Let change histogram min/max values via context menu; 4. Support Z-scale zooming with TScatter; 5. Implement ""haxis"" draw option for histogram to draw only axes for hbar; 6. Implement ""axisg"" and ""haxisg"" to draw axes with grids; 7. Support TH1 marker, text and line drawing superimposed with ""haxis""; 8. Support `TBox`, `TLatex`, `TLine`, `TMarker` drawing on ""frame"", support drawing on swapped axes; 9. `TProfile` and `TProfile2D` projections https://github.com/root-project/root/issues/15851; 10. Draw total histogram from TEfficiency when draw option starts with 'b'; 11. Let redraw TEfficiency, THStack and TMultiGraph with different draw options via hist context menu; 12. Support 'pads' draw options for TMultiGraph, support context menu for it; 13. Let drop object on sub-pads; 14. Properly loads ES6 modules for web canvas; 15. Improve performance of TH3/RH3 drawing by using THREE.InstancedMesh; 16. Implement batch mode with '&batch' URL parameter to create SVG/PNG images with default GUI; 17. Adjust node.js implementation to produce identical output with normal browser; 18. Create necessary infrastructure for testing with 'puppeteer'; 19. Support inject of ES6 modules via '&inject=path.mjs'; 20. Using importmap for 'jsroot' in all major HTML files and in demos; 21. Implement `settings.CutAxisLabels` flag to remove labels which may exceed graphical range; 22. Let disable usage of TAxis custom labels via context menu; 23. Let configure default draw options via context menu, they can be preserved in the local storage; 24. Let save canvas as JSON file from context menu, object as JSON from inspector; 25. Upgrade three.js r162 -> r168, use r162 only in node.js because of ""gl"" module; 26. Create unified svg2pdf/jspdf ES6 modules, integrate in jsroot builds; 27. Let create multipage PDF document - in TWebCanv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:969,load,loads,969,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loads']
Performance,"# LLVM TableGen Kernel. This notebook is running `llvm-tblgen`. ```tablegen; %reset; // This is some tablegen; class Foo {}; ```. ------------- Classes -----------------; class Foo {; }; ------------- Defs -----------------. Errors printed to stderr are shown. ```tablegen; %reset; This is not tablegen.; ```. <stdin>:1:1: error: Unexpected token at top level; This is not tablegen.; ^. Add some classes to get some output. ```tablegen; %reset; class Stuff {}; def thing : Stuff {}; ```. ------------- Classes -----------------; class Stuff {; }; ------------- Defs -----------------; def thing {	// Stuff; }. By default cells are connected. Meaning that we cache the code and magic directives from the previously run cells. This means that the next cell still sees the `Stuff` class. ```tablegen; def other_thing : Stuff {}; ```. ------------- Classes -----------------; class Stuff {; }; ------------- Defs -----------------; def other_thing {	// Stuff; }; def thing {	// Stuff; }. You can use the magic `%reset` to clear this cache and start fresh. ```tablegen; %reset; def other_thing : Stuff {}; ```. <stdin>:1:19: error: Couldn't find class 'Stuff'; def other_thing : Stuff {}; ^. You can also configure the default reset behaviour using the `%config` magic. ```tablegen; %config cellreset on; class Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------. ```tablegen; // The cache is reset here so this is an error.; def AThing: Thing {}; ```. <stdin>:2:13: error: Couldn't find class 'Thing'; def AThing: Thing {}; ^. The default value is `off`, meaning cells are connected. If you want to override the default for one cell only, use the `%reset` or `%noreset` magic. These always override the default. ```tablegen; class Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------. ```tablegen; %noreset; // This works because of the noreset above.; def AThing: Thing {}; ```. --------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md:658,cache,cache,658,interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,1,['cache'],['cache']
Performance,"# Latency tests for RWebWindow. Provide round-trip test under different conditions.; To run, execute `root ""ping.cxx(10,0)""`, where first argument is number of connections tested and; second argument is running mode. Can be tested:; 0 - default communication, no extra threads; 1 - minimal timer for THttpServer, should reduce round-trip significantly; 2 - use special thread for process requests in THttpServer, web window also runs in the thread; 3 - in addition to special THttpThread also window starts own thread; 4 - let invoke webwindow callbacks in the civetweb threads, expert mode only. One also can perform same tests with longpoll emulation of web sockets, if adding 10 to second parameter. When running in batch mode, function blocked until 200 round-trip packets send by the client; or 50s elappsed. Therefore ping.cxx test can be used for RWebWindow functionality tests ; like `root -l -b ""ping.cxx(10,2)"" -q`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/ping/Readme.md:610,perform,perform,610,tutorials/webgui/ping/Readme.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/ping/Readme.md,1,['perform'],['perform']
Performance,"# Linear Algebra in ROOT. The linear algebra package is supposed to give a complete environment in; ROOT to perform calculations like equation solving and eigenvalue; decompositions. Most calculations are performed in double precision. For; backward compatibility, some classes are also provided in single; precision like **`TMatrixF`**, **`TMatrixFSym`** and **`TVectorF`**.; Copy constructors exist to transform these into their double precision; equivalent, thereby allowing easy access to decomposition and eigenvalue; classes, only available in double precision. The choice was made not to provide the less frequently used complex; matrix classes. If necessary, users can always reformulate the; calculation in 2 parts, a real one and an imaginary part. Although, a; linear equation involving complex numbers will take about a factor of 8; more computations, the alternative of introducing a set of complex; classes in this non-template library would create a major maintenance; challenge. Another choice was to fill in both the upper-right corner and the; bottom-left corner of a symmetric matrix. Although most algorithms use; only the upper-right corner, implementation of the different matrix; views was more straightforward this way. When stored only the; upper-right part is written to file. For a detailed description of the interface, the user should look at the; root reference guide at: <http://root.cern.ch/root/Reference.html>. ## Overview of Matrix Classes. The figure below shows an overview of the classes available in the; linear algebra library,` libMatrix.so`. At the center is the base class; **`TMatrixDBase`** from which three different matrix classes,; **`TMatrixD`**, **`TMatrixDSym`** and **`TMatrixDFSparse`** derive. The; user can define customized matrix operations through the classes; **`TElementActionD`** and **`TElementsPosActionD`**. ![Overview of matrix classes](pictures/0300012D.png). Reference to different views of the matrix can be created through the; clas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md:108,perform,perform,108,documentation/users-guide/LinearAlgebra.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md,2,['perform'],"['perform', 'performed']"
Performance,"# MLGO Python Utilities. This folder contains MLGO Python utilities, particularly infrastructure; to help enable ML applications within LLVM, especially tooling to extract; corpora that can be used in downstream projects to train ML models and perform; other tasks that benefit from having a large amount of data. ### Python Versioning. Due to type annotations, the MLGO tooling currently only supports a Python; version greater than 3.8, deviating from the current LLVM project-wide; minimum supported version of Python 3.6.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/README.md:244,perform,perform,244,interpreter/llvm-project/llvm/utils/mlgo-utils/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/README.md,1,['perform'],['perform']
Performance,"# Math Libraries in ROOT. The aim of Math libraries in ROOT is to provide and to support a; coherent set of mathematical and statistical functions. The latest; developments have been concentrated in providing first versions of the; `MathCore` and `MathMore` libraries, included in ROOT v5.08. Other; recent developments include the new version of `MINUIT`, which has been; re-designed and re-implemented in the C++ language. It is integrated in; ROOT. In addition, an optimized package for describing small matrices; and vector with fixed sizes and their operation has been developed; (`SMatrix`). The structure is shown in the following picture. ![Math libraries and packages](pictures/02000109.jpg). ## MathCore Library. `MathCore` provides a collection of functions and C++ classes for; numerical computing. This library includes only the basic mathematical; functions and algorithms and not all the functionality required by the; physics community. A more advanced mathematical functionality is; provided by the `MathMore` library. The current set of included classes,; which are provided in the `ROOT::Math` namespace are:. - Basic special functions like the gamma, beta and error function. - Mathematical functions used in statistics, such as the probability; density functions and the cumulative distributions functions (lower; and upper integral of the pdf's). - Generic function classes and interfaces; for evaluating one-dimensional (`ROOT::Math::IBaseFunctiononeDim`) and multi-dimensional functions; (`ROOT::Math::IBaseFunctionMultiDim`) and parametric function interfaces for evaluating functions with parameters in one; (`ROOT::Math::IParametricFunctionOneDim`) or multi dimensions (`ROOT::Math::IParametricFunctionMultiDim`).; 	A set of user convenient wrapper classes, such as `ROOT::Math::Functor` is provided for wrapping user-classes in the needed interface,; 	required to use the algorithms of the `ROOT` Mathematical libraries. - Numerical algorithms interfaces and in same cases ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:468,optimiz,optimized,468,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['optimiz'],['optimized']
Performance,"# PROOF: Parallel Processing; \index{PROOF}; \index{parallel processing}. The Parallel ROOT Facility, PROOF, is an extension of ROOT allowing; transparent analysis of large sets of ROOT files in parallel on remote; computer clusters or multi-core computers. The main design goals for the; PROOF system are:. *Transparency* : there should be as little difference as possible; between a local ROOT based analysis session and a remote parallel PROOF; session, both being interactive and giving the same results. *Scalability* : the basic architecture should not put any implicit; limitations on the number of computers that can be used in parallel. *Adaptability* : the system should be able to adapt itself to variations; in the remote environment (changing load on the cluster nodes, network; interruptions, etc.). Being an extension of the ROOT system, PROOF is designed to work on; objects in ROOT data stores, though, for the time being, it mainly; addresses the case of **`TTree`** based object collections. PROOF is primarily meant as an interactive alternative to batch systems; for Central Analysis Facilities and departmental workgroups (Tier-2's).; However, thanks to a multi-tier architecture allowing multiple levels of; masters, it can be easily adapted to wide range virtual clusters; distributed over geographically separated domains and heterogeneous; machines (GRIDs). While pure interactivity might not always be possible when performing a; complicated analysis on a very large data set, PROOF still tries to give; the user the interactive experience with something we call ""interactive; batch"". With ""interactive batch"" the user can start very long running; queries, disconnect the client and at any time, any location and from; any computer reconnect to the query to monitor its progress or retrieve; the results. This feature gives it a distinct advantage over purely; batch based solutions, that only provide an answer once all sub-jobs; have been finished. ![The Multi-tier struct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PROOF.md:756,load,load,756,documentation/users-guide/PROOF.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PROOF.md,1,['load'],['load']
Performance,"# Physics Vectors. The physics vector classes describe vectors in three and four dimensions; and their rotation algorithms. The classes were ported to root from; CLHEP see:. <http://www.cern.ch/clhep/manual/UserGuide/Vector/vector.html>. ## The Physics Vector Classes. In order to use the physics vector classes you will have to load the; Physics library:. ``` {.cpp}; gSystem.Load(""libPhysics.so"");; ```. There are four classes in this package. They are:. **`TVector3`** is a general three-vector. A **`TVector3`** may be; expressed in Cartesian, polar, or cylindrical coordinates. Methods; include dot and cross products, unit vectors and magnitudes, angles; between vectors, and rotations and boosts. There are also functions of; particular use to HEP, like pseudo-rapidity, projections, and transverse; part of a **`TVector3`**, and kinetic methods on 4-vectors such as; Invariant Mass of pairs or containers of particles`.`. **`TLorentzVector`** is a general four-vector class, which can be used; either for the description of position and time (`x`, `y`, `z`, `t`) or; momentum and energy (`px`, `py`, `pz`, `E`). **`TRotation`** is a class; describing a rotation of a **`TVector3`** object. **`TLorentzRotation`**; is a class to describe the Lorentz transformations including Lorentz; boosts and rotations. In addition, a **`TVector2`** is a basic; implementation of a vector in two dimensions and is not part of the; CLHEP translation. ## TVector3. ![](pictures/030001A9.png). **`TVector3`** is a general three-vector; class, which can be used for description of different vectors in 3D.; Components of three vectors:. - $x$, $y$, $z$ = basic components. - $\theta$ = azimuth angle. - $\phi$ = polar angle. - magnitude = $mag$ = $\sqrt{x^2 + y^2 + z^2}$. - transverse component = $perp$ = $\sqrt{x^2 + y^2}$. Using the **`TVector3`** class, you should remember that it contains; only common features of three vectors and lacks methods specific for; some particular vector values. For example, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PhysicsVectors.md:329,load,load,329,documentation/users-guide/PhysicsVectors.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PhysicsVectors.md,1,['load'],['load']
Performance,"# Python Interface; \index{Python}. Python is a popular, open-source, dynamic programming language with an; interactive interpreter. Its interoperability with other programming; languages, both for extending Python as well as embedding it, is; excellent and many existing third-party applications and libraries have; therefore so-called ""Python bindings."" PyROOT provides Python bindings; for ROOT: it enables cross-calls from ROOT/Cling into Python and vice; versa, the intermingling of the two interpreters, and the transport of; user-level objects from one interpreter to the other. PyROOT enables; access from ROOT to any application or library that itself has Python; bindings, and it makes all ROOT functionality directly available from; the python interpreter. ## PyROOT Overview. The Python scripting language is widely used for scientific programming,; including high performance and distributed parallel code (see; <http://www.scipy.org>). It is the second most popular scripting; language (after Perl) and enjoys a wide-spread use as a ""glue language"":; practically every library and application these days comes with Python; bindings (and if not, they can be easily written or generated). `PyROOT`, a Python extension module, provides the bindings for the ROOT; class library in a generic way using the Cling dictionary. This way, it; allows the use of any ROOT classes from the Python interpreter, and thus; the ""glue-ing"" of ROOT libraries with any non-ROOT library or; applications that provide Python bindings. Further, `PyROOT` can be; loaded into the Cling interpreter to allow (as of now still rudimentary); access to Python classes. The best way to understand the benefits of; `PyROOT` is through a few examples. ### Glue-ing Applications. The `PyQt` library, see <http://www.riverbankcomputing.co.uk/pyqt>,; provides Python bindings for the Qt cross-platform GUI framework (; <http://www.trolltech.com>). With `PyROOT` and `PyQt`, adding ROOT; application layer code to a Qt GUI, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:877,perform,performance,877,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['perform'],['performance']
Performance,"# ROOT Development Practice. ## Overview. The development of ROOT almost exclusively happens using the [pull request](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests); model of github. A pull request (PR) should contain a set focused changes; organized in one or more [atomic commits](https://en.wikipedia.org/wiki/Atomic_commit#Revision_control).; PRs should be well-documented and well-tested in order to allow other community; members to use, maintain and modify. If the PR contains performance-critical; code consider writing a benchmark against the [rootbench repository](https://github.com/root-project/rootbench). ## Quality Assurance. Each contribution should contain developer documentation in the form of code; comments and sufficient amount of tests in the form of unit and/or integration; tests. Unit tests are relatively small and quick programs focused to check if; small pieces of code and API work as expected. Integration tests are checks; which ensure the synergy between different (unit tested) components. Put in; practice, unit tests verify (member) function behavior whereas integration tests; check classes and their cooperation. The boundary between both kinds of testing; is blurred. ROOT has support for both kinds of tests in the [roottest repository](https://github.com/root-project/roottest); and supports ""inline"" unit tests in each component's `test` folder. Unit testing; uses the [GTest and GMock](https://github.com/google/googletest) infrastructure; along with small ROOT-specific extensions located in; [TestSupport](../core/test_support/). The documentation of GTest; and GMock is rather extensive and we will describe some of the features of; ROOT::TestSupport. In order to write an inline unit test, add a new file in the; nearest to the tested component's `test` folder and call `ROOT_ADD_GTEST` in the; `CMakeLists.txt` file. In many cases using standard GTest facility is sufficient to write a good test.; How",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/DEVELOPMENT.md:536,perform,performance-critical,536,README/DEVELOPMENT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/DEVELOPMENT.md,1,['perform'],['performance-critical']
Performance,"# ROOT Macros #. You know how other books go on and on about programming fundamentals and; finally work up to building a complete, working program ? Let's skip all; that. In this guide, we will describe macros executed by the ROOT C++; interpreter Cling. It is relatively easy to compile a macro, either as a pre-compiled; library to load into ROOT, or as a stand-alone application, by adding; some include statements for header file or some ""dressing code"" to any; macro. ## General Remarks on ROOT macros ##. If you have a number of lines which you were able to execute at the ROOT; prompt, they can be turned into a ROOT macro by giving them a name which; corresponds to the file name without extension. The general structure; for a macro stored in file `MacroName.C` is. ``` {.cpp}; void MacroName() {; < ...; your lines of C++ code; ... >; }; ```. The macro is executed by typing. ``` {.cpp}; > root MacroName.C; ```. at the system prompt, or executed using `.x`. ``` {.cpp}; > root; root [0] .x MacroName.C; ```. at the ROOT prompt. or it can be loaded into a ROOT session and then; be executed by typing. ``` {.cpp}; root [0].L MacroName.C; root [1] MacroName();; ```. at the ROOT prompt. Note that more than one macro can be loaded this; way, as each macro has a unique name in the ROOT name space. A small set; of options can help making your plot nicer. ``` {.cpp}; gROOT->SetStyle(""Plain""); // set plain TStyle; gStyle->SetOptStat(111111); // draw statistics on plots,; // (0) for no output; gStyle->SetOptFit(1111); // draw fit results on plot,; // (0) for no ouput; gStyle->SetPalette(57); // set color map; gStyle->SetOptTitle(0); // suppress title box; ...; ```. Next, you should create a canvas for graphical output, with size,; subdivisions and format suitable to your needs, see documentation of; class `TCanvas`:. ``` {.cpp}; TCanvas c1(""c1"",""<Title>"",0,0,400,300); // create a canvas, specify position and size in pixels; c1.Divide(2,2); //set subdivisions, called pads; c1.cd(1); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/your_first_ROOT_macro.md:334,load,load,334,documentation/primer/your_first_ROOT_macro.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/your_first_ROOT_macro.md,1,['load'],['load']
Performance,"# ROOTR Users Guide. ## DESCRIPTION; ROOT R is an interface in ROOT to call R functions using an R C++ interface (Rcpp, see http://dirk.eddelbuettel.com/code/rcpp.html).; This interface opens the possibility in ROOT to use the very large set of mathematical and statistical tools provided by R.; With ROOTR you can perform a conversion from ROOT's C++ objects to R's objects, transform the returned R objects into ROOT's C++ objects, then; the R functionality can be used directly for statistical studies in ROOT. ## ROOTR BASICS; ROOTR creates a working environment to execute R coding called from `C++`. It allows to translate some datatypes from `C++` to R; inside the R environment and vice versa in an easy way to get the most from both R and ROOT.; To ease the sending and receiving of data in both environments, I overloaded the operators `<<`,`>>` and `[]`; which make look the job as a flow of data between environments, we will see more of that later.; With this tool you ca use any library or R package wich allows you to access a big amount of benefits to make statistical analysis.; ROOTR also has a R events processing system, which allows to use the R graphical system from `C++`. ## INSTALLATION; To install ROOTR please read first. - [https://root.cern.ch/building-root](https://root.cern.ch/building-root); - [https://root.cern.ch/build-prerequisites](https://root.cern.ch/build-prerequisites). ### COMPILING ROOTR ON MAC WITH CMAKE:; **NOTE:** Mac OSX Yosemite last xcode and without macports. **Prerequisites**. - xcode; - [xquartz](http://xquartz.macosforge.org/); - [R last version](https://www.r-project.org); - [cmake](https://cmake.org/download/). To compile with cmake added into ~/.profile. ~~~{.sh}; export PATH=$PATH:/Applications/CMake.app/Contents/bin/; ~~~; and. ~~~{.sh}; source ~/.profile; ~~~. Install needed R packages, open R and in the prompt type. ~~~{.sh}; install.packages(c('Rcpp','RInside')); ~~~; select a mirror and install. Install the next additional pac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/r/doc/users-guide/ROOTR_Users_Guide.md:315,perform,perform,315,bindings/r/doc/users-guide/ROOTR_Users_Guide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/r/doc/users-guide/ROOTR_Users_Guide.md,1,['perform'],['perform']
Performance,"# Speculative Load Hardening. ## A Spectre Variant #1 Mitigation Technique. Author: Chandler Carruth - [chandlerc@google.com](mailto:chandlerc@google.com). ## Problem Statement. Recently, Google Project Zero and other researchers have found information leak; vulnerabilities by exploiting speculative execution in modern CPUs. These; exploits are currently broken down into three variants:; * GPZ Variant #1 (a.k.a. Spectre Variant #1): Bounds check (or predicate) bypass; * GPZ Variant #2 (a.k.a. Spectre Variant #2): Branch target injection; * GPZ Variant #3 (a.k.a. Meltdown): Rogue data cache load. For more details, see the Google Project Zero blog post and the Spectre research; paper:; * https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html; * https://spectreattack.com/spectre.pdf. The core problem of GPZ Variant #1 is that speculative execution uses branch; prediction to select the path of instructions speculatively executed. This path; is speculatively executed with the available data, and may load from memory and; leak the loaded values through various side channels that survive even when the; speculative execution is unwound due to being incorrect. Mispredicted paths can; cause code to be executed with data inputs that never occur in correct; executions, making checks against malicious inputs ineffective and allowing; attackers to use malicious data inputs to leak secret data. Here is an example,; extracted and simplified from the Project Zero paper:; ```; struct array {; unsigned long length;; unsigned char data[];; };; struct array *arr1 = ...; // small array; struct array *arr2 = ...; // array of size 0x400; unsigned long untrusted_offset_from_caller = ...;; if (untrusted_offset_from_caller < arr1->length) {; unsigned char value = arr1->data[untrusted_offset_from_caller];; unsigned long index2 = ((value&1)*0x100)+0x200;; unsigned char value2 = arr2->data[index2];; }; ```. The key of the attack is to call this with `untrusted_off",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:591,cache,cache,591,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,"['cache', 'load']","['cache', 'load']"
Performance,"# The Geometry Package; \index{Geometry}. The new ROOT geometry package is a tool for building, browsing,; navigating and visualizing detector geometries. The code works; standalone with respect to any tracking Monte-Carlo engine; therefore,; it does not contain any constraints related to physics. However, the; navigation features provided by the package are designed to optimize; particle transport through complex geometries, working in correlation; with simulation packages such as GEANT3, GEANT4 and FLUKA. ## Quick Start: Creating the ""world"". This chapter will provide a detailed description on how to build valid; geometries as well as the ways to optimize them. There are several; components gluing together the geometrical model, but for the time being; let us get used with the most basic concepts. The basic bricks for building-up the model are called; `volumes`**.**These represent the un-positioned pieces of the geometry; puzzle. The difference is just that the relationship between the pieces; is not defined by neighbors, but by `containment`. In other words,; volumes are put one inside another making an in-depth hierarchy. From; outside, the whole thing looks like a big pack that you can open finding; out other smaller packs nicely arranged waiting to be opened at their; turn. The biggest one containing all others defines the ""`world`"" of the; model. We will often call this `master reference system (MARS)`. Going; on and opening our packs, we will obviously find out some empty ones,; otherwise, something is very wrong... We will call these leaves (by; analogy with a tree structure). On the other hand, any volume is a small world by itself - what we need; to do is to take it out and to ignore all the rest since it is a; self-contained object. In fact, the modeller can act like this,; considering a given volume as temporary MARS, but we will describe this; feature later on. Let us focus on the biggest pack - it is mandatory to; define one. Consider the simplest geom",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:373,optimiz,optimize,373,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,2,['optimiz'],['optimize']
Performance,"# Threads. A thread is an independent flow of control that operates within the same; address space as other independent flows of controls within a process.; In most UNIX systems, thread and process characteristics are grouped; into a single entity called a process. Sometimes, threads are called; ""lightweight processes''. Note: This introduction is adapted from the AIX 4.3 Programmer's Manual. ## Threads and Processes. In traditional single-threaded process systems, a process has a set of; properties. In multi-threaded systems, these properties are divided; between processes and threads. ### Process Properties. A process in a multi-threaded system is the changeable entity. It must; be considered as an execution frame. It has all traditional process; attributes, such as:. - Process ID, process group ID, user ID, and group ID. - Environment. - Working directory. A process also provides a common address space and common system; resources:. - File descriptors. - Signal actions. - Shared libraries. - Inter-process communication tools (such as message queues, pipes,; semaphores, or shared memory). ### Thread Properties. A thread is the schedulable entity. It has only those properties that; are required to ensure its independent flow of control. These include; the following properties:. - Stack. - Scheduling properties (such as policy or priority). - Set of pending and blocked signals. - Some thread-specific data (TSD). An example of thread-specific data is the error indicator, `errno`. In; multi-threaded systems, `errno` is no longer a global variable, but; usually a subroutine returning a thread-specific `errno` value. Some; other systems may provide other implementations of `errno`. With respect; to ROOT, a thread specific data is for example the ***`gPad`*** pointer,; which is treated in a different way, whether it is accessed from any; thread or the main thread. Threads within a process must not be considered as a group of processes; (even though in Linux each thread re",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:509,multi-thread,multi-threaded,509,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,2,['multi-thread'],['multi-threaded']
Performance,"# Trees. ## Why Should You Use a Tree?. In the ""Input/Output"" chapter, we saw how objects can be saved in ROOT; files. In case you want to store large quantities of same-class objects,; ROOT has designed the **`TTree`** and **`TNtuple`** classes specifically; for that purpose. The **`TTree`** class is optimized to reduce disk; space and enhance access speed. A **`TNtuple`** is a **`TTree`** that is; limited to only hold floating-point numbers; a **`TTree`** on the other; hand can hold all kind of data, such as objects or arrays in addition to; all the simple types. When using a **`TTree`**, we fill its branch buffers with leaf data and; the buffers are written to disk when it is full. Branches, buffers, and; leafs, are explained a little later in this chapter, but for now, it is; important to realize that each object is not written individually, but; rather collected and written a bunch at a time. This is where the **`TTree`** takes advantage of compression and will; produce a much smaller file than if the objects were written; individually. Since the unit to be compressed is a buffer, and the; **`TTree`** contains many same-class objects, the header of the objects; can be compressed. The **`TTree`** reduces the header of each object, but it still contains; the class name. Using compression, the class name of each same-class; object has a good chance of being compressed, since the compression; algorithm recognizes the bit pattern representing the class name. Using; a **`TTree`** and compression the header is reduced to about 4 bytes; compared to the original 60 bytes. However, if compression is turned; off, you will not see these large savings. The **`TTree`** is also used to optimize the data access. A tree uses a; hierarchy of branches, and each branch can be read independently from; any other branch. Now, assume that `Px` and `Py` are data members of the; event, and we would like to compute `Px2 + Py2` for every event; and histogram the result. If we had saved the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:303,optimiz,optimized,303,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['optimiz'],['optimized']
Performance,"# Workaround for MSVC ARM64 performance regression:; # https://developercommunity.visualstudio.com/t/Compiling-a-specific-code-for-ARM64-with/10444970; # Since /O1 and /O2 represent a set of optimizations,; # our goal is to disable the /Og flag while retaining the other optimizations from the /O1|/O2 set; if(MSVC AND NOT CMAKE_CXX_COMPILER_ID MATCHES Clang; AND MSVC_VERSION VERSION_GREATER_EQUAL 1932; AND CMAKE_SYSTEM_PROCESSOR MATCHES ""ARM64""). string(TOUPPER ""${CMAKE_BUILD_TYPE}"" uppercase_CMAKE_BUILD_TYPE); string(REGEX MATCHALL ""/[Oo][12]"" opt_flags ""${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_${uppercase_CMAKE_BUILD_TYPE}}""); if (opt_flags); if(opt_flags MATCHES ""1$""); set(opt_flags ""/Od;/Os;/Oy;/Ob2;/GF;/Gy""); elseif (opt_flags MATCHES ""2$""); set(opt_flags ""/Od;/Oi;/Ot;/Oy;/Ob2;/GF;/Gy""); endif(); set_source_files_properties(StandardLibrary.cpp PROPERTIES COMPILE_OPTIONS ""${opt_flags}""); endif(); endif(). add_clang_library(clangToolingInclusionsStdlib; StandardLibrary.cpp. LINK_LIBS; clangAST; ); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/Inclusions/Stdlib/CMakeLists.txt:28,perform,performance,28,interpreter/llvm-project/clang/lib/Tooling/Inclusions/Stdlib/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/Inclusions/Stdlib/CMakeLists.txt,3,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"# llvm-exegesis. `llvm-exegesis` is a benchmarking tool that accepts or assembles a snippet and; can measure characteristics of that snippet by executing it while keeping track; of performance counters. ### Currently Supported Platforms. `llvm-exegesis` is quite platform-dependent and currently only supports a couple; platform configurations for benchmarking. The limitations are listed below.; Analysis mode in `llvm-exegesis` is supported on all platforms on which LLVM is. #### Currently Supported Operating Systems for Benchmarking. Currently, `llvm-exegesis` only supports benchmarking on Linux. This is mainly; due to a dependency on the Linux perf subsystem for reading performance; counters. The subprocess execution mode and memory annotations currently only supports; Linux due to a heavy reliance on many Linux specific syscalls/syscall; implementations. #### Currently Supported Architectures for Benchmarking. Currently, using `llvm-exegesis` for benchmarking is supported on the following; architectures:; * x86; * 64-bit only due to this being the only implemented calling convention; in `llvm-exegesis` currently.; * ARM; * AArch64 only; * MIPS; * PowerPC (PowerPC64LE only). Note that not benchmarking functionality is guaranteed to work on all platforms. Memory annotations are currently only supported on 64-bit X86. There is no; inherent limitations for porting memory annotations to other architectures, but; parts of the test harness are implemented as MCJITed assembly that is generated; in `./lib/X86/Target.cpp` that would need to be implemented on other architectures; to bring up support.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/README.md:181,perform,performance,181,interpreter/llvm-project/llvm/tools/llvm-exegesis/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/README.md,2,['perform'],['performance']
Performance,"# rlibmap. The tools used to generate rootmap files are rootcling and genreflex. The; rlibmap tool is not present any more in ROOT starting from release 6.00.00. ### Rootmap files. To enhance the set of functionalities offered by ROOT and its new interpreter,; the format of the rootmaps evolved. Rootmap in the old format cannot be; produced anymore but only read. The new rootmaps can be still be concatenated.; A rootmap file now contains:. - One (or more) section for forward declarations. These are real C++; forward declarations of templates and namespaces. This is needed for Cling; to be able to parse templates' instantiations and for some autoloading; functionalities.; - One (or more) libraries sections. These sections describe the ensamble of; the autoload keys related to one or more shared libraries. An autoload key; can be a class name, a namespace name, a typedef or alias or a header file name.; - Single line comments, which start with a ""#"" character. At ROOT startup, a check is performed on autoload keys. If the same key (which is not a template instantiation) refers to two different libraries (or sets of libraries) a warning is issued.; A typical Rootmap file look like:; ``` {.cpp}; { decls }; fwd declaration 1;; fwd declaration 2;; [...]; fwd declaration N;. [ libraryName1 libraryName2 ... ]; class className1; class className2; ...; typedef typedefName1; typedef typedefName2; ...; header headerName1; header headerName2; ... ```. ### TROOT. The list returned by `GetListOfTypes` is no longer filled when the dictionary; are loaded but instead are filled on demand, when the user explicitly (directly; or indirectly) request each typedef. In particular this means that. ``` {.cpp}; gROOT->GetListOfTypes()->ls(); // or Print(); ```. no longer prints the list of all available typedef but instead list only the; typedefs that have been previously accessed throught the list (plus the builtins; types). ### ACliC. ACLiC has the following backward incompatibilities:. - S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v600/index.md:10175,perform,performed,10175,core/doc/v600/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v600/index.md,1,['perform'],['performed']
Performance,"# rootreadspeed. `rootreadspeed` is a tool used to help identify bottlenecks in root analysis programs; by providing an idea of what throughput you can expect when reading ROOT files in; certain configurations. It does this by providing information about the number of bytes read from your files,; how long this takes, and the different throughputs in MB/s, both in total and per thread. ## Compressed vs Uncompressed Throughput:. Throughput speeds are provided as compressed and uncompressed - ROOT files are usually; saved in compressed format, so these will often differ. Compressed bytes is the total; number of bytes read from TFiles during the readspeed test (possibly including meta-data).; Uncompressed bytes is the number of bytes processed by reading the branch values in the TTree.; Throughput is calculated as the total number of bytes over the total runtime (including; decompression time) in the uncompressed and compressed cases. ## Interpreting results:. ### There are three possible scenarios when using rootreadspeed, namely:. - The 'Real Time' is significantly lower than your own analysis runtime.; This would imply your actual application code is dominating the runtime of your analysis,; ie. your analysis logic or framework is taking up the time.; The best way to decrease the runtime would be to optimize your code (or the framework's),; parallelize it onto multiple threads if possible (for example with; [RDataFrame](https://root.cern/doc/master/classROOT_1_1RDataFrame.html); and [EnableImplicitMT](https://root.cern/doc/master/namespaceROOT.html#a06f2b8b216b615e5abbc872c9feff40f)); or switch to a machine with a more performant CPU.; - The 'Real Time' is significantly higher than 'CPU Time / number of threads'*.; If the real time is higher than the CPU time per core it implies the reading of data is the; bottleneck, as the CPU cores are wasting time waiting for data to arrive from your disk/drive; or network connection in order to decompress it.; The best way to dec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md:65,bottleneck,bottlenecks,65,tree/readspeed/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md,3,"['bottleneck', 'throughput']","['bottlenecks', 'throughput', 'throughputs']"
Performance,"## Core Libraries. ### Interpreter. The new interface `TInterpreter::Declare(const char* code)` will declare the; code to the interpreter with all interpreter extensions disabled, i.e. as; ""proper"" C++ code. No autoloading or synamic lookup will be performed. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v602/index.md:249,perform,performed,249,core/doc/v602/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v602/index.md,1,['perform'],['performed']
Performance,"## OPENUI5 Panel example. This is simplest way to use openui5 widget with RWebWindow. It is normal xml::View, but controller should be derived from rootui5/panel/Controller.; This class provides methods, which simplify handling of communication between server and client. First of all, when creating RWebWindow, one should configure panel name. Like:. auto win = ROOT::RWebWindow::Create();. win->SetPanelName(""localapp.view.TestPanel"");. Namespace ""localapp"" in this case corresponds to openui5 files, which will be loaded from current directory.; Therefore `""localapp.view.TestPanel""` means view, which will be loaded from `./view/TestPanel.view.xml` file. Controller is configured in the XML file and called `""localapp.controller.TestPanel""`.; Means it will be loaded from `./controller/TestPanel.controller.js` file. In the controller one use `onPanelInit` and `onPanelExit` methods to handle initialization and close of widget.; Method `panelSend` should be used to send string data to the server. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/panel/Readme.md:517,load,loaded,517,tutorials/webgui/panel/Readme.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/panel/Readme.md,3,['load'],['loaded']
Performance,"## Tree Libraries. ### TTreeReader. ROOT offers a new class `TTreeReader` that gives simple, safe and fast access to the content of a `TTree`.; Using it is trivial:. ``` {.cpp}; #include ""TFile.h""; #include ""TH1F.h""; #include ""TTreeReader.h""; #include ""TTreeReaderValue.h"". void hsimpleReader() {; TH1F *myHist = new TH1F(""h1"",""ntuple"",100,-4,4);; TFile *myFile = TFile::Open(""hsimple.root"");. // Create a TTreeReader for the tree, for instance by passing the; // TTree's name and the TDirectory / TFile it is in.; TTreeReader myReader(""ntuple"", myFile);. // The branch ""px"" contains floats; access them as myPx.; TTreeReaderValue<Float_t> myPx(myReader, ""px"");; // The branch ""py"" contains floats, too; access those as myPy.; TTreeReaderValue<Float_t> myPy(myReader, ""py"");. // Loop over all entries of the TTree or TChain.; while (myReader.Next()) {; // Just access the data as if myPx and myPy were iterators (note the '*'; // in front of them):; myHist->Fill(*myPx + *myPy);; }. myHist->Draw();; }; ```. TTreeReader checks whether the type that you expect can be extracted from the tree's branch and will clearly complain if not.; It reads on demand: only data that are actually needed are read, there is no need for `SetBranchStatus()`, `SetBranchAddress()`, `LoadTree()` or anything alike.; It uses the memory management of TTree, removing possible double deletions or memory leaks and relieveing you from the need to manage the memory yourself.; It turns on the tree cache, accelerating the reading of data.; It has been extensively tested on all known types of TTree branches and is thus a generic, fits-all access method for data stored in TTrees. ### TTreePlayer. - The TEntryList for ||-Coord plot was not defined correctly. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v600/index.md:1474,cache,cache,1474,tree/doc/v600/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v600/index.md,1,['cache'],['cache']
Performance,"### TTree Behavior change. #### Merging. Added fast cloning support to TTree::MergeTrees and TTree::Merge(TCollection*,Option_t*). #### TTreeCache. The TTreeCache is now enabled by default. The default size of the TTreeCache; is the estimated size of a cluster size for the TTree. The TTreeCache; prefilling is also enabled by default; when in learning phase rather than; reading each requested branch individually, the TTreeCache will read all the; branches thus trading off the latencies inherent to multiple small reads for; the potential of requesting more data than needed by read from the disk or; server the baskets for too many branches. The default behavior can be changed by either updating one of the rootrc files; or by setting environment variables. The rootrc files, both the global and the; local ones, now support the following the resource variable TTreeCache.Size; which set the default size factor for auto sizing TTreeCache for TTrees. The; estimated cluster size for the TTree and this factor is used to give the cache; size. If option is set to zero auto cache creation is disabled and the default; cache size is the historical one (equivalent to factor 1.0). If set to; non zero auto cache creation is enabled and both auto created and; default sized caches will use the configured factor: 0.0 no automatic cache; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of defa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:13435,cache,cache,13435,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cache']
Performance,"%6:gr32 = ADD32rm %4, %2, 4, killed %5, 0, $noreg, implicit-def dead $eflags :: (load 4 from %ir.addr2); %7:gr32 = SUB32rr %6, %0, implicit-def $eflags, debug-location !5; JB_1 %bb.1, implicit $eflags, debug-location !5; JMP_1 %bb.2, debug-location !5. bb.2.bb2:; %8:gr32 = MOV32r0 implicit-def dead $eflags; $eax = COPY %8, debug-location !5; RET 0, $eax, debug-location !5. Observe first that there is a DBG_VALUE instruction for every ``llvm.dbg.value``; intrinsic in the source IR, ensuring no source level assignments go missing.; Then consider the different ways in which variable locations have been recorded:. * For the first dbg.value an immediate operand is used to record a zero value.; * The dbg.value of the PHI instruction leads to a DBG_VALUE of virtual register; ``%0``.; * The first GEP has its effect folded into the first load instruction; (as a 4-byte offset), but the variable location is salvaged by folding; the GEPs effect into the DIExpression.; * The second GEP is also folded into the corresponding load. However, it is; insufficiently simple to be salvaged, and is emitted as a ``$noreg``; DBG_VALUE, indicating that the variable takes on an undefined location.; * The final dbg.value has its Value placed in virtual register ``%1``. Instruction Scheduling; ----------------------. A number of passes can reschedule instructions, notably instruction selection; and the pre-and-post RA machine schedulers. Instruction scheduling can; significantly change the nature of the program -- in the (very unlikely) worst; case the instruction sequence could be completely reversed. In such; circumstances LLVM follows the principle applied to optimizations, that it is; better for the debugger not to display any state than a misleading state.; Thus, whenever instructions are advanced in order of execution, any; corresponding DBG_VALUE is kept in its original position, and if an instruction; is delayed then the variable is given an undefined location for the duration; of the d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:32408,load,load,32408,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['load'],['load']
Performance,%`; * - llvm/examples/Kaleidoscope/BuildingAJIT/Chapter2; - `2`; - `1`; - `1`; - :part:`50%`; * - llvm/examples/Kaleidoscope/BuildingAJIT/Chapter3; - `2`; - `1`; - `1`; - :part:`50%`; * - llvm/examples/Kaleidoscope/BuildingAJIT/Chapter4; - `2`; - `0`; - `2`; - :none:`0%`; * - llvm/examples/Kaleidoscope/Chapter2; - `1`; - `1`; - `0`; - :good:`100%`; * - llvm/examples/Kaleidoscope/Chapter3; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/Chapter4; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/Chapter5; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/Chapter6; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/Chapter7; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/Chapter8; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/Chapter9; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/include; - `1`; - `1`; - `0`; - :good:`100%`; * - llvm/examples/Kaleidoscope/MCJIT/cached; - `2`; - `0`; - `2`; - :none:`0%`; * - llvm/examples/Kaleidoscope/MCJIT/complete; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/MCJIT/initial; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/Kaleidoscope/MCJIT/lazy; - `2`; - `0`; - `2`; - :none:`0%`; * - llvm/examples/ModuleMaker; - `1`; - `0`; - `1`; - :none:`0%`; * - llvm/examples/OrcV2Examples; - `1`; - `1`; - `0`; - :good:`100%`; * - llvm/examples/OrcV2Examples/LLJITDumpObjects; - `1`; - `1`; - `0`; - :good:`100%`; * - llvm/examples/OrcV2Examples/LLJITWithCustomObjectLinkingLayer; - `1`; - `1`; - `0`; - :good:`100%`; * - llvm/examples/OrcV2Examples/LLJITWithExecutorProcessControl; - `1`; - `1`; - `0`; - :good:`100%`; * - llvm/examples/OrcV2Examples/LLJITWithGDBRegistrationListener; - `1`; - `1`; - `0`; - :good:`100%`; * - llvm/examples/OrcV2Examples/LLJITWithInitializers; - `1`; - `1`; - `0`; - :good:`100%`; * - llvm/examples/OrcV2Examples/LLJITWithLazyReexports; - `1`; - `1`; - `0`; - :good:`100%`; * -,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst:59701,cache,cached,59701,interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,1,['cache'],['cached']
Performance,"%d to i64; %j = trunc i64 %i to i32; store i32 %j, ptr @glbi ; %d is captured. ret ptr %e ; %e is captured; }. 2. The call stores any bit of the pointer carrying information into a place,; and the stored bits can be safely read from the place by another thread via; synchronization. .. code-block:: llvm. @lock = global i1 true. define void @f(ptr %a) {; store ptr %a, ptr* @glb; store atomic i1 false, ptr @lock release ; %a is captured because another thread can safely read @glb; store ptr null, ptr @glb; ret void; }. 3. The call's behavior depends on any bit of the pointer carrying information. .. code-block:: llvm. @glb = global i8 0. define void @f(ptr %a) {; %c = icmp eq ptr %a, @glb; br i1 %c, label %BB_EXIT, label %BB_CONTINUE ; escapes %a; BB_EXIT:; call void @exit(); unreachable; BB_CONTINUE:; ret void; }. 4. The pointer is used in a volatile access as its address. .. _volatile:. Volatile Memory Accesses; ------------------------. Certain memory accesses, such as :ref:`load <i_load>`'s,; :ref:`store <i_store>`'s, and :ref:`llvm.memcpy <int_memcpy>`'s may be; marked ``volatile``. The optimizers must not change the number of; volatile operations or change their order of execution relative to other; volatile operations. The optimizers *may* change the order of volatile; operations relative to non-volatile operations. This is not Java's; ""volatile"" and has no cross-thread synchronization behavior. A volatile load or store may have additional target-specific semantics.; Any volatile operation can have side effects, and any volatile operation; can read and/or modify state which is not accessible via a regular load; or store in this module. Volatile operations may use addresses which do; not point to memory (like MMIO registers). This means the compiler may; not use a volatile operation to prove a non-volatile access to that; address has defined behavior. The allowed side-effects for volatile accesses are limited. If a; non-volatile store to a given address would be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:145869,load,load,145869,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"%eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5460,perform,perform,5460,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['perform'],['perform']
Performance,"%esp), %esi; 	addl	%edx, %esi; 	imull	20(%esp), %ecx; 	movl	%esi, %edx; 	addl	%ecx, %edx; 	popl	%esi; 	ret. This looks like a scheduling deficiency and lack of remat of the load from; the argument area. ICC apparently produces:. movl 8(%esp), %ecx; imull 12(%esp), %ecx; movl 16(%esp), %eax; imull 4(%esp), %eax ; addl %eax, %ecx ; movl 4(%esp), %eax; mull 12(%esp) ; addl %ecx, %edx; ret. Note that it remat'd loads from 4(esp) and 12(esp). See this GCC PR:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17236. //===---------------------------------------------------------------------===//. We can fold a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:. 	movsd	(%esp), %xmm0; 	xorpd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp). Since we 'know' that this is a 'neg', we can actually ""fold"" the spill into; the neg/abs instruction, turning it into an *integer* operation, like this:. 	xorl 2147483648, [mem+4] ## 2147483648 = (1 << 31). you could also use xorb, but xorl is less likely to lead to a partial register; stall. Here is a contrived testcase:. double a, b, c;; void test(double *P) {; double X = *P;; a = X;; bar();; X = -X;; b = X;; bar();; c = X;; }. //===---------------------------------------------------------------------===//. The generated code on x86 for checking for signed overflow on a multiply the; obvious way is much longer than it needs to be. int x(int a, int b) {; long long prod = (long long)a*b;; return prod > 0x7FFFFFFF || prod < (-0x7FFFFFFF-1);; }. See PR2053 for more det",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:22310,load,load,22310,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['load'],['load']
Performance,"%i\n"", Y);; }. If we want to know how ``function_to_test`` behaves when we change the behavior; of ``function_to_mock`` we can test it by writing a test harness:. .. code-block:: c. void function_to_test();. int function_to_mock(int X) {; printf(""used mock utility function\n"");; return 42;; }. int main(int argc, char *argv[]) {; function_to_test():; return 0;; }. Under normal circumstances these objects could not be linked together:; ``function_to_test`` is static and could not be resolved outside; ``test_code.o``, the two ``function_to_mock`` functions would result in a; duplicate definition error, and ``irrelevant_external`` is undefined.; However, using ``-harness`` and ``-phony-externals`` we can run this code; with:. .. code-block:: sh. % clang -c -o test_code_harness.o test_code_harness.c; % llvm-jitlink -phony-externals test_code.o -harness test_code_harness.o; used mock utility function; Y is 42. The ``-harness`` option may be of interest to people who want to perform some; very late testing on build products to verify that compiled code behaves as; expected. On basic C test cases this is relatively straightforward. Mocks for; more complicated languages (e.g. C++) are much trickier: Any code involving; classes tends to have a lot of non-trivial surface area (e.g. vtables) that; would require great care to mock. Tips for JITLink backend developers; -----------------------------------. #. Make liberal use of assert and ``llvm::Error``. Do *not* assume that the input; object is well formed: Return any errors produced by libObject (or your own; object parsing code) and validate as you construct. Think carefully about the; distinction between contract (which should be validated with asserts and; llvm_unreachable) and environmental errors (which should generate; ``llvm::Error`` instances). #. Don't assume you're linking in-process. Use libSupport's sized,; endian-specific types when reading/writing content in the ``LinkGraph``. As a ""minimum viable"" JITLink wrappe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:42490,perform,perform,42490,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['perform'],['perform']
Performance,"%indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbol",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31795,load,load,31795,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['load'],['load']
Performance,"%r = call <2 x i1> @llvm.vp.is.fpclass.v2f16(<2 x half> %x, i32 3, <2 x i1> %m, i32 %evl); %t = call <vscale x 2 x i1> @llvm.vp.is.fpclass.nxv2f16(<vscale x 2 x half> %x, i32 3, <vscale x 2 x i1> %m, i32 %evl). .. _int_mload_mstore:. Masked Vector Load and Store Intrinsics; ---------------------------------------. LLVM provides intrinsics for predicated vector load and store operations. The predicate is specified by a mask operand, which holds one bit per vector element, switching the associated vector lane on or off. The memory addresses corresponding to the ""off"" lanes are not accessed. When all bits of the mask are on, the intrinsic is identical to a regular vector load or store. When all bits are off, no memory is accessed. .. _int_mload:. '``llvm.masked.load.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The loaded data is a vector of any integer, floating-point or pointer data type. ::. declare <16 x float> @llvm.masked.load.v16f32.p0(ptr <ptr>, i32 <alignment>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x double> @llvm.masked.load.v2f64.p0(ptr <ptr>, i32 <alignment>, <2 x i1> <mask>, <2 x double> <passthru>); ;; The data is a vector of pointers; declare <8 x ptr> @llvm.masked.load.v8p0.p0(ptr <ptr>, i32 <alignment>, <8 x i1> <mask>, <8 x ptr> <passthru>). Overview:; """""""""""""""""". Reads a vector from memory according to the provided mask. The mask holds a bit for each vector lane, and is used to prevent memory accesses to the masked-off lanes. The masked-off lanes in the result vector are taken from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the alignment of the source location. It must be a power of two constant integer value. The third operand, mask, is a vector of boolean values with the same number of elements as the return type. The fourth is a pass-through value that is used to fil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:843374,load,load,843374,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"%xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12150,load,loads,12150,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,2,['load'],"['load', 'loads']"
Performance,"& 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsig",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24926,optimiz,optimized,24926,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimized']
Performance,"' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; easy to reuse individual parts or replace pieces if desired. For example, to; build a preprocessor, you take the Basic and Lexer libraries. If you want an; indexer, you take those plus the Parser library and provide some actions for; indexing. If you want a refactoring, static analysis, or source-to-source; compiler tool, it makes sense to take those plus the AST building and semantic; analyzer library. Finally, if you want to use this with the LLVM backend,; you'd take these components plus the AST to LLVM lowering code.; ; In the future I hope this toolkit will grow to include new and interesting; components, including a C++ front-end, ObjC support, and a whole lot of other; things. Finally, it should be pointed out that the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2997,optimiz,optimization,2997,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,1,['optimiz'],['optimization']
Performance,"': argument list whose types match the function; signature argument types and parameter attributes. All arguments must; be of :ref:`first class <t_firstclass>` type. If the function signature; indicates the function accepts a variable number of arguments, the; extra arguments can be specified.; #. '``normal label``': the label reached when the called function; executes a '``ret``' instruction.; #. '``exception label``': the label reached when a callee returns via; the :ref:`resume <i_resume>` instruction or other exception handling; mechanism.; #. The optional :ref:`function attributes <fnattrs>` list.; #. The optional :ref:`operand bundles <opbundles>` list. Semantics:; """""""""""""""""""". This instruction is designed to operate as a standard '``call``'; instruction in most regards. The primary difference is that it; establishes an association with a label, which is used by the runtime; library to unwind the stack. This instruction is used in languages with destructors to ensure that; proper cleanup is performed in the case of either a ``longjmp`` or a; thrown exception. Additionally, this is important for implementation of; '``catch``' clauses in high-level languages that support them. For the purposes of the SSA form, the definition of the value returned; by the '``invoke``' instruction is deemed to occur on the edge from the; current block to the ""normal"" label. If the callee unwinds then no; return value is available. Example:; """""""""""""""". .. code-block:: llvm. %retval = invoke i32 @Test(i32 15) to label %Continue; unwind label %TestCleanup ; i32:retval set; %retval = invoke coldcc i32 %Testfnptr(i32 15) to label %Continue; unwind label %TestCleanup ; i32:retval set. .. _i_callbr:. '``callbr``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = callbr [cconv] [ret attrs] [addrspace(<num>)] <ty>|<fnty> <fnptrval>(<function args>) [fn attrs]; [operand bundles] to label <fallthrough label> [indirect labels]. Overview:; """""""""""""""""". The '``callbr``' instruc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:365048,perform,performed,365048,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"'; ...; # While this is running, do `killall -SIGUSR1 my_fuzzer` in another console; ==9015== INFO: libFuzzer: exiting as requested. # This will leave the file SomeLocalPath with the partial state of the merge.; # Now, you can continue the merge by executing the same command. The merge; # will continue from where it has been interrupted.; % ./my_fuzzer CORPUS1 CORPUS2 -merge=1 -merge_control_file=SomeLocalPath; ...; MERGE-OUTER: non-empty control file provided: 'SomeLocalPath'; MERGE-OUTER: control file ok, 32 files total, first not processed file 20; ... Options; =======. To run the fuzzer, pass zero or more corpus directories as command line; arguments. The fuzzer will read test inputs from each of these corpus; directories, and any new test inputs that are generated will be written; back to the first corpus directory:. .. code-block:: console. ./fuzzer [-flag1=val1 [-flag2=val2 ...] ] [dir1 [dir2 ...] ]. If a list of files (rather than directories) are passed to the fuzzer program,; then it will re-run those files as test inputs but will not perform any fuzzing.; In this mode the fuzzer binary can be used as a regression test (e.g. on a; continuous integration system) to check the target function and saved inputs; still work. The most important command line options are:. ``-help``; Print help message (``-help=1``).; ``-seed``; Random seed. If 0 (the default), the seed is generated.; ``-runs``; Number of individual test runs, -1 (the default) to run indefinitely.; ``-max_len``; Maximum length of a test input. If 0 (the default), libFuzzer tries to guess; a good value based on the corpus (and reports it).; ``-len_control``; Try generating small inputs first, then try larger inputs over time.; Specifies the rate at which the length limit is increased (smaller == faster).; Default is 100. If 0, immediately try inputs with size up to max_len.; ``-timeout``; Timeout in seconds, default 1200. If an input takes longer than this timeout,; the process is treated as a failur",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LibFuzzer.rst:9882,perform,perform,9882,interpreter/llvm-project/llvm/docs/LibFuzzer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LibFuzzer.rst,1,['perform'],['perform']
Performance,"'s impossible to trigger a pointer escape from; within the checker. Approach (1): If only we enabled ``ProgramState::bindLoc(..., notifyChanges=true)``; to cause pointer escapes (not only region changes) (which sounds like the right; thing to do anyway) such checker would be able to solve the false positives by; triggering escapes when binding list elements to the list. However, it'd be as; conservative as the current patch's solution. Ideally, we do not want escapes to; happen so early. Instead, we'd prefer them to be delayed until the list itself; escapes. So i believe that escaping metadata symbols whenever their base regions escape; would be the right thing to do. Currently we didn't think about that because we; had neither pointer-type metadatas nor non-pointer escapes. Approach (2): We could teach the Store to scan itself for bindings to; metadata-symbolic-based regions during scanReachableSymbols() whenever a region; turns out to be reachable. This requires no work on checker side, but it sounds; performance-heavy. Approach (3): We could let checkers maintain the set of active metadata symbols; in the program state (ideally somewhere in the Store, which sounds weird but; causes the smallest amount of layering violations), so that the core knew what; to escape. This puts a stress on the checkers, but with a smart data map it; wouldn't be a problem. Approach (4): We could allow checkers to trigger pointer escapes in arbitrary; moments. If we allow doing this within ``checkPointerEscape`` callback itself, we; would be able to express facts like ""when this region escapes, that metadata; symbol attached to it should also escape"". This sounds like an ultimate freedom,; with maximum stress on the checkers - still not too much stress when we have; smart data maps. I'm personally liking the approach (2) - it should be possible to avoid; performance overhead, and clarity seems nice. **Gabor:**. At this point, I am a bit wondering about two questions. * When should somet",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst:3738,perform,performance-heavy,3738,interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,1,['perform'],['performance-heavy']
Performance,"'s precompiled; headers (PCH) and modules. If you are interested in the end-user view, please; see the :ref:`User's Manual <usersmanual-precompiled-headers>`. Using Precompiled Headers with ``clang``; ----------------------------------------. The Clang compiler frontend, ``clang -cc1``, supports two command line options; for generating and using PCH files. To generate PCH files using ``clang -cc1``, use the option `-emit-pch`:. .. code-block:: bash. $ clang -cc1 test.h -emit-pch -o test.h.pch. This option is transparently used by ``clang`` when generating PCH files. The; resulting PCH file contains the serialized form of the compiler's internal; representation after it has completed parsing and semantic analysis. The PCH; file can then be used as a prefix header with the `-include-pch`; option:. .. code-block:: bash. $ clang -cc1 -include-pch test.h.pch test.c -o test.s. Design Philosophy; -----------------. Precompiled headers are meant to improve overall compile times for projects, so; the design of precompiled headers is entirely driven by performance concerns.; The use case for precompiled headers is relatively simple: when there is a; common set of headers that is included in nearly every source file in the; project, we *precompile* that bundle of headers into a single precompiled; header (PCH file). Then, when compiling the source files in the project, we; load the PCH file first (as a prefix header), which acts as a stand-in for that; bundle of headers. A precompiled header implementation improves performance when:. * Loading the PCH file is significantly faster than re-parsing the bundle of; headers stored within the PCH file. Thus, a precompiled header design; attempts to minimize the cost of reading the PCH file. Ideally, this cost; should not vary with the size of the precompiled header file. * The cost of generating the PCH file initially is not so large that it; counters the per-source-file performance improvement due to eliminating the; need to parse th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:1271,perform,performance,1271,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['performance']
Performance,"'s two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) {; entry:; 	br label %cond_true. cond_true:		; preds = %cond_true, %entry; 	%x.0.0 = phi i32 [ 0, %entry ], [ %tmp9, %cond_true ]		; <i32> [#uses=3]; 	%t_addr.0.0 = phi i32 [ %t, %entry ], [ %tmp7, %cond_true ]		; <i32> [#uses=1]; 	%tmp2 = getelementptr i32* %a, i32 %x.0.0		; <i32*> [#uses=1]; 	%tmp3 = load i32* %tmp2		; <i32> [#uses=1]; 	%tmp5 = add i32 %t_addr.0.0, %x.0.0		; <i32> [#uses=1]; 	%tmp7 = add i32 %tmp5, %tmp3		; <i32> [#uses=2]; 	%tmp9 = add i32 %x.0.0, 1		; <i32> [#uses=2]; 	%tmp = icmp sgt i32 %tmp9, 39		; <i1> [#uses=1]; 	br i1 %tmp, label %bb12, label %cond_true. bb12:		; preds = %cond_true; 	ret i32 %tmp7; }; is pessimized by -loop-reduce and -indvars. //===---------------------------------------------------------------------===//. u32 to float conversion improvement:. float uint32_2_float( unsigned u ) {; float fl = (int) (u & 0xffff);; float fh = (int) (u >> 16);; fh *= 0x1.0p16f;; return fh + fl;; }. 00000000 subl $0x04,%esp; 00000003 movl 0x08(%esp,1),%eax; 00000007 movl %eax,%ecx; 00000009 shrl $0x10,%ecx; 0000000c cvtsi2ss %ecx,%xmm0; 00000010 andl $0x0000ffff,%eax; 00000015 cvtsi2ss %eax,%xmm1; 00000019 mulss 0x00000078,%xmm0; 00000021 addss %xmm1,%xmm0; 00000025 movss %xmm0,(%esp,1); 0000002a flds (%esp,1); 0000002d addl $0x04,%esp; 00000030 ret. //===--------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:8412,load,load,8412,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['load'],['load']
Performance,"'t have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13313,optimiz,optimized,13313,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,2,"['load', 'optimiz']","['loads', 'optimized']"
Performance,"(%eax), %eax; ret. You can also use the GCC compatibility macros ``__seg_fs`` and ``__seg_gs`` for; the same purpose. The preprocessor symbols ``__SEG_FS`` and ``__SEG_GS``; indicate their support. PowerPC Language Extensions; ---------------------------. Set the Floating Point Rounding Mode; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; PowerPC64/PowerPC64le supports the builtin function ``__builtin_setrnd`` to set; the floating point rounding mode. This function will use the least significant; two bits of integer argument to set the floating point rounding mode. .. code-block:: c++. double __builtin_setrnd(int mode);. The effective values for mode are:. - 0 - round to nearest; - 1 - round to zero; - 2 - round to +infinity; - 3 - round to -infinity. Note that the mode argument will modulo 4, so if the integer argument is greater; than 3, it will only use the least significant two bits of the mode.; Namely, ``__builtin_setrnd(102))`` is equal to ``__builtin_setrnd(2)``. PowerPC cache builtins; ^^^^^^^^^^^^^^^^^^^^^^. The PowerPC architecture specifies instructions implementing cache operations.; Clang provides builtins that give direct programmer access to these cache; instructions. Currently the following builtins are implemented in clang:. ``__builtin_dcbf`` copies the contents of a modified block from the data cache; to main memory and flushes the copy from the data cache. **Syntax**:. .. code-block:: c. void __dcbf(const void* addr); /* Data Cache Block Flush */. **Example of Use**:. .. code-block:: c. int a = 1;; __builtin_dcbf (&a);. Extensions for Static Analysis; ==============================. Clang supports additional attributes that are useful for documenting program; invariants and rules for static analysis tools, such as the `Clang Static; Analyzer <https://clang-analyzer.llvm.org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; =====",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:156887,cache,cache,156887,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['cache'],['cache']
Performance,"(%edx); 	movw $257, 8(%edx). It might be better to generate. 	movl $16843009, %eax; 	movl %eax, 4(%edx); 	movl %eax, (%edx); 	movw al, 8(%edx); 	; when we can spare a register. It reduces code size. //===---------------------------------------------------------------------===//. Evaluate what the best way to codegen sdiv X, (2^C) is. For X/8, we currently; get this:. define i32 @test1(i32 %X) {; %Y = sdiv i32 %X, 8; ret i32 %Y; }. _test1:; movl 4(%esp), %eax; movl %eax, %ecx; sarl $31, %ecx; shrl $29, %ecx; addl %ecx, %eax; sarl $3, %eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:4968,tune,tuned,4968,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['tune'],['tuned']
Performance,"() now accepts a new argument ShowProgress() that will print a dot for every; function evaluation performed in the process of creating the plot. This can be useful when plotting very expensive; functions such as profile likelihoods; Automatic handling of constraint terms; It is no longer necessary to add a Constrain() argument to fitTo() calls to have internal constraints; applied. Any pdf term appearing in a product that does not contain an observable and shares one or more parameters; with another pdf term in the same product that does contain an observable is automatically picked up as a constraint term.; For example given a dataset D(x) which defines variable x as observable, the default logic works out as follows. F(x,a,b)*G(a,a0,a1) --> G is constraint term (a also appears in F(x)); F(x,a,b)*G(y,c,d) --> G is dropped (factorizing term). A Constrain(y) term in the above example will still force term G(y,c,d) to be interpreted as constraint term; Automatic caching of numeric integral calculations; Integrals that require numeric integrations in two of more dimensions are now automatically cached in the expensive object store.; The expensive object store allows to cache such values between difference instance of integral objects that represent the; same configuration. If integrals are created from an object (function or pdf) that live in a RooWorkspace the ; expensive object cache of the workspace will be used instead of the global store instance, and values stored in the workspace; store will also be persisted if the workspace is persisted. The global caching behavior of integral objects can be ; controlled through RooRealIntegral::setCacheAllNumeric(Int_t nDimNumMin). Miscellaneous improvements data classes. The RooAbsData::tree() method has been restored. It will only return a TTree* pointer for datasets; that are based on a RooTreeDataStore implementation, i.e. not for the composite datasets mentioned below; A new composite data storage class RooCompositeDataS",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html:7819,cache,cached,7819,roofit/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html,1,['cache'],['cached']
Performance,"(). See section ""RooFit Libraries"" for instructions on how to use [`RooAbsReal::evaluateSpan()`](https://root.cern/doc/v624/classRooAbsReal.html#a1e5129ffbc63bfd04c01511fd354b1b8).; - `TTreeProcessorMT::SetMaxTasksPerFilePerWorker` has been deprecated in favour of `TTreeProcessorMT::SetTasksPerWorkerHint`. ## Core Libraries. Due to internal changes required to comply with the deprecation of Intel TBB's `task_scheduler_init` and related; interfaces in recent TBB versions, as of v6.24 ROOT will not honor a maximum concurrency level set with; `tbb::task_scheduler_init` but will require instead the usage of `tbb::global_control`:. ```cpp; //tbb::task_scheduler_init init(2); // does not affect the number of threads ROOT will use anymore. tbb::global_control c(tbb::global_control::max_allowed_parallelism, 2);; ROOT::TThreadExecutor p1; // will use 2 threads; ROOT::TThreadExecutor p2(/*nThreads=*/8); // will still use 2 threads; ```. Note that the preferred way to steer ROOT's concurrency level is still through; [`ROOT::EnableImplicitMT`](https://root.cern/doc/master/namespaceROOT.html#a06f2b8b216b615e5abbc872c9feff40f); or by passing the appropriate parameter to executors' constructors, as in; [`TThreadExecutor::TThreadExecutor`](https://root.cern/doc/master/classROOT_1_1TThreadExecutor.html#ac7783d52c56cc7875d3954cf212247bb). See the discussion at [ROOT-11014](https://sft.its.cern.ch/jira/browse/ROOT-11014) for more context. ### Dynamic Path: `ROOT_LIBRARY_PATH`. A new way to set ROOT's ""Dynamic Path"" was added: the; environment variable `ROOT_LIBRARY_PATH`. On Unix it should contain a colon; separated list of paths, on Windows a semicolon separated list. It is; intended to be cross platform and to be specific to ROOT (and thus not; interfere with the system's shared linker).; The final ""Dynamic Path"" is now composed of these sources in order:; 1. `ROOT_LIBRARY_PATH` environment variable; 2. System specific shared linker environment variables like; `LD_LIBRARY_PATH`, `LIB",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:2457,concurren,concurrency,2457,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['concurren'],['concurrency']
Performance,"();; uintptr_t all_zeros_mask = 0;; void leak(int data);; void example(int* pointer1, int* pointer2) {; uintptr_t predicate_state = all_ones_mask;; if (condition) {; // Assuming ?: is implemented using branchless logic...; predicate_state = !condition ? all_zeros_mask : predicate_state;; // ... lots of code ...; //; // Harden the pointer so it can't be loaded; pointer1 &= predicate_state;; leak(*pointer1);; } else {; predicate_state = condition ? all_zeros_mask : predicate_state;; // ... more code ...; //; // Alternative: Harden the loaded value; int value2 = *pointer2 & predicate_state;; leak(value2);; }; }; ```. The result should be that if the `if (condition) {` branch is mis-predicted,; there is a *data* dependency on the condition used to zero out any pointers; prior to loading through them or to zero out all of the loaded bits. Even; though this code pattern may still execute speculatively, *invalid* speculative; executions are prevented from leaking secret data from memory (but note that; this data might still be loaded in safe ways, and some regions of memory are; required to not hold secrets, see below for detailed limitations). This; approach only requires the underlying hardware have a way to implement a; branchless and unpredicted conditional update of a register's value. All modern; architectures have support for this, and in fact such support is necessary to; correctly implement constant time cryptographic primitives. Crucial properties of this approach:; * It is not preventing any particular side-channel from working. This is; important as there are an unknown number of potential side channels and we; expect to continue discovering more. Instead, it prevents the observation of; secret data in the first place.; * It accumulates the predicate state, protecting even in the face of nested; *correctly* predicted control flows.; * It passes this predicate state across function boundaries to provide; [interprocedural protection](#interprocedural-checking).; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:4660,load,loaded,4660,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"(...); A; C. This can increase the size of the code exponentially (doubling it every time a; loop is unswitched) so we only unswitch if the resultant code will be smaller; than a threshold. This pass expects :ref:`LICM <passes-licm>` to be run before it to hoist; invariant conditions out of the loop, to make the unswitching opportunity; obvious. ``strip``: Strip all symbols from a module; ------------------------------------------. Performs code stripping. This transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-debug-info``: Strip debug info for unused symbols; --------------------------------------------------------------. .. FIXME: this description is the same as for -strip. performs code stripping. this transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. note that this transformation makes code much less readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-prototypes``: Strip Unused Function Prototypes; -----------------------------------------------------------. This pass loops over all of the functions in the input module, looking for dead; declarations and removes them. Dead declarations are declarations of functions; for which no implementation is available (i.e., declarations for unused library; functions). ``strip-debug-declare``: Strip all ``llvm.dbg.declare`` intrinsics; ------------------------------------------------------------------. .. FIXME: this description is the same as for -strip. This pass implements code stripping. Specifically, it can delete:. #. names",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:36706,perform,performs,36706,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performs']
Performance,"(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed befor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:367935,cache,caches,367935,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes.; **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:256326,cache,cache,256326,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:320084,load,load,320084,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"(0x8b03bf0, LLVM BB @0x8b032d0, ID#5):; Predecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4); %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>. Note ADDri is not a two-address instruction. However, its result %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived poin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:1748,load,load,1748,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['load'],['load']
Performance,"(CurTok == ',') {; getNextToken();; Step = ParseExpression();; if (!Step); return nullptr;; }. if (CurTok != tok_in); return LogError(""expected 'in' after for"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<ForExprAST>(IdName, std::move(Start),; std::move(End), std::move(Step),; std::move(Body));; }. And again we hook it up as a primary expression:. .. code-block:: c++. static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; case tok_for:; return ParseForExpr();; }; }. LLVM IR for the 'for' Loop; --------------------------. Now we get to the good part: the LLVM IR we want to generate for this; thing. With the simple example above, we get this LLVM IR (note that; this dump is generated with optimizations disabled for clarity):. .. code-block:: llvm. declare double @putchard(double). define double @printstar(double %n) {; entry:; ; initial value = 1.0 (inlined into phi); br label %loop. loop: ; preds = %loop, %entry; %i = phi double [ 1.000000e+00, %entry ], [ %nextvar, %loop ]; ; body; %calltmp = call double @putchard(double 4.200000e+01); ; increment; %nextvar = fadd double %i, 1.000000e+00. ; termination test; %cmptmp = fcmp ult double %i, %n; %booltmp = uitofp i1 %cmptmp to double; %loopcond = fcmp one double %booltmp, 0.000000e+00; br i1 %loopcond, label %loop, label %afterloop. afterloop: ; preds = %loop; ; loop always returns 0.0; ret double 0.000000e+00; }. This loop contains all the same constructs we saw before: a phi node,; several expressions, and some basic blocks. Let's see how this fits; together. Code Generation for the 'for' Loop; ----------------------------------. The first part of codegen is very simple: we just output the sta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:19500,optimiz,optimizations,19500,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,1,['optimiz'],['optimizations']
Performance,"(FIND CMAKE_FIND_LIBRARY_SUFFIXES ${shared_lib_suffix} shared_lib_suffix_idx); if(NOT ${shared_lib_suffix_idx} EQUAL -1); list(REMOVE_AT CMAKE_FIND_LIBRARY_SUFFIXES ${shared_lib_suffix_idx}); endif(); endforeach(); endif(). # Use libtool instead of ar if you are both on an Apple host, and targeting Apple.; if(CMAKE_HOST_APPLE AND APPLE); include(UseLibtool); endif(). # Override the default target with an environment variable named by LLVM_TARGET_TRIPLE_ENV.; set(LLVM_TARGET_TRIPLE_ENV CACHE STRING ""The name of environment variable to override default target. Disabled by blank.""); mark_as_advanced(LLVM_TARGET_TRIPLE_ENV). if(CMAKE_SYSTEM_NAME MATCHES ""BSD|Linux|OS390""); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR_default ON); else(); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR_default OFF); endif(); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR ${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR_default} CACHE BOOL; ""Enable per-target runtimes directory""). set(LLVM_PROFDATA_FILE """" CACHE FILEPATH; ""Profiling data file to use when compiling in order to improve runtime performance.""). if(LLVM_INCLUDE_TESTS); # Lit test suite requires at least python 3.6; set(LLVM_MINIMUM_PYTHON_VERSION 3.6); else(); # FIXME: it is unknown if this is the actual minimum bound; set(LLVM_MINIMUM_PYTHON_VERSION 3.0); endif(). # Find python before including config-ix, since it needs to be able to search; # for python modules.; find_package(Python3 ${LLVM_MINIMUM_PYTHON_VERSION} REQUIRED; COMPONENTS Interpreter). # All options referred to from HandleLLVMOptions have to be specified; # BEFORE this include, otherwise options will not be correctly set on; # first cmake run; include(config-ix). # By default, we target the host, but this can be overridden at CMake; # invocation time. Except on 64-bit AIX, where the system toolchain; # expect 32-bit objects by default.; if(""${LLVM_HOST_TRIPLE}"" MATCHES ""^powerpc64-ibm-aix""); string(REGEX REPLACE ""^powerpc64"" ""powerpc"" LLVM_DEFAULT_TARGET_TRIPLE_DEFAULT ""${LLVM_HOST_TRIPLE}""); els",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:37310,perform,performance,37310,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['perform'],['performance']
Performance,"(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and store",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23977,optimiz,optimizations,23977,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimizations']
Performance,"(a * b) + c. Hardware-Loop Intrinsics; ------------------------. LLVM support several intrinsics to mark a loop as a hardware-loop. They are; hints to the backend which are required to lower these intrinsics further to target; specific instructions, or revert the hardware-loop to a normal loop if target; specific restriction are not met and a hardware-loop can't be generated. These intrinsics may be modified in the future and are not intended to be used; outside the backend. Thus, front-end and mid-level optimizations should not be; generating these intrinsics. '``llvm.set.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare void @llvm.set.loop.iterations.i32(i32); declare void @llvm.set.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.set.loop.iterations.*``' intrinsics are used to specify the; hardware-loop trip count. They are placed in the loop preheader basic block and; are marked as ``IntrNoDuplicate`` to avoid optimizers duplicating these; instructions. Arguments:; """""""""""""""""""". The integer operand is the loop trip count of the hardware-loop, and thus; not e.g. the loop back-edge taken count. Semantics:; """""""""""""""""""". The '``llvm.set.loop.iterations.*``' intrinsics do not perform any arithmetic; on their operand. It's a hint to the backend that can use this to set up the; hardware-loop count with a target specific instruction, usually a move of this; value to a special register or a hardware-loop instruction. '``llvm.start.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare i32 @llvm.start.loop.iterations.i32(i32); declare i64 @llvm.start.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.start.loop.iterations.*``' intrinsics are similar to the; '``llvm.set.loop.iterations.*``' intrinsics, used to specify the; hardware-loop trip count but also produce a value identical to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:643547,optimiz,optimizers,643547,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"(c1_n/sum_1, c2_n/sum_2). The result overlap distribution is a percentage number, ranging from 0.0% to; 100.0%, where 0.0% means there is no overlap and 100.0% means a perfect; overlap. Here is an example, if *base profile file* has counts of {400, 600}, and; *test profile file* has matched counts of {60000, 40000}. The *overlap* is 80%. OPTIONS; ^^^^^^^. .. option:: --function=<string>. Print details for a function if the function's name contains the given string. .. option:: --help. Print a summary of command line options. .. option:: --output=<output>, -o. Specify the output file name. If *output* is ``-`` or it isn't specified,; then the output is sent to standard output. .. option:: --value-cutoff=<n>. Show only those functions whose max count values are greater or equal to ``n``.; By default, the value-cutoff is set to max of unsigned long long. .. option:: --cs. Only show overlap for the context sensitive profile counts. The default is to show; non-context sensitive profile counts. .. program:: llvm-profdata order. .. _profdata-order:. ORDER; -------. SYNOPSIS; ^^^^^^^^. :program:`llvm-profdata order` [*options*] [*filename*]. DESCRIPTION; ^^^^^^^^^^^. :program:`llvm-profdata order` uses temporal profiling traces from a profile and; finds a function order that reduces the number of page faults for those traces.; This output can be directly passed to ``lld`` via ``--symbol-ordering-file=``; for ELF or ``-order-file`` for Mach-O. If the traces found in the profile are; representative of the real world, then this order should improve startup; performance. OPTIONS; ^^^^^^^. .. option:: --help. Print a summary of command line options. .. option:: --output=<output>, -o. Specify the output file name. If *output* is ``-`` or it isn't specified,; then the output is sent to standard output. EXIT STATUS; -----------. :program:`llvm-profdata` returns 1 if the command is omitted or is invalid,; if it cannot read input files, or if there is a mismatch between their data.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst:14148,perform,performance,14148,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst,1,['perform'],['performance']
Performance,"(c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. un",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25167,optimiz,optimized,25167,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimized']
Performance,"(e.g., entire applications, benchmarks,; etc) should be added to the ``llvm-test`` test suite. The llvm-test suite is; for coverage (correctness, performance, etc) testing, not feature or regression; testing. Release Notes; -------------. Many projects in LLVM communicate important changes to users through release; notes, typically found in ``docs/ReleaseNotes.rst`` for the project. Changes to; a project that are user-facing, or that users may wish to know about, should be; added to the project's release notes at the author's or code reviewer's; discretion, preferably as part of the commit landing the changes. Examples of; changes that would typically warrant adding a release note (this list is not; exhaustive):. * Adding, removing, or modifying command-line options.; * Adding, removing, or regrouping a diagnostic.; * Fixing a bug that potentially has significant user-facing impact (please link; to the issue fixed in the bug database).; * Adding or removing optimizations that have widespread impact or enables new; programming paradigms.; * Modifying a C stable API.; * Notifying users about a potentially disruptive change expected to be made in; a future release, such as removal of a deprecated feature. In this case, the; release note should be added to a ``Potentially Breaking Changes`` section of; the notes with sufficient information and examples to demonstrate the; potential disruption. Additionally, any new entries to this section should be; announced in the `Announcements <https://discourse.llvm.org/c/announce/>`_; channel on Discourse. See :ref:`breaking` for more details. Code reviewers are encouraged to request a release note if they think one is; warranted when performing a code review. Quality; -------. The minimum quality standards that any change must satisfy before being; committed to the main development branch are:. #. Code must adhere to the `LLVM Coding Standards <CodingStandards.html>`_. #. Code must compile cleanly (no errors, no warnings) on at le",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst:11544,optimiz,optimizations,11544,interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst,1,['optimiz'],['optimizations']
Performance,"(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:54299,perform,performance,54299,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,"['optimiz', 'perform']","['optimizer', 'performance']"
Performance,"(m_A+m_B+m_C)). Constrained split can also be; specified in product of categories. In that case the name of the; remainder state follows the syntax {State1;State2} where State1; and State2 are the state names of the two spitting categories. Additional; functionality exists to work with multiple prototype p.d.f.s simultaneously. ; Improved infrastructure for caching p.d.f and functions. The infrastructure that exists for caching p.d.f.s, i.e. p.d.f that precalculate their value; for all observable values at one and cache those in a histogram that is returned as p.d.f shape; (with optional interpolation), has been expanded. This infrastructure comprises RooAbsCached; the base class for all caching p.d.fs, RooAbsSelfCachedPdf a base class for end-user; caching p.d.f implementations that simply cache the result of evaluate() and RooCachedPdf; that can wrap and cache any input p.d.f specified in its constructor. . By default a p.d.f is sampled and cached in all observables in any; given use context, with no need to specify what those are in advance.; The internal code has also been changed such that all cache; histograms now store pre-normalized p.d.f, which is more efficient; than 'raw' p.d.f histograms that are explicitly post-normalized; through integration. Multiple different use cases (e.g. definitions; of what are observables vs parameters) can be cached; simultaneously. Now it is also possible to specify that p.d.f.s; should be sampled and cached in one or more parameter dimensions; in addition to the automatically determined set of observables.; as well. Also a complete new line of classes with similar functionality has been added inheriting from RooAbsReal.; These are RooAbsCachedReal,RooAbsSelfCachedReal and RooCachedReal. A newly; added class RooHistFunc presents these shapes and is capable of handling negative entries. New PDF error handling structure. New infrastructure has been put into place to propagate and process p.d.f evaluation errors during fitting.;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:14079,cache,cached,14079,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,1,['cache'],['cached']
Performance,"(non-coherent) with; the PTE C-bit set or MTYPE UC (uncached) for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by coherent requests. Coherent requests are caused; by GPU accesses to pages with the PTE C-bit set, by CPU accesses over; XGMI, and by PCIe requests that are configured to be coherent requests.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter and and the PTE C-bit being set.; * Since all work-groups on the same agent share the same L2, no L2; invalidation or writeback is required for coherence.; * To ensure coherence of local and remote memory writes of work-groups in; different agents a ``buffer_wbl2`` is required. It will writeback dirty L2; cache lines of MTYPE RW (used for local coarse grain memory) and MTYPE NC; ()used for remote coarse grain memory). Note that MTYPE CC (used for local; fine grain memory) causes write through to DRAM, and MTYPE UC (used for; remote fine grain memory) bypasses the L2, so both will never result in; dirty L2 cache lines.; * To ensure coherence of local and remote memory reads of work-groups in; different agents a ``buffer_invl2`` is required. It will invalidate L2; cache lines with MTYPE NC (used for remote coarse grain memory). Note that; MTYPE CC (used for local fine grain memory) and MTYPE RW (used for local; coarse memory) cause local reads to be invalidated by remote writes with; with the PTE C-bit so these cache lines are not invalidated. Note that; MTYPE UC (used for remote fine grain memory) bypasses the L2, so will; never result in L2 cache lines that need to be invalidated. * PCIe access from the GPU to the CPU memory is kept coherent by using the; MTYPE UC (uncached) which bypasses the L2. Scalar memory operations are on",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:238762,cache,cache,238762,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"(see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes.; **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; ato",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:256533,load,load,256533,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"(set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17614,load,load,17614,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,1,['load'],['load']
Performance,"(vic. 1) for this flag enables; the AT&T (vic. Intel) assembly format for the code printed out by the tool in; the analysis report. .. option:: -print-imm-hex. Prefer hex format for numeric literals in the output assembly printed as part; of the report. .. option:: -dispatch=<width>. Specify a different dispatch width for the processor. The dispatch width; defaults to field 'IssueWidth' in the processor scheduling model. If width is; zero, then the default dispatch width is used. .. option:: -register-file-size=<size>. Specify the size of the register file. When specified, this flag limits how; many physical registers are available for register renaming purposes. A value; of zero for this flag means ""unlimited number of physical registers"". .. option:: -iterations=<number of iterations>. Specify the number of iterations to run. If this flag is set to 0, then the; tool sets the number of iterations to a default value (i.e. 100). .. option:: -noalias=<bool>. If set, the tool assumes that loads and stores don't alias. This is the; default behavior. .. option:: -lqueue=<load queue size>. Specify the size of the load queue in the load/store unit emulated by the tool.; By default, the tool assumes an unbound number of entries in the load queue.; A value of zero for this flag is ignored, and the default load queue size is; used instead. .. option:: -squeue=<store queue size>. Specify the size of the store queue in the load/store unit emulated by the; tool. By default, the tool assumes an unbound number of entries in the store; queue. A value of zero for this flag is ignored, and the default store queue; size is used instead. .. option:: -timeline. Enable the timeline view. .. option:: -timeline-max-iterations=<iterations>. Limit the number of iterations to print in the timeline view. By default, the; timeline view prints information for up to 10 iterations. .. option:: -timeline-max-cycles=<cycles>. Limit the number of cycles in the timeline view, or use 0 for no limit. By",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:4046,load,loads,4046,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['loads']
Performance,"(x, 0.25)``; may be replaced with ``sqrt(sqrt(x))``, despite being an inexact result; in cases where ``x`` is ``-0.0`` or ``-inf``.; Defaults to ``-fno-approx-func``. .. option:: -f[no-]signed-zeros. Allow optimizations that ignore the sign of floating point zeros.; Defaults to ``-fsigned-zeros``. .. option:: -f[no-]associative-math. Allow floating point operations to be reassociated.; Defaults to ``-fno-associative-math``. .. option:: -f[no-]reciprocal-math. Allow division operations to be transformed into multiplication by a; reciprocal. This can be significantly faster than an ordinary division; but can also have significantly less precision. Defaults to; ``-fno-reciprocal-math``. .. option:: -f[no-]unsafe-math-optimizations. Allow unsafe floating-point optimizations.; ``-funsafe-math-optimizations`` also implies:. * ``-fapprox-func``; * ``-fassociative-math``; * ``-freciprocal-math``; * ``-fno-signed-zeros``; * ``-fno-trapping-math``; * ``-ffp-contract=fast``. ``-fno-unsafe-math-optimizations`` implies:. * ``-fno-approx-func``; * ``-fno-associative-math``; * ``-fno-reciprocal-math``; * ``-fsigned-zeros``; * ``-ftrapping-math``; * ``-ffp-contract=on``; * ``-fdenormal-fp-math=ieee``. There is ambiguity about how ``-ffp-contract``,; ``-funsafe-math-optimizations``, and ``-fno-unsafe-math-optimizations``; behave when combined. Explanation in :option:`-fno-fast-math` also applies; to these options. Defaults to ``-fno-unsafe-math-optimizations``. .. option:: -f[no-]finite-math-only. Allow floating-point optimizations that assume arguments and results are; not NaNs or +-Inf. ``-ffinite-math-only`` defines the; ``__FINITE_MATH_ONLY__`` preprocessor macro.; ``-ffinite-math-only`` implies:. * ``-fno-honor-infinities``; * ``-fno-honor-nans``. ``-ffno-inite-math-only`` implies:. * ``-fhonor-infinities``; * ``-fhonor-nans``. Defaults to ``-fno-finite-math-only``. .. option:: -f[no-]rounding-math. Force floating-point operations to honor the dynamically-set rounding mode by de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:59503,optimiz,optimizations,59503,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,") &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0);",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:367466,load,load,367466,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,") infinite loop should a; server go offline that is subject to a locate request display.Client sideBetter handling of errno, especially for parallel streamsAllow the client to cycle through all the remaining valid security protocols in the list of protocols returned by the serverMake the readahead strategy more conservativeFix a rare race condition happening when destroying instances with outstanding open requestsEnforce cache coherency in the case of reads+writes in the same fileCorrectly guess the filesize of a file opened for writing in sync modeMake server host name check more flexible for GSI authenticationFix some relevant issues with cache handling on the client, including a rare but fatal bug in; determining the cache holes list and the end of a cache lookupMore complete detection of async read errorsGeneralFix problem in handling the return code; of X509_REQ_verify; in XrdCryptosslX509Req.ccAvoid SEGV when doing an lsd admin command with; authenticated xrootd clientsClose race conditions that allowed a supervisor/manager; to subscribe without declaring a data port. Initialize nostage state in; XrdCmsState to prevent erroneous state declaration during; initialization.Fix a problem with the subject name of proxies of level; > 1; this was creating a failure when a Globus application was; trying to use the proxy certificateFix a problem with cache refreshing in XrdSutCache; affecting automatic reloading of password filesFor now, turn off IPV6 processing as it seems to create; several problems.Fix a few issues with the available releases of gcc 4.4Fix a few issues with the 'icc' compilerFix several issues in GSI and PWD authentication modulesNew featuresNew File Residency Manager (frm), replacement for the MPS scriptsScripts are now provided toautomatically donwload a CRL certificate; (utils/getCRLcert)install the recommended verion of OpenSSL and build it; with the options optimal for usage in XROOTD/SCALLA; (utils/installOpenSSL.sh)install the recommended verio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v524/index.html:2824,race condition,race conditions,2824,net/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v524/index.html,1,['race condition'],['race conditions']
Performance,") rather than comparisons (`==`). Expert users can resort to compiled callables if they absolutely have to assign to column values (not recommended). See [ROOT-11009](https://sft.its.cern.ch/jira/browse/ROOT-11009) for further discussion.; - For some `TTrees`, `RDataFrame::GetColumnNames` might now returns multiple valid spellings for a given column. For example, leaf `""l""` under branch `""b""` might now be mentioned as `""l""` as well as `""b.l""`, while only one of the two spellings might have been recognized before.; - Certain RDF-related types in the `ROOT::Detail` and `ROOT::Internal` namespaces have been renamed, most notably `RCustomColumn` is now `RDefine`. This does not impact code that only makes use of entities in the public ROOT namespace, and should not impact downstream code unless it was patching or reusing internal `RDataFrame` types. ### Notable bug fixes and improvements. - A critical issue has been fixed that could potentially result in wrong data being silently read in multi-thread runs when an input `TChain` contained more than one `TTree` coming from the _same_ input file. More details are available at [#7143](https://github.com/root-project/root/issues/7143).; - The start-up time of event loops with large computation graphs with many just-in-time-compiled expressions (e.g. thousands of string `Filter`s and `Define`s) has been greatly reduced. See [the corresponding pull request](https://github.com/root-project/root/pull/7651) for more details. The full list of bug fixes for this release is available below. ### Distributed computing with RDataFrame; ROOT 6.24 introduces `ROOT.RDF.Experimental.Distributed`, an experimental python package that enhances RDataFrame with distributed computing capabilities. The new package allows distributing RDataFrame applications through one of the supported distributed backends. The package was designed so that different backends can be easily plugged in. Currently the [Apache Spark](http://spark.apache.org/) backend is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:9473,multi-thread,multi-thread,9473,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['multi-thread'],['multi-thread']
Performance,"), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%es",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:18728,optimiz,optimizations,18728,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['optimiz'],['optimizations']
Performance,"), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===--------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8304,load,load,8304,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,2,['load'],['load']
Performance,"), std::move(Then),; std::move(Else));; }. Next we hook it up as a primary expression:. .. code-block:: c++. static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; }; }. LLVM IR for If/Then/Else; ------------------------. Now that we have it parsing and building the AST, the final piece is; adding LLVM code generation support. This is the most interesting part; of the if/then/else example, because this is where it starts to; introduce new concepts. All of the code above has been thoroughly; described in previous chapters. To motivate the code we want to produce, let's take a look at a simple; example. Consider:. ::. extern foo();; extern bar();; def baz(x) if x then foo() else bar();. If you disable optimizations, the code you'll (soon) get from; Kaleidoscope looks like this:. .. code-block:: llvm. declare double @foo(). declare double @bar(). define double @baz(double %x) {; entry:; %ifcond = fcmp one double %x, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; %calltmp = call double @foo(); br label %ifcont. else: ; preds = %entry; %calltmp1 = call double @bar(); br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ %calltmp, %then ], [ %calltmp1, %else ]; ret double %iftmp; }. To visualize the control flow graph, you can use a nifty feature of the; LLVM '`opt <https://llvm.org/cmds/opt.html>`_' tool. If you put this LLVM; IR into ""t.ll"" and run ""``llvm-as < t.ll | opt -passes=view-cfg``"", `a; window will pop up <../../ProgrammersManual.html#viewing-graphs-while-debugging-code>`_ and you'll; see this graph:. .. figure:: LangImpl05-cfg.png; :align: center; :alt: Example CFG. Example CFG. Another way to get this is to call ""``F->viewCFG()``"" or; ""``F->viewCFGOnly()",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:5296,optimiz,optimizations,5296,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,1,['optimiz'],['optimizations']
Performance,"). NCycles No 3000 − Number of training cycles. HiddenLayers No N,N-1 − Specification of hidden layer architecture. Configuration options for MVA method :. Configuration options reference for MVA method: KNN. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). nkNN No 20 − Number of k-nearest neighbors. BalanceDepth No 6 − Binary tree balance depth. ScaleFrac No 0.8 − Fraction of events used to compute variable width. SigmaFact No 1 − Scale factor for sigma in Gaussian kernel. Kernel No Gaus − Use polynomial (=Poln) or Gaussian (=Gaus) kernel. Trim No False − Use equal number of signal and background events. UseKernel No False − Use polynomial kernel weight. UseWeight No True − Use weight to count kNN events. UseLDA No False − Use local linear discriminant - experimental feature. Configuration options for MVA method :. Configuration options reference for MVA method: BDT. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:10352,perform,performance,10352,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,1,['perform'],['performance']
Performance,"). std::multimap is useful if you want to map a key to multiple values, but has all; the drawbacks of std::map. A sorted vector or some other approach is almost; always better. .. _ds_bit:. Bit storage containers; ------------------------------------------------------------------------. There are several bit storage containers, and choosing when to use each is; relatively straightforward. One additional option is ``std::vector<bool>``: we discourage its use for two; reasons 1) the implementation in many common compilers (e.g. commonly; available versions of GCC) is extremely inefficient and 2) the C++ standards; committee is likely to deprecate this container and/or change it significantly; somehow. In any case, please don't use it. .. _dss_bitvector:. BitVector; ^^^^^^^^^. The BitVector container provides a dynamic size set of bits for manipulation.; It supports individual bit setting/testing, as well as set operations. The set; operations take time O(size of bitvector), but operations are performed one word; at a time, instead of one bit at a time. This makes the BitVector very fast for; set operations compared to other containers. Use the BitVector when you expect; the number of set bits to be high (i.e. a dense set). .. _dss_smallbitvector:. SmallBitVector; ^^^^^^^^^^^^^^. The SmallBitVector container provides the same interface as BitVector, but it is; optimized for the case where only a small number of bits, less than 25 or so,; are needed. It also transparently supports larger bit counts, but slightly less; efficiently than a plain BitVector, so SmallBitVector should only be used when; larger counts are rare. At this time, SmallBitVector does not support set operations (and, or, xor), and; its operator[] does not provide an assignable lvalue. .. _dss_sparsebitvector:. SparseBitVector; ^^^^^^^^^^^^^^^. The SparseBitVector container is much like BitVector, with one major difference:; Only the bits that are set, are stored. This makes the SparseBitVector much; m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:97150,perform,performed,97150,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performed']
Performance,"); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo_0: ; entry; mflr r11; *** stw r11, 8(r1); bl ""L00000$pb""; ""L00000$pb"":; mflr r2; addis r2, r2, ha16(.CPI_foo_0-""L00000$pb""); lfs f0, lo16(.CPI_foo_0-""L00000$pb"")(r2); fadds f1, f1, f0; *** lwz r11, 8(r1); mtlr r11; blr. This is functional, but there is no reason to spill the LR register all the way; to the stack (the two marked instrs): spilling it to a GPR is quite enough. Implementing this will require some codegen improvements. Nate writes:. ""So basically what we need to support the ""no stack frame save and restore"" is a; generalization of the LR optimization to ""callee-save regs"". Currently, we have LR marked as a callee-save reg. The register allocator sees; that it's callee save, and spills it directly to the stack. Ideally, something like this would happen:. LR would be in a separate register class from the GPRs. The class of LR would be; marked ""unspillable"". When the register allocator came across an unspillable; reg, it would ask ""what is the best class to copy this into that I *can* spill""; If it gets a class back, which it will in this case (the gprs), it grabs a free; register of that class. If it is then later necessary to spill that reg, so be; it. ===-------------------------------------------------------------------------===. We compile this:; int test(_Bool X) {; return X ? 524288 : 0;; }. to: ; _test:; cmplwi cr0, r3, 0; lis r2, 8; li r3, 0; beq cr0, LBB1_2 ;entry; LBB1_1: ;entry; mr r3, r2; LBB1_2: ;entry; blr . instead of:; _test:; addic r2,r3,-1; subfe r0,r2,r3; slwi r3,r0,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:7159,optimiz,optimization,7159,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,1,['optimiz'],['optimization']
Performance,");; ```. ### Disable copy assignment for RooAbsArg and derived types. Copy assignment for RooAbsArgs was implemented in an unexpected and; inconsistent way. While one would expect that the copy assignment is copying; the object, it said in the documentation of `RooAbsArg::operator=` that it will; ""assign all boolean and string properties of the original bject. Transient; properties and client-server links are not assigned."" This contradicted with; the implementation, where the server links were actually copied too.; Furthermore, in `RooAbsRealLValue`, the assigment operator was overloaded by a; function that only assigns the value of another `RooAbsReal`. With all these inconsistencies, it was deemed safer to disable copy assignment; of RooAbsArgs from now on. ### RooBrowser: a graphical user interface for workspace exploration, visualization, and analysis. This experimental new feature utilises the technology from ROOT's familiar `TBrowser` in order to create an interface for graphically exploring and visualizing the content of a workspace, as well as perform basic fitting operations with the models and datasets. ![Demonstration of RooBrowser using json workspace from the roofit tutorials directory](RooBrowser.png). ### Removal of deprecated HistFactory functionality. #### Removal of HistoToWorkspaceFactory (non-Fast version). The original `HistoToWorkspaceFactory` produced models that consisted of a; Poisson term for each bin. In this ""number counting form"" the dataset has one; row and the collumns corresponded to the number of events for each bin. This; led to severe performance problems in statistical tools that generated; pseudo-experiments and evaluated likelihood ratio test statistics. Nowadays, everyone uses the faster `HistoToWorkspaceFactoryFast` implementation that; produces a model in the ""standard form"" where the dataset has one row for each; event, and the column corresponds to the value of the observable in the; histogram. Therefore, the original `His",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md:22278,perform,perform,22278,README/ReleaseNotes/v628/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md,1,['perform'],['perform']
Performance,")`` method does not have ``REQUIRES``, so the; analysis issues a warning. Thread safety analysis is not inter-procedural, so; caller requirements must be explicitly declared.; There is also a warning in ``transferFrom()``, because although the method; locks ``this->mu``, it does not lock ``b.mu``. The analysis understands; that these are two separate mutexes, in two different objects. Finally, there is a warning in the ``withdraw()`` method, because it fails to; unlock ``mu``. Every lock must have a corresponding unlock, and the analysis; will detect both double locks, and double unlocks. A function is allowed to; acquire a lock without releasing it, (or vice versa), but it must be annotated; as such (using ``ACQUIRE``/``RELEASE``). Running The Analysis; --------------------. To run the analysis, simply compile with the ``-Wthread-safety`` flag, e.g. .. code-block:: bash. clang -c -Wthread-safety example.cpp. Note that this example assumes the presence of a suitably annotated; :ref:`mutexheader` that declares which methods perform locking,; unlocking, and so on. Basic Concepts: Capabilities; ============================. Thread safety analysis provides a way of protecting *resources* with; *capabilities*. A resource is either a data member, or a function/method; that provides access to some underlying resource. The analysis ensures that; the calling thread cannot access the *resource* (i.e. call the function, or; read/write the data) unless it has the *capability* to do so. Capabilities are associated with named C++ objects which declare specific; methods to acquire and release the capability. The name of the object serves; to identify the capability. The most common example is a mutex. For example,; if ``mu`` is a mutex, then calling ``mu.Lock()`` causes the calling thread; to acquire the capability to access data that is protected by ``mu``. Similarly,; calling ``mu.Unlock()`` releases that capability. A thread may hold a capability either *exclusively* or *shared",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst:3411,perform,perform,3411,interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,1,['perform'],['perform']
Performance,")``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of *a* by *b*, and produces the quotient.; Division by 0 produces an error. Division of INT64_MIN by -1 produces an error. ``!empty(``\ *a*\ ``)``; This operator produces 1 if the string, list, or DAG *a* is empty; 0 otherwise.; A dag is empty if it has no arguments; the operator does not count. ``!eq(`` *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``, or; record values. Use ``!cast<string>`` to compare other types of objects. ``!exists<``\ *type*\ ``>(``\ *name*\ ``)``; This operator produces 1 if a record of the given *type* whose name is *name*; exists; 0 otherwise. *name* should be of type *string*. ``!filter(``\ *var*\ ``,`` *list*\ ``,`` *predicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably; refers to *var*. The predicate must; produce a boolean value (``bit``, ``bits``, or ``int``). The value is; interpreted as with ``!if``:; if the value is 0, the element is not included in the new list. If the value; is anything else, the element is included. ``!find(``\ *string1*\ ``,`` *string2*\ [``,`` *start*]\ ``)``; This operator searches for *string2* in *string1* and produces its; position. The starting position of the search may be specified by *start*,; which can range between 0 and the length of *string1*; the default is 0.; If the string is not found, the result is -1. ``!foldl(``\ *init*\ ``,`` *list*\ ``,`` *acc*\ ``,`` *var*\ ``,`` *expr*\ ``)``; This operator performs a left-fold over the items in *list*. The; variable *acc* acts as the accumulator and is initialized to *init*.; The variable *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:63112,perform,perform,63112,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['perform']
Performance,"* `SPARC V8 ABI <http://sparc.org/standards/psABI3rd.pdf>`_. SystemZ; -------. * `z/Architecture Principles of Operation (registration required, free sign-up) <http://www-01.ibm.com/support/docview.wss?uid=isg2b9de5f05a9d57819852571c500428f9a>`_. VE; --. * `NEC SX-Aurora TSUBASA ISA Guide <https://www.hpc.nec/documents/guide/pdfs/Aurora_ISA_guide.pdf>`_; * `NEC SX-Aurora TSUBASA manuals and documentation <https://www.hpc.nec/documentation>`_. X86; ---. * `AMD processor manuals <http://developer.amd.com/resources/developer-guides-manuals/>`_; * `Intel 64 and IA-32 manuals <http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html>`_; * `Intel Itanium documentation <http://www.intel.com/design/itanium/documentation.htm?iid=ipp_srvr_proc_itanium2+techdocs>`_; * `X86 and X86-64 SysV psABI <https://github.com/hjl-tools/x86-psABI/wiki/X86-psABI>`_; * `Calling conventions for different C++ compilers and operating systems <http://www.agner.org/optimize/calling_conventions.pdf>`_. XCore; -----. * `The XMOS XS1 Architecture (ISA) <https://www.xmos.ai/download/The-XMOS-XS1-Architecture%281.0%29.pdf>`_; * `The XMOS XS2 Architecture (ISA) <https://www.xmos.ai/download/xCORE-200:-The-XMOS-XS2-Architecture-%28ISA%29%281.1%29.pdf>`_; * `Tools Development Guide (includes ABI) <https://www.xmos.ai/download/Tools-Development-Guide%282.1%29.pdf>`_. Hexagon; -------. * `Hexagon Programmer's Reference Manuals and Hexagon ABI Specification (registration required, free sign-up) <https://developer.qualcomm.com/software/hexagon-dsp-sdk/tools>`_. Other relevant lists; --------------------. * `GCC reading list <http://gcc.gnu.org/readings.html>`_. ABI; ===. * `System V Application Binary Interface <http://www.sco.com/developers/gabi/latest/contents.html>`_; * `Itanium C++ ABI <http://itanium-cxx-abi.github.io/cxx-abi/>`_ (This is used for all non-Windows targets.). Linux; -----. * `Linux extensions to gabi <https://github.com/hjl-tools/linux-abi/wiki/Linux-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompilerWriterInfo.rst:6332,optimiz,optimize,6332,interpreter/llvm-project/llvm/docs/CompilerWriterInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompilerWriterInfo.rst,1,['optimiz'],['optimize']
Performance,"**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default makes it; easy to use ORC as an in-memory compiler for an existing JIT (similar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of reloc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:2913,concurren,concurrently,2913,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['concurren'],['concurrently']
Performance,"**`TPosixThread`** ). **`TMutex`** class implements `mutex` locks. A mutex is a mutually; exclusive lock. The platform dependent implementation is in the; **`TMutexImp`** class and its descendant classes (e.g.; **`TPosixMutex`**). **`TCondition`** class implements a condition variable. Use a condition; variable to signal threads. The platform dependent implementation is in; the **`TConditionImp`** and **`TPosixCondition`** classes . **`TSemaphore`** class implements a counting semaphore. Use a semaphore; to synchronize threads. The platform dependent implementation is in the; **`TMutexImp`** and **`TConditionImp`** classes. ### TThread for Pedestrians. To run a thread in ROOT, follow these steps:. 1. Initialization. Add these lines to your `rootlogon.C`:. ``` {.cpp}; {; ...; // The next line may be unnecessary on some platforms; gSystem->Load(""/usr/lib/libpthread.so"");; gSystem->Load(""$ROOTSYS/lib/libThread.so"");; ...; }; ```. This loads the library with the **`TThread`** class and the `pthread`; specific implementation file for `Posix` threads. 2. Coding. Define a function (e.g. `void* UserFun(void* UserArgs))` that should run; as a thread. The code for the examples is at the web site of the authors; (Jörn Adamczewski, Marc Hemberger). After downloading the code from this; site, you can follow the example below:. <http://www-linux.gsi.de/~go4/HOWTOthreads/howtothreadsbody.html>. 3. Loading. Start an interactive ROOT session. Load the shared library:. ``` {.cpp}; root[] gSystem->Load(""mhs3.so""); // or; root[] gSystem->Load(""CalcPiThread.so"");; ```. 4. Creating. Create a thread instance (see also example `RunMhs3.C `or` RunPi.C`); with:. ``` {.cpp}; root[] TThread *th = new TThread(UserFun,UserArgs);; ```. When called from the interpreter, this gives the name ""`UserFun`"" to the; thread. This name can be used to retrieve the thread later. However,; when called from compiled code, this method does not give any name to; the thread. So give a name to the thread in compile",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:4635,load,loads,4635,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['load'],['loads']
Performance,"*. Apple Clang Builds (A More Complex Bootstrap); =============================================. Apple's Clang builds are a slightly more complicated example of the simple; bootstrapping scenario. Apple Clang is built using a 2-stage build. The stage1 compiler is a host-only compiler with some options set. The stage1; compiler is a balance of optimization vs build time because it is a throwaway.; The stage2 compiler is the fully optimized compiler intended to ship to users. Setting up these compilers requires a lot of options. To simplify the; configuration the Apple Clang build settings are contained in CMake Cache files.; You can build an Apple Clang compiler using the following commands:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/Apple-stage1.cmake <path to source>/llvm; $ ninja stage2-distribution. This CMake invocation configures the stage1 host compiler, and sets; CLANG_BOOTSTRAP_CMAKE_ARGS to pass the Apple-stage2.cmake cache script to the; stage2 configuration step. When you build the stage2-distribution target it builds the minimal stage1; compiler and required tools, then configures and builds the stage2 compiler; based on the settings in Apple-stage2.cmake. This pattern of using cache scripts to set complex settings, and specifically to; make later stage builds include cache scripts is common in our more advanced; build configurations. Multi-stage PGO; ===============. Profile-Guided Optimizations (PGO) is a really great way to optimize the code; clang generates. Our multi-stage PGO builds are a workflow for generating PGO; profiles that can be used to optimize clang. At a high level, the way PGO works is that you build an instrumented compiler,; then you run the instrumented compiler against sample source files. While the; instrumented compiler runs it will output a bunch of files containing; performance counters (.profraw files). After generating all the profraw files; you use llvm-profdata to merge the files into a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:4276,cache,cache,4276,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['cache']
Performance,"*. However, there; are certain functionalities that work with the assumption that the used; lengths are expressed in centimeters. This is the case for shape; capacity or volume weight computation. The same is valid when using the; ROOT geometry as navigator for an external transport MC package (e.g.; GEANT) via the VMC interface. Other units in use: All angles used for defining rotation matrices or; some shape parameters are expressed in **degrees**. Material density is; expressed in [**g/cm3**]. ### Primitive Shapes. #### Boxes - TGeoBBox Class. Normally a box has to be built only with 3 parameters: `DX,DY,DZ`; representing the half-lengths on X, Y and Z-axes. In this case, the; origin of the box will match the one of its reference frame and the box; will range from: `-DX` to `DX` on X-axis, from `-DY` to `DY` on Y and; from `-DZ` to `DZ` on Z. On the other hand, any other shape needs to; compute and store the parameters of their minimal bounding box. The; bounding boxes are essential to optimize navigation algorithms.; Therefore all other primitives derive from **`TGeoBBox`**. Since the; minimal bounding box is not necessary centered in the origin, any box; allows an origin translation `(Ox`,`Oy`,`Oz)`. All primitive; constructors automatically compute the bounding box parameters. Users; should be aware that building a translated box that will represent a; primitive shape by itself would affect any further positioning of other; shapes inside. Therefore it is highly recommendable to build; non-translated boxes as primitives and translate/rotate their; corresponding volumes only during positioning stage. ``` {.cpp}; TGeoBBox(Double_t dx,Double_t dy,Double_t dz,Double_t *origin=0);; ```. ![TGeoBBox class](pictures/060001B6.png). #### Parallelepiped - TGeoPara class. A parallelepiped is a shape having 3 pairs of parallel faces out of; which one is parallel with the XY plane (Z faces). All faces are; parallelograms in the general case. The Z faces have 2 edges parallel;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:26340,optimiz,optimize,26340,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['optimiz'],['optimize']
Performance,"*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``MemoryPhi``\ s don't correspond to any; concrete operation. As such, ``BasicBlock``\ s are mapped to ``MemoryPhi``\ s; inside ``MemorySSA``, whereas ``Instruction``\ s are mapped to ``MemoryUse``\ s; and ``MemoryDef``\ s. Note also that in SSA, Phi nodes merge must-reach definitions (that is,; definitions that *must* be new versions of variables). In MemorySSA, PHI nodes; merge may-reach definitions (that is, until disambiguated, the versions that; reach a phi node may or may not clobber a given variable). ``MemoryUse``\ s are operations which use but don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:3717,load,load,3717,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['load'],['load']
Performance,"*MI)``, and; ``DFAPacketizer::canReserveResources(MachineInstr *MI)``. These functions allow; a target packetizer to add an instruction to an existing packet and to check; whether an instruction can be added to a packet. See; ``llvm/CodeGen/DFAPacketizer.h`` for more information. Implementing a Native Assembler; ===============================. Though you're probably reading this because you want to write or maintain a; compiler backend, LLVM also fully supports building a native assembler.; We've tried hard to automate the generation of the assembler from the .td files; (in particular the instruction syntax and encodings), which means that a large; part of the manual and repetitive data entry can be factored and shared with the; compiler. Instruction Parsing; -------------------. .. note::. To Be Written. Instruction Alias Processing; ----------------------------. Once the instruction is parsed, it enters the MatchInstructionImpl function.; The MatchInstructionImpl function performs alias processing and then does actual; matching. Alias processing is the phase that canonicalizes different lexical forms of the; same instructions down to one representation. There are several different kinds; of alias that are possible to implement and they are listed below in the order; that they are processed (which is in order from simplest/weakest to most; complex/powerful). Generally you want to use the first alias mechanism that; meets the needs of your instruction, because it will allow a more concise; description. Mnemonic Aliases; ^^^^^^^^^^^^^^^^. The first phase of alias processing is simple instruction mnemonic remapping for; classes of instructions which are allowed with two different mnemonics. This; phase is a simple and unconditionally remapping from one input mnemonic to one; output mnemonic. It isn't possible for this form of alias to look at the; operands at all, so the remapping must apply for all forms of a given mnemonic.; Mnemonic aliases are defined simply, for ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:81575,perform,performs,81575,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['perform'],['performs']
Performance,"*none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. buffer_wbl2 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/ge",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:313806,perform,performing,313806,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"*none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:263467,perform,performing,263467,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"*option="""");; ~~~. The drawing/animation time range is a global variable that can be; directly set:. ~~~{.cpp}; gGeoManager->SetTminTmax(tmin, tmax);; // without arguments resets the time range to the maximum value; ~~~. Once set, the time range will be active both for individual or global; track drawing. For animation, this range is divided to the desired; number of frames and will be automatically updated at each frame in; order to get the animation effect. The option provided to all track-drawing methods can trigger different; track selections:. `default: `A track (or all primary tracks) drawn without daughters. `/D:` Track and first level descendents only are drawn. `/*: ` Track and all descendents are drawn. `/Ntype:` All tracks having `name=type` are drawn. Generally several options can be concatenated in the same string (E.g.; `""/D /Npion-""`). For animating tracks, additional options can be added:. `/G:`Geometry animate. Generally when drawing or animating tracks, one; has to first perform a normal drawing of the geometry as convenient. The; tracks will be drawn over the geometry. The geometry itself will be; animated (camera moving and rotating in order to ""catch"" the majority of; current track segments.). `/S:`Save all frames in gif format in the current folder. This option; allows creating a movie based on individual frames. \anchor GP03; ## Checking the Geometry. Several checking methods are accessible from the context menu of volume; objects or of the manager class. They generally apply only to the; visible parts of the drawn geometry in order to ease geometry checking,; and their implementation is in the TGeoChecker class. The checking; package contains an overlap checker and several utility methods that; generally have visualization outputs. \anchor GP03a; ### The Overlap Checker. An overlap is any region in the Euclidian space being contained by more; than one positioned volume. Due to the containment scheme used by the; modeller, all points inside a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:89427,perform,perform,89427,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['perform']
Performance,"+ A - P; ``R_AMDGPU_GOTPCREL32_LO`` Static 8 ``word32`` (G + GOT + A - P) & 0xFFFFFFFF; ``R_AMDGPU_GOTPCREL32_HI`` Static 9 ``word32`` (G + GOT + A - P) >> 32; ``R_AMDGPU_REL32_LO`` Static 10 ``word32`` (S + A - P) & 0xFFFFFFFF; ``R_AMDGPU_REL32_HI`` Static 11 ``word32`` (S + A - P) >> 32; *reserved* 12; ``R_AMDGPU_RELATIVE64`` Dynamic 13 ``word64`` B + A; ``R_AMDGPU_REL16`` Static 14 ``word16`` ((S + A - P) - 4) / 4; ========================== ======= ===== ========== ==============================. ``R_AMDGPU_ABS32_LO`` and ``R_AMDGPU_ABS32_HI`` are only supported by; the ``mesa3d`` OS, which does not support ``R_AMDGPU_ABS64``. There is no current OS loader support for 32-bit programs and so; ``R_AMDGPU_ABS32`` is not used. .. _amdgpu-loaded-code-object-path-uniform-resource-identifier:. Loaded Code Object Path Uniform Resource Identifier (URI); ---------------------------------------------------------. The AMD GPU code object loader represents the path of the ELF shared object from; which the code object was loaded as a textual Uniform Resource Identifier (URI).; Note that the code object is the in memory loaded relocated form of the ELF; shared object. Multiple code objects may be loaded at different memory; addresses in the same process from the same ELF shared object. The loaded code object path URI syntax is defined by the following BNF syntax:. .. code::. code_object_uri ::== file_uri | memory_uri; file_uri ::== ""file://"" file_path [ range_specifier ]; memory_uri ::== ""memory://"" process_id range_specifier; range_specifier ::== [ ""#"" | ""?"" ] ""offset="" number ""&"" ""size="" number; file_path ::== URI_ENCODED_OS_FILE_PATH; process_id ::== DECIMAL_NUMBER; number ::== HEX_NUMBER | DECIMAL_NUMBER | OCTAL_NUMBER. **number**; Is a C integral literal where hexadecimal values are prefixed by ""0x"" or ""0X"",; and octal values by ""0"". **file_path**; Is the file's path specified as a URI encoded UTF-8 string. In URI encoding,; every character that is not in the regular expre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:82856,load,loader,82856,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],"['loaded', 'loader']"
Performance,"++++++. For GFX10-GFX11:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple work-group processors (WGP).; * Each WGP has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same; WGP. In CU wavefront execution mode the wavefronts may be executed by; different SIMDs in the same CU. In WGP wavefront execution mode the; wavefronts may be executed by different SIMDs in different CUs in the same; WGP.; * Each WGP has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a WGP are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; WGP. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:336049,perform,performed,336049,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"++; DEFINE_FLAG(std::string, example_flag, """", ""A sample flag."");. void Example() {; bool x = GetFlag(FLAGS_example_flag).empty();; f();; if (x) {; g();; } else {; h();; }; }; ```. The tool would simplify the code to:. ```c++; void Example() {; f();; g();; }; ```. We can solve this problem with a classic constant propagation lattice combined; with symbolic evaluation. ## Example: finding inefficient usages of associative containers. Real-world code often accidentally performs repeated lookups in associative; containers:. ```c++; map<int, Employee> xs;; xs[42]->name = ""..."";; xs[42]->title = ""..."";; ```. To find the above inefficiency we can use the available expressions analysis to; understand that `m[42]` is evaluated twice. ```c++; map<int, Employee> xs;; Employee &e = xs[42];; e->name = ""..."";; e->title = ""..."";; ```. We can also track the `m.contains()` check in the flow condition to find; redundant checks, like in the example below. ```c++; std::map<int, Employee> xs;; if (!xs.contains(42)) {; xs.insert({42, someEmployee});; }; ```. ## Example: refactoring types that implicitly convert to each other. Refactoring one strong type to another is difficult, but the compiler can help:; once you refactor one reference to the type, the compiler will flag other places; where this information flows with type mismatch errors. Unfortunately this; strategy does not work when you are refactoring types that implicitly convert to; each other, for example, replacing `int32_t` with `int64_t`. Imagine that we want to change user IDs from 32 to 64-bit integers. In other; words, we need to find all integers tainted with user IDs. We can use data flow; analysis to implement taint analysis. ```c++; void UseUser(int32_t user_id) {; int32_t id = user_id;; // Variable `id` is tainted with a user ID.; ...; }; ```. Taint analysis is very well suited to this problem because the program rarely; branches on user IDs, and almost certainly does not perform any computation; (like arithmetic).; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md:30341,perform,perform,30341,interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,1,['perform'],['perform']
Performance,"+-----------------------------------------------+; | reserved | ``3`` | Unused. |; +---------------+--------------+-----------------------------------------------+. EndOfBuffer Records; -------------------. An EndOfBuffer record type indicates that there is no more trace data in this; buffer. The reader is expected to seek past the remaining buffer_size expressed; before the start of buffer and look for either another header or EOF. Format Grammar and Invariants; =============================. Not all sequences of Metadata records and Function records are valid data. A; sequence should be parsed as a state machine. The expectations for a valid; format can be expressed as a context free grammar. This is an attempt to explain the format with statements in EBNF format. - Format := Header ThreadBuffer* EOF. - ThreadBuffer := NewBuffer WallClockTime NewCPUId BodySequence* End. - BodySequence := NewCPUId | TSCWrap | Function | CustomEvent. - Function := (Function_Entry_Args CallArgument*) | Function_Other_Type. - CustomEvent := CustomEventMarker CustomEventUnstructuredMemory. - End := EndOfBuffer RemainingBufferSizeToSkip. Function Record Order; ---------------------. There are a few clarifications that may help understand what is expected of; Function records. - Functions with an Exit are expected to have a corresponding Entry or; Entry_Args function record precede them in the trace. - Tail_Exit Function records record the Function ID of the function whose return; address the program counter will take. In other words, the final function that; would be popped off of the call stack if tail call optimization was not used. - Not all functions marked for instrumentation are necessarily in the trace. The; tracer uses heuristics to preserve the trace for non-trivial functions. - Not every entry must have a traced Exit or Tail Exit. The buffer may run out; of space or the program may request for the tracer to finalize toreturn the; buffer before an instrumented function exits.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst:16653,optimiz,optimization,16653,interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst,1,['optimiz'],['optimization']
Performance,"+i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never; taken, the optimizer can change the conditional branch into an; unconditional one. If a is loop is annotated with; :ref:`llvm.loop.mustprogress <langref_llvm_loop_mustprogress>` metadata,; the compiler is allowed to assume that it will eventually terminate, even; if it cannot prove it. For instance, it may remove a mustprogress-loop; that does not have any side-effect in its body even though the program; could be stuck in that loop forever. Languages such as C and; `C++ <https://eel.is/c++draft/intro.progress#1>`_ have such; forward-progress guarantees for some loops. Also see the; :ref:`mustprogress <langref_mustprogress>` and; :ref:`willreturn <langref_willreturn>` function attributes, as well as; the older :ref:`llvm.sideeffect <llvm_sideeffect>` intrinsic. * The number of executions of the loop header before leaving the loop is; the **loop trip count** (or **",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:7160,optimiz,optimizer,7160,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['optimiz'],['optimizer']
Performance,", ""i386"", {3}, object; 5: bind-arch, ""x86_64"", {3}, object; 6: lipo, {4, 5}, object; 7: input, ""t1.c"", c; 8: preprocessor, {7}, cpp-output; 9: compiler, {8}, assembler; 10: assembler, {9}, object; 11: bind-arch, ""i386"", {10}, object; 12: bind-arch, ""x86_64"", {10}, object; 13: lipo, {11, 12}, object. After this stage is complete the compilation process is divided into; a simple set of actions which need to be performed to produce; intermediate or final outputs (in some cases, like ``-fsyntax-only``,; there is no ""real"" final output). Phases are well known compilation; steps, such as ""preprocess"", ""compile"", ""assemble"", ""link"", etc. #. **Bind: Tool & Filename Selection**. This stage (in conjunction with the Translate stage) turns the tree; of Actions into a list of actual subprocess to run. Conceptually, the; driver performs a top down matching to assign Action(s) to Tools. The; ToolChain is responsible for selecting the tool to perform a; particular action; once selected the driver interacts with the tool; to see if it can match additional actions (for example, by having an; integrated preprocessor). Once Tools have been selected for all actions, the driver determines; how the tools should be connected (for example, using an inprocess; module, pipes, temporary files, or user provided filenames). If an; output file is required, the driver also computes the appropriate; file name (the suffix and file location depend on the input types and; options such as ``-save-temps``). The driver interacts with a ToolChain to perform the Tool bindings.; Each ToolChain contains information about all the tools needed for; compilation for a particular architecture, platform, and operating; system. A single driver invocation may query multiple ToolChains; during one compilation in order to interact with tools for separate; architectures. The results of this stage are not computed directly, but the driver; can print the results via the ``-ccc-print-bindings`` option. For; example:. .. c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst:8787,perform,perform,8787,interpreter/llvm-project/clang/docs/DriverInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst,1,['perform'],['perform']
Performance,", 4B items (list frame) | Frame preamble encoding |; | Maximum field / type version | 4B | Field meta-data encoding |; | Maximum number of fields, columns | 4B (foreseen: <10M) | 32bit column / field IDs, list frame limit |; | Maximum number of cluster groups | 4B (foreseen: <10k) | List frame limits |; | Maximum number of clusters per group | 4B (foreseen: <10k) | List frame limits, cluster group summary encoding |; | Maximum number of pages per cluster per column | 4B | List frame limits |; | Maximum number of entries per cluster | 2^56 | Cluster summary encoding |; | Maximum string length (meta-data) | 4GB | String encoding |; | Maximum RBlob size | 128 PiB | 1GiB / 8B * 1GiB (with maxKeySize=1GiB, offsetSize=8B) |. ## Glossary. ### Anchor. The anchor is a data block that represents the entry point to an RNTuple.; The anchor is specific to the RNTuple container in which the RNTuple data are embedded (e.g., a ROOT file or an object store).; The anchor must provide the information to load the header and the footer **envelopes**. ### Cluster. A cluster is a set of **pages** that contain all the data belonging to an entry range.; The data set is partitioned in clusters.; A typical cluster size is tens to hundreds of megabytes. ### Column. A column is a storage backed vector of a number of **elements** of a simple type.; Column elements have a fixed bit-length that depends on the column type.; Some column types allow setting the bit lengths within specific limits (e.g. for floats with truncated mantissa). ### Envelope. An envelope is a data block with RNTuple meta-data, such as the header and the footer. ### Field. A field describes a serialized C++ type.; A field can have a hierarchy of subfields representing a composed C++ type (e.g., a vector of integers).; A field has zero, one, or multiple **columns** attached to it.; The columns contain the data related to the field but not to its subfields, which have their own columns. ### Frame. A frame is a byte range with m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/BinaryFormatSpecification.md:50206,load,load,50206,tree/ntuple/v7/doc/BinaryFormatSpecification.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/BinaryFormatSpecification.md,1,['load'],['load']
Performance,", Arg1 is a compile-time constant.; // These callbacks are emitted by -fsanitize-coverage=trace-cmp since 2017-08-11; void __sanitizer_cov_trace_const_cmp1(uint8_t Arg1, uint8_t Arg2);; void __sanitizer_cov_trace_const_cmp2(uint16_t Arg1, uint16_t Arg2);; void __sanitizer_cov_trace_const_cmp4(uint32_t Arg1, uint32_t Arg2);; void __sanitizer_cov_trace_const_cmp8(uint64_t Arg1, uint64_t Arg2);. // Called before a switch statement.; // Val is the switch operand.; // Cases[0] is the number of case constants.; // Cases[1] is the size of Val in bits.; // Cases[2:] are the case constants.; void __sanitizer_cov_trace_switch(uint64_t Val, uint64_t *Cases);. // Called before a division statement.; // Val is the second argument of division.; void __sanitizer_cov_trace_div4(uint32_t Val);; void __sanitizer_cov_trace_div8(uint64_t Val);. // Called before a GetElemementPtr (GEP) instruction; // for every non-constant array index.; void __sanitizer_cov_trace_gep(uintptr_t Idx);. // Called before a load of appropriate size. Addr is the address of the load.; void __sanitizer_cov_load1(uint8_t *addr);; void __sanitizer_cov_load2(uint16_t *addr);; void __sanitizer_cov_load4(uint32_t *addr);; void __sanitizer_cov_load8(uint64_t *addr);; void __sanitizer_cov_load16(__int128 *addr);; // Called before a store of appropriate size. Addr is the address of the store.; void __sanitizer_cov_store1(uint8_t *addr);; void __sanitizer_cov_store2(uint16_t *addr);; void __sanitizer_cov_store4(uint32_t *addr);; void __sanitizer_cov_store8(uint64_t *addr);; void __sanitizer_cov_store16(__int128 *addr);. Tracing control flow; ====================. With ``-fsanitize-coverage=control-flow`` the compiler will create a table to collect; control flow for each function. More specifically, for each basic block in the function,; two lists are populated. One list for successors of the basic block and another list for; non-intrinsic called functions. **TODO:** in the current implementation, indirect calls are not ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SanitizerCoverage.rst:11177,load,load,11177,interpreter/llvm-project/clang/docs/SanitizerCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SanitizerCoverage.rst,1,['load'],['load']
Performance,", ROOT objects can be send with POST request and used as arguments of; objects method execution in exe.bin and exe.json requests. Request and response HTTP headers are now directly accessible in THttpCallArg class. When command is registered with THttpServer::RegisterCommand() method,; one could configure additional arguments which should be submitted when; command is executed with cmd.json requests. Introduce restriction rules for objects access with THttpServer::Restrict() method.; Up to now general read-only flag was applied - either; everything read-only or everything is fully accessible.; Now one could restrict access to different parts of; objects hierarchy or even fully 'hide' them from the client.; Restriction based on user account name, which is applied; when htdigest authentication is configured.; One also able to allow execution of selected methods. Implement multi.bin and multi.json requests.; One could request many items with single HTTP request.; Let optimize communication between server and client. With *SNIFF* tag in ClassDef() comments one could expose different properties,; which than exposed by the TRootSniffer to the client with h.json requests.; Such possibility ease implementation of client-side code for custom classes. Allow to bind http port with loopback address.; This restrict access to http server only from localhost.; One could either specify 'loopback' option in constructor:; new THttpServer(""http:8080?loopback""); or in clear text specify IP address to which http socket should be bind:; new THttpServer(""http:127.0.0.1:8080""); If host has several network interfaces, one could select one for binding:; new THttpServer(""http:192.168.1.17:8080""). ### TNetXNGFileStager; Fixed ROOT-7703. This restores the behavior of Locate() to that found with; TXNetFileStager: Rather than return only the xrootd server's reply, the endpoint; hostname is looked up and Locate() returns the full url, including the path. ### TWebFile; Fixed ROOT-7809. Returns an er",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md:19729,optimiz,optimize,19729,README/ReleaseNotes/v606/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md,1,['optimiz'],['optimize']
Performance,", Selu, Sigmoid, Softmax, Tanh, LeakyRelu; - BatchNormalization; - MaxPool, AveragePool, GlobalAverage; - ConvTranspose; - Gather; - Expand, Reduce; - Neg, Exp, Sqrt, Reciprocal; - Add, Sum, Mul, Div; - Reshape, Flatten, Transpose; - Squeeze, Unsqueeze, Slice; - Concat, Reduce; - Identity; - Shape; - Custom; - Error; - Log. #### SOFIE Keras Parser; - The Swish Activation function is now supported in the SOFIE Keras parser. ## 2D Graphics Libraries. - Introduce `TAxis::ChangeLabelByValue` to set custom label defined by axis value. It works also; when axis zooming changes and position and index of correspondent axis label changes as well.; `TAxis::ChangeLabel` method to change axis label by index works as before. - Introduce `TCanvas::SaveAll` method. Allows to store several pads at once into different image file formats.; File name can include printf qualifier to code pad number. Also allows to store all pads in single PDF; or single ROOT file. Significantly improves performance when creating many image files using web graphics. - Introduce `TCanvas::UpdateAsync` method. In case of web-based canvas triggers update of the canvas on the client side,; but does not wait that real update is completed. Avoids blocking of caller thread.; Have to be used if called from other web-based widget to avoid logical dead-locks.; In case of normal canvas just canvas->Update() is performed. - The Delaunay triangles (used by TGraph2D) were computed by the external package `triangle.c`; included in the ROOT distribution. This package had several issues:; - It was not maintained anymore.; - Its license was not compatible with LGPL; This code is now replaced by the [CDT package](https://github.com/artem-ogre/CDT) which is; properly maintained and has a license (MLP) compatible with LGPL. It will appear in 6.03.02. ## Machine Learning integration. - ROOT now offers functionality to extract batches of events out of a dataset for use in common ML training workflows. For example, one can gene",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v630/index.md:19810,perform,performance,19810,README/ReleaseNotes/v630/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v630/index.md,1,['perform'],['performance']
Performance,", \; Oksana Shadura, UNL/CMS,\; Enric Tejedor Saavedra, CERN/SFT,\; Christian Tacke, GSI, \; Matevz Tadel, UCSD/CMS,\; Vassil Vassilev, Princeton/CMS,\; Wouter Verkerke, NIKHEF/Atlas,\; Stefan Wunsch, CERN/SFT,\; Anirudh Dagar, CERN-HSF/GSoC. ## Deprecation and Removal. - [`RooAbsReal::evaluateBatch()`](https://root.cern/doc/v624/classRooAbsReal.html#a261580dfe94f2b107f9b9a77cad78a62) has been removed in favour of the faster evaluateSpan(). See section ""RooFit Libraries"" for instructions on how to use [`RooAbsReal::evaluateSpan()`](https://root.cern/doc/v624/classRooAbsReal.html#a1e5129ffbc63bfd04c01511fd354b1b8).; - `TTreeProcessorMT::SetMaxTasksPerFilePerWorker` has been deprecated in favour of `TTreeProcessorMT::SetTasksPerWorkerHint`. ## Core Libraries. Due to internal changes required to comply with the deprecation of Intel TBB's `task_scheduler_init` and related; interfaces in recent TBB versions, as of v6.24 ROOT will not honor a maximum concurrency level set with; `tbb::task_scheduler_init` but will require instead the usage of `tbb::global_control`:. ```cpp; //tbb::task_scheduler_init init(2); // does not affect the number of threads ROOT will use anymore. tbb::global_control c(tbb::global_control::max_allowed_parallelism, 2);; ROOT::TThreadExecutor p1; // will use 2 threads; ROOT::TThreadExecutor p2(/*nThreads=*/8); // will still use 2 threads; ```. Note that the preferred way to steer ROOT's concurrency level is still through; [`ROOT::EnableImplicitMT`](https://root.cern/doc/master/namespaceROOT.html#a06f2b8b216b615e5abbc872c9feff40f); or by passing the appropriate parameter to executors' constructors, as in; [`TThreadExecutor::TThreadExecutor`](https://root.cern/doc/master/classROOT_1_1TThreadExecutor.html#ac7783d52c56cc7875d3954cf212247bb). See the discussion at [ROOT-11014](https://sft.its.cern.ch/jira/browse/ROOT-11014) for more context. ### Dynamic Path: `ROOT_LIBRARY_PATH`. A new way to set ROOT's ""Dynamic Path"" was added: the; environment variable `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:1990,concurren,concurrency,1990,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['concurren'],['concurrency']
Performance,", `Getp2f2funcname(void*)`, `Setgvp(Long_t)`, `SetRTLD_NOW()`, `SetRTLD_LAZY()`.; - `SetFCN(void*)` from TVirtualFitter, TFitter, TBackCompFitter, TMinuit; - `TFoam::SetRhoInt(void*)`. ### Core. - The enum constant `TRef::kNotComputed`, `TLink::kObjIsParent` were never used and have been removed.; - The enum constant `TClonesArray::kNoSplit` has not been used since v2.26 and has been removed. ## Interpreter. - Automatic declaration of variables (`h = new TH1F(...)`) is *only* available at the prompt. The side-effects of relying on this in source files is simply too grave. Due to a bug (ROOT-8538), automatically declared variables must currently reside on the top-most scope, i.e. not inside an `if` block etc.; - Improved the stack frame information generated by the JIT. By avoiding interleaving of the memory associated to multiple JIT module, the generation of stack trace involving jitted code and the catching of exception going through jitted code has been repaired.; - Interpreted code is now optimized; `.O 0/1/2/3` can be used to change the optimization level, as well as `#pragma cling optimize`.; - The prompt colors are now much more visible, both on terminals with light and dark background.; - Significant speedup of `TMethodCall`.; - One can now run `.x 12file-with@funny=name.C`; it will expect a function called `_12file_with_funny_name()`. ## Core Libraries. - See ""Build, Configuration and Testing Infrastructure"" below for changes in the directory structure.; - libCling now exports only a minimal set of symbols.; - Add support for std::array_view also for C++11 builds. The implementation has been modified to work before C++14.; - Added TCollection::Notify to allow notifying more than one object.; ```{.cpp}; TList formulas;; // Add several TTreeFormula to the list;; chain.SetNotify(&formulas);; ```; - For classes that need the `ClassDef` support, `ClassDefInline(ClassName, Version)` now provides it without the need for a dictionary source: all members injected by ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md:1909,optimiz,optimized,1909,README/ReleaseNotes/v610/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md,1,['optimiz'],['optimized']
Performance,", and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18073,optimiz,optimizations,18073,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['optimiz'],['optimizations']
Performance,", has been; developed for this case: it basically contains a list of standard; packetizers (one for each dataset) and loops over them (syntax:; ""dataset1,dataset2,..."" or dataset1 dataset2 ..."").; In; both cases, entry-list can be applied using the syntax; ""dataset<<entrylist"", e.g.; ""dataset1<<el1|dataset2<<el2|"".; The datasets to be processed can also be specified on one or multiple lines in a text file.; Add; support for automatic download of a package when available on the; master but not locally. The downloaded packages are store under <sandbox>/packages/downloaded; and automatically checked for updates against the master repository. If; a local version of the same package is created (using the; UploadPackage) the entry in downloaded is; cleared, so that the behaviour is unchanged.; Add; the possibility to remap the server for the files in a dataset. This; allows, for example, to reuse the dataset information for the same; files stored in a different cluster.; Add a local cache for; TDataSetManagerFile. This is mainly used to improve the speed of; TDataSetManager::ShowDataSets, which is run very often by users and may; be very slow if the number of dataset is large. The cache is also used; to cache frequently received dataset objects.Add the possibility to audit the activity on the nodes via syslog. .; New packetizer TPacketizerFile generating packets which contain a single; file path to be used in processing single files. Used, for example, in; tasks generating files. The files are specified into a TMap - named; 'PROOF_FilesToProcess' - containing the list of files to be generated; per host (the key is the host name, the value the TList of TObjString; (or TFileInfo) with the files names - or a TFileCollection: the output; of TFileCollection::GetFilesPerServer() can be directly passed as files; map). Workers are first assigned files belonging to; the list with host name matching the worker name. The map is; distributed to the master via the input list.Add suppor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v528/index.html:1491,cache,cache,1491,proof/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v528/index.html,1,['cache'],['cache']
Performance,", i32 0, i8 addrspace(1)* %obj); %obj.relocated = call coldcc i8 addrspace(1)* @llvm.experimental.gc.relocate.p1i8(token %0, i32 7, i32 7); ret i8 addrspace(1)* %obj.relocated; }. During lowering, this will result in an instruction selection DAG that looks; something like:. ::. CALLSEQ_START; ...; GC_TRANSITION_START (lowered i32 *@Flag), SRCVALUE i32* Flag; STATEPOINT; GC_TRANSITION_END (lowered i32 *@Flag), SRCVALUE i32 *Flag; ...; CALLSEQ_END. In order to generate the necessary transition code, the backend for each target; supported by ""hypothetical-gc"" must be modified to lower ``GC_TRANSITION_START``; and ``GC_TRANSITION_END`` nodes appropriately when the ""hypothetical-gc""; strategy is in use for a particular function. Assuming that such lowering has; been added for X86, the generated assembly would be:. .. code-block:: gas. 	 .globl	test1; 	 .align	16, 0x90; 	 pushq	%rax; 	 movl $1, %fs:Flag@TPOFF; 	 callq	foo; 	 movl $0, %fs:Flag@TPOFF; .Ltmp1:; 	 movq	(%rsp), %rax # This load is redundant (oops!); 	 popq	%rdx; 	 retq. Note that the design as presented above is not fully implemented: in particular,; strategy-specific lowering is not present, and all GC transitions are emitted as; as single no-op before and after the call instruction. These no-ops are often; removed by the backend during dead machine instruction elimination. Before the abstract machine model is lowered to the explicit statepoint model; of relocations by the :ref:`RewriteStatepointsForGC` pass it is possible for; any derived pointer to get its base pointer and offset from the base pointer; by using the ``gc.get.pointer.base`` and the ``gc.get.pointer.offset``; intrinsics respectively. These intrinsics are inlined by the; :ref:`RewriteStatepointsForGC` pass and must not be used after this pass. .. _statepoint-stackmap-format:. Stack Map Format; ================. Locations for each pointer value which may need read and/or updated by; the runtime or collector are provided in a separate section of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:19133,load,load,19133,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['load'],['load']
Performance,", i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.maxnum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.maxnum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point IEEE-754 maxNum of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.maxnum``' intrinsic performs floating-point maximum (:ref:`maxnum <i_maxnum>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.maxnum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.maxnum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_minimum:. '``llvm.vp.minimum.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.minimum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.minimum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.minimum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <ve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:728123,perform,performed,728123,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,", i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.minnum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.minnum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point IEEE-754 minNum of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.minnum``' intrinsic performs floating-point minimum (:ref:`minnum <i_minnum>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.minnum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.minnum.v4f32(<4 x float> %a, <4 x float> %b); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_maxnum:. '``llvm.vp.maxnum.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.maxnum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.maxnum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.maxnum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """"""""""",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:726471,perform,performed,726471,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,", location of JSROOT modules should be specified; as `/jsrootsys/modules/main.mjs`. And then trying to access files from local disk, one should specify `/currentdir/` folder:. ```javascript; jQuery.sap.registerModulePath(""NavExample"", ""/currentdir/"");; ```. JSROOT provides [example](https://root.cern/js/latest/demo/openui5/) showing usage of JSROOT drawing in the OpenUI5,; [source code](https://github.com/root-project/jsroot/tree/master/demo/openui5) can be found in repository. ### Migration v6 -> v7. * Core functionality should be imported from `main.mjs` module like:. ```javascript; import { create, parse, createHistogram, redraw } from 'https://root.cern/js/7.0.0/modules/main.mjs';; ```. * It is still possible to use `JSRoot.core.js` script, which provides very similar (but not identical!) functionality as with `v6` via global `JSROOT` object. * `JSROOT.define()` and `JSROOT.require()` functions only available after `JSRoot.core.js` loading. * Support of `require.js` and `openui5` loaders was removed. * Global hierarchy painter `JSROOT.hpainter` no longer existing, one can use `getHPainter` function:. ```javascript; import { getHPainter } from 'https://root.cern/js/7.0.0/modules/main.mjs';; let hpainter = getHPainter();; ```. * All math functions previously available via `JSROOT.Math` should be imported from `base/math.mjs` module:. ```javascript; import * as math from 'https://root.cern/js/7.0.0/modules/base/math.mjs';; ```. * Indication of batch mode `JSROOT.batch_mode` should be accessed via functions:. ```javascript; import { isBatchMode, setBatchMode } from 'https://root.cern/js/7.0.0/modules/main.mjs';; let was_batch = isBatchMode();; if (!was_batch) setBatchMode(true);; ```. * `JSROOT.extend()` function was removed, use `Object.assign()` instead. ### Migration v5 -> v6. * Main script was renamed to `JSRoot.core.js`. Old `JSRootCore.js` was deprecated and removed in v6.2. All URL parameters for main script ignored now, to load JSROOT functionality one shoul",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:47785,load,loaders,47785,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['load'],['loaders']
Performance,", omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:216678,load,loads,216678,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,", so a TGeoVolumeAssembly does not need to have a; medium. Due to the self-containment of assemblies, they are very; practical to use when a container is hard to define due to possible; overlaps during positioning. For instance, it is very easy creating; honeycomb structures. A very useful example for creating and using; assemblies can be found at: assembly.C. Creation of an assembly is very easy: one has just to create a; TGeoVolumeAssembly object and position the components inside as; for any volume:. ~~~{.cpp}; TGeoVolume *vol = new TGeoVolumeAssembly(name);; vol->AddNode(vdaughter1, cpy1, matrix1);; vol->AddNode(vdaughter2, cpy2, matrix2);; ~~~. Note that components cannot be declared as ""overlapping"" and that a; component can be an assembly volume. For existing flat volume; structures, one can define assemblies to force a hierarchical structure; therefore optimizing the performance. Usage of assemblies does NOT imply; penalties in performance, but in some cases, it can be observed that it; is not as performing as bounding the structure in a container volume; with a simple shape. Choosing a normal container is therefore; recommended whenever possible. \image html geometry006.png ""Assemblies of volumes"" width=600px. \anchor GP01c; ### Geometrical Transformations. All geometrical transformations handled by the modeller are provided as; a built-in package. This was designed to minimize memory requirements; and optimize performance of point/vector master-to-local and; local-to-master computation. We need to have in mind that a; transformation in **`TGeo`** has two major use-cases. The first one is; for defining the placement of a volume with respect to its container; reference frame. This frame will be called 'master' and the frame of the; positioned volume - 'local'. If `T` is a transformation used for; positioning volume daughters, then: `MASTER = T * LOCAL`. Therefore `T `is used to perform a local to master conversion, while; `T-1` for a master to local conversi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:51002,perform,performance,51002,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,2,['perform'],"['performance', 'performing']"
Performance,", specify the minimum version supported by your; application. .. option:: --print-supported-cpus. Print out a list of supported processors for the given target (specified; through ``--target=<architecture>`` or :option:`-arch` ``<architecture>``). If no; target is specified, the system default target will be used. .. option:: -mcpu=?, -mtune=?. Acts as an alias for :option:`--print-supported-cpus`. .. option:: -mcpu=help, -mtune=help. Acts as an alias for :option:`--print-supported-cpus`. .. option:: -march=<cpu>. Specify that Clang should generate code for a specific processor family; member and later. For example, if you specify -march=i486, the compiler is; allowed to generate instructions that are valid on i486 and later processors,; but which may not exist on earlier ones. Code Generation Options; ~~~~~~~~~~~~~~~~~~~~~~~. .. option:: -O0, -O1, -O2, -O3, -Ofast, -Os, -Oz, -Og, -O, -O4. Specify which optimization level to use:. :option:`-O0` Means ""no optimization"": this level compiles the fastest and; generates the most debuggable code. :option:`-O1` Somewhere between :option:`-O0` and :option:`-O2`. :option:`-O2` Moderate level of optimization which enables most; optimizations. :option:`-O3` Like :option:`-O2`, except that it enables optimizations that; take longer to perform or that may generate larger code (in an attempt to; make the program run faster). :option:`-Ofast` Enables all the optimizations from :option:`-O3` along; with other aggressive optimizations that may violate strict compliance with; language standards. :option:`-Os` Like :option:`-O2` with extra optimizations to reduce code; size. :option:`-Oz` Like :option:`-Os` (and thus :option:`-O2`), but reduces code; size further. :option:`-Og` Like :option:`-O1`. In future versions, this option might; disable different optimizations in order to improve debuggability. :option:`-O` Equivalent to :option:`-O1`. :option:`-O4` and higher. Currently equivalent to :option:`-O3`. .. option:: -g, -gline-table",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:10534,optimiz,optimization,10534,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,1,['optimiz'],['optimization']
Performance,", the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ignored if; the optimization is not safe to apply. There are loop hints that control transformations (e.g. vectorization, loop; unrolling) and there are loop hints that set transformation options (e.g.; ``vectorize_width``, ``unroll_count``). Pragmas setting transformation options; imply the transformation is enabled, as if it was enabled via the corresponding; transformation pragma (e.g. ``vectorize(enable)``). If the transformation is; disabled (e.g. ``vectorize(disable)``), that takes precedence over; transformations option pragmas implying that transformation. Vectorization, Interleaving, and Predication; ---------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:162239,optimiz,optimizations,162239,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizations']
Performance,", this is the total number of; virtual cores. For some applications and machine configurations this; may be too aggressive, in which case the amount of parallelism can; be reduced to ``N`` via:. - gold:; ``-Wl,-plugin-opt,jobs=N``; - ld64:; ``-Wl,-mllvm,-threads=N``; - ld.lld, ld64.lld:; ``-Wl,--thinlto-jobs=N``; - lld-link:; ``/opt:lldltojobs=N``. Other possible values for ``N`` are:. - 0:; Use one thread per physical core (default); - 1:; Use a single thread only (disable multi-threading); - all:; Use one thread per logical core (uses all hyper-threads). Incremental; -----------; .. _incremental:. ThinLTO supports fast incremental builds through the use of a cache,; which currently must be enabled through a linker option. - gold (as of LLVM 4.0):; ``-Wl,-plugin-opt,cache-dir=/path/to/cache``; - ld64 (supported since clang 3.9 and Xcode 8) and Mach-O ld64.lld (as of LLVM; 15.0):; ``-Wl,-cache_path_lto,/path/to/cache``; - ELF ld.lld (as of LLVM 5.0):; ``-Wl,--thinlto-cache-dir=/path/to/cache``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocache:/path/to/cache``. Cache Pruning; -------------. To help keep the size of the cache under control, ThinLTO supports cache; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value pairs separated by ``:`` characters.; Possible key-value pairs are:. - ``cache_size=X%``: The maximum size for the cache directory is ``X`` percent; of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is inva",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:4947,cache,cache-dir,4947,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,2,['cache'],"['cache', 'cache-dir']"
Performance,", to declare variables). - **addToCodeBody()**: adds the input string to the squashed code body. If a; class implements a translate function that wants to emit something to the; squashed code body, it must call this function with the code it wants to; emit. In case of loops, it automatically determines if the code needs to be; stored inside or outside the scope of that loop. - **makeValidVarName()**: takes a string (e.g., a variable name) and converts; it into a valid C++ variable name by replacing any forbidden characters with; underscores. - **buildArg()**: helps convert RooFit objects into arrays or other C++; representations for efficient computation. - **addResult()**: adds (or overwrites) the string representing the result of; a node. > For each `translate()` function, it is important to call `addResult()` since; this is what enables the squashing to happen. - **getResult()**: gets the result for the given node using the node name.; This node also performs the necessary code generation through recursive calls; to `translate()`. - **assembleCode()**: combines the generated code statements into the final; code body of the squashed function. These functions will appear again in this document with more contextual; examples. For detailed in-line documentation (code comments), please see:. > [roofit/roofitcore/src/RooFit/Detail/CodeSquashContext.cxx](https://github.com/root-project/root/blob/master/roofit/roofitcore/src/RooFit/Detail/CodeSquashContext.cxx). ### b. RooFuncWrapper. > [roofit/roofitcore/inc/RooFuncWrapper.h](https://github.com/root-project/root/blob/master/roofit/roofitcore/inc/RooFuncWrapper.h). This class wraps the generated C++ code in a RooFit object, so that it can be; used like other RooFit objects. It takes a function body as input and creates a callable function from it.; This allows users to evaluate the function and its derivatives efficiently. #### Helper Functions. - **loadParamsAndData()** extracts parameters and observables from the; prov",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md:32617,perform,performs,32617,roofit/doc/developers/roofit_ad.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md,1,['perform'],['performs']
Performance,", volatile T *addr);; int __builtin_arm_stlex(T val, volatile T *addr);; void __builtin_arm_clrex(void);. The types ``T`` currently supported are:. * Integer types with width at most 64 bits (or 128 bits on AArch64).; * Floating-point types; * Pointer types. Note that the compiler does not guarantee it will not insert stores which clear; the exclusive monitor in between an ``ldrex`` type operation and its paired; ``strex``. In practice this is only usually a risk when the extra store is on; the same cache line as the variable being modified and Clang will only insert; stack stores on its own, so it is best not to use these operations on variables; with automatic storage duration. Also, loads and stores may be implicit in code written between the ``ldrex`` and; ``strex``. Clang will not necessarily mitigate the effects of these either, so; care should be exercised. For these reasons the higher level atomic primitives should be preferred where; possible. Non-temporal load/store builtins; --------------------------------. Clang provides overloaded builtins allowing generation of non-temporal memory; accesses. .. code-block:: c. T __builtin_nontemporal_load(T *addr);; void __builtin_nontemporal_store(T value, T *addr);. The types ``T`` currently supported are:. * Integer types.; * Floating-point types.; * Vector types. Note that the compiler does not guarantee that non-temporal loads or stores; will be used. C++ Coroutines support builtins; --------------------------------. .. warning::; This is a work in progress. Compatibility across Clang/LLVM releases is not; guaranteed. Clang provides experimental builtins to support C++ Coroutines as defined by; https://wg21.link/P0057. The following four are intended to be used by the; standard library to implement the ``std::coroutine_handle`` type. **Syntax**:. .. code-block:: c. void __builtin_coro_resume(void *addr);; void __builtin_coro_destroy(void *addr);; bool __builtin_coro_done(void *addr);; void *__builtin_coro_promise",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:144744,load,load,144744,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['load'],['load']
Performance,", we see the counter for; SCHEDQ reports 272 cycles. This counter is incremented every time the dispatch; logic is unable to dispatch a full group because the scheduler's queue is full. Looking at the *Dispatch Logic* table, we see that the pipeline was only able to; dispatch two micro opcodes 51.5% of the time. The dispatch group was limited to; one micro opcode 44.6% of the cycles, which corresponds to 272 cycles. The; dispatch statistics are displayed by either using the command option; ``-all-stats`` or ``-dispatch-stats``. The next table, *Schedulers*, presents a histogram displaying a count,; representing the number of micro opcodes issued on some number of cycles. In; this case, of the 610 simulated cycles, single opcodes were issued 306 times; (50.2%) and there were 7 cycles where no opcodes were issued. The *Scheduler's queue usage* table shows that the average and maximum number of; buffer entries (i.e., scheduler queue entries) used at runtime. Resource JFPU01; reached its maximum (18 of 18 queue entries). Note that AMD Jaguar implements; three schedulers:. * JALU01 - A scheduler for ALU instructions.; * JFPU01 - A scheduler floating point operations.; * JLSAGU - A scheduler for address generation. The dot-product is a kernel of three floating point instructions (a vector; multiply followed by two horizontal adds). That explains why only the floating; point scheduler appears to be used. A full scheduler queue is either caused by data dependency chains or by a; sub-optimal usage of hardware resources. Sometimes, resource pressure can be; mitigated by rewriting the kernel using different instructions that consume; different scheduler resources. Schedulers with a small queue are less resilient; to bottlenecks caused by the presence of long data dependencies. The scheduler; statistics are displayed by using the command option ``-all-stats`` or; ``-scheduler-stats``. The next table, *Retire Control Unit*, presents a histogram displaying a count,; representing t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:31489,queue,queue,31489,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,", with the aim to provide to the LHC experiments a stand-alone and; high performant matrix package for reconstruction. The API of the current package differs; from the original one, in order to be compliant to the %ROOT coding conventions. SMatrix contains generic \ref SMatrixSVector to describe matrix and vector of arbitrary; dimensions and of arbitrary type. The classes are templated on the scalar type and on the; size of the matrix (number of rows and columns) or the vector. Therefore, the size has to; be known at compile time. Since the release 5.10, SMatrix supports symmetric matrices using; a storage class (ROOT::Math::MatRepSym) which contains only the N*(N+1)/2 independent element; of a NxN symmetric matrix.; It is not in the mandate of this package to provide a complete linear algebra functionality; for these classes. What is provided are basic \ref MatrixFunctions and \ref VectFunction,; such as the matrix-matrix, matrix-vector, vector-vector operations, plus some extra; functionality for square matrices, like inversion, which is based on the optimized Cramer; method for squared matrices of size up to 6x6, and determinant calculation.; For a more detailed descriptions and usage examples see:. * \ref SVectorDoc; * \ref SMatrixDoc; * \ref MatVecFunctions. The SMatrix package contains only header files. Normally one does not need to build any library.; In the %ROOT distribution a library, _libSmatrix_ is produced with the C++ dictionary information; for vectors, symmetric and squared matrices for double, float types up to dimension 7.; The current version of SMatrix can be downloaded from [here](../SMatrix.tar.gz). If you want; to install the header files or run the test _configure_ script and then _make install_ or; _make check_ to build the tests. No dictionary library is built in this case. ## References. 1. T. Veldhuizen, [_Expression Templates_](http://osl.iu.edu/~tveldhui/papers/Expression-Templates/exprtmpl.html),; C++ Report, 1995.; 2. T. Glebe, _SMat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/smatrix/doc/index.md:1931,optimiz,optimized,1931,math/smatrix/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/smatrix/doc/index.md,1,['optimiz'],['optimized']
Performance,", with the; proviso that of course the object cannot be legally read after the object's; lifetime ends. :arc-term:`Moving` occurs in specific situations where an lvalue is ""moved; from"", meaning that its current pointee will be used but the object may be left; in a different (but still valid) state. This arises with ``__block`` variables; and rvalue references in C++. For ``__strong`` lvalues, moving is equivalent; to loading the lvalue with primitive semantics, writing a null pointer to it; with primitive semantics, and then releasing the result of the load at the end; of the current full-expression. For all other lvalues, moving is equivalent to; reading the object. .. _arc.ownership.restrictions:. Restrictions; ------------. .. _arc.ownership.restrictions.weak:. Weak-unavailable types; ^^^^^^^^^^^^^^^^^^^^^^. It is explicitly permitted for Objective-C classes to not support ``__weak``; references. It is undefined behavior to perform an operation with weak; assignment semantics with a pointer to an Objective-C object whose class does; not support ``__weak`` references. .. admonition:: Rationale. Historically, it has been possible for a class to provide its own; reference-count implementation by overriding ``retain``, ``release``, etc.; However, weak references to an object require coordination with its class's; reference-count implementation because, among other things, weak loads and; stores must be atomic with respect to the final release. Therefore, existing; custom reference-count implementations will generally not support weak; references without additional effort. This is unavoidable without breaking; binary compatibility. A class may indicate that it does not support weak references by providing the; ``objc_arc_weak_reference_unavailable`` attribute on the class's interface declaration. A; retainable object pointer type is **weak-unavailable** if; is a pointer to an (optionally protocol-qualified) Objective-C class ``T`` where; ``T`` or one of its superclas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:41153,perform,perform,41153,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['perform']
Performance,", you must use the pointer to the; current **`TTree`** to call the method `GetEntry(entry)`. The; parameter `entry` is always the local entry number in the current; tree. Assuming that `fChain` is the pointer to the **`TChain`**; being processed, use. ``` {.cpp}; fChain->GetTree()->GetEntry(entry);; ```. To create a selector call:. ``` {.cpp}; root[] T->MakeSelector(""MySelector"");; ```. Where `T` is the **`TTree`** and `MySelector` is the name of created; class and the name of the `.h` and `.C` files. The resulting; **`TSelector`** is the argument to **`TTree::Process`**. The argument can; be the file name or a pointer to the selector object. ``` {.cpp}; root[] T->Process(""MySelector.C"","""",1000,100);; ```. This call will interpret the class defined in `MySelector.C` and process; 1000 entries beginning with entry 100. The file name can be appended; with a ""+"" or a ""++"" to use `ACLiC`. ``` {.cpp}; root[] T->Process(""MySelector.C++"","""",1000,100);; ```. When appending a ""++"", the class will be compiled and dynamically; loaded. ``` {.cpp}; root[] T->Process(""MySelector.C+"","""",1000,100);; ```. When appending a ""+"", the class will also be compiled and dynamically; loaded. When it is called again, it recompiles only if the macro; (`MySelector.C`) has changed since it was compiled last. If not, it; loads the existing library. The next example shows how to create a; selector with a pointer:. ``` {.cpp}; MySelector *selector = (MySelector *)TSelector::GetSelector(""MySelector.C+"");; T->Process(selector);; ```. `Using this form, you can do things like:`. ``` {.cpp}; selector->public_attribute1 = init_value;; for (int i=0; i<limit; i++) {; T->Process(selector);; selector->public_attribute1 =; function(selector->public_attribute2);; }; ```. `TTree::Process()` is aware of PROOF, ROOT parallel processing facility.; If PROOF is setup, it divides the processing amongst the slave CPUs. ### Performance Benchmarks; \index{benchmarks}. The program `$ROOTSYS/test/bench.cxx` compares the I/O",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:134231,load,loaded,134231,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loaded']
Performance,",...]>. With **kind** being one of the options in the following list. .. code-block:: text. =Base: Base type (integer, boolean, etc).; =Const: Constant specifier.; =Enumerator: Enumerator.; =Import: Import declaration.; =ImportDeclaration: Import declaration.; =ImportModule: Import module.; =Pointer: Pointer type.; =PointerMember: Pointer to member function.; =Reference: Reference type.; =Restrict: Restrict specifier.; =RvalueReference: R-value reference.; =Subrange: Array subrange.; =TemplateParam: Template parameter.; =TemplateTemplateParam: Template template parameter.; =TemplateTypeParam: Template type parameter.; =TemplateValueParam: Template value parameter.; =Typedef: Type definition.; =Unspecified: Unspecified type.; =Volatile: Volatile specifier. .. _compare_:. COMPARE; ~~~~~~~; When dealing with debug information, there are situations when the; printing of the elements is not the correct approach. That is the case,; when we are interested in the effects caused by different versions of; the same toolchain, or the impact of specific compiler optimizations. For those cases, we are looking to see which elements have been added; or removed. Due to the complicated debug information format, it is very; difficult to use a regular diff tool to find those elements; even; impossible when dealing with different debug formats. :program:`llvm-debuginfo-analyzer` supports a logical element comparison,; allowing to find semantic differences between logical views, produced by; different toolchain versions or even debug information formats. When comparing logical views created from different debug formats, its; accuracy depends on how close the debug information represents the; user code. For instance, a logical view created from a binary file with; DWARF debug information may include more detailed data than a logical; view created from a binary file with CodeView/COFF debug information. The following options describe the elements to compare. .. option:: --compare=<value[,v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst:19614,optimiz,optimizations,19614,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,1,['optimiz'],['optimizations']
Performance,",10);; TDecompSVD svd(m);; TVectorD sig = svd.GetSig(); sig.Sqr();; // Symmetric matrix EigenVector algorithm; TMatrixDSym mtm(TMatrixDBase::kAtA,m);; const TMatrixDSymEigen eigen(mtm);; const TVectorD eigenVal = eigen.GetEigenValues();; const Bool_t ok = VerifyVectorIdentity(sig,eigenVal,1,1.-e-14);; ```. ## Speed Comparisons. Speed of four matrix operations have been compared between four matrix; libraries, `GSL` `CLHEP`, `ROOT v3.10` and `ROOT v4.0`. Next figure; shows the `CPU` time for these four operations as a function of the; matrix size:. 1. `A*B` The execution time is measured for the sum of A \* Bsym,; Bsym\* A and A \* B. Notice the matrix\_size3 dependence of execution; time. `CLHEP` results are hampered by a poor implementation of symmetric; matrix multiplications. For instance, for general matrices of size; 100x100, the time is 0.015 sec. while A \* Bsym takes 0.028 sec and; Bsym\* A takes 0.059 sec. Both `GSL` and `ROOT v4.0` can be setup to use the hardware-optimized; multiplication routines of the `BLAS` libraries. It was tested on a G4; PowerPC. The improvement becomes clearly visible around sizes of (50x50); were the execution speed improvement of the Altivec processor becomes; more significant than the overhead of filling its pipe. 2. $A^{-1}$ Here, the time is measured for an in-place matrix inversion. Except for `ROOT v3.10`, the algorithms are all based on an; `LU `factorization followed by forward/back-substitution. `ROOT v3.10`; is using the slower Gaussian elimination method. The numerical accuracy; of the `CLHEP` routine is poor:. - up to 6x6 the numerical imprecise Cramer multiplication is hard-coded.; For instance, calculating `U=H*H-1`, where `H` is a (5x5) Hilbert; matrix, results in off-diagonal elements of $10^{-7}$ instead of the $10^{-13}$; using an `LU `according to `Crout`. - scaling protection is non-existent and limits are hard-coded, as a; consequence inversion of a Hilbert matrix for `sizes>(12x12)` fails. In; order to gain s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md:50288,optimiz,optimized,50288,documentation/users-guide/LinearAlgebra.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md,1,['optimiz'],['optimized']
Performance,",m,s one can now do. RooWorkspace w(""w"",true) ; // workspace with CINT interface activated; // ... fill workspace with RooGaussian gauss(x,m,s) ...; RooPlot* frame = w::x.frame() ;; w::gauss.plotOn(frame) ;. to access the workspace contents. Each reference has the correct type, e.g. w::gauss is; a RooGaussian&. If a workspace is deleted from memory, the corresponding CINT namespace; is removed as well. Note that this feature is strictly available in interpreted C++ only; A new tutorial macro has been added to illustrate this functionality in more detail: rf509_wsinteractive.C.; writeToFile -- A new utility method RooWorkspace::writeToFile() has been added; to simplify the process of saving a workspace to file; Named sets and parameter snapshots -- It is now possible to define and retrieve; named RooArgSets of objects that live in the workspace through methods; defineSet() and set(). While named sets merely group objects logically, methods loadSnapshot and; saveSnapshot allow to make copies of the values, errors and 'constant' status of; sets of variable objects that live in the workspace. A newly added tutorial macro rf510_namedsets.C illustrates the functionality of both; of these features.; Improved printing of contents -- Many operator p.d.f. and function components now show; a more intuitive natural representation of their contents (these changes are mostly in the; respective p.d.f.s, but are most relevant in the context of a workspace). New object factory interface to workspace to facilitate script driven model definition; A object factory has been added to RooFit to simplify the process of creating p.d.f.; and function expressions consisting of multiple objects. The factory has two goals:; the first is to provide a back-end for higher level factories and tools to process; the creation of objects. The second is to provide a simple end-user language to; populate a RooWorkspace with function and p.d.f. objects. For the latter purpose the object creation language ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html:17548,load,loadSnapshot,17548,roofit/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html,1,['load'],['loadSnapshot']
Performance,"- *Colors*: creates a new canvas showing the color palette. - *Markers*: creates a new canvas showing the various marker styles. - *Iconify*: create the canvas window icon, does not close the; canvas. - *View With...*: If the last selected pad contains a 3-d structure,; a new canvas is created with a 3-D picture according to the; selection made from the cascaded menu: X3D or OpenGL. The 3-D; image can be interactively rotated, zoomed in wire-frame, solid,; hidden line or stereo mode. ![](pictures/0300000C.png). #### Options Menu. - *Auto Resize Canvas*: turns auto-resize of the canvas *on/off*:. - *on* - the canvas fits to the window when changing the window; size;; - *off* - the canvas stays fixed when changing the window size. - *Resize Canvas*: resizes and fits the canvas to the window size. - *Move Opaque*: if selected, graphics objects are moved in opaque; mode; otherwise, only the outline of objects is drawn when moving; them. The option opaque produces the best effect but it requires a; reasonably fast workstation or response time. - *Resize Opaque*: if selected, graphics objects are resized in; opaque mode; otherwise, only the outline of objects is drawn when; resizing them. - *Interrupt*: interrupts the current drawing process. - *Refresh*: redraws the canvas contents. - *Pad Auto Exec*: executes the list of **`TExecs`** in the current; pad. - *Statistics*: toggles the display of the histogram statistics box. - *Histogram Title*: toggles the display of the histogram title. - *Fit Parameters*: toggles the display of the histogram or graph; fit parameters. - *Can Edit Histogram*: enables/disables the possibility to edit; histogram bin contents. ![](pictures/0300000D.png). #### Inspect Menu. - *ROOT*: inspects the top-level ***`gROOT`*** object (in a new; canvas). - *Start Browser*: starts a new object browser (in a separate; window). - *GUI Builder*: starts the GUI builder application (in a separate; window). ![](pictures/0300000E.png). #### Help Menu. - *Can",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md:10290,response time,response time,10290,documentation/users-guide/GettingStarted.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md,1,['response time'],['response time']
Performance,"- All elements of the vector have identical scalar elements. This; operation may also be known as a ""broadcast"" or ""duplicate"" in target assembly.; The shufflevector IR instruction may change the vector length, so this operation; may map to multiple SelectionDAG nodes including ``shuffle_vector``,; ``concat_vectors``, ``insert_subvector``, and ``extract_subvector``. Prior to the existence of the Legalize passes, we required that every target; `selector`_ supported and handled every operator and type even if they are not; natively supported. The introduction of the Legalize phases allows all of the; canonicalization patterns to be shared across targets, and makes it very easy to; optimize the canonicalized code because it is still in the form of a DAG. .. _optimizations:; .. _Optimize SelectionDAG:; .. _selector:. SelectionDAG Optimization Phase: the DAG Combiner; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The SelectionDAG optimization phase is run multiple times for code generation,; immediately after the DAG is built and once after each legalization. The first; run of the pass allows the initial code to be cleaned up (e.g. performing; optimizations that depend on knowing that the operators have restricted type; inputs). Subsequent runs of the pass clean up the messy code generated by the; Legalize passes, which allows Legalize to be very simple (it can focus on making; code legal instead of focusing on generating *good* and legal code). One important class of optimizations performed is optimizing inserted sign and; zero extension instructions. We currently use ad-hoc techniques, but could move; to more rigorous techniques in the future. Here are some good papers on the; subject:. ""`Widening integer arithmetic <http://www.eecs.harvard.edu/~nr/pubs/widen-abstract.html>`_"" :raw-html:`<br>`; Kevin Redwine and Norman Ramsey :raw-html:`<br>`; International Conference on Compiler Construction (CC) 2004. ""`Effective sign extension elimination <http://portal.acm.org/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:45245,optimiz,optimization,45245,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"- If OpenCL, omit lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - system - global 1. buffer_wbl2; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after any; preceding; global/generic; load/store/load; atom",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:258297,load,load,258297,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"- If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; req",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:325444,load,load,325444,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"- Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:347734,load,load,347734,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"- for 3D vectors implementing the `x()`, `y()` and `z()` methods. - for Lorentz vectors implementing the `x()`, `y()`, `z()` and `t()`; methods. ``` {.cpp}; CLHEP::Hep3Vector hv;; XYZVector v1(hv); //create 3D vector from; //CLHEP 3D Vector; HepGeom::Point3D hp;; XYZPoint p1(hp); //create a 3D p; ```. ## Linear Algebra: SMatrix Package. The ROOT Linear algebra package is documented in a separate chapter (see; ""Linear Algebra in ROOT""). `SMatrix` is a C++ package, for high; performance vector and matrix computations. It has been introduced in; ROOT v5.08. It is optimized for describing small matrices and vectors; and It can be used only in problems when the size of the matrices is; known at compile time, like in the tracking reconstruction of physics; experiments. It is based on a C++ technique, called expression; templates, to achieve an high level optimization. The C++ templates can; be used to implement vector and matrix expressions such that these; expressions can be transformed at compile time to code which is; equivalent to hand optimized code in a low-level language like FORTRAN; or C (see for example T. Veldhuizen, Expression Templates, C++ Report,; 1995). The `SMatrix` has been developed initially by T. Glebe in; Max-Planck-Institut, Heidelberg, as part of the `HeraB` analysis; framework. A subset of the original package has been now incorporated in; the ROOT distribution, with the aim to provide a stand-alone and high; performance matrix package. The API of the current package differs from; the original one, in order to be compliant to the ROOT coding; conventions. `SMatrix` contains the generic **`ROOT::Math::SMatrix`** and; **`ROOT::Math::SVector`** classes for describing matrices and vectors of; arbitrary dimensions and of arbitrary type. The classes are templated on; the scalar type and on the size, like number of rows and columns for a; matrix . Therefore, the matrix/vector dimension has to be known at; compile time. An advantage of using the dimension ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:99637,optimiz,optimized,99637,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['optimiz'],['optimized']
Performance,"- middle end --------------------┼------ backend ----┤; │ │ │ │; └--- parsing ---- sema -----┴--- optimizations --- IPO ---- optimizations---┴--- optimizations -┘. ┌-----------------------------------------------------------------------------------------------┐; │ │; │ source file │; │ │; └-----------------------------------------------------------------------------------------------┘; ┌---------------------------------------┐; │ │; │ │; │ imported code │; │ │; │ │; └---------------------------------------┘. It would be very unfortunate if we end up with worse performance after using modules.; The main concern is that when we compile a source file, the compiler needs to see the function body; of imported module units so that it can perform IPO (InterProcedural Optimization, primarily inlining; in practice) to optimize functions in current source file with the help of the information provided by; the imported module units.; In other words, the imported code would be processed again and again in importee units; by optimizations (including IPO itself).; The optimizations before IPO and the IPO itself are the most time-consuming part in whole compilation process.; So from this perspective, we might not be able to get the improvements described in the theory.; But we could still save the time for optimizations after IPO and the whole backend. Overall, at ``O0`` the implementations of functions defined in a module will not impact module users,; but at higher optimization levels the definitions of such functions are provided to user compilations for the; purposes of optimization (but definitions of these functions are still not included in the use's object file)-; this means the build speedup at higher optimization levels may be lower than expected given ``O0`` experience,; but does provide by more optimization opportunities. Interoperability with Clang Modules; -----------------------------------. We **wish** to support clang modules and standard c++ modules at the same t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:42601,optimiz,optimizations,42601,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['optimiz'],['optimizations']
Performance,"- wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/glo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:245757,load,load,245757,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"- wavefront - generic; store atomic monotonic - workgroup - global 1. buffer/global/flat_store; - generic sc0=1; store atomic monotonic - agent - global 1. buffer/global/flat_store; - generic sc1=1; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic sc1=1; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load sc0=1; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load sc0=1; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:295656,load,load,295656,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"-+; | 0x110000000000|0x200000000000 | origin 2 |; +---------------+---------------+--------------------+; | 0x100000000000|0x110000000000 | unused |; +---------------+---------------+--------------------+; | 0x010000000000|0x100000000000 | shadow 2 |; +---------------+---------------+--------------------+; | 0x000000000000|0x010000000000 | application 1 |; +---------------+---------------+--------------------+. Each byte of application memory corresponds to a single byte of shadow; memory, which is used to store its taint label. We map memory, shadow, and; origin regions to each other with these masks and offsets:. * shadow_addr = memory_addr ^ 0x500000000000. * origin_addr = shadow_addr + 0x100000000000. As for LLVM SSA registers, we have not found it necessary to associate a label; with each byte or bit of data, as some other tools do. Instead, labels are; associated directly with registers. Loads will result in a union of; all shadow labels corresponding to bytes loaded, and stores will; result in a copy of the label of the stored value to the shadow of all; bytes stored to. Propagating labels through arguments; ------------------------------------. In order to propagate labels through function arguments and return values,; DataFlowSanitizer changes the ABI of each function in the translation unit.; There are currently two supported ABIs:. * Args -- Argument and return value labels are passed through additional; arguments and by modifying the return type. * TLS -- Argument and return value labels are passed through TLS variables; ``__dfsan_arg_tls`` and ``__dfsan_retval_tls``. The main advantage of the TLS ABI is that it is more tolerant of ABI mismatches; (TLS storage is not shared with any other form of storage, whereas extra; arguments may be stored in registers which under the native ABI are not used; for parameter passing and thus could contain arbitrary values). On the other; hand the args ABI is more efficient and allows ABI mismatches to be more easily; i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst:9728,load,loaded,9728,interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizerDesign.rst,1,['load'],['loaded']
Performance,"--'. And the compilation process for module units are like:. .. code-block:: text. src1.cpp ----------------------------------------+> clang++ src1.cpp -------> src1.o -,; (header unit) hdr1.h -> clang++ hdr1.h ... -> hdr1.pcm --' +-> clang++ src1.o mod1.o src2.o -> executable; mod1.cppm -> clang++ mod1.cppm ... -> mod1.pcm --,--> clang++ mod1.pcm ... -> mod1.o -+; src2.cpp ----------------------------------------+> clang++ src2.cpp -------> src2.o -'. As the diagrams show, we need to compile the BMI from module units to object files and link the object files.; (But we can't do this for the BMI from header units. See the later section for the definition of header units). If we want to create a module library, we can't just ship the BMIs in an archive.; We must compile these BMIs(``*.pcm``) into object files(``*.o``) and add those object files to the archive instead. Consistency Requirement; ~~~~~~~~~~~~~~~~~~~~~~~. If we envision modules as a cache to speed up compilation, then - as with other caching techniques -; it is important to keep cache consistency.; So **currently** Clang will do very strict check for consistency. Options consistency; ^^^^^^^^^^^^^^^^^^^. The language option of module units and their non-module-unit users should be consistent.; The following example is not allowed:. .. code-block:: c++. // M.cppm; export module M;. // Use.cpp; import M;. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; $ clang++ -std=c++23 Use.cpp -fprebuilt-module-path=. The compiler would reject the example due to the inconsistent language options.; Not all options are language options.; For example, the following example is allowed:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 Use.cpp -fprebuilt-module-path=.; # Inconsistent debugging level.; $ clang++ -std=c++20 -g Use.cpp -fprebuilt-module-path=. Although the two examples have inconsistent optimization",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:15364,cache,cache,15364,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,2,['cache'],['cache']
Performance,"--+---------------------------------------+; | | `TMatrixDColumn(A,j1) *=` | multiply column $j2$ with column $j1$ |; | | `TMatrixDColumn const(A,j2)` | element wise |; +-----------------------+---------------------------------+---------------------------------------+; | multiply matrix slice | `TMatrixDDiag(A) *=` | multiply $B$ diagonal with $A$ |; | | `TMatrixDDiag const(B)` | diagonal element wise |; +-----------------------+---------------------------------+---------------------------------------+; | | `TMatrixDSub(A,i1,l1,j1,k1) *=` | multiply sub matrix of $A$ with |; | | `TMatrixDSub(B,i2,l2,j2,k2)` | sub matrix of $B$ |; +-----------------------+---------------------------------+---------------------------------------+; | | `TMatrixDSub(A,i,l,j,k) *= B` | multiply sub matrix of $A$ with |; | | | matrix of $B$ |; +-----------------------+---------------------------------+---------------------------------------+. In the current implementation of the matrix views, the user could; perform operations on a symmetric matrix that violate the symmetry. No; checking is done. For instance, the following code violates the; symmetry. ``` {.cpp}; TMatrixDSym A(5);; A.UnitMatrix();; TMatrixDRow(A,1)[0] = 1;; TMatrixDRow(A,1)[2] = 1;; ```. ### View Examples. Inserting row `i1 `into row` i2` of matrix $A$; can easily accomplished through:. ``` {.cpp}; TMatrixDRow(A,i1) = TMatrixDRow(A,i2); ```. Which more readable than:. ``` {.cpp}; const Int_t ncols = A.GetNcols();; Double_t *start = A.GetMatrixArray();; Double_t *rp1 = start+i*ncols;; const Double_t *rp2 = start+j*ncols;; while (rp1 < start+ncols) *rp1++ = *rp2++;; ```. Check that the columns of a Haar -matrix of order `order` are indeed; orthogonal:. ``` {.cpp}; const TMatrixD haar = THaarMatrixD(order);; TVectorD colj(1<<order);; TVectorD coll(1<<order);; for (Int_t j = haar.GetColLwb(); j <= haar.GetColUpb(); j++) {; colj = TMatrixDColumn_const(haar,j);; Assert(TMath::Abs(colj*colj-1.0) <= 1.0e-15);. for (Int_t l = j+1;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md:32073,perform,perform,32073,documentation/users-guide/LinearAlgebra.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md,1,['perform'],['perform']
Performance,"--- optimizations---┴--- optimizations -┘. ┌-----------------------------------------------------------------------------------------------┐; │ │; │ source file │; │ │; └-----------------------------------------------------------------------------------------------┘; ┌---------------------------------------┐; │ │; │ │; │ imported code │; │ │; │ │; └---------------------------------------┘. It would be very unfortunate if we end up with worse performance after using modules.; The main concern is that when we compile a source file, the compiler needs to see the function body; of imported module units so that it can perform IPO (InterProcedural Optimization, primarily inlining; in practice) to optimize functions in current source file with the help of the information provided by; the imported module units.; In other words, the imported code would be processed again and again in importee units; by optimizations (including IPO itself).; The optimizations before IPO and the IPO itself are the most time-consuming part in whole compilation process.; So from this perspective, we might not be able to get the improvements described in the theory.; But we could still save the time for optimizations after IPO and the whole backend. Overall, at ``O0`` the implementations of functions defined in a module will not impact module users,; but at higher optimization levels the definitions of such functions are provided to user compilations for the; purposes of optimization (but definitions of these functions are still not included in the use's object file)-; this means the build speedup at higher optimization levels may be lower than expected given ``O0`` experience,; but does provide by more optimization opportunities. Interoperability with Clang Modules; -----------------------------------. We **wish** to support clang modules and standard c++ modules at the same time,; but the mixed using form is not well used/tested yet. Please file new github issues as you find interoperability pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:42644,optimiz,optimizations,42644,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['optimiz'],['optimizations']
Performance,"--------------+. Neither approach is perfect, and choosing one boils down to choosing the lesser of two evils. The issue with lane ordering, it was decided, would have to change target-agnostic compiler passes and would result in a strange IR in which lane indices were reversed. It was decided that this was worse than the changes that would have to be made to support ``LD1``, so ``LD1`` was chosen as the canonical vector load instruction (and by inference, ``ST1`` for vector stores). Implementation; ==============. There are 3 parts to the implementation:. 1. Predicate ``LDR`` and ``STR`` instructions so that they are never allowed to be selected to generate vector loads and stores. The exception is one-lane vectors [1]_ - these by definition cannot have lane ordering problems so are fine to use ``LDR``/``STR``. 2. Create code generation patterns for bitconverts that create ``REV`` instructions. 3. Make sure appropriate bitconverts are created so that vector values get passed over call boundaries as 1-element vectors (which is the same as if they were loaded with ``LDR``). Bitconverts; -----------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type before storing, the round trip will currently be broken. Take for example this code sequence::. %0 = load <4 x i32> %x; %1 = bitcast <4 x i32> %0 to <2 x i64>; store <2 x i64> %1, <2 x i64>* %y. This would produce a code sequence such as that in the figure on the right. The mismatched ``LD1`` and ``ST1`` cause the stored data to differ from the loaded data. .. container:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:9733,load,loaded,9733,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loaded']
Performance,"-------------------------+. To configure LLVM, follow these steps:. #. Change directory into the object root directory:. .. code-block:: console. % cd OBJ_ROOT. #. Run the ``cmake``:. .. code-block:: console. % cmake -G ""Unix Makefiles"" -DCMAKE_BUILD_TYPE=<type> -DCMAKE_INSTALL_PREFIX=/install/path; [other options] SRC_ROOT. Compiling the LLVM Suite Source Code; ------------------------------------. Unlike with autotools, with CMake your build type is defined at configuration.; If you want to change your build type, you can re-run cmake with the following; invocation:. .. code-block:: console. % cmake -G ""Unix Makefiles"" -DCMAKE_BUILD_TYPE=<type> SRC_ROOT. Between runs, CMake preserves the values set for all options. CMake has the; following build types defined:. Debug. These builds are the default. The build system will compile the tools and; libraries unoptimized, with debugging information, and asserts enabled. Release. For these builds, the build system will compile the tools and libraries; with optimizations enabled and not generate debug info. CMakes default; optimization level is -O3. This can be configured by setting the; ``CMAKE_CXX_FLAGS_RELEASE`` variable on the CMake command line. RelWithDebInfo. These builds are useful when debugging. They generate optimized binaries with; debug information. CMakes default optimization level is -O2. This can be; configured by setting the ``CMAKE_CXX_FLAGS_RELWITHDEBINFO`` variable on the; CMake command line. Once you have LLVM configured, you can build it by entering the *OBJ_ROOT*; directory and issuing the following command:. .. code-block:: console. % make. If the build fails, please `check here`_ to see if you are using a version of; GCC that is known not to compile LLVM. If you have multiple processors in your machine, you may wish to use some of the; parallel build options provided by GNU Make. For example, you could use the; command:. .. code-block:: console. % make -j2. There are several special targets which are",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:29110,optimiz,optimizations,29110,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimizations']
Performance,"---------------------------------------------------------------------+; | .llvm.offloading | Embedded device object file for the target device and architecture |; +----------------------------------+------------------------------------------------------------------------------+. .. _Device Linking:. Linking Target Device Code; --------------------------. Objects containing :ref:`table-offloading_sections` require special handling to; create an executable device image. This is done using a Clang tool, see; :doc:`ClangLinkerWrapper` for more information. This tool works as a wrapper; over the host linking job. It scans the input object files for the offloading; section ``.llvm.offloading``. The device files stored in this section are then; extracted and passed to the appropriate linking job. The linked device image is; then :ref:`wrapped <Device Binary Wrapping>` to create the symbols used to load; the device image and link it with the host. The linker wrapper tool supports linking bitcode files through link time; optimization (LTO). This is used whenever the object files embedded in the host; contain LLVM bitcode. Bitcode will be embedded for architectures that do not; support a relocatable object format, such as AMDGPU or SPIR-V, or if the user; requested it using the ``-foffload-lto`` flag. .. _Device Binary Wrapping:. Device Binary Wrapping; ----------------------. Various structures and functions are used to create the information necessary to; offload code on the device. We use the :ref:`linked device executable <Device; Linking>` with the corresponding offloading entries to create the symbols; necessary to load and execute the device image. Structure Types; ^^^^^^^^^^^^^^^. Several different structures are used to store offloading information. The; :ref:`device image structure <table-device_image_structure>` stores a single; linked device image and its associated offloading entries. The offloading; entries are stored using the ``__start_omp_offloading_entries``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:14265,optimiz,optimization,14265,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['optimiz'],['optimization']
Performance,"---------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omit dlc=1. store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:346104,load,load,346104,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"---------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/ato",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:224658,load,load,224658,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"---------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - workgroup - generic; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214264,load,load,214264,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3599,concurren,concurrent,3599,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['concurren'],['concurrent']
Performance,"-------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's together. The issue with the addition only occurs in 64-bit mode, and appears to be at; least partially caused by Scalar Evolution not keeping its cache updated: it; returns the ""wrong"" result immediately after indvars runs, but figures out the; expected result if it is run from scratch on IR resulting from running indvars. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:59366,cache,cache,59366,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['cache'],['cache']
Performance,"------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------==",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:22623,optimiz,optimized,22623,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimized']
Performance,"-----------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to local have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:219949,load,load,219949,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"--------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5397,load,load,5397,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,1,['load'],['load']
Performance,"----------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe poin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2240,load,load,2240,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['load'],['load']
Performance,"--------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single imm",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8629,perform,performance,8629,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['perform'],['performance']
Performance,"-------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM whe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19910,optimiz,optimized,19910,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['optimiz'],['optimized']
Performance,"------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a bit like this:. int foo(char *P, char *Q, int x, int y) {; if (P[0] != Q[0]); return P[0] < Q[0];; if (P[1] != Q[1]); return P[1] < Q[1];; if (P[2] != Q[2]); return P[2] < Q[2];; return P[3] < Q[3];; }. In the real code, we get a lot more wrong than this. However, even in this; code we generate:. _foo: ## @foo; ## %bb.0: ## %entry; 	movb	(%rsi), %al; 	movb	(%rdi), %cl; 	cmpb	%al, %cl; 	je	LBB0_2; LBB0_1: ## %if.then; 	cmpb	%al, %cl; 	jmp	LBB0_5; LBB0_2: ## %if.end; 	movb	1(%rsi), %al; 	movb	1(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.3: ## %if.end38; 	movb	2(%rsi), %al; 	movb	2(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.4: ## %if.end60; 	movb	3(%rdi), %al; 	cmpb	3(%rsi), %al; LBB0_5: ## %if.end60; 	setl	%al; 	movzbl	%al, %eax; 	ret. Note that we generate jumps to LBB0_1 which does a redundant compare. The; redundant compare also forces the register values to be live, which prevents; folding one of the loads into the compare. In contrast, GCC 4.2 produces:. _foo:; 	movzbl	(%rsi), %eax; 	cmpb	%al, (%rdi); 	jne	L10; L12:; 	movzbl	1(%rsi), %eax; 	cmpb	%al, 1(%rdi); 	jne	L10; 	movzbl	2(%rsi), %eax; 	cmpb	%al, 2(%rdi); 	jne	L10; 	movzbl	3(%rdi), %eax; 	cmpb	3(%rsi), %al; L10:; 	setl	%al; 	movzbl	%al, %eax; 	ret. which is ""perfect"". //===---------------------------------------------------------------------===//. For the branch in the following code:; int a();; int b(int x, int y) {; if (x & (1<<(y&7))); return a();; return y;; }. We currently generate:; 	movb	%sil, %al; 	andb	$7, %al; 	movzbl	%al, %eax; 	btl	%eax, %edi; 	jae	.LBB0_2. movl+andl would be shorter than the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; return u.x + u.y;; }. We currently generate:; 	movdqa	%xmm0, %xmm1; 	pshufd	$1, %xmm0, %xmm0 # xmm0 = ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:40696,load,loads,40696,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['load'],['loads']
Performance,"---------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33479,load,loadpre,33479,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['load'],['loadpre']
Performance,"--------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:355722,perform,performing,355722,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"-------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, lr}; add r7, sp, #8; mov r4, r0; mov r5, r1; fldd d0, LCPI1_0; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:3517,load,load,3517,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['load'],['load']
Performance,"------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:17998,optimiz,optimization,17998,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,3,['optimiz'],"['optimization', 'optimized']"
Performance,"------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36486,load,loading,36486,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['load'],['loading']
Performance,"------------------------------------------===//. Given the following C code:; int f(int a, int b) { return (signed char)a == (signed char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %sext = shl i32 %a, 24 ; <i32> [#uses=1]; %conv1 = ashr i32 %sext, 24 ; <i32> [#uses=1]; %sext6 = shl i32 %b, 24 ; <i32> [#uses=1]; %conv4 = ashr i32 %sext6, 24 ; <i32> [#uses=1]; %cmp = icmp eq i32 %conv1, %conv4 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to dete",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:37476,load,load,37476,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['load'],['load']
Performance,"-----------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify is currently a greedy algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===--------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:5184,load,loads,5184,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,1,['load'],['loads']
Performance,"----------------------------------------. Some collectors need to be informed when the mutator (the program that needs; garbage collection) either reads a pointer from or writes a pointer to a field; of a heap object. The code fragments inserted at these points are called *read; barriers* and *write barriers*, respectively. The amount of code that needs to; be executed is usually quite small and not on the critical path of any; computation, so the overall performance impact of the barrier is tolerable. Barriers often require access to the *object pointer* rather than the *derived; pointer* (which is a pointer to the field within the object). Accordingly,; these intrinsics take both pointers as separate arguments for completeness. In; this snippet, ``%object`` is the object pointer, and ``%derived`` is the derived; pointer:. .. code-block:: llvm. ;; An array type.; %class.Array = type { %class.Object, i32, [0 x %class.Object*] }; ... ;; Load the object pointer from a gcroot.; %object = load %class.Array** %object_addr. ;; Compute the derived pointer.; %derived = getelementptr %object, i32 0, i32 2, i32 %n. LLVM does not enforce this relationship between the object and derived pointer; (although a particular :ref:`collector strategy <plugin>` might). However, it; would be an unusual collector that violated it. The use of these intrinsics is naturally optional if the target GC does not; require the corresponding barrier. The GC strategy used with such a collector; should replace the intrinsic calls with the corresponding ``load`` or; ``store`` instruction if they are used. One known deficiency with the current design is that the barrier intrinsics do; not include the size or alignment of the underlying operation performed. It is; currently assumed that the operation is of pointer size and the alignment is; assumed to be the target machine's default alignment. Write barrier: ``llvm.gcwrite``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. void @llvm.gcwrite(i8* ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:13744,load,load,13744,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['load']
Performance,"--------------------------------------. Clang can be provided with inputs written in non-C-family languages. In such; cases, an external tool will be used to compile the input. The; currently-supported languages are:. * Ada (``-x ada``, ``.ad[bs]``); * Fortran (``-x f95``, ``.f``, ``.f9[05]``, ``.for``, ``.fpp``, case-insensitive); * Java (``-x java``). In each case, GCC will be invoked to compile the input. Assembler; ---------. Clang can either use LLVM's integrated assembler or an external system-specific; tool (for instance, the GNU Assembler on GNU OSes) to produce machine code from; assembly.; By default, Clang uses LLVM's integrated assembler on all targets where it is; supported. If you wish to use the system assembler instead, use the; ``-fno-integrated-as`` option. Linker; ------. Clang can be configured to use one of several different linkers:. * GNU ld; * GNU gold; * LLVM's `lld <https://lld.llvm.org>`_; * MSVC's link.exe. Link-time optimization is natively supported by lld, and supported via; a `linker plugin <https://llvm.org/docs/GoldPlugin.html>`_ when using gold. The default linker varies between targets, and can be overridden via the; ``-fuse-ld=<linker name>`` flag. Runtime libraries; =================. A number of different runtime libraries are required to provide different; layers of support for C family programs. Clang will implicitly link an; appropriate implementation of each runtime library, selected based on; target defaults or explicitly selected by the ``--rtlib=`` and ``--stdlib=``; flags. The set of implicitly-linked libraries depend on the language mode. As a; consequence, you should use ``clang++`` when linking C++ programs in order; to ensure the C++ runtimes are provided. .. note::. There may exist other implementations for these components not described; below. Please let us know how well those other implementations work with; Clang so they can be added to this list!. .. FIXME: Describe Objective-C runtime libraries; .. FIXME: Des",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Toolchain.rst:4441,optimiz,optimization,4441,interpreter/llvm-project/clang/docs/Toolchain.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Toolchain.rst,1,['optimiz'],['optimization']
Performance,"--------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768))); Line[i++] = Stack[--StackPtr];; if (LineLen > 32768); {; while (S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9163,load,load,9163,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['load'],['load']
Performance,"---------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:246132,load,load,246132,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"-------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2259,load,load,2259,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['load'],['load']
Performance,"------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:11435,load,load,11435,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['load'],['load']
Performance,"----------------------------; .. _parallelism:. By default, the ThinLTO link step will launch as many; threads in parallel as there are cores. If the number of; cores can't be computed for the architecture, then it will launch; ``std::thread::hardware_concurrency`` number of threads in parallel.; For machines with hyper-threading, this is the total number of; virtual cores. For some applications and machine configurations this; may be too aggressive, in which case the amount of parallelism can; be reduced to ``N`` via:. - gold:; ``-Wl,-plugin-opt,jobs=N``; - ld64:; ``-Wl,-mllvm,-threads=N``; - ld.lld, ld64.lld:; ``-Wl,--thinlto-jobs=N``; - lld-link:; ``/opt:lldltojobs=N``. Other possible values for ``N`` are:. - 0:; Use one thread per physical core (default); - 1:; Use a single thread only (disable multi-threading); - all:; Use one thread per logical core (uses all hyper-threads). Incremental; -----------; .. _incremental:. ThinLTO supports fast incremental builds through the use of a cache,; which currently must be enabled through a linker option. - gold (as of LLVM 4.0):; ``-Wl,-plugin-opt,cache-dir=/path/to/cache``; - ld64 (supported since clang 3.9 and Xcode 8) and Mach-O ld64.lld (as of LLVM; 15.0):; ``-Wl,-cache_path_lto,/path/to/cache``; - ELF ld.lld (as of LLVM 5.0):; ``-Wl,--thinlto-cache-dir=/path/to/cache``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocache:/path/to/cache``. Cache Pruning; -------------. To help keep the size of the cache under control, ThinLTO supports cache; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:4634,cache,cache,4634,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache']
Performance,"---------------------------. This builtin function stores a value in a WebAssembly table.; It takes three arguments.; The first argument is the table to store a value into, the second; argument is the index to which to store the value into, and the; third argument is a value of reference type to store in the table.; It returns nothing. .. code-block:: c++. static __externref_t table[0];; extern __externref_t JSObj;. void store(int index) {; __builtin_wasm_table_set(table, index, JSObj);; }. ``__builtin_wasm_table_get``; ----------------------------. This builtin function is the counterpart to ``__builtin_wasm_table_set``; and loads a value from a WebAssembly table of reference typed values.; It takes 2 arguments.; The first argument is a table of reference typed values and the; second argument is an index from which to load the value. It returns; the loaded reference typed value. .. code-block:: c++. static __externref_t table[0];. __externref_t load(int index) {; __externref_t Obj = __builtin_wasm_table_get(table, index);; return Obj;; }. ``__builtin_wasm_table_size``; -----------------------------. This builtin function returns the size of the WebAssembly table.; Takes the table as an argument and returns an unsigned integer (``size_t``); with the current table size. .. code-block:: c++. typedef void (*__funcref funcref_t)();; static __funcref table[0];. size_t getSize() {; return __builtin_wasm_table_size(table);; }. ``__builtin_wasm_table_grow``; -----------------------------. This builtin function grows the WebAssembly table by a certain amount.; Currently, as all WebAssembly tables created in C/C++ are zero-sized,; this always needs to be called to grow the table. It takes three arguments. The first argument is the WebAssembly table; to grow. The second argument is the reference typed value to store in; the new table entries (the initialization value), and the third argument; is the amount to grow the table by. It returns the previous table size; or -1. It wil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:92991,load,load,92991,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['load'],['load']
Performance,"--------------------------. For most target platforms, half precision floating-point is a; storage-only format. This means that it is a dense encoding (in memory); but does not support computation in the format. This means that code must first load the half-precision floating-point; value as an i16, then convert it to float with; :ref:`llvm.convert.from.fp16 <int_convert_from_fp16>`. Computation can; then be performed on the float value (including extending to double; etc). To store the value back to memory, it is first converted to float; if needed, then converted to i16 with; :ref:`llvm.convert.to.fp16 <int_convert_to_fp16>`, then storing as an; i16 value. .. _int_convert_to_fp16:. '``llvm.convert.to.fp16``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i16 @llvm.convert.to.fp16.f32(float %a); declare i16 @llvm.convert.to.fp16.f64(double %a). Overview:; """""""""""""""""". The '``llvm.convert.to.fp16``' intrinsic function performs a conversion from a; conventional floating-point type to half precision floating-point format. Arguments:; """""""""""""""""""". The intrinsic function contains single argument - the value to be; converted. Semantics:; """""""""""""""""""". The '``llvm.convert.to.fp16``' intrinsic function performs a conversion from a; conventional floating-point format to half precision floating-point format. The; return value is an ``i16`` which contains the converted number. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call i16 @llvm.convert.to.fp16.f32(float %a); store i16 %res, i16* @x, align 2. .. _int_convert_from_fp16:. '``llvm.convert.from.fp16``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare float @llvm.convert.from.fp16.f32(i16 %a); declare double @llvm.convert.from.fp16.f64(i16 %a). Overview:; """""""""""""""""". The '``llvm.convert.from.fp16``' intrinsic function performs a; conversion from half precision floating-point format to single precision; floating-point format. Arguments:; """""""""""""""""""". The intrinsi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:681744,perform,performs,681744,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"---------------------. If you encounter a bug that leads to crashes in the LLVM LTO phase when using; the ``-flto`` option, follow these steps to diagnose and report the issue:. Compile your source file to a ``.bc`` (Bitcode) file with the following options,; in addition to your existing compilation options:. .. code-block:: bash. export CFLAGS=""-flto -fuse-ld=lld"" CXXFLAGS=""-flto -fuse-ld=lld"" LDFLAGS=""-Wl,-plugin-opt=save-temps"". These options enable LTO and save temporary files generated during compilation; for later analysis. On Windows, you should be using lld-link as the linker. Adjust your compilation ; flags as follows:; * Add ``/lldsavetemps`` to the linker flags.; * When linking from the compiler driver, add ``/link /lldsavetemps`` in order to forward that flag to the linker. Using the specified flags will generate four intermediate bytecode files:. #. a.out.0.0.preopt.bc (Before any link-time optimizations (LTO) are applied); #. a.out.0.2.internalize.bc (After initial optimizations are applied); #. a.out.0.4.opt.bc (After an extensive set of optimizations); #. a.out.0.5.precodegen.bc (After LTO but before translating into machine code). Execute one of the following commands to identify the source of the problem:. #. ``opt ""-passes=lto<O3>"" a.out.0.2.internalize.bc``; #. ``llc a.out.0.5.precodegen.bc``. If one of these do crash, you should be able to reduce; this with :program:`llvm-reduce`; command line (use the bc file corresponding to the command above that failed):. .. code-block:: bash. llvm-reduce --test reduce.sh a.out.0.2.internalize.bc. Example of reduce.sh script. .. code-block:: bash. $ cat reduce.sh; #!/bin/bash -e. path/to/not --crash path/to/opt ""-passes=lto<O3>"" $1 -o temp.bc 2> err.log; grep -q ""It->second == &Insn"" err.log. Here we have grepped the failed assert message. Please run this, then file a bug with the instructions and reduced .bc file; that llvm-reduce emits. .. _miscompiling:. Miscompilations; ===============. If clang successf",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst:6988,optimiz,optimizations,6988,interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,1,['optimiz'],['optimizations']
Performance,"--------------------. Once the release process starts, the Release Manager will ask for volunteers,; and it'll be the role of each volunteer to:. * Test and benchmark the previous release. * Test and benchmark each release candidate, comparing to the previous release; and candidates. * Identify, reduce and report every regression found during tests and benchmarks. * Make sure the critical bugs get fixed and merged to the next release candidate. Not all bugs or regressions are show-stoppers and it's a bit of a grey area what; should be fixed before the next candidate and what can wait until the next; release. It'll depend on:. * The severity of the bug, how many people it affects and if it's a regression; or a known bug. Known bugs are ""unsupported features"" and some bugs can be; disabled if they have been implemented recently. * The stage in the release. Less critical bugs should be considered to be; fixed between RC1 and RC2, but not so much at the end of it. * If it's a correctness or a performance regression. Performance regression; tends to be taken more lightly than correctness. .. _scripts:. Scripts; =======. The scripts are in the ``utils/release`` directory. test-release.sh; ---------------. This script will check-out, configure and compile LLVM+Clang (+ most add-ons,; like ``compiler-rt``, ``libcxx``, ``libomp`` and ``clang-extra-tools``) in; three stages, and will test the final stage.; It'll have installed the final binaries on the Phase3/Releasei(+Asserts); directory, and that's the one you should use for the test-suite and other; external tests. To run the script on a specific release candidate run::. ./test-release.sh \; -release 3.3 \; -rc 1 \; -no-64bit \; -test-asserts \; -no-compare-files. Each system will require different options. For instance, x86_64 will; obviously not need ``-no-64bit`` while 32-bit systems will, or the script will; fail. The important flags to get right are:. * On the pre-release, you should change ``-rc 1`` to ``-final``. On ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ReleaseProcess.rst:1424,perform,performance,1424,interpreter/llvm-project/llvm/docs/ReleaseProcess.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ReleaseProcess.rst,1,['perform'],['performance']
Performance,"-----------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0) bar();; else if (x == 1) qux();; }. currently compiles into:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	testl	%eax, %eax; 	jne	LBB0_4. the testl could be removed:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	jb	LBB0_4. 0 is the only unsigned number < 1. //===---------------------------------------------------------------------===//. This code:. %0 = type { i32, i1 }. define i32 @add32carry(i32 %sum, i32 %x) nounwind readnone ssp {; entry:; %uadd = tail call %0 @llvm.uadd.with.overflow.i32(i32 %sum, i32 %x); %cmp = extractvalue",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:38233,load,loads,38233,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['load'],['loads']
Performance,"----------------. The most basic form of input mutation is to use the built in mutators of; LibFuzzer. These simply treat the input corpus as a bag of bits and make random; mutations. This type of fuzzer is good for stressing the surface layers of a; program, and is good at testing things like lexers, parsers, or binary; protocols. Some of the in-tree fuzzers that use this type of mutator are `clang-fuzzer`_,; `clang-format-fuzzer`_, `llvm-as-fuzzer`_, `llvm-dwarfdump-fuzzer`_,; `llvm-mc-assemble-fuzzer`_, and `llvm-mc-disassemble-fuzzer`_. .. _fuzzing-llvm-protobuf:. Structured Fuzzing using ``libprotobuf-mutator``; ------------------------------------------------. We can use libprotobuf-mutator_ in order to perform structured fuzzing and; stress deeper layers of programs. This works by defining a protobuf class that; translates arbitrary data into structurally interesting input. Specifically, we; use this to work with a subset of the C++ language and perform mutations that; produce valid C++ programs in order to exercise parts of clang that are more; interesting than parser error handling. To build this kind of fuzzer you need `protobuf`_ and its dependencies; installed, and you need to specify some extra flags when configuring the build; with :doc:`CMake <CMake>`. For example, `clang-proto-fuzzer`_ can be enabled by; adding ``-DCLANG_ENABLE_PROTO_FUZZER=ON`` to the flags described in; :ref:`building-fuzzers`. The only in-tree fuzzer that uses ``libprotobuf-mutator`` today is; `clang-proto-fuzzer`_. .. _libprotobuf-mutator: https://github.com/google/libprotobuf-mutator; .. _protobuf: https://github.com/google/protobuf. .. _fuzzing-llvm-ir:. Structured Fuzzing of LLVM IR; -----------------------------. We also use a more direct form of structured fuzzing for fuzzers that take; :doc:`LLVM IR <LangRef>` as input. This is achieved through the ``FuzzMutate``; library, which was `discussed at EuroLLVM 2017`_. The ``FuzzMutate`` library is used to structurally fuzz backen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst:6750,perform,perform,6750,interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,1,['perform'],['perform']
Performance,"----------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===---------------------------------------------------------------------===//. memcpyopt should turn this:. define i8* @test10(i32 %x) {; %alloc = call noalias i8* @malloc(i32 %x) nounwind; call void @llvm.memset.p0i8.i32(i8* %alloc, i8 0, i32 %x, i32 1, i1 false); ret i8* %alloc; }. into a call to calloc. We should make sure that we analyze calloc as; aggressively as malloc though. //===---------------------------------------------------------------------===//. clang -O3 doesn't optimize this:. void f1(int* begin, int* end) {; std::fill(begin, end, 0);; }. into a memset. This is PR8942. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(int N) {; std::vector<int> v(N);. extern void sink(void*); sink(&v);; }. into. define void @_Z1fi(i32 %N) nounwind {; entry:; %v2 = alloca [3 x i32*], align 8; %v2.sub = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 0; %tmpcast = bitcast [3 x i32*]* %v2 to %""class.std::vector""*; %conv = sext i32 %N to i64; store i32* null, i32** %v2.sub, align 8, !tbaa !0; %tmp3.i.i.i.i.i = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 1; store i32* null, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0; %tmp4.i.i.i.i.i = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 2; store i32* null, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0; %cmp.i.i.i.i = icmp eq i32 %N, 0; br i1 %cmp.i.i.i.i, label %_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.thread.i.i, la",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:55725,optimiz,optimize,55725,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimize']
Performance,"---------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - workgroup - generic; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214093,load,load,214093,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"--------------. The role of debug information is to provide meta information normally stripped; away during the compilation process. This meta information provides an LLVM; user a relationship between generated code and the original program source; code. Currently, there are two backend consumers of debug info: DwarfDebug and; CodeViewDebug. DwarfDebug produces DWARF suitable for use with GDB, LLDB, and; other DWARF-based debuggers. :ref:`CodeViewDebug <codeview>` produces CodeView,; the Microsoft debug info format, which is usable with Microsoft debuggers such; as Visual Studio and WinDBG. LLVM's debug information format is mostly derived; from and inspired by DWARF, but it is feasible to translate into other target; debug info formats such as STABS. It would also be reasonable to use debug information to feed profiling tools; for analysis of generated code, or, tools for reconstructing the original; source from generated code. .. _intro_debugopt:. Debug information and optimizations; -----------------------------------. An extremely high priority of LLVM debugging information is to make it interact; well with optimizations and analysis. In particular, the LLVM debug; information provides the following guarantees:. * LLVM debug information **always provides information to accurately read; the source-level state of the program**, regardless of which LLVM; optimizations have been run. :doc:`HowToUpdateDebugInfo` specifies how debug; info should be updated in various kinds of code transformations to avoid; breaking this guarantee, and how to preserve as much useful debug info as; possible. Note that some optimizations may impact the ability to modify the; current state of the program with a debugger, such as setting program; variables, or calling functions that have been deleted. * As desired, LLVM optimizations can be upgraded to be aware of debugging; information, allowing them to update the debugging information as they; perform aggressive optimizations. This means",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:3619,optimiz,optimizations,3619,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizations']
Performance,"--------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:10787,cache,cache,10787,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['cache'],"['cache', 'cacheline']"
Performance,"--------------┐; | │; | source file │; | │; └---------------------------------------------------------------------------------------┘. ┌--------┐; │ │; │imported│; │ │; │ code │; │ │; └--------┘. Here we can see that the source file (could be a non-module unit or a module unit) would get processed by the; whole pipeline.; But the imported code would only get involved in semantic analysis, which is mainly about name lookup,; overload resolution and template instantiation.; All of these processes are fast relative to the whole compilation process.; More importantly, the imported code only needs to be processed once in frontend code generation,; as well as the whole middle end and backend.; So we could get a big win for the compilation time in O0. But with optimizations, things are different:. (we omit ``code generation`` part for each end due to the limited space). .. code-block:: none. ├-------- frontend ---------┼--------------- middle end --------------------┼------ backend ----┤; │ │ │ │; └--- parsing ---- sema -----┴--- optimizations --- IPO ---- optimizations---┴--- optimizations -┘. ┌-----------------------------------------------------------------------------------------------┐; │ │; │ source file │; │ │; └-----------------------------------------------------------------------------------------------┘; ┌---------------------------------------┐; │ │; │ │; │ imported code │; │ │; │ │; └---------------------------------------┘. It would be very unfortunate if we end up with worse performance after using modules.; The main concern is that when we compile a source file, the compiler needs to see the function body; of imported module units so that it can perform IPO (InterProcedural Optimization, primarily inlining; in practice) to optimize functions in current source file with the help of the information provided by; the imported module units.; In other words, the imported code would be processed again and again in importee units; by optimizations (including IPO its",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:41671,optimiz,optimizations,41671,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,3,['optimiz'],['optimizations']
Performance,"------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===-------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1272,load,load,1272,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['load'],['load']
Performance,"-----------. Unary operators require a single operand, execute an operation on; it, and produce a single value. The operand might represent multiple; data, as is the case with the :ref:`vector <t_vector>` data type. The; result value has the same type as its operand. .. _i_fneg:. '``fneg``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = fneg [fast-math flags]* <ty> <op1> ; yields ty:result. Overview:; """""""""""""""""". The '``fneg``' instruction returns the negation of its operand. Arguments:; """""""""""""""""""". The argument to the '``fneg``' instruction must be a; :ref:`floating-point <t_floating>` or :ref:`vector <t_vector>` of; floating-point values. Semantics:; """""""""""""""""""". The value produced is a copy of the operand with its sign bit flipped.; The value is otherwise completely identical; in particular, if the input is a; NaN, then the quiet/signaling bit and payload are perfectly preserved. This instruction can also take any number of :ref:`fast-math; flags <fastmath>`, which are optimization hints to enable otherwise; unsafe floating-point optimizations:. Example:; """""""""""""""". .. code-block:: text. <result> = fneg float %val ; yields float:result = -%var. .. _binaryops:. Binary Operations; -----------------. Binary operators are used to do most of the computation in a program.; They require two operands of the same type, execute an operation on; them, and produce a single value. The operands might represent multiple; data, as is the case with the :ref:`vector <t_vector>` data type. The; result value has the same type as its operands. There are several different binary operators:. .. _i_add:. '``add``' Instruction; ^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = add <ty> <op1>, <op2> ; yields ty:result; <result> = add nuw <ty> <op1>, <op2> ; yields ty:result; <result> = add nsw <ty> <op1>, <op2> ; yields ty:result; <result> = add nuw nsw <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``add``' instruction returns the sum of its two",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:376499,optimiz,optimization,376499,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"----------. A |generic fuzzer| for the Itanium demangler used in various LLVM tools. We've; fuzzed __cxa_demangle to death, why not fuzz LLVM's implementation of the same; function!. llvm-isel-fuzzer; ----------------. A |LLVM IR fuzzer| aimed at finding bugs in instruction selection. This fuzzer accepts flags after `ignore_remaining_args=1`. The flags match; those of :doc:`llc <CommandGuide/llc>` and the triple is required. For example,; the following command would fuzz AArch64 with :doc:`GlobalISel/index`:. .. code-block:: shell. % bin/llvm-isel-fuzzer <corpus-dir> -ignore_remaining_args=1 -mtriple aarch64 -global-isel -O0. Some flags can also be specified in the binary name itself in order to support; OSS Fuzz, which has trouble with required arguments. To do this, you can copy; or move ``llvm-isel-fuzzer`` to ``llvm-isel-fuzzer--x-y-z``, separating options; from the binary name using ""--"". The valid options are architecture names; (``aarch64``, ``x86_64``), optimization levels (``O0``, ``O2``), or specific; keywords, like ``gisel`` for enabling global instruction selection. In this; mode, the same example could be run like so:. .. code-block:: shell. % bin/llvm-isel-fuzzer--aarch64-O0-gisel <corpus-dir>. llvm-opt-fuzzer; ---------------. A |LLVM IR fuzzer| aimed at finding bugs in optimization passes. It receives optimization pipeline and runs it for each fuzzer input. Interface of this fuzzer almost directly mirrors ``llvm-isel-fuzzer``. Both; ``mtriple`` and ``passes`` arguments are required. Passes are specified in a; format suitable for the new pass manager. You can find some documentation about; this format in the doxygen for ``PassBuilder::parsePassPipeline``. .. code-block:: shell. % bin/llvm-opt-fuzzer <corpus-dir> -ignore_remaining_args=1 -mtriple x86_64 -passes instcombine. Similarly to the ``llvm-isel-fuzzer`` arguments in some predefined configurations; might be embedded directly into the binary file name:. .. code-block:: shell. % bin/llvm-opt-fuzze",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst:3056,optimiz,optimization,3056,interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,1,['optimiz'],['optimization']
Performance,"----------. If, in the formal computation history of the program, an object ``X``; has been deallocated by the time of an observable side-effect, then; ARC must cause ``X`` to be deallocated by no later than the occurrence; of that side-effect, except as influenced by the re-ordering of the; destruction of objects. .. admonition:: Rationale. This rule is intended to prohibit ARC from observably extending the; lifetime of a retainable object, other than as specified in this; document. Together with the rule limiting the transformation of; releases, this rule requires ARC to eliminate retains and release; only in pairs. ARC's power to reorder the destruction of objects is critical to its; ability to do any optimization, for essentially the same reason that; it must retain the power to decrease the lifetime of an object.; Unfortunately, while it's generally poor style for the destruction; of objects to have arbitrary side-effects, it's certainly possible.; Hence the caveat. .. _arc.optimization.precise:. Precise lifetime semantics; --------------------------. In general, ARC maintains an invariant that a retainable object pointer held in; a ``__strong`` object will be retained for the full formal lifetime of the; object. Objects subject to this invariant have :arc-term:`precise lifetime; semantics`. By default, local variables of automatic storage duration do not have precise; lifetime semantics. Such objects are simply strong references which hold; values of retainable object pointer type, and these values are still fully; subject to the optimizations on values under local control. .. admonition:: Rationale. Applying these precise-lifetime semantics strictly would be prohibitive.; Many useful optimizations that might theoretically decrease the lifetime of; an object would be rendered impossible. Essentially, it promises too much. A local variable of retainable object owner type and automatic storage duration; may be annotated with the ``objc_precise_lifetime`` attribut",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:83470,optimiz,optimization,83470,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"----------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should mak",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42395,load,load,42395,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['load'],['load']
Performance,"---------. If :program:`FileCheck` verifies that the file matches the expected contents,; it exits with 0. Otherwise, if not, or if an error occurs, it will exit with a; non-zero value. TUTORIAL; --------. FileCheck is typically used from LLVM regression tests, being invoked on the RUN; line of the test. A simple example of using FileCheck from a RUN line looks; like this:. .. code-block:: llvm. ; RUN: llvm-as < %s | llc -march=x86-64 | FileCheck %s. This syntax says to pipe the current file (""``%s``"") into ``llvm-as``, pipe; that into ``llc``, then pipe the output of ``llc`` into ``FileCheck``. This; means that FileCheck will be verifying its standard input (the llc output); against the filename argument specified (the original ``.ll`` file specified by; ""``%s``""). To see how this works, let's look at the rest of the ``.ll`` file; (after the RUN line):. .. code-block:: llvm. define void @sub1(i32* %p, i32 %v) {; entry:; ; CHECK: sub1:; ; CHECK: subl; %0 = tail call i32 @llvm.atomic.load.sub.i32.p0i32(i32* %p, i32 %v); ret void; }. define void @inc4(i64* %p) {; entry:; ; CHECK: inc4:; ; CHECK: incq; %0 = tail call i64 @llvm.atomic.load.add.i64.p0i64(i64* %p, i64 1); ret void; }. Here you can see some ""``CHECK:``"" lines specified in comments. Now you can; see how the file is piped into ``llvm-as``, then ``llc``, and the machine code; output is what we are verifying. FileCheck checks the machine code output to; verify that it matches what the ""``CHECK:``"" lines specify. The syntax of the ""``CHECK:``"" lines is very simple: they are fixed strings that; must occur in order. FileCheck defaults to ignoring horizontal whitespace; differences (e.g. a space is allowed to match a tab) but otherwise, the contents; of the ""``CHECK:``"" line is required to match some thing in the test file exactly. One nice thing about FileCheck (compared to grep) is that it allows merging; test cases together into logical groups. For example, because the test above; is checking for the ""``sub1:``""",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:8597,load,load,8597,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['load'],['load']
Performance,"---------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===--------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3246,optimiz,optimization,3246,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,1,['optimiz'],['optimization']
Performance,"--------. Clang supports ARC-style weak and unsafe references in Objective-C even; outside of ARC mode. Weak references must be explicitly enabled with; the ``-fobjc-weak`` option; use ``__has_feature((objc_arc_weak))``; to test whether they are enabled. Unsafe references are enabled; unconditionally. ARC-style weak and unsafe references cannot be used; when Objective-C garbage collection is enabled. Except as noted below, the language rules for the ``__weak`` and; ``__unsafe_unretained`` qualifiers (and the ``weak`` and; ``unsafe_unretained`` property attributes) are just as laid out; in the :doc:`ARC specification <AutomaticReferenceCounting>`.; In particular, note that some classes do not support forming weak; references to their instances, and note that special care must be; taken when storing weak references in memory where initialization; and deinitialization are outside the responsibility of the compiler; (such as in ``malloc``-ed memory). Loading from a ``__weak`` variable always implicitly retains the; loaded value. In non-ARC modes, this retain is normally balanced; by an implicit autorelease. This autorelease can be suppressed; by performing the load in the receiver position of a ``-retain``; message send (e.g. ``[weakReference retain]``); note that this performs; only a single retain (the retain done when primitively loading from; the weak reference). For the most part, ``__unsafe_unretained`` in non-ARC modes is just the; default behavior of variables and therefore is not needed. However,; it does have an effect on the semantics of block captures: normally,; copying a block which captures an Objective-C object or block pointer; causes the captured pointer to be retained or copied, respectively,; but that behavior is suppressed when the captured variable is qualified; with ``__unsafe_unretained``. Note that the ``__weak`` qualifier formerly meant the GC qualifier in; all non-ARC modes and was silently ignored outside of GC modes. It now; means the ARC-st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:72561,load,loaded,72561,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['load'],['loaded']
Performance,"--------. Currently, the ``LoopInterchange`` pass does not use any metadata. Ambiguous Transformation Order; ==============================. If there multiple transformations defined, the order in which they are; executed depends on the order in LLVM's pass pipeline, which is subject; to change. The default optimization pipeline (anything higher than; ``-O0``) has the following order. When using the legacy pass manager:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - VersioningLICM (if enabled); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). When using the legacy pass manager with LTO:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopVectorizer; - LoopUnroll (partial and runtime unrolling). When using the new pass manager:. - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). Leftover Transformations; ========================. Forced transformations that have not been applied after the last; transformation pass should be reported to the user. The transformation; passes themselves cannot be responsible for this reporting because they; might not be in the pipeline, there might be multiple passes able to; apply a transformation (e.g. ``LoopInterchange`` and Polly) or a; transformation attribute may be 'hidden' inside another passes' followup; attribute. The pass ``-transform-warning`` (``WarnMissedTransformationsPass``); emits such warnings. It should be placed after the last transformation; pass. The current pass pipeline has a fixed order in which transformations; passes are executed. A transformation can be in the followup of a pass; that is executed later and thus leftover. For instance, a loop nest; cannot be distributed and then interchanged with the current pass; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:14326,perform,performs,14326,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,1,['perform'],['performs']
Performance,"--------. First, configure and build LLVM. Next, you need to create a new directory; somewhere in the LLVM source base. For this example, we'll assume that you; made ``lib/Transforms/Hello``. Finally, you must set up a build script; that will compile the source code for the new pass. To do this,; copy the following into ``CMakeLists.txt``:. .. code-block:: cmake. add_llvm_library( LLVMHello MODULE; Hello.cpp. PLUGIN_TOOL; opt; ). and the following line into ``lib/Transforms/CMakeLists.txt``:. .. code-block:: cmake. add_subdirectory(Hello). (Note that there is already a directory named ``Hello`` with a sample ""Hello""; pass; you may play with it -- in which case you don't need to modify any; ``CMakeLists.txt`` files -- or, if you want to create everything from scratch,; use another name.). This build script specifies that ``Hello.cpp`` file in the current directory; is to be compiled and linked into a shared object ``$(LEVEL)/lib/LLVMHello.so`` that; can be dynamically loaded by the :program:`opt` tool via its :option:`-load`; option. If your operating system uses a suffix other than ``.so`` (such as; Windows or macOS), the appropriate extension will be used. Now that we have the build scripts set up, we just need to write the code for; the pass itself. .. _writing-an-llvm-pass-basiccode:. Basic code required; -------------------. Now that we have a way to compile our new pass, we just have to write it.; Start out with:. .. code-block:: c++. #include ""llvm/Pass.h""; #include ""llvm/IR/Function.h""; #include ""llvm/Support/raw_ostream.h"". Which are needed because we are writing a `Pass; <https://llvm.org/doxygen/classllvm_1_1Pass.html>`_, we are operating on; `Function <https://llvm.org/doxygen/classllvm_1_1Function.html>`_\ s, and we will; be doing some printing. Next we have:. .. code-block:: c++. using namespace llvm;. ... which is required because the functions from the include files live in the; llvm namespace. Next we have:. .. code-block:: c++. namespace {. ... whic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:3447,load,loaded,3447,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,2,['load'],"['load', 'loaded']"
Performance,"--------. The Apple stage1 cache configures a two stage build similar to how Apple builds; the clang shipped with Xcode. The build files generated from this invocation has; a target named ""stage2"" which performs an LTO build of clang. The Apple-stage2 cache can be used directly to match the build settings Apple; uses in shipping builds without doing a full bootstrap build. PGO; ---. The PGO CMake cache can be used to generate a multi-stage instrumented compiler.; You can configure your build directory with the following invocation of CMake:. cmake -G <generator> -C <path_to_clang>/cmake/caches/PGO.cmake <source dir>. After configuration the following additional targets will be generated:. stage2-instrumented:; Builds a stage1 x86 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. stage2-instrumented-generate-profdata:; Depends on ""stage2-instrumented"" and will use the instrumented compiler to; generate profdata based on the training files in <clang>/utils/perf-training. stage2:; Depends on ""stage2-instrumented-generate-profdata"" and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. stage2-check-llvm:; Depends on stage2 and runs check-llvm using the stage3 compiler. stage2-check-clang:; Depends on stage2 and runs check-clang using the stage3 compiler. stage2-check-all:; Depends on stage2 and runs check-all using the stage3 compiler. stage2-test-suite:; Depends on stage2 and runs the test-suite using the stage3 compiler (requires; in-tree test-suite). 3-stage; -------. This cache file can be used to generate a 3-stage clang build. You can configure; using the following CMake command:. cmake -C <path to clang>/cmake/caches/3-stage.cmake -G Ninja <path to llvm>. You can then run ""ninja stage3-clang"" to build stage1, stage2 and stage3 clangs. This is useful for finding non-determinism the compiler by verifying that stage2; and stage3 are identical.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt:1695,optimiz,optimized,1695,interpreter/llvm-project/clang/cmake/caches/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt,3,"['cache', 'optimiz']","['cache', 'caches', 'optimized']"
Performance,"--------; class Thing {; }; ------------- Defs -----------------. ```tablegen; // The cache is reset here so this is an error.; def AThing: Thing {}; ```. <stdin>:2:13: error: Couldn't find class 'Thing'; def AThing: Thing {}; ^. The default value is `off`, meaning cells are connected. If you want to override the default for one cell only, use the `%reset` or `%noreset` magic. These always override the default. ```tablegen; class Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------. ```tablegen; %noreset; // This works because of the noreset above.; def AThing: Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------; def AThing {	// Thing; }. ```tablegen; // This does not because we're not changing the default.; def AnotherThing: Thing {}; ```. <stdin>:2:19: error: Couldn't find class 'Thing'; def AnotherThing: Thing {}; ^. ```tablegen; %config cellreset off; %reset; // Here we have an empty cache and default reset behaviour.; ```. ------------- Classes -----------------; ------------- Defs -----------------. It is not valid to have `%reset` and `%noreset` in the same cell. ```tablegen; %reset; %noreset; ```. %reset and %noreset in the same cell is not allowed. Use only one, or neither. Consider setting `cellreset` to the majority usecase for your notebook. For example a tutorial building a large example across many cells will likely want it `off`. One with many standalone examples, `on`. There is a ""magic"" directive `%args` that you can use to send command line arguments to `llvm-tblgen`. For example, here we have some code that shows a warning. ```tablegen; %reset; class Thing <int A, int B> {; int num = A;; }; ```. <stdin>:1:25: warning: unused template argument: Thing:B; class Thing <int A, int B> {; ^. We can pass an argument to ignore that warning. ```tablegen; %args --no-warn-on-unused-template-args; ```. ------------- Classes -----------------; cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md:2370,cache,cache,2370,interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,1,['cache'],['cache']
Performance,"-------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type before storing, the round trip will currently be broken. Take for example this code sequence::. %0 = load <4 x i32> %x; %1 = bitcast <4 x i32> %0 to <2 x i64>; store <2 x i64> %1, <2 x i64>* %y. This would produce a code sequence such as that in the figure on the right. The mismatched ``LD1`` and ``ST1`` cause the stored data to differ from the loaded data. .. container:: clearer. When we see a bitcast from type ``X`` to type ``Y``, what we need to do is to change the in-register representation of the data to be *as if* it had just been loaded by a ``LD1`` of type ``Y``. .. image:: ARM-BE-bitcastsuccess.png; :align: right. Conceptually this is simple - we can insert a ``REV`` undoing the ``LD1`` of type ``X`` (converting the in-register representation to the same as if it had been loaded by ``LDR``) and then insert another ``REV`` to change the representation to be as if it had been loaded by an ``LD1`` of type ``Y``. For the previous example, this would be::. LD1 v0.4s, [x]. REV64 v0.4s, v0.4s // There is no REV128 instruction, so it must be synthesizedcd; EXT v0.16b, v0.16b, v0.16b, #8 // with a REV64 then an EXT to swap the two 64-bit elements. REV64 v0.2d, v0.2d; EXT v0.16b, v0.16b, v0.16b, #8. ST1 v0.2d, [y]. It turns out that these ``REV`` pairs can, in almost all cases, be squashed together into a single ``REV``. For the example above, a ``REV128 4s`` + ``REV128 2d`` is actually a ``REV64 4s``, as shown in the figure on the right. .. [1] One lane ve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:10835,load,loaded,10835,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loaded']
Performance,"-------. :doc:`WritingAnLLVMPass`; Information on how to write LLVM transformations and analyses. :doc:`WritingAnLLVMNewPMPass`; Information on how to write LLVM transformations under the new pass; manager. :doc:`Passes`; A list of optimizations and analyses implemented in LLVM. :doc:`StackSafetyAnalysis`; This document describes the design of the stack safety analysis of local; variables. :doc:`MergeFunctions`; Describes functions merging optimization. :doc:`AliasAnalysis`; Information on how to write a new alias analysis implementation or how to; use existing analyses. :doc:`MemorySSA`; Information about the MemorySSA utility in LLVM, as well as how to use it. :doc:`LoopTerminology`; A document describing Loops and associated terms as used in LLVM. :doc:`CycleTerminology`; A document describing cycles as a generalization of loops. :doc:`Vectorizers`; This document describes the current status of vectorization in LLVM. :doc:`LinkTimeOptimization`; This document describes the interface between LLVM intermodular optimizer; and the linker and its design. :doc:`GoldPlugin`; How to build your programs with link-time optimization on Linux. :doc:`Remarks`; A reference on the implementation of remarks in LLVM. :doc:`Source Level Debugging with LLVM <SourceLevelDebugging>`; This document describes the design and philosophy behind the LLVM; source-level debugger. :doc:`How to Update Debug Info <HowToUpdateDebugInfo>`; This document specifies how to correctly update debug info in various kinds; of code transformations. :doc:`InstrRefDebugInfo`; This document explains how LLVM uses value tracking, or instruction; referencing, to determine variable locations for debug info in the final; stages of compilation. :doc:`RemoveDIsDebugInfo`; This is a migration guide describing how to move from debug info using; intrinsics such as dbg.value to using the non-instruction DPValue object. :doc:`InstrProfileFormat`; This document explains two binary formats of instrumentation-based profile",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst:3648,optimiz,optimizer,3648,interpreter/llvm-project/llvm/docs/UserGuides.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst,1,['optimiz'],['optimizer']
Performance,"-----; ``undef`` is the LLVM way of representing a value that is not defined. You; can get these if you do not initialize a variable before you use it. For; example, the C function:. .. code-block:: c. int X() { int i; return i; }. Is compiled to ""``ret i32 undef``"" because ""``i``"" never has a value specified; for it. Why does instcombine + simplifycfg turn a call to a function with a mismatched calling convention into ""unreachable""? Why not make the verifier reject it?; ----------------------------------------------------------------------------------------------------------------------------------------------------------; This is a common problem run into by authors of front-ends that are using; custom calling conventions: you need to make sure to set the right calling; convention on both the function and on each call to the function. For; example, this code:. .. code-block:: llvm. define fastcc void @foo() {; ret void; }; define void @bar() {; call void @foo(); ret void; }. Is optimized to:. .. code-block:: llvm. define fastcc void @foo() {; ret void; }; define void @bar() {; unreachable; }. ... with ""``opt -instcombine -simplifycfg``"". This often bites people because; ""all their code disappears"". Setting the calling convention on the caller and; callee is required for indirect calls to work, so people often ask why not; make the verifier reject this sort of thing. The answer is that this code has undefined behavior, but it is not illegal.; If we made it illegal, then every transformation that could potentially create; this would have to ensure that it doesn't, and there is valid code that can; create this sort of construct (in dead code). The sorts of things that can; cause this to happen are fairly contrived, but we still need to accept them.; Here's an example:. .. code-block:: llvm. define fastcc void @foo() {; ret void; }; define internal void @bar(void()* %FP, i1 %cond) {; br i1 %cond, label %T, label %F; T:; call void %FP(); ret void; F:; call fastcc void ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst:9698,optimiz,optimized,9698,interpreter/llvm-project/llvm/docs/FAQ.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst,1,['optimiz'],['optimized']
Performance,"----. * Upgrade cppyy-cling to 6.18.2; * Various patches to upstream's pre-compiled header generation and use; * Instantiate templates with larger integer types if argument values require; * Improve cppyy.interactive and partially enable it on PyPy, IPython, etc.; * Let ``__overload__`` be more flexible in signature matching; * Make list filtering of dir(cppyy.gbl) on Windows same as Linux/Mac; * Extended documentation. 2019-08-18: 1.5.0; -----------------. * Upgrade cppyy-cling to 6.18.0; * Allow python-derived classes to be used in templates; * Stricter template resolution and better caching/performance; * Detailed memory management for make_shared and shared_ptr; * Two-way memory management for cross-inherited objects; * Reduced memory footprint of proxy objects in most common cases; * Allow implicit conversion from a tuple of arguments; * Data set on namespaces reflected on C++ even if data not yet bound; * Generalized resolution of binary operators in wrapper generation; * Proper naming of arguments in namespaces for ``std::function<>``; * Cover more cases of STL-liker iterators; * Allow ``std::vector`` initialization with a list of constructor arguments; * Consistent naming of ``__cppname__`` to ``__cpp_name__``; * Added ``__set_lifeline__`` attribute to overloads; * Fixes to the cmake fragments for Ubuntu; * Fixes linker errors on Windows in some configurations; * Support C++ naming of typedef of bool types; * Basic views of 2D arrays of builtin types; * Extended documentation. 2019-07-01 : 1.4.12; -------------------. * Automatic conversion of python functions to ``std::function`` arguments; * Fix for templated operators that can map to different python names; * Fix on p3 crash when setting a detailed exception during exception handling; * Fix lookup of ``std::nullopt``; * Fix bug that prevented certain templated constructors from being considered; * Support for enum values as data members on ""enum class"" enums; * Support for implicit conversion when passing ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst:17740,perform,performance,17740,bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,1,['perform'],['performance']
Performance,"----. LLVM includes several alias-analysis driven transformations which can be used; with any of the implementations above. The ``-adce`` pass; ^^^^^^^^^^^^^^^^^^. The ``-adce`` pass, which implements Aggressive Dead Code Elimination uses the; ``AliasAnalysis`` interface to delete calls to functions that do not have; side-effects and are not used. The ``-licm`` pass; ^^^^^^^^^^^^^^^^^^. The ``-licm`` pass implements various Loop Invariant Code Motion related; transformations. It uses the ``AliasAnalysis`` interface for several different; transformations:. * It uses mod/ref information to hoist or sink load instructions out of loops if; there are no instructions in the loop that modifies the memory loaded. * It uses mod/ref information to hoist function calls out of loops that do not; write to memory and are loop-invariant. * It uses alias information to promote memory objects that are loaded and stored; to in loops to live in a register instead. It can do this if there are no may; aliases to the loaded/stored memory location. The ``-argpromotion`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-argpromotion`` pass promotes by-reference arguments to be passed in; by-value instead. In particular, if pointer arguments are only loaded from it; passes in the value loaded instead of the address to the function. This pass; uses alias information to make sure that the value loaded from the argument; pointer is not modified between the entry of the function and any load of the; pointer. The ``-gvn``, ``-memcpyopt``, and ``-dse`` passes; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. These passes use AliasAnalysis information to reason about loads and stores. .. _the clients:. Clients for debugging and evaluation of implementations; -------------------------------------------------------. These passes are useful for evaluating the various alias analysis; implementations. You can use them with commands like:. .. code-block:: bash. % opt -ds-aa -aa-eval foo.bc -disable-output -sta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:28174,load,loaded,28174,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,1,['load'],['loaded']
Performance,"----middle end----------------┼----backend----┤; │ │ │ │; └---parsing----sema----codegen--┴----- transformations ---- codegen ----┴---- codegen --┘. ┌---------------------------------------------------------------------------------------┐; | │; | source file │; | │; └---------------------------------------------------------------------------------------┘. ┌--------┐; │ │; │imported│; │ │; │ code │; │ │; └--------┘. Here we can see that the source file (could be a non-module unit or a module unit) would get processed by the; whole pipeline.; But the imported code would only get involved in semantic analysis, which is mainly about name lookup,; overload resolution and template instantiation.; All of these processes are fast relative to the whole compilation process.; More importantly, the imported code only needs to be processed once in frontend code generation,; as well as the whole middle end and backend.; So we could get a big win for the compilation time in O0. But with optimizations, things are different:. (we omit ``code generation`` part for each end due to the limited space). .. code-block:: none. ├-------- frontend ---------┼--------------- middle end --------------------┼------ backend ----┤; │ │ │ │; └--- parsing ---- sema -----┴--- optimizations --- IPO ---- optimizations---┴--- optimizations -┘. ┌-----------------------------------------------------------------------------------------------┐; │ │; │ source file │; │ │; └-----------------------------------------------------------------------------------------------┘; ┌---------------------------------------┐; │ │; │ │; │ imported code │; │ │; │ │; └---------------------------------------┘. It would be very unfortunate if we end up with worse performance after using modules.; The main concern is that when we compile a source file, the compiler needs to see the function body; of imported module units so that it can perform IPO (InterProcedural Optimization, primarily inlining; in practice) to optimize functio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:41396,optimiz,optimizations,41396,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['optimiz'],['optimizations']
Performance,"--.; | 0x00001000 | BUCKETS[0]; | 0x00002000 | BUCKETS[1]; | 0x00002200 | BUCKETS[2]; | 0x000034f0 | BUCKETS[3]; | | ...; | 0xXXXXXXXX | BUCKETS[n_buckets]; '------------'. So for ``bucket[3]`` in the example above, we have an offset into the table; 0x000034f0 which points to a chain of entries for the bucket. Each bucket must; contain a next pointer, full 32 bit hash value, the string itself, and the data; for the current string value. .. code-block:: none. .------------.; 0x000034f0: | 0x00003500 | next pointer; | 0x12345678 | 32 bit hash; | ""erase"" | string value; | data[n] | HashData for this bucket; |------------|; 0x00003500: | 0x00003550 | next pointer; | 0x29273623 | 32 bit hash; | ""dump"" | string value; | data[n] | HashData for this bucket; |------------|; 0x00003550: | 0x00000000 | next pointer; | 0x82638293 | 32 bit hash; | ""main"" | string value; | data[n] | HashData for this bucket; `------------'. The problem with this layout for debuggers is that we need to optimize for the; negative lookup case where the symbol we're searching for is not present. So; if we were to lookup ""``printf``"" in the table above, we would make a 32-bit; hash for ""``printf``"", it might match ``bucket[3]``. We would need to go to; the offset 0x000034f0 and start looking to see if our 32 bit hash matches. To; do so, we need to read the next pointer, then read the hash, compare it, and; skip to the next bucket. Each time we are skipping many bytes in memory and; touching new pages just to do the compare on the full 32 bit hash. All of; these accesses then tell us that we didn't have a match. Name Hash Tables; """""""""""""""""""""""""""""""". To solve the issues mentioned above we have structured the hash tables a bit; differently: a header, buckets, an array of all unique 32 bit hash values,; followed by an array of hash value data offsets, one for each hash value, then; the data for all hash values:. .. code-block:: none. .-------------.; | HEADER |; |-------------|; | BUCKETS |; |-------------|",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:62164,optimiz,optimize,62164,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimize']
Performance,"--===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===---------------------------------------------------------------------===//. This code:. typedef struct {; int f1:1;; int f2:1;; int f3:1;; int f4:29;; } t1;. typedef struct {; int f1:1;; int f2:1;; int f3:30;; } t2;. t1 s1;; t2 s2;. void func1(void); {; s1.f1 = s2.f1;; s1.f2 = s2.f2;; }. Compiles into this IR (on x86-64 at least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; ret. Note that bb1 and bb2 are the same. This doesn't happen at the IR level; because one call is passing an i32 and the o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66279,load,load,66279,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['load'],['load']
Performance,"--enable-gold=default`` to the above configure invocation; to automatically install the newly built gold as the default linker with; ``make install``. * Build the LLVMgold plugin. Run CMake with; ``-DLLVM_BINUTILS_INCDIR=/path/to/binutils/include``. The correct include; path will contain the file ``plugin-api.h``. Usage; =====. You should produce bitcode files from ``clang`` with the option; ``-flto``. This flag will also cause ``clang`` to look for the gold plugin in; the ``lib`` directory under its prefix and pass the ``-plugin`` option to; ``ld``. It will not look for an alternate linker without ``-fuse-ld=gold``,; which is why you otherwise need gold to be the installed system linker in; your path. ``ar`` and ``nm`` also accept the ``-plugin`` option and it's possible to; to install ``LLVMgold.so`` to ``/usr/lib/bfd-plugins`` for a seamless setup.; If you built your own gold, be sure to install the ``ar`` and ``nm-new`` you; built to ``/usr/bin``. Example of link time optimization; ---------------------------------. The following example shows a worked example of the gold plugin mixing LLVM; bitcode and native code. .. code-block:: c. --- a.c ---; #include <stdio.h>. extern void foo1(void);; extern void foo4(void);. void foo2(void) {; printf(""Foo2\n"");; }. void foo3(void) {; foo4();; }. int main(void) {; foo1();; }. --- b.c ---; #include <stdio.h>. extern void foo2(void);. void foo1(void) {; foo2();; }. void foo4(void) {; printf(""Foo4"");; }. .. code-block:: bash. --- command lines ---; $ clang -flto a.c -c -o a.o # <-- a.o is LLVM bitcode file; $ ar q a.a a.o # <-- a.a is an archive with LLVM bitcode; $ clang b.c -c -o b.o # <-- b.o is native object file; $ clang -flto a.a b.o -o main # <-- link with LLVMgold plugin. Gold informs the plugin that foo3 is never referenced outside the IR,; leading LLVM to delete that function. However, unlike in the :ref:`libLTO; example <libLTO-example>` gold does not currently eliminate foo4. Quickstart for using LTO with autotoo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GoldPlugin.rst:3730,optimiz,optimization,3730,interpreter/llvm-project/llvm/docs/GoldPlugin.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GoldPlugin.rst,1,['optimiz'],['optimization']
Performance,"-. The advertised premise of Numba is that it ""makes Python code fast.""; However, there is a much more compelling reason: Numba allows developers to; stay in their chosen ecosystem, be it Python or C++, in mixed environments,; without paying for their choice in lost performance.; For example, a Python developer using Numba does not need to rewrite a kernel; into C++ just to run performantly in a C++ framework.; Similarly, a C++ developer can use Numba to compile and create function; pointers to Python code for easy, performant, access.; This becomes even more compelling if the deployment target is a GPU, which; would otherwise certainly require a rewrite of the Python code.; Add that Numba, as a JIT-compiler, is fully run-time just like ``cppyy``,; and the use case for integration is clear.; (Numba does not currently provide support for C++.). Usage; -------. ``cppyy`` does not use Numba extension hooks to minimize accidental; dependencies.; Instead, it requires that the extensions are loaded explicitly by any code; that uses it::. import cppyy.numba_ext. After that, Numba is able to trace ``cppyy`` bound code when applying the; usual ``numba.njit`` decorator. Numba type declarations are done lazily, with the ``numba_ext`` module only; initially registering hooks on proxy base classes, to keep overheads in; Numba's type-resolution to a minimum.; On use in a JITed trace, each C++ type or function call is refined to the; actual, concrete types and type-specific overloads, with templates; instantiated as-needed.; Where possible, lowering is kept generic to reduce the number of callbacks; in Numba's compilation chain. Examples; --------. The following, non-exhaustive, set of examples gives an idea of the; current level of support.; More examples can be found in the `test suite`_. C++ free (global) functions can be called and overloads will be selected, or; a template will be instantiated, based on the provided types.; Exact type matches are fully supported, there is some",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst:2379,load,loaded,2379,bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,1,['load'],['loaded']
Performance,"-===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:60598,load,load,60598,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['load'],['load']
Performance,"-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to checking with the OptBisect class to see if the pass should be; skipped, the skipFunction(), skipLoop() and skipBasicBlock() helper functions; also look for the presence of the ""optnone"" function attribute. The calling; pass will be unable to determine whether it is being skipped because the; ""optnone"" attribute is present or because the opt-bisect-limit has been; reached. This is desirable because the behavior should be the same in either; case. The majority of LLVM passes which can be skipped have already been instrumented; in the manner described above. If you are adding a new pass or believe you; have found a pass which is not being included in the opt-bisect process but; should be, you can add it as described above. Adding Finer Granularity; ========================. Once the pass in which an incorrect transformation is performed has been; determined, it may be useful to perform further analysis in order to determine; which specific transformation is causing the problem. Debug counters; can be used for this purpose.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:8752,perform,performed,8752,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,2,['perform'],"['perform', 'performed']"
Performance,"-block:: c. #include <SomeLib.h>. The implementation is handled separately by linking against the appropriate library. For example, by passing ``-lSomeLib`` to the linker. Modules provide an alternative, simpler way to use software libraries that provides better compile-time scalability and eliminates many of the problems inherent to using the C preprocessor to access the API of a library. Problems with the current model; -------------------------------; The ``#include`` mechanism provided by the C preprocessor is a very poor way to access the API of a library, for a number of reasons:. * **Compile-time scalability**: Each time a header is included, the; compiler must preprocess and parse the text in that header and every; header it includes, transitively. This process must be repeated for; every translation unit in the application, which involves a huge; amount of redundant work. In a project with *N* translation units; and *M* headers included in each translation unit, the compiler is; performing *M x N* work even though most of the *M* headers are; shared among multiple translation units. C++ is particularly bad,; because the compilation model for templates forces a huge amount of; code into headers. * **Fragility**: ``#include`` directives are treated as textual; inclusion by the preprocessor, and are therefore subject to any; active macro definitions at the time of inclusion. If any of the; active macro definitions happens to collide with a name in the; library, it can break the library API or cause compilation failures; in the library header itself. For an extreme example,; ``#define std ""The C++ Standard""`` and then include a standard; library header: the result is a horrific cascade of failures in the; C++ Standard Library's implementation. More subtle real-world; problems occur when the headers for two different libraries interact; due to macro collisions, and users are forced to reorder; ``#include`` directives or introduce ``#undef`` directives to break; t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:1504,perform,performing,1504,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['perform'],['performing']
Performance,"-block:: c. lto_module_dispose(lto_module_t). The linker can introspect the non-native object file by getting the number of; symbols and getting the name and attributes of each symbol via:. .. code-block:: c. lto_module_get_num_symbols(lto_module_t); lto_module_get_symbol_name(lto_module_t, unsigned int); lto_module_get_symbol_attribute(lto_module_t, unsigned int). The attributes of a symbol include the alignment, visibility, and kind. Tools working with object files on Darwin (e.g. lipo) may need to know properties like the CPU type:. .. code-block:: c. lto_module_get_macho_cputype(lto_module_t mod, unsigned int *out_cputype, unsigned int *out_cpusubtype). ``lto_code_gen_t``; ------------------. Once the linker has loaded each non-native object files into an; ``lto_module_t``, it can request ``libLTO`` to process them all and generate a; native object file. This is done in a couple of steps. First, a code generator; is created with:. .. code-block:: c. lto_codegen_create(). Then, each non-native object file is added to the code generator with:. .. code-block:: c. lto_codegen_add_module(lto_code_gen_t, lto_module_t). The linker then has the option of setting some codegen options. Whether or not; to generate DWARF debug info is set with:. .. code-block:: c. lto_codegen_set_debug_model(lto_code_gen_t). which kind of position independence is set with:. .. code-block:: c. lto_codegen_set_pic_model(lto_code_gen_t). And each symbol that is referenced by a native object file or otherwise must not; be optimized away is set with:. .. code-block:: c. lto_codegen_add_must_preserve_symbol(lto_code_gen_t, const char*). After all these settings are done, the linker requests that a native object file; be created from the modules with the settings using:. .. code-block:: c. lto_codegen_compile(lto_code_gen_t, size*). which returns a pointer to a buffer containing the generated native object file.; The linker then parses that and links it with the rest of the native object; files.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:10822,optimiz,optimized,10822,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimized']
Performance,"-block:: text. {<2 x i64>, <2 x i64>} llvm.experimental.vector.deinterleave2.v4i64(<4 x i64> <i64 0, i64 1, i64 2, i64 3>); ==> {<2 x i64> <i64 0, i64 2>, <2 x i64> <i64 1, i64 3>}. Arguments:; """""""""""""""""""". The argument is a vector whose type corresponds to the logical concatenation of; the two result types. '``llvm.experimental.vector.interleave2``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x double> @llvm.experimental.vector.interleave2.v4f64(<2 x double> %vec1, <2 x double> %vec2); declare <vscale x 8 x i32> @llvm.experimental.vector.interleave2.nxv8i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2). Overview:; """""""""""""""""". The '``llvm.experimental.vector.interleave2``' intrinsic constructs a vector; by interleaving two input vectors. This intrinsic works for both fixed and scalable vectors. While this intrinsic; supports all vector types the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. <4 x i64> llvm.experimental.vector.interleave2.v4i64(<2 x i64> <i64 0, i64 2>, <2 x i64> <i64 1, i64 3>); ==> <4 x i64> <i64 0, i64 1, i64 2, i64 3>. Arguments:; """"""""""""""""""""; Both arguments must be vectors of the same type whereby their logical; concatenation matches the result type. '``llvm.experimental.cttz.elts``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ```llvm.experimental.cttz.elts```; on any vector of integer elements, both fixed width and scalable. ::. declare i8 @llvm.experimental.cttz.elts.i8.v8i1(<8 x i1> <src>, i1 <is_zero_poison>). Overview:; """""""""""""""""". The '``llvm.experimental.cttz.elts``' intrinsic counts the number of trailing; zero elements of a vector. Arguments:; """""""""""""""""""". The first argument is the vector to be counted. This argument must be a vector;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:668736,optimiz,optimization,668736,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2083,load,load,2083,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,1,['load'],['load']
Performance,"-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry poi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17414,concurren,concurrency,17414,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,2,['concurren'],"['concurrency', 'concurrently']"
Performance,"-fimplicit-module-maps -fmodules-cache-path=prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -DENABLE_A; find prebuilt -name ""*.pcm""; # prebuilt/1AYBIGPM8R2GA/A-3L1K4LUA6O31.pcm; # prebuilt/1AYBIGPM8R2GA/B-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/A-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -DENABLE_A. Finally we want to allow implicit modules for configurations that were not prebuilt. When using the clang driver a module cache path is implicitly selected. Using ``-cc1``, we simply add use the ``-fmodules-cache-path`` option. .. code-block:: sh. clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A -DOTHER_OPTIONS. This way, a single directory containing multiple variants of modules can be prepared and reused. The options configuring the module cache are independent of other options. Module Semantics; ================. Modules are modeled as if each submodule were a separate translation unit, and a module import makes names from the other translation unit visible. Each submodule starts with a new preprocessor state and an empty translation unit. .. note::. This behavior is currently only approximated when building a module with submodules. Entities within a submodule that has already been built are visibl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:23539,cache,cache-path,23539,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,2,['cache'],"['cache', 'cache-path']"
Performance,"-gfx11-table`. .. table:: AMDHSA Memory Model Code Sequences GFX10-GFX11; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx10-gfx11-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX10-GFX11; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_load; glc=1 dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:344173,load,load,344173,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to en",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:337440,cache,caches,337440,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"-id-x"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.x intrinsic. ""amdgpu-no-workgroup-id-y"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.y intrinsic. ""amdgpu-no-workgroup-id-z"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.z intrinsic. ""amdgpu-no-dispatch-ptr"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.dispatch.ptr intrinsic. ""amdgpu-no-implicitarg-ptr"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.implicitarg.ptr intrinsic. ""amdgpu-no-dispatch-id"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.dispatch.id intrinsic. ""amdgpu-no-queue-ptr"" Similar to amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.queue.ptr intrinsic. Note that unlike the other ABI hint; attributes, the queue pointer may be required in situations where the; intrinsic call does not directly appear in the program. Some subtargets; require the queue pointer for to handle some addrspacecasts, as well; as the llvm.amdgcn.is.shared, llvm.amdgcn.is.private, llvm.trap, and; llvm.debug intrinsics. ""amdgpu-no-hostcall-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to the hostcall buffer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-heap-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to an initialized memory buffer; that conforms to the requirements of the malloc/free device library V1; version implementation. If this attribute is absent, then the; amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-multigrid-sync-arg"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the multigrid synchronization pointer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-default-queue"" Similar to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:49887,queue,queue,49887,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"-memory-model-single-thread-optimization-constraints-table. ============ ==============================================================; LLVM Memory Optimization Constraints; Ordering; ============ ==============================================================; unordered *none*; monotonic *none*; acquire - If a load atomic/atomicrmw then no following load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved before the acquire.; - If a fence then same as load atomic, plus no preceding; associated fence-paired-atomic can be moved after the fence.; release - If a store atomic/atomicrmw then no preceding load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved after the release.; - If a fence then same as store atomic, plus no following; associated fence-paired-atomic can be moved before the; fence.; acq_rel Same constraints as both acquire and release.; seq_cst - If a load atomic then same constraints as acquire, plus no; preceding sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved after the; seq_cst.; - If a store atomic then the same constraints as release, plus; no following sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved before the; seq_cst.; - If an atomicrmw/fence then same constraints as acq_rel.; ============ ==============================================================. The code sequences used to implement the memory model are defined in the; following sections:. * :ref:`amdgpu-amdhsa-memory-model-gfx6-gfx9`; * :ref:`amdgpu-amdhsa-memory-model-gfx90a`; * :ref:`amdgpu-amdhsa-memory-model-gfx942`; * :ref:`amdgpu-amdhsa-memory-model-gfx10-gfx11`. .. _amdgpu-amdhsa-memory-model-gfx6-gfx9:. Memory Model GFX6-GFX9; ++++++++++++++++++++++. For GFX6-GFX9:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:205362,load,load,205362,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"-null pointers tagged with; ``dereferenceable_or_null(<n>)`` are ``dereferenceable(<n>)``.; For address space 0 ``dereferenceable_or_null(<n>)`` implies that; a pointer is exactly one of ``dereferenceable(<n>)`` or ``null``,; and in other address spaces ``dereferenceable_or_null(<n>)``; implies that a pointer is at least one of ``dereferenceable(<n>)``; or ``null`` (i.e. it may be both ``null`` and; ``dereferenceable(<n>)``). This attribute may only be applied to; pointer typed parameters. ``swiftself``; This indicates that the parameter is the self/context parameter. This is not; a valid attribute for return values and can only be applied to one; parameter. .. _swiftasync:. ``swiftasync``; This indicates that the parameter is the asynchronous context parameter and; triggers the creation of a target-specific extended frame record to store; this pointer. This is not a valid attribute for return values and can only; be applied to one parameter. ``swifterror``; This attribute is motivated to model and optimize Swift error handling. It; can be applied to a parameter with pointer to pointer type or a; pointer-sized alloca. At the call site, the actual argument that corresponds; to a ``swifterror`` parameter has to come from a ``swifterror`` alloca or; the ``swifterror`` parameter of the caller. A ``swifterror`` value (either; the parameter or the alloca) can only be loaded and stored from, or used as; a ``swifterror`` argument. This is not a valid attribute for return values; and can only be applied to one parameter. These constraints allow the calling convention to optimize access to; ``swifterror`` variables by associating them with a specific register at; call boundaries rather than placing them in memory. Since this does change; the calling convention, a function which uses the ``swifterror`` attribute; on a parameter is not ABI-compatible with one which does not. These constraints also allow LLVM to assume that a ``swifterror`` argument; does not alias any other mem",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:63197,optimiz,optimize,63197,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimize']
Performance,"-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:4291,optimiz,optimizations,4291,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,2,['optimiz'],['optimizations']
Performance,"-point; operations, such as fused multiply-add (FMA). Fused operations are; permitted to produce more precise results than performing the same; operations separately. The C standard permits intermediate floating-point results within an; expression to be computed with more precision than their type would; normally allow. This permits operation fusing, and Clang takes advantage; of this by default. This behavior can be controlled with the ``FP_CONTRACT``; and ``clang fp contract`` pragmas. Please refer to the pragma documentation; for a description of how the pragmas interact with this option. Valid values are:. * ``fast`` (fuse across statements disregarding pragmas, default for CUDA); * ``on`` (fuse in the same statement unless dictated by pragmas, default for languages other than CUDA/HIP); * ``off`` (never fuse); * ``fast-honor-pragmas`` (fuse across statements unless dictated by pragmas, default for HIP). .. option:: -f[no-]honor-infinities. Allow floating-point optimizations that assume arguments and results are; not +-Inf.; Defaults to ``-fhonor-infinities``. If both ``-fno-honor-infinities`` and ``-fno-honor-nans`` are used,; has the same effect as specifying ``-ffinite-math-only``. .. option:: -f[no-]honor-nans. Allow floating-point optimizations that assume arguments and results are; not NaNs.; Defaults to ``-fhonor-nans``. If both ``-fno-honor-infinities`` and ``-fno-honor-nans`` are used,; has the same effect as specifying ``-ffinite-math-only``. .. option:: -f[no-]approx-func. Allow certain math function calls (such as ``log``, ``sqrt``, ``pow``, etc); to be replaced with an approximately equivalent set of instructions; or alternative math function calls. For example, a ``pow(x, 0.25)``; may be replaced with ``sqrt(sqrt(x))``, despite being an inexact result; in cases where ``x`` is ``-0.0`` or ``-inf``.; Defaults to ``-fno-approx-func``. .. option:: -f[no-]signed-zeros. Allow optimizations that ignore the sign of floating point zeros.; Defaults to ``-fsig",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:57770,optimiz,optimizations,57770,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"-project/root/issues/15156)] - Segfault in TMVA::Experimental::SOFIE::RModel::Streamer; * [[#15139](https://github.com/root-project/root/issues/15139)] - cmake option -Dall=YES breaks the cmake; * [[#15118](https://github.com/root-project/root/issues/15118)] - jsoninterface does not build if provided with RapidYAML; * [[#15108](https://github.com/root-project/root/issues/15108)] - Turn off clang-format for Linkdef files; * [[#15090](https://github.com/root-project/root/issues/15090)] - TClass::GetClassInfo() is not thread safe; * [[#15077](https://github.com/root-project/root/issues/15077)] - Passing different floating point types to `RVec` utility functions; * [[#15048](https://github.com/root-project/root/issues/15048)] - [ntuple] Handling of virtual inheritance broken; * [[#15040](https://github.com/root-project/root/issues/15040)] - [RDataFrame] Inaccurate example of progress bar from documentation; * [[#15028](https://github.com/root-project/root/issues/15028)] - [RDataFrame] Unable to cacheread remote file; * [[#15027](https://github.com/root-project/root/issues/15027)] - spurrious cmake message about AfterImage with -Dminimal=ON; * [[#14981](https://github.com/root-project/root/issues/14981)] - RVecs leak memory with np.asarray in pyROOT; * [[#14964](https://github.com/root-project/root/issues/14964)] - ROOT-HEAD fails with ""cling interactive line includer >>>: fatal error: module file '[snip]/Vc.pcm' not found: module file not found""; * [[#14958](https://github.com/root-project/root/issues/14958)] - ROOT_HEAD failed with error message: Fail to detect cryptographic random generator; * [[#14921](https://github.com/root-project/root/issues/14921)] - ROOT Fails to build macOS 14.4 arm64 Xcode 15.3; * [[#14914](https://github.com/root-project/root/issues/14914)] - VecOps::Take with default argument doesn't check correctly the out of boundary condition; * [[#14910](https://github.com/root-project/root/issues/14910)] - hadd issue when using parallelization together",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:27272,cache,cacheread,27272,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['cache'],['cacheread']
Performance,"-state`). For; GFX9-GFX11 the aperture base addresses are directly available as inline; constant registers ``SRC_SHARED_BASE/LIMIT`` and ``SRC_PRIVATE_BASE/LIMIT``.; In 64-bit address mode the aperture sizes are 2^32 bytes and the base is; aligned to 2^32 which makes it easier to convert from flat to segment or; segment to flat. A global address space address has the same value when used as a flat address; so no conversion is needed. **Global and Constant**; The global and constant address spaces both use global virtual addresses,; which are the same virtual address space used by the CPU. However, some; virtual addresses may only be accessible to the CPU, some only accessible; by the GPU, and some by both. Using the constant address space indicates that the data will not change; during the execution of the kernel. This allows scalar read instructions to; be used. As the constant address space could only be modified on the host; side, a generic pointer loaded from the constant address space is safe to be; assumed as a global pointer since only the device global memory is visible; and managed on the host side. The vector and scalar L1 caches are invalidated; of volatile data before each kernel dispatch execution to allow constant; memory to change values between kernel dispatches. **Region**; The region address space uses the hardware Global Data Store (GDS). All; wavefronts executing on the same device will access the same memory for any; given region address. However, the same region address accessed by wavefronts; executing on different devices will access different memory. It is higher; performance than global memory. It is allocated by the runtime. The data; store (DS) instructions can be used to access it. **Local**; The local address space uses the hardware Local Data Store (LDS) which is; automatically allocated when the hardware creates the wavefronts of a; work-group, and freed when all the wavefronts of a work-group have; terminated. All wavefronts belongin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:25154,load,loaded,25154,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.h; clang-tools-extra/clang-tidy/readability/ConstReturnTypeCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.h; clang-tools-extra/clang-tidy/readability/ContainerDataPointerCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerDataPointerCheck.h; clang-tools-extra/clang-tidy/readability/ContainerSizeEmptyCheck,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:66241,perform,performance,66241,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,". ## Glossary. The following glossary is adapted from the description of the Rogue Wave; `Threads.h`++ package. A **`process`** is a program that is loaded into memory and prepared for; execution. Each process has a private address space. Processes begin; with a single thread. A **`thread`** is a sequence of instructions being executed in a; program. A thread has a program counter and a private stack to keep; track of local variables and return addresses. A multithreaded process; is associated with one or more threads. Threads execute independently.; All threads in a given process share the private address space of that; process. **`Concurrency`** exists when at least two threads are in progress at; the same time. A system with only a single processor can support; concurrency by switching execution contexts among multiple threads. **`Parallelism`** arises when at least two threads are executing; simultaneously. This requires a system with multiple processors.; Parallelism implies concurrency, but not vice-versa. A function is **`reentrant`** if it will behave correctly even if a; thread of execution enters the function while one or more threads are; already executing within the function. These could be the same thread,; in the case of recursion, or different threads, in the case of; concurrency. **`Thread-specific data`** (**`TSD`**) is also known as thread-local; storage (TLS). Normally, any data that has lifetime beyond the local; variables on the thread's private stack are shared among all threads; within the process. Thread-specific data is a form of static or global; data that is maintained on a per-thread basis. That is, each thread gets; its own private copy of the data. Left to their own devices, threads execute independently.; **`Synchronization`** is the work that must be done when there are, in; fact, interdependencies that require some form of communication among; threads. Synchronization tools include mutexes, semaphores, condition; variables, and other",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:20524,concurren,concurrency,20524,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['concurren'],['concurrency']
Performance,". **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimizati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1522,optimiz,optimizing,1522,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizing']
Performance,". .. _arc.misc.interior:. Interior pointers; -----------------. An Objective-C method returning a non-retainable pointer may be annotated with; the ``objc_returns_inner_pointer`` attribute to indicate that it returns a; handle to the internal data of an object, and that this reference will be; invalidated if the object is destroyed. When such a message is sent to an; object, the object's lifetime will be extended until at least the earliest of:. * the last use of the returned pointer, or any pointer derived from it, in the; calling function or; * the autorelease pool is restored to a previous state. .. admonition:: Rationale. Rationale: not all memory and resources are managed with reference counts; it; is common for objects to manage private resources in their own, private way.; Typically these resources are completely encapsulated within the object, but; some classes offer their users direct access for efficiency. If ARC is not; aware of methods that return such ""interior"" pointers, its optimizations can; cause the owning object to be reclaimed too soon. This attribute informs ARC; that it must tread lightly. The extension rules are somewhat intentionally vague. The autorelease pool; limit is there to permit a simple implementation to simply retain and; autorelease the receiver. The other limit permits some amount of; optimization. The phrase ""derived from"" is intended to encompass the results; both of pointer transformations, such as casts and arithmetic, and of loading; from such derived pointers; furthermore, it applies whether or not such; derivations are applied directly in the calling code or by other utility code; (for example, the C library routine ``strchr``). However, the implementation; never need account for uses after a return from the code which calls the; method returning an interior pointer. As an exception, no extension is required if the receiver is loaded directly; from a ``__strong`` object with :ref:`precise lifetime semantics; <arc.optimizatio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:100537,optimiz,optimizations,100537,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizations']
Performance,". .. code-block:: c++. struct HostS { ... };; struct DeviceS { ... };. #ifdef __NVCC__; #ifndef __CUDA_ARCH__; __host__ HostS MakeStruct() { return HostS(); }; #else; __device__ DeviceS MakeStruct() { return DeviceS(); }; #endif; #else; __host__ HostS MakeStruct() { return HostS(); }; __device__ DeviceS MakeStruct() { return DeviceS(); }; #endif. // Now host and device code can call MakeStruct(). Hopefully you don't have to do this sort of thing often. Optimizations; =============. Modern CPUs and GPUs are architecturally quite different, so code that's fast; on a CPU isn't necessarily fast on a GPU. We've made a number of changes to; LLVM to make it generate good GPU code. Among these changes are:. * `Straight-line scalar optimizations <https://goo.gl/4Rb9As>`_ -- These; reduce redundancy within straight-line code. * `Aggressive speculative execution; <https://llvm.org/docs/doxygen/html/SpeculativeExecution_8cpp_source.html>`_; -- This is mainly for promoting straight-line scalar optimizations, which are; most effective on code along dominator paths. * `Memory space inference; <https://llvm.org/doxygen/NVPTXInferAddressSpaces_8cpp_source.html>`_ --; In PTX, we can operate on pointers that are in a particular ""address space""; (global, shared, constant, or local), or we can operate on pointers in the; ""generic"" address space, which can point to anything. Operations in a; non-generic address space are faster, but pointers in CUDA are not explicitly; annotated with their address space, so it's up to LLVM to infer it where; possible. * `Bypassing 64-bit divides; <https://llvm.org/docs/doxygen/html/BypassSlowDivision_8cpp_source.html>`_ --; This was an existing optimization that we enabled for the PTX backend. 64-bit integer divides are much slower than 32-bit ones on NVIDIA GPUs.; Many of the 64-bit divides in our benchmarks have a divisor and dividend; which fit in 32-bits at runtime. This optimization provides a fast path for; this common case. * Aggressive loop unroll",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst:18700,optimiz,optimizations,18700,interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,1,['optimiz'],['optimizations']
Performance,". .. code-block:: text. #ifdef GET_CTable_DECL; const CEntry *lookupCEntryByEncoding(uint16_t Encoding);; #endif. #ifdef GET_CTable_IMPL; constexpr CEntry CTable[] = {; { ""Apple"", CFoo, 0xA }, // 0; { ""Apple"", CBar, 0xD }, // 1; { ""Pear"", CBaz, 0xF }, // 2; };. const CEntry *lookupCEntryByEncoding(uint16_t Encoding) {; struct KeyType {; uint16_t Encoding;; };; KeyType Key = { Encoding };; auto Table = ArrayRef(CTable);; auto Idx = std::lower_bound(Table.begin(), Table.end(), Key,; [](const CEntry &LHS, const KeyType &RHS) {; if (LHS.Encoding < RHS.Encoding); return true;; if (LHS.Encoding > RHS.Encoding); return false;; return false;; });. if (Idx == Table.end() ||; Key.Encoding != Idx->Encoding); return nullptr;; return &*Idx;; }. The ``PrimaryKeyEarlyOut`` field, when set to 1, modifies the lookup; function so that it tests the first field of the primary key to determine; whether it is within the range of the collected records' primary keys. If; not, the function returns the null pointer without performing the binary; search. This is useful for tables that provide data for only some of the; elements of a larger enum-based space. The first field of the primary key; must be an integral type; it cannot be a string. Adding ``let PrimaryKeyEarlyOut = 1`` to the ``ATable`` above:. .. code-block:: text. def ATable : GenericTable {; let FilterClass = ""AEntry"";; let Fields = [""Str"", ""Val1"", ""Val2""];; let PrimaryKey = [""Val1"", ""Val2""];; let PrimaryKeyName = ""lookupATableByValues"";; let PrimaryKeyEarlyOut = 1;; }. causes the lookup function to change as follows:. .. code-block:: text. const AEntry *lookupATableByValues(uint8_t Val1, uint16_t Val2) {; if ((Val1 < 0x2) ||; (Val1 > 0x5)); return nullptr;. struct KeyType {; ... We can construct two GenericTables with the same ``FilterClass``, so that they; select from the same overall set of records, but assign them with different; ``FilterClassField`` values so that they include different subsets of the; records of that class. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:29397,perform,performing,29397,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,1,['perform'],['performing']
Performance,". ; Networking. NET; ; TWebFile. Several pptimizations in TWebFile improving performance especially for TTree::Map() by about 35%. This has been achieved with a better caching strategy for request strings (especially avoiding to recalculate the auth base64 encoding), and with a drastic optimization in reading the response headers.; Fixes in the counting of the bytes read. TWebSystem. New implementation of TSystem allowing to use TSystem::AccessPathName() and GetPathInfo() to check if a web file exists and to get its size. Directory browsing is not available yet. NETX; ; TXNetFile. Several fixes and optimisations, mainly in the use of the cache; Fix an offset issue affecting the use of the cache with files in archives. TXNetSystem. A few optimizations in the use of retry mechanism, path locality checks, file online checks. XROOTD. Import a new version of XROOTD (20091202-0509); ; Fixes in bulk prepare and sync readv operations; Add support for 'make install' / 'make uninstall' and; other improvements in configure.classic; Several improvements / fixes:; ; reduced memory and CPU consumption;; extreme cp optimizations;; windows porting; new cache policies on the client side; new listing features implemented recently in the 'cns' module.; optimizations in cmsd and cnsd (performance improvements); support for openssl 1.0.0 (required by Fedora 12). Support for if/else if/else/fi constructs; Several portability fixes; ; Support 32-bit builds with icc on 64-bit platforms; Improved detection of libreadline and lib(n)curses. Increase the flexibility for configuring with an external xrootd; ; Add standard switches to disentangle lib and inc dirs;       --with-xrootd-incdir=<path_to dir_containing_XrdVersion.hh>;       --with-xrootd-libdir=<path_to_dir_containing_xrootd_plugins_and_libs>; ; When; passing a global xrootd dir with --with-xrootd, check both; src/XrdVersion.hh and include/xrootd/XrdVersion.hh so that both build; and install distributions are supported. Fix a problem ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v526/index.html:77,perform,performance,77,net/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v526/index.html,5,"['cache', 'optimiz', 'perform']","['cache', 'optimization', 'optimizations', 'performance']"
Performance,". ; PROOF. Warning. The; classes; TProofDataSetManager; and TProofDataSetManagerFile have been renamed; TDataSetManager; and TDataSetManagerFile. New; functionality. Add support for session; queuing in the scheduler. This; allows to control the number of sessions allowed to process queries; concurrently. The feature is enabled by a new parameter 'queue:fifo' in; the 'xpd.schedparam'; directive. In case of static worker assignment; (default, random,; round-robin) the max number of running sessions can be limited by; another new parameter 'mxrun';; for; example;            ;     xpd.schedparam default; mxrun:3 queue:fifo; will run concurrently only 3 sessions. Additional requests are queued; and run as soon as one of the running; sessions goes idle. The current policy is FIFO, so that there is a; rotation among queued; sessions. In the case of load-based worker assignment, the max number; of running; queries is determined dynamically.; Add support for repeat functionality in the xrd.worker; directive. To avoid repeating the same line N times; one can just add; 'repeat=N'; in the line; for; example;            ;     xpd.worker worker; proofwrks:2093 repeat=4; will define 4 workers on port 2093 of machine 'proofwrks'.; Add support for port specification via the directive; 'xpd.port'; Enable variable; substitution in 'xpd.' directives using the standard; Scalla mechanism described in; http://xrootd.slac.stanford.edu/doc/dev/Syntax_config.htm .; Build also a binary named 'xproofd' which runs; a xrootd; daemon with only the XrdProofdProtocol (i.e. no data serving).; This simplifies setups when data serving is not needed and also allows; to better disantagle problems related to one specific protocol. The new; binary accepts the same arguments as 'xrootd' and parses the same; directives form the same configuration file, with the exception of; 'xpd.protocol xproofd libXrdProofd.so' which should now be dropped. AN; alternative port can be specified via the new 'xpd.port' direct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html:292,concurren,concurrently,292,proof/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html,7,"['concurren', 'load', 'queue']","['concurrently', 'load-based', 'queue', 'queued']"
Performance,". ; Proof. New functionality. TProofMgr. Add support for the following functionality:. sandbox file listing and browsing; sandbox file removal; file upload, download. TProofDraw. Allow to set a color, size, size, width for lines,; area, markers; the attributes are transmitted via the input list and; automatically derived from the ones of the chain. Support automatic creation of a dataset out of files; created on the worker nodes by worker processes. The implementation is; an extension of the functionality of the class TProofOutputFile used; for merging via file.; Add the possibility to enable/disable the tree cache and; to change its size on per-query base; two new parameters are available:. PROOF_UseTreeCache   ; Int_t       ; Enable (0) or Disable (1) the tree cache (default 1); PROOF_CacheSize      ; Long64_t     Cache size in bytes; (default 10000000). Examples:;        ; a) to disable the cache for the next run enter:;                                 ; proof->SetParameter(""PROOF_UseTreeCache"", 0);        ; b) to set the cache size to 20M;                                 ; proof->SetParameter(""PROOF_CacheSize"", 20000000);  Add the parameter; PROOF_UseParallelUnzip to toggle the use of the parallel unzip; (default off for now); to enable it add the following call;            ;            ;        ;  proof->SetParameter(""PROOF_UseParallelUnzip"", 1).  Add the possibility to give indications about; the number of workers at startup.;  E.g.;        1. To; start max 5 workers;             ; TProof::Open(""<master>"",""workers=5"");        2. To; start max 2 workers per physical machine;             ; TProof::Open(""<master>"",""workers=2x"");      This is useful in general when; running tests (equivalent but quicker then full startup;      followed by; TProof::SetParallel(n) or TProof::DeactivateWorker(...)).; Add support for the worker SysInfo_t in TSlaveInfo; (obtained via TProof::GetListOfSlaveInfos()); Add new submerger functionality to speed up the merging; phase. At the e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:617,cache,cache,617,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,2,['cache'],['cache']
Performance,". ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-by",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2465,optimiz,optimizations,2465,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,1,['optimiz'],['optimizations']
Performance,". An alloca instruction can be used to represent a function scoped stack slot,; but can also represent dynamic frame expansion. When representing function; scoped variables or locations, placing alloca instructions at the beginning of; the entry block should be preferred. In particular, place them before any; call instructions. Call instructions might get inlined and replaced with; multiple basic blocks. The end result is that a following alloca instruction; would no longer be in the entry basic block afterward. The SROA (Scalar Replacement Of Aggregates) and Mem2Reg passes only attempt; to eliminate alloca instructions that are in the entry basic block. Given; SSA is the canonical form expected by much of the optimizer; if allocas can; not be eliminated by Mem2Reg or SROA, the optimizer is likely to be less; effective than it could be. Avoid loads and stores of large aggregate type; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. LLVM currently does not optimize well loads and stores of large :ref:`aggregate; types <t_aggregate>` (i.e. structs and arrays). As an alternative, consider; loading individual fields from memory. Aggregates that are smaller than the largest (performant) load or store; instruction supported by the targeted hardware are well supported. These can; be an effective way to represent collections of small packed fields. Prefer zext over sext when legal; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. On some architectures (X86_64 is one), sign extension can involve an extra; instruction whereas zero extension can be folded into a load. LLVM will try to; replace a sext with a zext when it can be proven safe, but if you have; information in your source language about the range of an integer value, it can; be profitable to use a zext rather than a sext. Alternatively, you can :ref:`specify the range of the value using metadata; <range-metadata>` and LLVM can do the sext to zext conversion for you. Zext GEP indices to machine register width; ^^^^^^^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:3114,optimiz,optimize,3114,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,2,"['load', 'optimiz']","['loads', 'optimize']"
Performance,". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10477,optimiz,optimization,10477,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,1,['optimiz'],['optimization']
Performance,". Available Checkers. Available Checkers; The analyzer performs checks that are categorized into families or ""checkers"". The; default set of checkers covers a variety of checks targeted at finding security; and API usage bugs, dead code, and other logic errors. See the; Default Checkers list below. In addition to; these, the analyzer contains a number of ; Experimental (Alpha) Checkers. Writeups with examples of some of the bugs that the analyzer finds. Bug Finding With Clang: 5 Resources To Get You Started; Finding Memory Leaks With The LLVM/Clang Static Analyzer; Under the Microscope - The Clang Static Analyzer; Mike Ash - Using the Clang Static Analyzer. Default Checkers. Core Checkers model core language features and perform general-purpose checks such as division by zero, null pointer dereference, usage of uninitialized values, etc.; C++ Checkers perform C++-specific checks; Dead Code Checkers check for unused code; Nullability Checkers ; Optin Checkers ; OS X Checkers perform Objective-C-specific checks and check the use of Apple's SDKs (OS X and iOS); Security Checkers check for insecure API usage and perform checks based on the CERT Secure Coding Standards; Unix Checkers check the use of Unix and POSIX APIs. Core Checkers. Name, DescriptionExample. core.CallAndMessage; (C, C++, ObjC); Check for logical errors for function calls and Objective-C message expressions; (e.g., uninitialized arguments, null function pointers). // C; struct S {; int x;; };. void f(struct S s);. void test() {; struct S s;; f(s); // warn: passed-by-value arg contain uninitialized data; }. // C; void test() {; void (*foo)(void);; foo(); // warn: function pointer is uninitialized; }. // C; void test() {; void (*foo)(void);; foo = 0;; foo(); // warn: function pointer is null; }. // C++; class C {; public:; void f();; };. void test() {; C *pc;; pc->f(); // warn: object pointer is uninitialized; }. // C++; class C {; public:; void f();; };. void test() {; C *pc = 0;; pc->f(); // warn: objec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/available_checks.html:55,perform,performs,55,interpreter/llvm-project/clang/www/analyzer/available_checks.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/available_checks.html,2,['perform'],"['perform', 'performs']"
Performance,". Checker Developer Manual. This Page Is Under Construction; Checker Developer Manual; The static analyzer engine performs path-sensitive exploration of the program and; relies on a set of checkers to implement the logic for detecting and; constructing specific bug reports. Anyone who is interested in implementing their own; checker, should check out the Building a Checker in 24 Hours talk; (slides; video); and refer to this page for additional information on writing a checker. The static analyzer is a; part of the Clang project, so consult Hacking on Clang; and LLVM Programmer's Manual; for developer guidelines and post your questions and proposals to the; Static Analyzer subcategory at; the official LLVM Discourse server. Getting Started; Static Analyzer Overview. Interaction with Checkers; Representing Values. Idea for a Checker; Checker Registration; Events, Callbacks, and Checker Class Structure; Custom Program States; Bug Reports; AST Visitors; Testing; Useful Commands/Debugging Hints. Attaching the Debugger; Narrowing Down the Problem; Visualizing the Analysis; Debug Prints and Tricks. Additional Sources of Information; Useful Links. Getting Started. To check out the source code and build the project, follow steps 1-4 of; the Clang Getting Started; page.; The analyzer source code is located under the Clang source tree:; ; $ cd llvm/tools/clang. See: include/clang/StaticAnalyzer, lib/StaticAnalyzer,; test/Analysis.; The analyzer regression tests can be executed from the Clang's build; directory:; ; $ cd ../../../; cd build/tools/clang; TESTDIRS=Analysis make test. Analyze a file with the specified checker:; ; $ clang -cc1 -analyze -analyzer-checker=core.DivideZero test.c. List the available checkers:; ; $ clang -cc1 -analyzer-checker-help. See the analyzer help for different output formats, fine tuning, and; debug options:; ; $ clang -cc1 -help | grep ""analyzer"". Static Analyzer Overview; The analyzer core performs symbolic execution of the given program. All t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html:114,perform,performs,114,interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html,1,['perform'],['performs']
Performance,". Clang - C++ Defect Report Status. C++ Defect Report Support in Clang. C++ defect report implementation status; This page tracks which C++ defect reports are implemented within Clang. Number; Status; Issue title; Available in Clang?. 1; TC1; What if two using-declarations refer to the same function but the declarations introduce different default-arguments?; No. 2; drafting; How can dependent names be used in member declarations that appear outside of the class template definition?; Not resolved. 3; NAD; The template compilation model rules render some explicit specialization declarations not visible during instantiation; Yes. 4; CD1; Does extern ""C"" affect the linkage of function names with internal linkage?; Clang 2.8. 5; CD1; CV-qualifiers and type conversions; Clang 3.1. 6; NAD; Should the optimization that allows a class object to alias another object also allow the case of a parameter in an inline function to alias its argument?; Unknown. 7; NAD; Can a class with a private virtual base class be derived from?; Clang 3.4. 8; CD1; Access to template arguments used in a function return type and in the nested name specifier; Duplicate of 45. 9; CD1; Clarification of access to base class members; Clang 2.8. 10; CD1; Can a nested class access its own class name as a qualified name if it is a private member of the enclosing class?; Duplicate of 45. 11; CD1; How do the keywords typename/template interact with using-declarations?; Yes. 12; dup; Default arguments on different declarations for the same function and the Koenig lookup; Superseded by 239. 13; NAD; extern ""C"" for Parameters of Function Templates; No. 14; NAD; extern ""C"" functions and declarations in different namespaces; Clang 3.4. 15; dup; Default arguments for parameters of function templates; Yes. 16; CD1; Access to members of indirect private base classes; Clang 2.8. 17; NAD; Footnote 99 should discuss the naming class when describing members that can be accessed from friends; Yes. 18; NAD; f(TYPE) where ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html:806,optimiz,optimization,806,interpreter/llvm-project/clang/www/cxx_dr_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html,1,['optimiz'],['optimization']
Performance,". Clang - Features and Goals. Clang - Features and Goals. This page describes the features and goals of; Clang in more detail and gives a more broad explanation about what we mean.; These features are:. End-User Features:. Fast compiles and low memory use; Expressive diagnostics; GCC compatibility. Utility and Applications:. Library based architecture; Support diverse clients; Integration with IDEs; Use the LLVM 'BSD' License. Internal Design and Implementation:. A real-world, production quality compiler; A simple and hackable code base; A single unified parser for C, Objective C, C++,; and Objective C++; Conformance with C/C++/ObjC and their; variants. End-User Features. Fast compiles and Low Memory Use. A major focus of our work on clang is to make it fast, light and scalable.; The library-based architecture of clang makes it straight-forward to time and; profile the cost of each layer of the stack, and the driver has a number of; options for performance analysis. Many detailed benchmarks can be found online.; Compile time performance is important, but when using clang as an API, often; memory use is even more so: the less memory the code takes the more code you can; fit into memory at a time (useful for whole program analysis tools, for; example).; In addition to being efficient when pitted head-to-head against GCC in batch; mode, clang is built with a library based; architecture that makes it relatively easy to adapt it and build new tools; with it. This means that it is often possible to apply out-of-the-box thinking; and novel techniques to improve compilation in various ways. Expressive Diagnostics. In addition to being fast and functional, we aim to make Clang extremely user; friendly. As far as a command-line compiler goes, this basically boils down to; making the diagnostics (error and warning messages) generated by the compiler; be as useful as possible. There are several ways that we do this, but the; most important are pinpointing exactly what is wrong i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html:780,scalab,scalable,780,interpreter/llvm-project/clang/www/features.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html,2,"['perform', 'scalab']","['performance', 'scalable']"
Performance,". Cling Website. Cling Website; Interactive Compiler Interface. Home; Download; News; Docs. Documentation. Developer. Doxygen; Extensions; Error Recovery; Late Binding. End User. User Manual; Status of ObjectiveC[++]. Get Involved. Latest News; Cling goes public; July 25th, 2011; Cling was officially announced to the Clang community Read more. New website launched; July 1st, 2011; Welcome to the new website of the project. Read more; Useful Links. CERN. Cling. Welcome to our interactive C++ interpreter, built on the top of LLVM and Clang libraries. Its advantages over the standard interpreters are that it has command line prompt and uses just-in-time (JIT) compiler for compilation. An interactive prompt is usually referred to as a read eval print loop or repl. Many of the developers (e.g. Mono in their project called CSharpRepl) of such kind of software applications name them interactive compilers.; . One of Cling's main goals is to provide contemporary, high-performance alternative of the current C++ interpreter in the ROOT project - CINT. The backward-compatibility with CINT is major priority during the development.; ; How to use it; You can start typing not only C++ top level declaratons but statements, too. ; **** Welcome to the cling prototype! ****; * Type C code and press enter to run it *; * Type .q, exit or ctrl+D to quit *; *****************************************; [cling]$. Statements and expressions could take more than one input line. The interactive prompt changes from ""[cling]$"" to ""[cling]$ ?"".; ; [cling]$ #include ""math.h""; [cling]$ #include ""stdio.h""; [cling]$ for (unsigned i = 0; i < 5; ++i) {; [cling]$ ? printf(""%f\n"", sin(i));; [cling]$ ? }; 0.000000; 0.841471; 0.909297; 0.141120; -0.756802; Grammar; Cling is able to parse everything that clang can. Current clang status can be found here. At the moment, there are use cases only for C++ that's why cling is best in working with C++. Clang has support of C, objC, objC++ and we are looking forward t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/old/index.html:974,perform,performance,974,interpreter/cling/www/old/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/old/index.html,1,['perform'],['performance']
Performance,". Cling. Cling interprets C++. ****************** CLING ******************; * Type C++ code and press enter to run it *; * Type .q to exit *; *******************************************; [cling]$ #include <string>; [cling]$ std::string s(""abc"");; [cling]$ s.find('b'); (std::basic_string<char, std::char_traits<char>, std::allocator<char> >::size_type) 1; [cling]$. Cling is built on the top of LLVM and Clang libraries. In addition to standard interpreters it has a command line prompt and uses just-in-time (JIT) compiler. This kind of software application is commonly known as an interactive compiler.; ; Cling started off as a contemporary, high-performance alternative of the current C++ interpreter in the ROOT project - CINT.; ; Why interpreting C++ with Cling?. Learning C++ . ; One use case of cling is to aid the C++ learning process. Offering imediate feedback the user can easily get familiar with the structures and spelling of the language. ; . Creating scripts; ; The power of an interpreter lays as well in the compactness and ease of repeatedly running a small snippet of code - aka a script. This can be done in cling by inserting the bash-like style line: ; . #!/usr/bin/cling; . Rapid Application Development (RAD) . ; Cling can be used successfully for Rapid Application Development allowing for prototyping and proofs of concept taking advantage of dynamicity and feedback during the implementation process.; . Runtime-Generated Code ; ; Sometime it's convenient to create code as a reaction to input; (user/network/configuration).; Runtime-generated code can interface with C++ libraries.; . Embedding Cling . The functionality of an application can be enriched by embedding Cling. To embed Cling, the main program has to be provided. One of the things this main program has to do is initialize the Cling interpreter. There are optional calls to pass command line arguments to Cling. Afterwards, you can call the interpreter from any anywhere within the application. For compila",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/index.html:650,perform,performance,650,interpreter/cling/www/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/index.html,1,['perform'],['performance']
Performance,". Core Libraries; ROOT Error Handlers; There is a new rootrc variable which allows to control the; installation of the ROOT error handlers. By default the handlers; are activated:. Root.ErrorHandlers: 1. but setting the value to 0 result in no error handlers being installed; and the originals remaining in place. This can be useful if ROOT is used in; conjunction with other frameworks that already installed their own handlers. TString; TString::Hash() and thus also TMath::Hash() now use MurmurHash3_x64_128; from http://code.google.com/p/smhasher/ which is public domain.; To accelerate the hash in the case of pointers even further, pointers (and same-sized texts) are hashed using a simple bitwise xor.; This dramatically increases the hash performance for long texts, and still by a factor 5 for pointers.; The pointer case is most visible for certain I/O operations (TExMap).; TColor; Add the method SetAlpha() to set the alpha value (transparency; level) for an existing color. TStyle. The default font set by gStyle->SetLegendFont() was ignored. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v534/index.html:747,perform,performance,747,core/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v534/index.html,1,['perform'],['performance']
Performance,". Core. New class TBase64 providing Base64 encoding and decoding. Base64 encoded; messages are typically used in authentication protocols and to pack binary; data in HTTP or mail messages. New method in TSystem:. TString TSystem::GetFromPipe(const char *command). which executes ""command"" in the shell and returns the output in the TString.; Multi-line output is separated by \n's. Add proper support for Microsoft Visual C++ 9.0; Add support for 'unix' sockets on Windows.; New method TString::Clear() to reset the string but not to resize it to the default; (small) size. Useful when the string was pre-allocated to a large size and; has to be re-used.; Insure that ROOT's autoloader is always enabled whenever loading rootmap files.; Add function void TAttAxis::SetNdivisions(Int_t n1, Int_t n2, Int_t n3, Bool_t optim); ; Enable autoloading of typedef.; The statically linked roota executable and libRoot.a are currently; only supported on Linux platforms. We hope to extend this to MacOS X; soon. Meta. Add new macro ClassDefNV (ClassDef Non Virtual) which does not define any virtual function. ClassDef does define IsA, Streamer and ShowMember as virtual. This should be used only in classes that are never inherited from!; Improve performance of TClass::GetMethod (and friends). ACLiC. Implement TClassEdit::InsertStd() which puts ""std::"" in front of all STL classes.; The generated library now always checks with which version of ROOT the library was build and rebuilt the library if the running version of ROOT is different.; Add support for '+' character embedded in the script's name or directory name.; The dependency tracking file (script_C.d) is now always created when the library is built.; The dependency tracking file now records with which version of ROOT the library was built and the library is now rebuilt if it is loaded in a different version of ROOT. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v524/index.html:713,load,loading,713,core/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v524/index.html,3,"['load', 'perform']","['loaded', 'loading', 'performance']"
Performance,". Core; Build system and Platform support. The build system has been extended to support out-of-source builds.; This means you can build different sets of binaries from one source tree, like:. mkdir root-debug; cd root-debug; ../root/configure --build=debug. The build system now supports cross compilation, where typically you need tools; like rootcint, rootmap, etc, to be compiled on the build platform to be able; to generate dictionaries for the target platform. ROOT has been ported to iOS. This port required the above mentioned; changes in the build system. To build ROOT for the iPhone/iPad sumilator do:. ./configure iossim; make. To build a native iOS armv7 version do:. ./configure ios; make. Both builds create a libRoot.a that can be used to create ROOT based iOS apps; (iOS does not allow apps to load non-system dynamic libraries at run time).; Some sample Xcode projects using ROOT will soon be made available. Base. Change TTime data member from Long_t to Long64_t. On 32-bit systems the; Long_t is 32-bits and too small to keep the time in milliseconds since the ROOT EPOCH (1-1-1995). Added new operators:. operator long long(); operator unsigned long long(). The existing operators long and unsigned long on 32-bit machines return; an error in case the stored time is larger then 32-bit and truncation; occurs (like was always the case till now, but silently). New method ExitOnException() which allows to set the behaviour of; TApplication in case of an exception (sigsegv, sigbus, sigill, sigfpe).; The default is to trap the signal and continue with the event loop,; using this method one can specify to exit with the signal number to the; shell, or to abort() which in addition generates a core dump. New command line argument -x which forces ROOT to exit on an exception.; Add TSystem::AddDynamicPath. Build. New option '-t' for rmkdepend to allow the caller to fully specify the name to be used as a target; This supersedes the name calculated from the input file name and t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v528/index.html:812,load,load,812,core/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v528/index.html,1,['load'],['load']
Performance,". Currently, the only; way to do this is to manually check each place in your front-end where; GetElementPtr operators are created. It's not possible to write a checker which could find all rule violations; statically. It would be possible to write a checker which works by instrumenting; the code with dynamic checks though. Alternatively, it would be possible to; write a static checker which catches a subset of possible problems. However, no; such checker exists today. Rationale; =========. Why is GEP designed this way?; -----------------------------. The design of GEP has the following goals, in rough unofficial order of; priority:. * Support C, C-like languages, and languages which can be conceptually lowered; into C (this covers a lot). * Support optimizations such as those that are common in C compilers. In; particular, GEP is a cornerstone of LLVM's `pointer aliasing; model <LangRef.html#pointeraliasing>`_. * Provide a consistent method for computing addresses so that address; computations don't need to be a part of load and store instructions in the IR. * Support non-C-like languages, to the extent that it doesn't interfere with; other goals. * Minimize target-specific information in the IR. Why do struct member indices always use ``i32``?; ------------------------------------------------. The specific type i32 is probably just a historical artifact, however it's wide; enough for all practical purposes, so there's been no need to change it. It; doesn't necessarily imply i32 address arithmetic; it's just an identifier which; identifies a field in a struct. Requiring that all struct indices be the same; reduces the range of possibilities for cases where two GEPs are effectively the; same but have distinct operand types. What's an uglygep?; ------------------. Some LLVM optimizers operate on GEPs by internally lowering them into more; primitive integer expressions, which allows them to be combined with other; integer expressions and/or split into multiple separat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:19710,load,load,19710,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['load'],['load']
Performance,". Due to the self-containment of assemblies, they are very; practical to use when a container is hard to define due to possible; overlaps during positioning. For instance, it is very easy creating; honeycomb structures. A very useful example for creating and using; assemblies can be found at:; <http://root.cern.ch/root/html/examples/assembly.C.html>`.`. Creation of an assembly is very easy: one has just to create a; **`TGeoVolumeAssembly`** object and position the components inside as; for any volume:. ``` {.cpp}; TGeoVolume *vol = new TGeoVolumeAssembly(name);; vol->AddNode(vdaughter1, cpy1, matrix1);; vol->AddNode(vdaughter2, cpy2, matrix2);; ```. Note that components cannot be declared as ""overlapping"" and that a; component can be an assembly volume. For existing flat volume; structures, one can define assemblies to force a hierarchical structure; therefore optimizing the performance. Usage of assemblies does NOT imply; penalties in performance, but in some cases, it can be observed that it; is not as performing as bounding the structure in a container volume; with a simple shape. Choosing a normal container is therefore; recommended whenever possible. ![Assemblies of volumes](pictures/080001CF.png). ### Geometrical Transformations. All geometrical transformations handled by the modeller are provided as; a built-in package. This was designed to minimize memory requirements; and optimize performance of point/vector master-to-local and; local-to-master computation. We need to have in mind that a; transformation in **`TGeo`** has two major use-cases. The first one is; for defining the placement of a volume with respect to its container; reference frame. This frame will be called 'master' and the frame of the; positioned volume - 'local'. If `T` is a transformation used for; positioning volume daughters, then: `MASTER = T * LOCAL`. Therefore `T `is used to perform a local to master conversion, while; `T-1` for a master to local conversion. The second use case is the;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:90706,perform,performance,90706,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,2,['perform'],"['performance', 'performing']"
Performance,". For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6618,load,load-pass-plugin,6618,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['load'],['load-pass-plugin']
Performance,". GUI; TRootCanvas. In SetWindowSize the event queue is flushed to make sure; the window size change is really done. TRootContextMenu. When creating the dialog from the context menu, skip arguments that are pointers (but not char *) and have a default value. This should avoid confusing input fields in dialog.; Implemented online help in root dialogs (the dialog boxes used with contextual menus) via a new ""Online Help"" button. This opens a Root HTML browser at the right class/method location in the Root reference guide on the web.; The base url can be changed with the Browser.StartUrl option in system.rootrc (by default: http://root.cern.ch/root/html/ClassIndex.html); Added a small '?' on the right of the context menu entries, giving access to online help. TGMenu. Add possibility to add a right aligned shortcut by using a tab character ('\t') before the shortcut string, as shown below:; fMenuFile->AddEntry(""&Open...\tCtrl+O"", kOpenFile);; Use new way of adding right aligned shortcuts in the menu entries in most of the GUI classes using shortcuts in their menu. TGSlider. Added HandleConfigureNotify() to handle resizing events. New Browser. Automatically browse ROOT files if there is any open when starting the browser.; Correct system files manipulations (copy, rename, delete) and automatic update of the list tree. GUIHTML; TGHtmlBrowser. Added ability to display single picture from the web and to open pdf files with external viewer (Windows only); Implemented anchor navigation (e.g. http://root.cern.ch/root/html/TH1.html#TH1:Multiply). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/doc/v524/index.html:47,queue,queue,47,gui/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/doc/v524/index.html,1,['queue'],['queue']
Performance,". Help for ROOT's Reference Guide. Help for ROOT's Reference Guide. Contents. Parts of the Reference Guide. Type Index; Class Index; Inheritance. Modules. What they are; List of Modules; Modules' Library Dependencies. Class Reference. Sections; Link Box; Info Box; List of Data and Function Members. Display Options; Access (public / protected / private); Inheritance. Class Charts. Inheritance; Inherited Members; Includes; Libraries. Member Function Documentation. Parts of the Reference Guide; Type Index; All basic types and typedefs are documented in the Type Index. Class Index; All classes and namespaces are listed alphabetically in the Class Index,; along with a short info of what they are used for.; You can jump to a class's entry using the shortcuts ontop of the page.; They are optimized for quick access: they are distributed evenly, and short.; Classes are grouped in modules; these, too, are listed ontop of the list of classes. Inheritance; All classes along with their derived and base classes are shown in the; Class Hierarchy.; The documentation for each class also shows the inheritance diagram.; The class hierarchy is meant to give an overview of available classes; and their relations. Modules; What they are; Classes are grouped into modules. For ROOT, this is done on a per-directory basis:; each module corresponds to a sub-directory. In other cases one module might represent; one library. Either way, modules are meant to combine similar or related classes,; allowing users to find classes close by context. If you need some functionality that; you cannot find in a class you know, you might want to check for classes in the same; module - maybe one of them does what you need. List of Modules; Modules are listed ontop of the Class Index and as part; of the Library Dependencies Chart. Modules' Library Dependencies; Each module is assumed to be part of a library. The dependencies of libraries are; not only relevant for linking, but often reflect also the contextual d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/etc/html/HELP.html:792,optimiz,optimized,792,etc/html/HELP.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/etc/html/HELP.html,1,['optimiz'],['optimized']
Performance,". Histogram Libraries; TH1. Improve performances of TH1::Merge in case of histograms with same bin limits. Now an copy of the; initial histogram is not done. These improvements have been applied also in the TH2, TH3 and TProfile's classes.; . TH2. Add a new option ""S"" in FitSlice which performs a sliding merge: merge n consecutive bins along Y accordingly to what value in option Gn is given.; . TProfile2D and TProfile3D. Implement SetBins for variable bin sizes; ; Add support for variable bins in TProjectionXY. TH2Poly. The values set by SetMaximum() and SetMinimum() were not; taken into account by GetMaximum() and GetMinimum().; The Palette and the statistics box were not pickable when TH2Poly was drawn; with option COLZ.; TH2Poly was wrongly picked in the canvas area after a zoom along axis. TEfficiency. list holding the associated functions is created only on demand; default constructor creates two dummy histograms; can now be filled with weights (only Bayesian methods and the normal; approximation are supported) ; update TEfficiency::SavePrimitive to store also the set bits. TGraphAsymmErrors. add option to TGraphAsymmErrors::Divide for interpreting the given; histograms as ratio of Poisson means. TMultiGraph. The following macro did not show the x-axis in TimeDisplay mode. The; mg->GetYaxis()->UnZoom(); command erased the TimeDisplay attribute of; the axis. (fix from beischer@physik.rwth-aachen.de). {; TMultiGraph* mg = new TMultiGraph;; TGraph* g = new TGraph;; for (int i = 0; i < 100; i++) g->SetPoint(i, 1289420808+i, i+2);; mg->Add(g, ""P"");; mg->Draw(""AP"");; mg->GetXaxis()->SetTimeDisplay(1);; mg->GetYaxis()->UnZoom();; gPad->Modified();; gPad->Update();; }. TPaletteAxis. In TPaletteAxis::Paint() now makes sure the min and max of the; palette are not 0 when the histogram content is 0. on Ubuntu the following macro crashed. A char variable was too small. {; TCanvas *tmp = new TCanvas();; TH2F *h1 = new TH2F(""h1"",""h1"",40,0.,10.,40,1.e-2,1.e2);; h1->Fill(5,10);;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v532/index.html:36,perform,performances,36,hist/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v532/index.html,2,['perform'],"['performances', 'performs']"
Performance,". Histogram package. Modify implementation of the Fit function in the histograms and data classes to use a new common implementation, defined in the file HFitImpl.cxx. This file provides the implementation of the function used for bin fitting of the data objects of the histogram package, TH1's, TGraph, TMultiGraph, TGraph2D, and un-binned maximum likelihood fit for TTree data (used by the TTree::UnbinnedFit method). HFitInterface.h; Header file defining functions in the namespace ROOT::Fit providing functions required for fitting the data objects of the histogram package. These functions are used for example from other libraries like the FitPanel or the TTreePlayer for performing the fits.; . ROOT::Fit::FitObject: function for fitting the various data objects. The user must pass in addition to a pointer to the fit object, the fit options (via the FOption class and not a string), the minimizer options and the fit data range.; ; ROOT::Fit::FillData: function for filling the fit data from the histogram data objects. Used for fitting an histogram using the ROOT::Fit::Fitter class.; ; ROOT::Fit::UnBinFit:: function for fitting an unbinned data sets, for example obtained from TTree data.; . TBackCompFitter; New class providing a backward compatible implementation of the; TVirtualFitter using the new fitting class. It is wrapping the functionality of the ROOT::Math::Fitter and it; can be used to retrieve the fit information (result and; configuration) via the; TVirtualFitter API from FitConfig and FitResult. A static instance of this class; is created after calling the histograms and graph Fit methods in order to retrieve the full fit information after having fit. This instace will be deleted only when the next fit is executed.; This gives full backward compatibility functionality in fitting.; This class in addition to the TVirtualFitter provides the following functionality:; ; access direct to references to ROOT::Fit::FitResult and ROOT::FitConfig objects via the member fu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v522/index.html:678,perform,performing,678,hist/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v522/index.html,1,['perform'],['performing']
Performance,". Histogram package; All Histogram classes (including the TProfile's). Add support for filling histograms with negative weights. Now the correct global histogram statistics (Mean, Standard; deviation, etc..) is computed, while before the abs(w) was used when computing the histogram statistics.; ; Updated in all the projection function the check for the compatibility with previously existing histograms, now; the limits are checked using a tolerance/; Fix a bug in computing the point errors when fitting a 2d (or 3D) histogram with a 1d (or 2D) function. TH1. Add support for weighted likelihood fit of histogram by using a new option, WL and suppress the old option; LL.; The histogram must have the sum of the weight squared stored bin by bin to use this fit option; (i.e. TH1::Sumw2() has been called before filling).; Now one can perform likelihoof fit to weighted or scaled histograms and get the correct errors in the fit parameters.; (see bug report 79754).; ; Fix for the bug 82562.; Fix a bug in TH1::Merge for histogram with labels (bug 75902).; Fix few bugs related with the Buffer. . TProfile. Fix a bug in TProfile::Merge when the kCanRebin bit is set; (bug 79675).; Fix a bug in LabelsDeflate (bug 77149). TH1. Add new method TH3::Rebin3D and alsoRebinX, RebinY and RebinZ thanks to Zhiyi Liu. THistPainter. TPad::SetTheta() and TPad::SetPhi() did not cause the; canvas redrawing.; Protection added in case two histograms were plotted in the same pad; using the option BOX (the 2nd one with option SAME).; The clipping was not correct when an interactive zoom was performed. The 2D functions, for instance a fit function, associated to a 2D; histogram were always drawn as scatter plots. This was very confusing.; Now they are drawn as a surface if the histogram plotting option is a; lego or a surface or as a contour plot for any other plotting options. When drawing scatter plot for TH2 or TH2Poly do not use gRandom, but an independent random generator instance,; to avoid interfe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v530/index.html:837,perform,perform,837,hist/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v530/index.html,1,['perform'],['perform']
Performance,". However, those dependencies can be; removed at register renaming stage (at the cost of allocating register aliases,; and therefore consuming physical registers). Table *Average Wait times* helps diagnose performance issues that are caused by; the presence of long latency instructions and potentially long data dependencies; which may limit the ILP. Last row, ``<total>``, shows a global average over all; instructions measured. Note that :program:`llvm-mca`, by default, assumes at; least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency; instructions, the number of cycles spent while in the *ready* state is expected; to be very small when compared with the total number of cycles spent in the; scheduler's queue. The difference between the two counters is a good indicator; of how large of an impact data dependencies had on the execution of the; instructions. When performance is mostly limited by the lack of hardware; resources, the delta between the two counters is small. However, the number of; cycles spent in the queue tends to be larger (i.e., more than 1-3cy),; especially when compared to other low latency instructions. Bottleneck Analysis; ^^^^^^^^^^^^^^^^^^^; The ``-bottleneck-analysis`` command line option enables the analysis of; performance bottlenecks. This analysis is potentially expensive. It attempts to correlate increases in; backend pressure (caused by pipeline resource pressure and data dependencies) to; dynamic dispatch stalls. Below is an example of ``-bottleneck-analysis`` output generated by; :program:`llvm-mca` for 500 iterations of the dot-product example on btver2. .. code-block:: none. Cycles with backend pressure increase [ 48.07% ]; Throughput Bottlenecks:; Resource Pressure [ 47.77% ]; - JFPA [ 47.77% ]; - JFPU0 [ 47.77% ]; Data Dependencies: [ 0.30% ]; - Register Dependencies [ 0.30% ]; - Memory Dependencies [ 0.00% ]. Critical sequence based on the simulation:. Instr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:26224,perform,performance,26224,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance,". I/O Libraries; TFileCacheRead. Support for multiple TFileCacheRead per TFile.; Multiple TFileCacheRead per TFile are supported by augmenting the existing TFile::SetCacheRead() function with an optional TObject* argument specifying the owner (i.e. tree) of the cache. This function will assign a TFileCacheRead to a TFile for the given TTree. A cache can be removed by setting the pointer TFileCacheRead to 0.; Similarly, in TFile::GetCacheRead() an optional TObject* argument was added to obtain the TFileCacheRead from a TFile.; In addition to the unassigned TFileCacheRead pointer, TFile will maintain a map of tree specific cache pointers.; Backward compatibility in both functions is handled by making the TObject* argument optional. If it is not specified in the TFile::SetCacheRead() call, only the unassigned TFileCacheRead pointer is updated, otherwise the map and the unassigned cache are updated. In TFile::GetCacheRead(), if an owner is not specified or doesn't exist in the file's cache map, the unassigned cache is returned, unless it is 0 and there is exactly one entry in the cache map.; Distinguish counter for bytes read and read calls for learning phase. TFileMerger. Improve efficiency of TFileMerger when merging a single file by doing a TFile::Cp rather than a load/write of the objects.; In TFileMerger and hadd when objects can not be merged do not overwrite the last object in the set with the first!; Renable warning about not being able to merge objects in TFileMerger and hadd.; Fix hadd problem where the incremental merging fails if the TTree are stored in sub-directories.; Improve the code used for forward compatibility (record the type as TDirectory even-though the class is now TDirectoryFile) by delaying the switching of the class name until it is written (to the buffer). This avoids problem where a TKey is created (by TFile::mkdir) and then immediately used for reading (this happens in the incremental file merger). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v534/index.html:262,cache,cache,262,io/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v534/index.html,8,"['cache', 'load']","['cache', 'load']"
Performance,". I/O. Add support for the Chirp filesystem. To configure and build, chirp 3.2.2 must be installed.; When a TFile object is deleted, make sure that CINT also 'removes' any global variables that might point to it.; Fix support for the automatic addition to the current directory (for TTree and TH1 for example) in TKey::Read(TObject*).; In TKey, properly handle error in the I/O routines.; Explicitly check the validity of the zipped buffer before calling R__unzip, this allow for better error recovery.; When double checking whether a checksum difference is sustantial, ignore the std namespace. Use CompareContent also in the case of where; the class is versioned but the 'current' streamerInfo has not yet been built.; Prevent the I/O engine from mistakenly applying schema evolution to the TObject::fBits.; Make sure that when a streamer info of a base class is used to stream memberwise that is always not-optimized. If the StreamerInfo on file; has the same version as the StreamerInfo in memory but the one on file need to be 'not optimized' while the one in memory is not yet built, make; sure it will not be optimized.; Fix the reading of empty collection of object when reading without the library.; If the sequence of actions for streaming member-wise is not created correctly (i.e. where fReadMemberWise was null previously),; we now explicitly issue a Fatal error:. Fatal in <ReadSequence>: The sequence of actions to read AliESDVertex:7 member-wise was not initialized.; aborting. Add new optional parameter maxbuf to TXMLEngine::ParseFile() allowing the specification of the XML file size to be parsed. This fixes issue #78864.; Add function TBuffer::AutoExpand to centralize the automatic buffer extension policy. This enable the ability to tweak it later (for example instead of always doubling the size, increasing by only at most 2Mb or take hints from the number of entries already; in a TBasket).; Migrate the class TFileMerger from the proofplayer library to ROOT I/O library and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html:910,optimiz,optimized,910,io/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html,1,['optimiz'],['optimized']
Performance,. I/O; File Format; The default for streaming the content of STL containers was changed from object-wise; to member-wise. We evaluated the impact of moving to MemberWise streaming using 5 different CMS data files:. cms1.root an older split using level 99 Reco file; cms2.root a more recent non split raw data file; cms3.root a more recent non split Reco file; cms4.root is an example of the lepton plus jet analysis format known as a user PAT-tuple (split); cms5.root is an example AOD (analysis object dataset) format file. It is not split because the objects here are a strict subset of the RECO objects. We rewrote all the files using the v5.26/00 new basket clustering algorithm using both memberwise streaming and objectwise streaming. In the table below the read time is the CPU time to completion including loading the libraries.; When reading the file where always in the os cache (since we are focusing evaluating cpu time). The number of event actually read varies from file set to file set but was calibrated to result; in about 10s of cpu time. The files are read using a library generated with TFile::MakeProject. The object-wise files are between 2% and 10% larger compared to their member-wise counterpart. The CPU time for reading object-wise files is 12% higher for split files; and 30% higher for non-split files. So the improvement is significant enough to warrant switch the default from objectwise to memberwise. Split files. FilenameMemberwiseSizeCpu Time To read. cms1.root N 17.5 Gb 10.55s +/- 0.15 (2200 entries) ; cms1.root Y 16.8 Gb 9.12s +/- 0.08 (2200 entries) ; cms4.root N 1.47 Gb 10.18s +/- 0.19 (2500 entries) ; cms4.root Y 1.43 Gb 9.24s +/- 0.06 (2500 entries) . Non Split files. FilenameMemberwiseSizeCpu Time To read. cms2.root N 1.65 Gb 10.95s +/- 0.05 (1000 entries) ; cms2.root Y 1.53 Gb 8.20s +/- 0.05 (1000 entries) ; cms3.root N 0.780 Gb 10.59s +/- 0.05 (700 entries) ; cms3.root Y 0.717 Gb 8.29s +/- 0.08 (700 entries) ; cms5.root N 1.55 Gb 10.20s +/- 0.17 (,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html:814,load,loading,814,io/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html,2,"['cache', 'load']","['cache', 'loading']"
Performance,". I/O; Schema Evolution. Change TExMap hash, key and values from (U)Long_t to (U)Long64_t. This makes TExMap streamable in a portable way. On 64-bit platforms there is; no difference, but on 32-bit platforms all values will now be 64-bit. This fixes a big portability issue with THnSparse which uses TExMap internally; where the versions created on a 32-bit platform could not be read on a 64-bit platform and vice versa.; Avoid reporting I/O error for members of a class that is used only for a transient member; Concrete implementation of TClassGenerator needs to be updated to also avoid the warnings.; Fix the rule lookup based on checksum; Extend support of the schema evolution rules to fixed length array.; Prevent a process abort (due to a call to Fatal) when we are missing the dictionary for (one of) the; content of an STL collection when this collection is 'only' use has a transient member.; Fix the case where the source and target of a rule have the same name.; Avoid using the 'current' StreamerInfo to read an older streamerInfo that is missing (in case of corrupted files). Misc. New TFile plugin for the Hadoop Distributed File System (protocol hdfs:); Unregister stack objects from their TDirectory when the TList tries to delete them.; When streaming a base class without StreamerNVirtual() use an external streamer if it was set.; Many improvement to the I/O run-time performance.; DCache:; Increase readahead size from 8k to 128k and make it settable via DCACHE_RA_BUFFER env var.; dCap client does not ignore ?filetpye=raw and other options, so remove it. The function TFile::GetRelOffset is now public instead of protected.; Corrected the reading of the TFile record of large files.; MakeProject: several updates to improve support for CMS and Atlas data files (add support for auto_ptr, bitset, class name longer than 255 characters, etc.). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v526/index.html:1390,perform,performance,1390,io/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v526/index.html,1,['perform'],['performance']
Performance,". If any instruction should have either of; these flags set, it should be done within the target's InstrPostProcess class.; For an example, look at the `X86InstrPostProcess::postProcessInstruction` method; within `llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp`. A load/store barrier consumes one entry of the load/store queue. A load/store; barrier enforces ordering of loads/stores. A younger load cannot pass a load; barrier. Also, a younger store cannot pass a store barrier. A younger load; has to wait for the memory/load barrier to execute. A load/store barrier is; ""executed"" when it becomes the oldest entry in the load/store queue(s). That; also means, by construction, all of the older loads/stores have been executed. In conclusion, the full set of load/store consistency rules are:. #. A store may not pass a previous store.; #. A store may not pass a previous load (regardless of ``-noalias``).; #. A store has to wait until an older store barrier is fully executed.; #. A load may pass a previous load.; #. A load may not pass a previous store unless ``-noalias`` is set.; #. A load has to wait until an older load barrier is fully executed. In-order Issue and Execute; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; In-order processors are modelled as a single ``InOrderIssueStage`` stage. It; bypasses Dispatch, Scheduler and Load/Store unit. Instructions are issued as; soon as their operand registers are available and resource requirements are; met. Multiple instructions can be issued in one cycle according to the value of; the ``IssueWidth`` parameter in LLVM's scheduling model. Once issued, an instruction is moved to ``IssuedInst`` set until it is ready to; retire. :program:`llvm-mca` ensures that writes are committed in-order. However,; an instruction is allowed to commit writes and retire out-of-order if; ``RetireOOO`` property is true for at least one of its writes. Custom Behaviour; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Due to certain instructions not being expressed perfec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:42954,load,load,42954,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,['load'],['load']
Performance,". In that case, no; *other* input may share the same register as the input tied to the early-clobber; (even when the other input has the same value). You may only tie an input to an output which has a register constraint, not a; memory constraint. Only a single input may be tied to an output. There is also an ""interesting"" feature which deserves a bit of explanation: if a; register class constraint allocates a register which is too small for the value; type operand provided as input, the input value will be split into multiple; registers, and all of them passed to the inline asm. However, this feature is often not as useful as you might think. Firstly, the registers are *not* guaranteed to be consecutive. So, on those; architectures that have instructions which operate on multiple consecutive; instructions, this is not an appropriate way to support them. (e.g. the 32-bit; SparcV8 has a 64-bit load, which instruction takes a single 32-bit register. The; hardware then loads into both the named register, and the next register. This; feature of inline asm would not be useful to support that.). A few of the targets provide a template string modifier allowing explicit access; to the second register of a two-register operand (e.g. MIPS ``L``, ``M``, and; ``D``). On such an architecture, you can actually access the second allocated; register (yet, still, not any subsequent ones). But, in that case, you're still; probably better off simply splitting the value into two separate operands, for; clarity. (e.g. see the description of the ``A`` constraint on X86, which,; despite existing only for use with this feature, is not really a good idea to; use). Indirect inputs and outputs; """""""""""""""""""""""""""""""""""""""""""""""""""""". Indirect output or input constraints can be specified by the ""``*``"" modifier; (which goes after the ""``=``"" in case of an output). This indicates that the asm; will write to or read from the contents of an *address* provided as an input; argument. (Note that in this way, i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:214850,load,loads,214850,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,". Internals - do not select pad (aka gPad) for objects drawing, always use assigned pad painter; 32. Fix - properly save zoomed ranges in drawingJSON(); 33. Fix - properly redraw TMultiGraph; 34. Fix - show empty bin in TProfile2D if it has entries #316; 35. Fix - unzooming on log scale was extending range forevever; 36. Fix - do not force style 8 for hist markers; 37. Fix - ensure minimal hist title height; 38. Fix - disable Bloom effects on Android TGeo displays; 39. Fix - handle reordering of fragments in multipart reply #319; 40. Fix - properly show non-zero entries #320; 41. Fix - display empty hist bin if fSumw2 not zero. ## Changes in 7.7.4; 1. Fix - TGraph Y range selection, do not cross 0; 2. Fix - correctly handle `#font[id]` in latex; 3. Fix - store canvas with embed geometry drawing; 4. Fix - upgrade rollup and import.meta polyfill. ## Changes in 7.7.3; 1. Fix - correctly handle in I/O empty std::map; 2. Fix - reading of small (<1KB) ROOT files; 3. Fix - race condition in zstd initialization #318; 4. Fix - deployment with zstd #317. ## Changes in 7.7.2; 1. Fix - hide empty title on the canvas; 2. Fix - properly handle zooming in THStack histogram; 3. Fix - always use 0 as minimum in THStack drawings; 4. Fix - always show all ticks for labeled axis; 5. Fix - draw TProfile2D bins content as text, not entries; 6. Fix - interactive zooming on log color palette; 7. Fix - keyboard handling while input dialog active; 8. Fix - legend entry with not configured fill attributes; 9. Fix - prevent that color palette exceed graphical range; 10. Fix - exponential log axis labels with kMoreLogLabels bit set. ## Changes in 7.7.1; 1. Fix - properly select TF1 range after zooming; 2. Fix - TH1 y-range selection; 3. Fix - add 'gl' and svg2pdf-related packages to dependencies in package.json. ## Changes in 7.7.0; 1. Let plot current time, file creation or modification time with `&optdate=[1,2,3]` URL parameters; 2. Let plot file name, full file name or item name with `&optfil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:3181,race condition,race condition,3181,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['race condition'],['race condition']
Performance,". It is capable of forming; memory addresses of the following expression directly in integer instructions; (which use ModR/M addressing):. ::. SegmentReg: Base + [1,2,4,8] * IndexReg + Disp32. In order to represent this, LLVM tracks no less than 5 operands for each memory; operand of this form. This means that the ""load"" form of '``mov``' has the; following ``MachineOperand``\s in this order:. ::. Index: 0 | 1 2 3 4 5; Meaning: DestReg, | BaseReg, Scale, IndexReg, Displacement Segment; OperandTy: VirtReg, | VirtReg, UnsImm, VirtReg, SignExtImm PhysReg. Stores, and all other instructions, treat the four memory operands in the same; way and in the same order. If the segment register is unspecified (regno = 0),; then no segment override is generated. ""Lea"" operations do not have a segment; register specified, so they only have 4 operands for their memory reference. X86 address spaces supported; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. x86 has a feature which provides the ability to perform loads and stores to; different address spaces via the x86 segment registers. A segment override; prefix byte on an instruction causes the instruction's memory access to go to; the specified segment. LLVM address space 0 is the default address space, which; includes the stack, and any unqualified memory accesses in a program. Address; spaces 1-255 are currently reserved for user-defined code. The GS-segment is; represented by address space 256, the FS-segment is represented by address space; 257, and the SS-segment is represented by address space 258. Other x86 segments; have yet to be allocated address space numbers. While these address spaces may seem similar to TLS via the ``thread_local``; keyword, and often use the same underlying hardware, there are some fundamental; differences. The ``thread_local`` keyword applies to global variables and specifies that they; are to be allocated in thread-local memory. There are no type qualifiers; involved, and these variables can be pointed to with norma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:91777,perform,perform,91777,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,"['load', 'perform']","['loads', 'perform']"
Performance,". It may seem that ``LD1`` should suffice to perform vector loads on a big endian machine. However there are pros and cons to the two approaches that make it less than simple which register format to pick. There are two options:. 1. The content of a vector register is the same *as if* it had been loaded with an ``LDR`` instruction.; 2. The content of a vector register is the same *as if* it had been loaded with an ``LD1`` instruction. Because ``LD1 == LDR + REV`` and similarly ``LDR == LD1 + REV`` (on a big endian system), we can simulate either type of load with the other type of load plus a ``REV`` instruction. So we're not deciding which instructions to use, but which format to use (which will then influence which instruction is best to use). .. The 'clearer' container is required to make the following section header come after the floated; images above.; .. container:: clearer. Note that throughout this section we only mention loads. Stores have exactly the same problems as their associated loads, so have been skipped for brevity. Considerations; ==============. LLVM IR Lane ordering; ---------------------. LLVM IR has first class vector types. In LLVM IR, the zero'th element of a vector resides at the lowest memory address. The optimizer relies on this property in certain areas, for example when concatenating vectors together. The intention is for arrays and vectors to have identical memory layouts - ``[4 x i8]`` and ``<4 x i8>`` should be represented the same in memory. Without this property there would be many special cases that the optimizer would have to cleverly handle. Use of ``LDR`` would break this lane ordering property. This doesn't preclude the use of ``LDR``, but we would have to do one of two things:. 1. Insert a ``REV`` instruction to reverse the lane order after every ``LDR``.; 2. Disable all optimizations that rely on lane layout, and for every access to an individual lane (``insertelement``/``extractelement``/``shufflevector``) reverse the lane ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:4526,load,loads,4526,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loads']
Performance,". It's inadvisable to store raw profiles for; long periods of time. * Tools must retain **backwards** compatibility with indexed profile formats.; These formats are not forwards-compatible: i.e, a tool which uses format; version X will not be able to understand format version (X+k). * Tools must also retain **backwards** compatibility with the format of the; coverage mappings emitted into instrumented binaries. These formats are not; forwards-compatible. * The JSON coverage export format has a (major, minor, patch) version triple.; Only a major version increment indicates a backwards-incompatible change. A; minor version increment is for added functionality, and patch version; increments are for bugfixes. Impact of llvm optimizations on coverage reports; ================================================. llvm optimizations (such as inlining or CFG simplification) should have no; impact on coverage report quality. This is due to the fact that the mapping; from source regions to profile counters is immutable, and is generated before; the llvm optimizer kicks in. The optimizer can't prove that profile counter; instrumentation is safe to delete (because it's not: it affects the profile the; program emits), and so leaves it alone. Note that this coverage feature does not rely on information that can degrade; during the course of optimization, such as debug info line tables. Using the profiling runtime without static initializers; =======================================================. By default the compiler runtime uses a static initializer to determine the; profile output path and to register a writer function. To collect profiles; without using static initializers, do this manually:. * Export a ``int __llvm_profile_runtime`` symbol from each instrumented shared; library and executable. When the linker finds a definition of this symbol, it; knows to skip loading the object which contains the profiling runtime's; static initializer. * Forward-declare ``void __llvm_profi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst:14953,optimiz,optimizer,14953,interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,1,['optimiz'],['optimizer']
Performance,". Lowers to a call to `objc_destroyWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-destroyweak-id-object>`_. '``llvm.objc.initWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.initWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_initWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-initweak>`_. '``llvm.objc.loadWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.loadWeak(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_loadWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-loadweak>`_. '``llvm.objc.loadWeakRetained``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.loadWeakRetained(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_loadWeakRetained <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-loadweakretained>`_. '``llvm.objc.moveWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.moveWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_moveWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-moveweak-id-dest-id-src>`_. '``llvm.objc.release``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.release(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_release <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-release-id-value>`_. '``llvm.objc.retain``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.retain(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_retain <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-retain>`_. '``llvm.objc.retainAutorelease``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:969181,load,loadweakretained,969181,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loadweakretained']
Performance,". MATLAB; A MATLAB/ROOT interface has been developped by Johannes Kissel.; It is a data interface for reading and writing ROOT files from within MATLAB; just as you do with MATLAB files. For more information, see this announcement at the ROOT Forum. PyROOT. Support for python 2.6 has been added. Older versions will compile fine, but will; have problems when using the buffer interface for C arrays. For user convenience, code was added to load a custom rootlogon.py/.C, if available.; This code is loaded on first use of the ROOT module, and the python rootlogon.py is; loaded as a module. The language was improved by added a _creates property to all MethodProxy methods.; By setting this value to True, objects returned by such methods will be owned (and; reference counted) by the python interpreter.; By default, the Clone() and DrawClone() methods will have _create equal to True. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v522/index.html:441,load,load,441,bindings/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v522/index.html,3,['load'],"['load', 'loaded']"
Performance,". Math Libraries. MathCore. Various fixes have been applied in the fitting classes:. Fix issue #46006 for normalization of error resulting from fitting a TGraph; Fix a problem in Chi2 calculation in case of overflow; Fix issue #46601 for avoiding crashes when a linear fit fails.; Fix in the FitData classes the bug #45909 occurring when setting a function range outside histogram range; Fix default integration method to be Gauss algorithm of MathCore instead of the GSL method, when libMathmore is not built or when the plug-in manager fails to load it.; Add a protection against negative log when fitting using the Poisson log likelihood function; Improve calculation of derivative in x for fitted function. This fixes some problem observed when fitting using the error on the coordinates.; Fitter class: add new methods for calculating the error matrix after minimization, Fitter::CalculateHessErrors() and for calculating the Minos errors Fitter::CalculateMinosErrors; FitConfig: add in the configuration the possibility to select a sub-set of the parameters for calculating the Minos errors by using the method FitConfig::SetMinosErrors( listOfParameters ). If no list is passed, by default the Minos error will be computed on all parameters.; UnBinData class: add new constructor for creating a unbin data set passing a range to select the data and copy in the internal array; FitResult: the class now stores a map of the Minos error using as key the parameter index. If the Minos error has not been calculated for the parameter, FitResult::LowerError(i) and FitResult::UpperError(i) returns the parabolic error; ; Add a new class, MinimTransformFunction to perform a transformation of the function object to deal with limited and fixed variables.; This class uses the same transformation which are also used inside Minuit, a sin transformation for double bounded variables and a sqrt transformation for single bound variable defined in the class MinimizerVariableTransformation.; These classes",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v524/index.html:547,load,load,547,math/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v524/index.html,1,['load'],['load']
Performance,". Math Libraries. MathCore; MathCore includes now classes which were previously contained in libCore, like TMath, TComplex and the TRandom classes. Furthermore, some of the algorithms implemented in the TF1 class have been moved to MathCore. This implies that all other ROOT library using one of these classes, such as libHist, have a direct dependency on the Mathcore library.; Linking with libMathCore is therefore required for running any major ROOT application. It has been added to the list of libraries obtained when doing root-config --libs. N.B.: users building ROOT applications and not using root-config MUST add libMathCore to their list of linking libraries. Together with the libraries merge, many changes have been applied to both TMath and the other mathcore classes.; TMath; A major clean-up and re-structuring has been done for the functions present in TMath. Some functions have been implemented using the STL algorithms, which have better performances in term of CPU time and a template interface has been also added.; Some of the basic special mathematical functions of TMath, like the error function or the gamma and beta functions use now the Cephes implementation from Stephen L. Moshier, which is used as well by the ROOT::Math functions. This implementation has been found to be more accurate and in some cases more efficient in term of CPU time. More detailed information on the new mathematical functions can be found in this presentation from M. Slawinska at a ROOT team meeting. define the functions as template functions instead of having the same re-definition for all the various basic types. This is done for TMath::Mean,TMath::GeomMean, TMath::Median, TMath::KOrdStat; Use STL to implement the following algorithms:; ; TMath::Sort is re-implemented using std::sort.; TMath::BinarySearch is re-implemented using the STL algorithm std::lower_bound. The STL algorithms have been found for these cases to be perform better in term of CPU time. For some other algorithms l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v520/index.html:958,perform,performances,958,math/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v520/index.html,1,['perform'],['performances']
Performance,". Math Libraries; MathCore; TMath. Add a new function TMath::Power(double, int) impelmented using std::pow(double,int) which is 100% faster than the; double version. KDTree. New KDTree class from C. Gumpert. which has different splitting rules using the data population. The splitting can; is based on the basket population, but any function of the data can be used. For example, in case of weighted data one can split according to; the basket total weight or the effective entries in the basket. In this way the class can support weighted data; sets. The splitting of the TKDTree class is instead fixed and based on the basket number of entries.; The tree can also be frozen to fix the splitting and behaving like a multi-dim histograms with bins with variables; hyper-volumes.; Auxiliary classes are provided like the node classes or the data point class, which can exists also with a compile timed fixed dimension; for better performances. . Fitter classes. Provide support for weighted likelihood unbinned fits; Provide support for extended likelihood unbinned fits; Provide support for not-extended binned likelihood fits (i.e. multinomial distribution instead of Poisson for bin; contents); In case of binned likelihood fit build a Poisson or (Multinomial) likelihood ratio with the saturated model. So a; constant term is subtracted to the likelihood. The formulae described in the Baker and Cousins; paper (N.I.M. 221 (1984) 437) are now used. The obtained negative likelihood ratio value from the fit and multiplied; by a factor 2 is now asymptotically distributed as a chi square. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v532/index.html:929,perform,performances,929,math/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v532/index.html,1,['perform'],['performances']
Performance,". MonteCarlo Libraries; VMC. Update of the VMC interfaces for multi-threading:; TVirtualMC and TVirtualMCApplication instances are now declared thread local.; Added new functions in TVirtualMC and TVirtualMCApplication for multi-threading; applications with default implementation. ; In TVirtualMC:. virtual Bool_t IsMT() const { return kFALSE; }. In TVirtualMCApplication:. virtual TVirtualMCApplication* CloneForWorker() const { return 0;}; virtual void InitForWorker() const {}; virtual void BeginWorkerRun() const {}; virtual void FinishWorkerRun() const {}; virtual void Merge(TVirtualMCApplication* /*localMCApplication*/) {}. Removed default implementation of newly added functions in TVirtualMC:. virtual Bool_t IsRootGeometrySupported() const = 0;; virtual Bool_t GetMaterial(Int_t imat, TString& name,...) = 0;; virtual Bool_t CurrentBoundaryNormal(..) = 0;. Removed deprecated functions from TVirtualMC:. // Return parameters for material specified by material number imat; // Deprecated - replaced with GetMaterial(); virtual void Gfmate(Int_t imat, char *name, Float_t &a, Float_t &z,; Float_t &dens, Float_t &radl, Float_t &absl,; Float_t *ubuf, Int_t &nbuf) = 0;. // Return parameters for material specified by material number imat; // (in double precision); // Deprecated - replaced with GetMaterial(); virtual void Gfmate(Int_t imat, char *name, Double_t &a, Double_t &z,; Double_t &dens, Double_t &radl, Double_t &absl,; Double_t *ubuf, Int_t &nbuf) = 0;. // Check the parameters of a tracking medium; // Deprecated; virtual void Gckmat(Int_t imed, char *name) = 0;. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/montecarlo/doc/v534/index.html:62,multi-thread,multi-threading,62,montecarlo/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/montecarlo/doc/v534/index.html,2,['multi-thread'],['multi-threading']
Performance,. NET release notes. XROOTD. Fixes:; . Fix race condition that would disable connections; . ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v522/index.html:43,race condition,race condition,43,net/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v522/index.html,1,['race condition'],['race condition']
Performance,". Networking. Class TWebFile now supports proxies. Set the proxy URL either via; static method:; TWebFile::SetProxy(const char *url);; or via the shell variable http_proxy, like is used for e.g. wget. Class TWebFile now supports the basic authentication (AuthType Basic) scheme.; The user name and password can be specified in the URL like this:; http://username:mypasswd@pcsalo.cern.ch/files/aap.root. XROOTDNew version 20090610-0430ImprovementsAdd the possibility of using the xrd command line from; batch scriptsAdd support for Adler32 checksum calculation of; a local unix file (including stdin) and file on a remote xrootd data; server.Add support for the so-called Xtreme copy, allowing xrdcp; to read multiple chunks from several servers, in parallel.Add possibility to use a different version of a given C++; compiler or linker (--with-cxx=..., etc)Increase flexibility in configuring openssl and openafs; supportIn GSI authentication, automatize the loading of CRL; the; information; about the URI is looked for either in the dedicated extension on the CA; certificate or from the file ""<CA hash>.crl_url"" and the; file; automatically downloaded and transformed in PEM formatFixesServer sideFix wrong reporting of the refresh option for LocateFix incorrect propagation of selected nodesPrevent potential long duration loop (15 mins) after client disconnectionsAvoid potential deadlocks when trying to remove a node from a clusterCorrect matching of incoming connection with previously dropped connectionCorrect export of cluster identificationCorrectly propagate information about files that could not be stagedPrevent endsess deadlock when parallel streams stall due to large WAN RTTFix infinite wait for primary login that will never; happen if you are a manager without a meta-managerPrevent annoying (but not deadly) infinite loop should a; server go offline that is subject to a locate request display.Client sideBetter handling of errno, especially for parallel streamsAllow the client ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v524/index.html:959,load,loading,959,net/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v524/index.html,1,['load'],['loading']
Performance,". Networking. TMessage now has schema evolution and TRef (i.e. ProcessID) support. To; enable; schema evolution for all messages call; TMessage::EnableSchemaEvolutionForAll(kTRUE).; To enable it only for a specific message call; mess->EnableSchemaEvolution(kTRUE).; The default for schema evolution is off. The streamer and process id; information are send only once per socket (and is supported for all; types of; sockets, TSocket, TPSocket and TXSocket). If you communicate between; two; ROOT based applications, check the version numbers on both sides. If; they; are not the same enable the schema evolution support (in case ROOT; objects; are transferred). . XROOTD. New version 20080621-0000 containing several improvements and fixes; Server:. New daemon 'cmsd' supposed to replace 'olbd' with improved performances; Improved polling strategy; Fix problem with handling writev creating unjustified disconnections ; Fix problem with setrlimit on MacOsX Leopard. Client:; ; Fix a nasty memory leak in XrdClientCacheRead affecting; processing via TChain; Optimized file closing recipe; Fix; potential cache thrashing problem with big blocks requests. Fixes / improvements in the GSI plug-in:; ; support for large (> 32 bits) certificate serial; numbers in CRL handling; support for an external function for DN-to-username; mapping function; provide example for an LDAP based search; fixed a few problem with return code checking. netx. TXNetFile:; . Enable dynamic cache size synchronization; ; Enable per-instance control of the cache parameters; also for RAW files; by; default cache is OFF for these files, but there maybe cases in which the cache can; improve performances.; Remove call to XrdClient::Sync in SysStat. Correctly honor the create/recreate options coming from TFile::Open(); Allow the size of the (written) file to be retrieved after the Close (solves several reported file size mismatches).; . TXNetSystem:; ; Fix problem with GetDirEntry: the entry object was; going out-of-scope",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html:808,perform,performances,808,net/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html,1,['perform'],['performances']
Performance,". Note that a store-store fence is not sufficient to; implement Release semantics; store-store fences are generally not exposed to; IR because they are extremely difficult to use correctly. AcquireRelease; --------------. AcquireRelease (``acq_rel`` in IR) provides both an Acquire and a Release; barrier (for fences and operations which both read and write memory). Relevant standard; This corresponds to the C++/C ``memory_order_acq_rel``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation, and vice versa. Notes for optimizers; In general, optimizers should treat this like a nothrow call; the possible; optimizations are usually not interesting. Notes for code generation; This operation has Acquire and Release semantics; see the sections on Acquire; and Release. SequentiallyConsistent; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:14496,load,loads,14496,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,". Note that we; describe this in the `instruction selection section`_ because it operates on; a `SelectionDAG`_. 3. `SSA-based Machine Code Optimizations`_ --- This optional stage consists of a; series of machine-code optimizations that operate on the SSA-form produced by; the instruction selector. Optimizations like modulo-scheduling or peephole; optimization work here. 4. `Register Allocation`_ --- The target code is transformed from an infinite; virtual register file in SSA form to the concrete register file used by the; target. This phase introduces spill code and eliminates all virtual register; references from the program. 5. `Prolog/Epilog Code Insertion`_ --- Once the machine code has been generated; for the function and the amount of stack space required is known (used for; LLVM alloca's and spill slots), the prolog and epilog code for the function; can be inserted and ""abstract stack location references"" can be eliminated.; This stage is responsible for implementing optimizations like frame-pointer; elimination and stack packing. 6. `Late Machine Code Optimizations`_ --- Optimizations that operate on ""final""; machine code can go here, such as spill code scheduling and peephole; optimizations. 7. `Code Emission`_ --- The final stage actually puts out the code for the; current function, either in the target assembler format or in machine; code. The code generator is based on the assumption that the instruction selector will; use an optimal pattern matching selector to create high-quality sequences of; native instructions. Alternative code generator designs based on pattern; expansion and aggressive iterative peephole optimization are much slower. This; design permits efficient compilation (important for JIT environments) and; aggressive optimization (used when generating code offline) by allowing; components of varying levels of sophistication to be used for any step of; compilation. In addition to these stages, target implementations can insert arbitrary; ta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:6894,optimiz,optimizations,6894,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimizations']
Performance,". Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation, and vice versa. Notes for optimizers; In general, optimizers should treat this like a nothrow call; the possible; optimizations are usually not interesting. Notes for code generation; This operation has Acquire and Release semantics; see the sections on Acquire; and Release. SequentiallyConsistent; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15007,perform,performance,15007,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['perform'],['performance']
Performance,". Overview:; """""""""""""""""". The '``atomicrmw``' instruction is used to atomically modify memory. Arguments:; """""""""""""""""""". There are three arguments to the '``atomicrmw``' instruction: an; operation to apply, an address whose value to modify, an argument to the; operation. The operation must be one of the following keywords:. - xchg; - add; - sub; - and; - nand; - or; - xor; - max; - min; - umax; - umin; - fadd; - fsub; - fmax; - fmin; - uinc_wrap; - udec_wrap. For most of these operations, the type of '<value>' must be an integer; type whose bit width is a power of two greater than or equal to eight; and less than or equal to a target-specific size limit. For xchg, this; may also be a floating point or a pointer type with the same size constraints; as integers. For fadd/fsub/fmax/fmin, this must be a floating point type. The; type of the '``<pointer>``' operand must be a pointer to that type. If; the ``atomicrmw`` is marked as ``volatile``, then the optimizer is not; allowed to modify the number or order of execution of this; ``atomicrmw`` with other :ref:`volatile operations <volatile>`. Note: if the alignment is not greater or equal to the size of the `<value>`; type, the atomic operation is likely to require a lock and have poor; performance. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. If unspecified, the alignment is assumed to be equal to the; size of the '<value>' type. Note that this default alignment assumption is; different from the alignment used for the load/store instructions when align; isn't specified. A ``atomicrmw`` instruction can also take an optional; "":ref:`syncscope <syncscope>`"" argument. Semantics:; """""""""""""""""""". The contents of memory at the location specified by the '``<pointer>``'; operand are atomically read, modified, and written back. The original; value at the location is returned. The modification is specified by the; operation argument:. - xchg: ``*ptr = val``; - add: ``*ptr = *ptr + val``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:430196,optimiz,optimizer,430196,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,". Overview:; """""""""""""""""". This intrinsic loads a 32-bit value from the address ``%ptr + %offset``,; adds ``%ptr`` to that value and returns it. The constant folder specifically; recognizes the form of this intrinsic and the constant initializers it may; load from; if a loaded constant initializer is known to have the form; ``i32 trunc(x - %ptr)``, the intrinsic call is folded to ``x``. LLVM provides that the calculation of such a constant initializer will; not overflow at link time under the medium code model if ``x`` is an; ``unnamed_addr`` function. However, it does not provide this guarantee for; a constant initializer folded into a function body. This intrinsic can be; used to avoid the possibility of overflows when loading from such a constant. .. _llvm_sideeffect:. '``llvm.sideeffect``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.sideeffect() inaccessiblememonly nounwind willreturn. Overview:; """""""""""""""""". The ``llvm.sideeffect`` intrinsic doesn't perform any operation. Optimizers; treat it as having side effects, so it can be inserted into a loop to; indicate that the loop shouldn't be assumed to terminate (which could; potentially lead to the loop being optimized away entirely), even if it's; an infinite loop with no other side effects. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". This intrinsic actually does nothing, but optimizers must assume that it; has externally observable side effects. '``llvm.is.constant.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use llvm.is.constant with any argument type. ::. declare i1 @llvm.is.constant.i32(i32 %operand) nounwind memory(none); declare i1 @llvm.is.constant.f32(float %operand) nounwind memory(none); declare i1 @llvm.is.constant.TYPENAME(TYPE %operand) nounwind memory(none). Overview:; """""""""""""""""". The '``llvm.is.constant``' intrinsic will return true if the argument; is known to be a manifest compile-time c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:951900,perform,perform,951900,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,". PROOF System. Added functionality; ; Added interface to simplify the creation of the performance; tree: two new methods TProof::SetPerfTree(""<file>"") and; TProof::SavePerfTree(""<file>"", ""<queryref>"") allow set; and/or save the information to a given file path. The perfomance tree; settim=ngs are diabled after each query, so they need to be enabled; each time.; Add support for a command line test run of 'proofserv'; this is; useful to test that the environment is setup correctly.; In TProofBench::DrawCPU, add possibility to extract of a couple; of numbers supposed to give an idea of the computing specs of the; cluster being benchmarked. These are the maximum rate for the standard; CPU intensive task and the normalized, per worker, rate. Both are; expressed in RNGPS (RaNdom Generation Per Second).; Add class TProofPerfAnalysis collecting a set of tools to; analyse the performance tree.; Add support for selector-by-object processing in PROOF. The; selector object, created and configured locally by the user, is added; to the input list and recuperated from there on the worker machines for; processing. Any input list setting in the selector itself is not; streamed but temporarly moved to then standard input list, so that user; can use the selector input list as container of processing information; if they find convenient to do so. Process(...) methods with the file; name argument replaced by 'TSelector *' have  introduced where; relevant (TProof, TProofPlayer and their derivatives, TDSet).  ; Add the possibility to force submerging at node level, i.e. one; submerger per physical machine. In this way the network traffic can be; minimized, for example when merging large output files. The new feature; is enabled by setting the Int_t parameter 'PROOF_MergersByHost' (or the; directive 'Proof.MergersByHost') to a non-null value.; Simplify enabling of basic feedback. In TProof::Process, add; support for switches ""fb=name1,name2,name3,... "" or; ""feedback=name1,name2,name3,... """,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v534/index.html:87,perform,performance,87,proof/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v534/index.html,2,['perform'],['performance']
Performance,". PROOF. New functionality. ; Dataset management. ; New class TProofDataSetManager definining the interface; of PROOF to dataset metainfo database; New class TProofDataSetManagerFile implementating; TProofDataSetManager using the file system as back-end;  the; separation is needed to load dataset menagers using different backends;; for example ATLAS foresees to have a MySQL-based implementation.; The instance of the appropriate TProofDataSetManager is; instantiated via the plugin manager; by default an instance; of TProofDataSetManagerFile; managing the <sandbox>/datasets; area is created. The directive 'Proof.DataSetManager' can be used to; modify the settings for TProofDataSetManagerFile or to load a; different dataset manager; for example, to '/pool/datasets' as area for; the dataset information, the following directive can be added to the; xrootd config file; xpd.putrc Proof.DataSetManager file dir:/pool/datasets. Interface to TProofMgr::GetSessionLogs() in the dialog; box. The graphics layout of the logbox has been re-designed, with new; buttons to grep the logs and to save them to a file. It is also; possible to choose the range of lines to be displayed and the subset of; nodes.; ; Support for connection control base on the UNIX group; (new directive 'xpd.allowedgroups; <grp1>,<grp2>, ...'). Improvements:. ; In the case of mismatch between the expected and actual; number of processed events, send back to the client the list of failed; packets.; Implement the classic strategy of the TPacketizer in; TPacketizerAdaptive; the strategy can be changed from adaptive; (default) to TPacketizer with: ""PROOF_PacketizerStrategy"" parameter to; PROOF; The max workers per node can now be also set in the; xrootd config file with.        xpd.putrc ; Packetizer.MaxWorkersPerNode: <desired number>. Make fCacheDir and fPackageDir controllable via directive; . Fixes. ; Two memory leaks in TProofServ affecting repeated runs; withing the same session. Fix a problem cleaning-up the in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v520/index.html:285,load,load,285,proof/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v520/index.html,2,['load'],['load']
Performance,". PyROOT. Null-pointers now carry type, rather than being the None object, to make sure; that correct overloads are selected.; Memory policy is settable on individual functions, rather than only globally,; through the _mempolicy data member that functions carry. In order to support PyPy analysis of PyROOT code, getter/setter methods have; been added to the proxies.; The pydoc tool already benefits from this, since PyROOT objects are now a bit; easier to inspect by such standard tools. By short-circuiting some paths during class proxy creation, loading of the; libPyROOT module is now faster. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v524/index.html:550,load,loading,550,bindings/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v524/index.html,1,['load'],['loading']
Performance,". PyROOT. This release contains two big new features: the ability to use PROOF with; python, and the ability to pickle (python serialize) ROOT objects.; Pickling of ROOT objects is straightforward: just hand them to pickle (or; cPickle) like any other python object.; To use PROOF with python, derive your custom class from TPySelector, override; the methods that you want to specialize, and put it in a file that is shipped; to the worker nodes, e.g.:. from ROOT import TPySelector. class MyPySelector( TPySelector ):; def Begin( self ):; print 'py: beginning'. def SlaveBegin( self, tree ):; print 'py: slave beginning'. def Process( self, entry ):; self.fChain.GetEntry( entry ); print 'py: processing', self.fChain.ipi; return 1. def SlaveTerminate( self ):; print 'py: slave terminating'. def Terminate( self ):; print 'py: terminating'. The file containing the class (e.g. mymodule.py) will be treated as a; python module and should be loadable through PYTHONPATH (typically '.') at; the worker node.; Setup PROOF as normal, and call:. dataset.Process( 'TPySelector', 'mymodule' ). PROOF will instantiate a TPySelector instance, which will in turn pick up; the python class from module 'mymodule' and forward all calls. There are several improvements in language mappings, as well as cleanup of; the code for python2.2 (Py_ssize_t handling) and MacOS 10.3. Additionally,; there are code cleanups (removing use of CINT internals) that should be; fully transparent to the end-user. The language mapping improvements are:. Abstract classes can no longer be instantiated (__init__ will raise an exception); Looping over empty STL(-like) containers will yield an immediate StopIteration; Unknown& is favored over Unknown* in function overloading; Implemented unary-, unary+ (__neg__ and __pos__); Mapped operator bool() to __nonzero__; Support for templated member functions; Implemented __setitem__ for unsigned int& and unsigned long& returns. The python presentation of ROOT objects (ObjectProxy) ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v520/index.html:942,load,loadable,942,bindings/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v520/index.html,1,['load'],['loadable']
Performance,. RWebWindow latency test. Halt; Draw; Main: Msg. ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/ping/ping.html:13,latency,latency,13,tutorials/webgui/ping/ping.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/ping/ping.html,1,['latency'],['latency']
Performance,". Reads a vector from memory according to the provided mask. The mask holds a bit for each vector lane, and is used to prevent memory accesses to the masked-off lanes. The masked-off lanes in the result vector are taken from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the alignment of the source location. It must be a power of two constant integer value. The third operand, mask, is a vector of boolean values with the same number of elements as the return type. The fourth is a pass-through value that is used to fill the masked-off lanes of the result. The return type, underlying type of the base pointer and the type of the '``passthru``' operand are the same vector types. Semantics:; """""""""""""""""""". The '``llvm.masked.load``' intrinsic is designed for conditional reading of selected vector elements in a single IR operation. It is useful for targets that support vector masked loads and allows vectorizing predicated basic blocks on these targets. Other targets may support this intrinsic differently, for example by lowering it into a sequence of branches that guard scalar load operations.; The result of this operation is equivalent to a regular vector load instruction followed by a 'select' between the loaded and the passthru values, predicated on the same mask. However, using this intrinsic prevents exceptions on memory access to masked-off lanes. ::. %res = call <16 x float> @llvm.masked.load.v16f32.p0(ptr %ptr, i32 4, <16 x i1>%mask, <16 x float> %passthru). ;; The result of the two following instructions is identical aside from potential memory access exception; %loadlal = load <16 x float>, ptr %ptr, align 4; %res = select <16 x i1> %mask, <16 x float> %loadlal, <16 x float> %passthru. .. _int_mstore:. '``llvm.masked.store.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The data stored in memory is a vecto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:844741,load,loads,844741,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,". RooFit Package. RooFit 3.50 has undergone a substantial amount of core engineering to improve computational efficiency; and improve algorithmic likelihood optimizations. The expected increases in execution speed range from; roughly 20% (for problems that were already implemented in a close-to optimal form) to more than 2000%; for certain type of problems. Below is a summary of the changes made. All of these changes are; transparent to end-use cases ; ; ; New implementation of RooFit data types. The implementation of data stored in RooDataSet and RooDataHist; was historically handled by ROOT TTrees (though class RooTreeDataStore). The default storage type; has now been changed to class RooVectorDataStore which stores the information in STL arrays. Existing; datasets based on trees can be read in transparently, and are converted to vector form in the ; persistent-to-transient conversion (the datafile is not modified in this operation); ; The vector store has two important advantages: 1) faster data access (raw data access times are 70 times ; faster than for TTrees), 2) ability to rewrite columns on the fly. The first advantage is important; for the existing constant-term precalculation optimization in roofit likelihoods as these are now; also stored in vectors rather than trees. The faster access speed of vectors make that the constant; term optimization inside likelihoods results in a larger speed increase. This is particulatly noticeable in pdfs with; many constant expressions from pdfs that were moderately fast to begin with (e.g. RooHistPdf).; The second advantages allows new types of algorithmic likelihood optimization in RooFit detailed below. New algorithmic optimization in the caching of pdfs. So far - in the likelihood - two classes of; objects are identified: those that change with every event (i.e. the pdf) and those that change; only with the parameters (typically pdf normalization integrals). Pdfs are always recalculated; for every event, whereas integr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:157,optimiz,optimizations,157,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,1,['optimiz'],['optimizations']
Performance,". See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent. * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs), or the same; work-group if executing in tgsplit mode, of an agent can be reordered; relative to each other. A ``s_waitcnt vmcnt(0)`` is required to ensure; synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache of one agent can be kept coherent with other agents by:; using the MTYPE RW (read-write) or MTYPE CC (cache-coherent) with the PTE; C-bit for memory local to the L2; and using the MTYPE NC (non-coherent) with; the PTE C-bit set or MTYPE UC (uncached) for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by coherent requests. Coherent requests are caused; by GPU accesses to pages with the PTE C-bit set, by CPU accesses over; XGMI, and by PCIe requests that are configured to be coherent requests.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter and and the PTE C-bit being set.; * Since all work-groups on the same agent share the same L2, no L2; invalidation or writeback is required for coherence.; * To ensure coherence of local and remote memory writes of work-groups in; different agents a ``buffer_wbl2`` is required. It will writeb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:237609,cache,cache,237609,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'cache-coherent']"
Performance,". Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <2 x double> @llvm.experimental.vector.splice.v2f64(<2 x double> %vec1, <2 x double> %vec2, i32 %imm); declare <vscale x 4 x i32> @llvm.experimental.vector.splice.nxv4i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2, i32 %imm). Overview:; """""""""""""""""". The '``llvm.experimental.vector.splice.*``' intrinsics construct a vector by; concatenating elements from the first input vector with elements of the second; input vector, returning a vector of the same type as the input vectors. The; signed immediate, modulo the number of elements in the vector, is the index; into the first vector from which to extract the result value. This means; conceptually that for a positive immediate, a vector is extracted from; ``concat(%vec1, %vec2)`` starting at index ``imm``, whereas for a negative; immediate, it extracts ``-imm`` trailing elements from the first vector, and; the remaining elements from ``%vec2``. These intrinsics work for both fixed and scalable vectors. While this intrinsic; is marked as experimental, the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. llvm.experimental.vector.splice(<A,B,C,D>, <E,F,G,H>, 1); ==> <B, C, D, E> index; llvm.experimental.vector.splice(<A,B,C,D>, <E,F,G,H>, -3); ==> <B, C, D, E> trailing elements. Arguments:; """""""""""""""""""". The first two operands are vectors with the same type. The start index is imm; modulo the runtime number of elements in the source vector. For a fixed-width; vector <N x eltty>, imm is a signed integer constant in the range; -N <= imm < N. For a scalable vector <vscale x N x eltty>, imm is a signed; integer constant in the range -X <= imm < X where X=vscale_range_min * N. '``llvm.experimental.stepvector``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This is an overloaded intrinsic. You can use ``llvm.exper",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:671497,scalab,scalable,671497,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,". TH1. Add support for weighted likelihood fit of histogram by using a new option, WL and suppress the old option; LL.; The histogram must have the sum of the weight squared stored bin by bin to use this fit option; (i.e. TH1::Sumw2() has been called before filling).; Now one can perform likelihoof fit to weighted or scaled histograms and get the correct errors in the fit parameters.; (see bug report 79754).; ; Fix for the bug 82562.; Fix a bug in TH1::Merge for histogram with labels (bug 75902).; Fix few bugs related with the Buffer. . TProfile. Fix a bug in TProfile::Merge when the kCanRebin bit is set; (bug 79675).; Fix a bug in LabelsDeflate (bug 77149). TH1. Add new method TH3::Rebin3D and alsoRebinX, RebinY and RebinZ thanks to Zhiyi Liu. THistPainter. TPad::SetTheta() and TPad::SetPhi() did not cause the; canvas redrawing.; Protection added in case two histograms were plotted in the same pad; using the option BOX (the 2nd one with option SAME).; The clipping was not correct when an interactive zoom was performed. The 2D functions, for instance a fit function, associated to a 2D; histogram were always drawn as scatter plots. This was very confusing.; Now they are drawn as a surface if the histogram plotting option is a; lego or a surface or as a contour plot for any other plotting options. When drawing scatter plot for TH2 or TH2Poly do not use gRandom, but an independent random generator instance,; to avoid interfering with gRandom (bug 83021).; Now the same random sequence is always used for drawing the same histograms, giving therefore exactly the same scatter plot for the same; histogram, while before a slightly different plot was obtained every time. TH2Poly. Add(const TH1 *h1, Double_t c1) has been implemented.; Reset() has been implemented.; The destructor has been completed. THStack. When the 1D histograms in a stack are painted with patterns or hatches; the histograms are first painted with the TFrame background color to avoid; the hatches overlaps. I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v530/index.html:1581,perform,performed,1581,hist/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v530/index.html,1,['perform'],['performed']
Performance,". TMVA. TMVA version 4.0.4 is included in this root release. Methods. A new Category method allowing the user to; separate the training data (and accordingly the application; data) into disjoint sub-populations exhibiting significantly; different properties. The separation into phase space regions is; done by applying requirements on the input and/or spectator; variables. In each of these disjoint regions (each event must; belong to one and only one region), an independent training is; performed using the most appropriate MVA method, training; options and set of training variables in that zone. The division; into categories in presence of distinct sub-populations reduces; the correlations between the training variables, improves the; modelling, and hence increases the classification and regression; performance. Presently, the Category method works for; classification only, but regression will follow soon. Please; contact us if urgently needed. An example scripts and data files illustrating how the new; Category method is configured and used. Please check the macros; test/TMVAClassificationCategory.C and; test/TMVAClassificationCategoryApplication.C or the; corresponding executables.; Regression functionality for gradient boosted trees using a Huber loss function. Comments. On Input Data: . New TMVA event vector building. The code for splitting the input; data into training and test samples for all classes and the; mixing of those samples to one training and one test sample has; been rewritten completely. The new code is more performant and; has a clearer structure. This fixes several bugs which have been; reported by the TMVA users. On Minimization: . Variables, targets and spectators are now checked if they are; constant. (The execution of TMVA is stopped for variables and; targets, a warning is given for spectators.). On Regression:; ; The analysis type is no longer defined by calling a dedicated; TestAllMethods-member-function of the Factory, but with the; option ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v526/index.html:491,perform,performed,491,tmva/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v526/index.html,2,['perform'],"['performance', 'performed']"
Performance,". The '``llvm.experimental.stepvector``' intrinsics are used to create vectors; of integers whose elements contain a linear sequence of values starting from 0; with a step of 1. This experimental intrinsic can only be used for vectors; with integer elements that are at least 8 bits in size. If the sequence value; exceeds the allowed limit for the element type then the result for that lane is; undefined. These intrinsics work for both fixed and scalable vectors. While this intrinsic; is marked as experimental, the recommended way to express this operation for; fixed-width vectors is still to generate a constant vector instead. Arguments:; """""""""""""""""""". None. '``llvm.experimental.get.vector.length``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.experimental.get.vector.length.i32(i32 %cnt, i32 immarg %vf, i1 immarg %scalable); declare i32 @llvm.experimental.get.vector.length.i64(i64 %cnt, i32 immarg %vf, i1 immarg %scalable). Overview:; """""""""""""""""". The '``llvm.experimental.get.vector.length.*``' intrinsics take a number of; elements to process and returns how many of the elements can be processed; with the requested vectorization factor. Arguments:; """""""""""""""""""". The first argument is an unsigned value of any scalar integer type and specifies; the total number of elements to be processed. The second argument is an i32; immediate for the vectorization factor. The third argument indicates if the; vectorization factor should be multiplied by vscale. Semantics:; """""""""""""""""""". Returns a positive i32 value (explicit vector length) that is unknown at compile; time and depends on the hardware specification.; If the result value does not fit in the result type, then the result is; a :ref:`poison value <poisonvalues>`. This intrinsic is intended to be used by loop vectorization with VP intrinsics; in order to get the number of elements to process on each loop iteration. The; result should be used",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:673793,scalab,scalable,673793,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,". The call to `pn1->Align()` will invalidate the pointer to the node `B_1`; in `pn2` object.. The way out is to either call `pn1->Align()` before; the creation of `pn2`, either to use a global method that will correct; all existing physical nodes:. ``` {.cpp}; void RefreshPhysicalNodes(Bool_t lock = kTRUE); ```. The method above will optionally lock the possibility of doing any; further misalignment. ## Geometry I/O. Once geometry is successfully built, it can be saved in a root file, as; C++ macro or as GDML file by calling:. ``` {.cpp}; TGeoManager::Export(const char *filename,const char*keyname="""",; Option_t *opt=""vg""); ```. - `Filename`is the name of the file to be written (mandatory).; Depending on the extension of the file, the geometry is exported; either as ,root file or .C(.cxx) macro or GDML file in case; extension is .gdml.; - `keyname`is the name of the key in the file (default """"); - `opt` = `""v""` is an export voxelization (default), otherwise; voxelization is recomputed after loading the geometry, `""g""` this; option (default) is taken into account only for exporting to gdml; file and it ensures compatibility with Geant4 (e.g. it adds extra; plane to incorrectly set polycone, it checks whether offset of Phi; division is in (-360;0\> range, ...), for this gdml export there are; two more option, that are not set by default: `""f""` and `""n""`. If; none of this two options are set, then names of solids and volumes; in resulting gdml file will have incremental suffix (e.g.; TGeoBBox\_0x1, TGeoBBox\_0x2, ...). If `""f""` option is set then then; suffix will contain pointer of object (e.g. TGeoBBox\_0xAAAAA01,; ...). Finally if option `""n""` is set then no suffix will be added,; though in this case uniqueness of the names is not ensured and it can; cause that file will be invalid. Loading geometry from a root file can be done in the same way as for any; other ROOT object, but a static method is also provided:. ``` {.cpp}; TGeoManager::Import(const char *filename,con",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:150653,load,loading,150653,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loading']
Performance,". The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_wbinvl1_vol`` is required as described in; the following item. * A ``buffer_wbinvl1_vol`` is required for coherence between wavefronts; executing in different work-groups as they may be executing on different; CUs. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent. * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs), or the same; work-group if executing in tgsplit mode, of an agent can be reordered; relative to each other. A ``s_waitcnt vmcnt(0)`` is required to ensure; synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache of one agent can be kept coherent with other agents by:; using the MTYPE RW (read-write) or MTYPE CC (cache-coherent) with the PTE; C-bit for memory local to the L2; and using the MTYPE NC (non-coherent) with; the PTE C-bit set or MTYPE UC (uncached) for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by coherent requests. Coherent requests are caused; by GPU accesses to pages with the PTE",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:237081,perform,performed,237081,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,". The first argument is an operand which is used as the returned value. Overview:; """""""""""""""""""". The ``llvm.ssa.copy`` intrinsic can be used to attach information to; operations by copying them and giving them new names. For example,; the PredicateInfo utility uses it to build Extended SSA form, and; attach various forms of information to operands that dominate specific; uses. It is not meant for general use, only for building temporary; renaming forms that require value splits at certain points. .. _type.test:. '``llvm.type.test``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.type.test(ptr %ptr, metadata %type) nounwind memory(none). Arguments:; """""""""""""""""""". The first argument is a pointer to be tested. The second argument is a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the giv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:937679,load,load,937679,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,". The index; of the current suspension point of the coroutine is emitted as `__coro_index`.; In the above example, the `__coro_index` value of `1` means the coroutine; stopped at the second suspend point (Note that `__coro_index` is zero indexed); which is the first `co_await await_counter{};` in `coro_task`. Note that the; first initial suspend point is the compiler generated; `co_await promise_type::initial_suspend()`. However, when optimizations are enabled, the printed result changes drastically:. .. parsed-literal::. {__resume_fn = 0x401280 <coro_task(int)>, __destroy_fn = 0x401390 <coro_task(int)>, __promise = {count = 1}, __int_32_0 = 43, __coro_index = 1 '\001'}. Unused values are optimized out, as well as the name of the local variable `a`.; The only information remained is the value of a 32 bit integer. In this simple; case, it seems to be pretty clear that `__int_32_0` represents `a`. However, it; is not true. An important note with optimization is that the value of a variable may not; properly express the intended value in the source code. For example:. .. code-block:: c++. static task coro_task(int v) {; int a = v;; co_await await_counter{};; a++; // __int_32_0 is 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here!; std::cout << a << ""\n"";; co_await await_counter{};; a++; // __int_32_0 is still 43 here!!; std::cout << a << ""\n"";; a++; // Why is __int_32_0 still 43 here?; std::cout << a << ""\n"";; }. When debugging step-by-step, the value of `__int_32_0` seemingly does not; change, despite being frequently incremented, and instead is always `43`.; While this might be surprising, this is a result of the optimizer recognizing; that it can eliminate most of the load/store operations. The above code gets; optimized to the equivalent of:. .. code-block:: c++. static task coro_task(int v) {; store v to __int_32_0 in the frame; co_await await_counter{};; a = load __int_32_0; std::cou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:8835,optimiz,optimization,8835,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,1,['optimiz'],['optimization']
Performance,". The most appealing way to harden loads is to mask out all of the bits loaded.; The key requirement is that for each bit loaded, along the misspeculated path; that bit is always fixed at either 0 or 1 regardless of the value of the bit; loaded. The most obvious implementation uses either an `and` instruction with; an all-zero mask along misspeculated paths and an all-one mask along correct; paths, or an `or` instruction with an all-one mask along misspeculated paths; and an all-zero mask along correct paths. Other options become less appealing; such as multiplying by zero, or multiple shift instructions. For reasons we; elaborate on below, we end up suggesting you use `or` with an all-ones mask,; making the x86 instruction sequence look like the following:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; movl (%rsi), %edi # Load potentially secret data from %rsi.; orl %eax, %edi; ```. Other useful patterns may be to fold the load into the `or` instruction itself; at the cost of a register-to-register copy. There are some challenges with deploying this approach:; 1. Many loads on x86 are folded into other instructions. Separating them would; add very significant and costly register pressure with prohibitive; performance cost.; 1. Loads may not target a general purpose register requiring extra instructions; to map the state value into the correct register class, and potentially more; expensive instructions to mask the value in some way.; 1. The flags registers on x86 are very likely to be live, and challenging to; preserve cheaply.; 1. There are many more values loaded than pointers & indices used for loads. As; a consequence, hardening the result of a load requires substantially more; instructions than hardening the address of the load (see below). Despite these challenges, hardening the result of the load critically allows; the load to proceed and thus has dramatically less impact on the total; speculative / out-of-order pote",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:23719,load,load,23719,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,". The rules mentioned in; this section only pertain to TBAA nodes living under the same root. .. _tbaa_node_semantics:. Semantics; """""""""""""""""". The TBAA metadata system, referred to as ""struct path TBAA"" (not to be; confused with ``tbaa.struct``), consists of the following high level; concepts: *Type Descriptors*, further subdivided into scalar type; descriptors and struct type descriptors; and *Access Tags*. **Type descriptors** describe the type system of the higher level language; being compiled. **Scalar type descriptors** describe types that do not; contain other types. Each scalar type has a parent type, which must also; be a scalar type or the TBAA root. Via this parent relation, scalar types; within a TBAA root form a tree. **Struct type descriptors** denote types; that contain a sequence of other type descriptors, at known offsets. These; contained type descriptors can either be struct type descriptors themselves; or scalar type descriptors. **Access tags** are metadata nodes attached to load and store instructions.; Access tags use type descriptors to describe the *location* being accessed; in terms of the type system of the higher level language. Access tags are; tuples consisting of a base type, an access type and an offset. The base; type is a scalar type descriptor or a struct type descriptor, the access; type is a scalar type descriptor, and the offset is a constant integer. The access tag ``(BaseTy, AccessTy, Offset)`` can describe one of two; things:. * If ``BaseTy`` is a struct type, the tag describes a memory access (load; or store) of a value of type ``AccessTy`` contained in the struct type; ``BaseTy`` at offset ``Offset``. * If ``BaseTy`` is a scalar type, ``Offset`` must be 0 and ``BaseTy`` and; ``AccessTy`` must be the same; and the access tag describes a scalar; access with scalar type ``AccessTy``. We first define an ``ImmediateParent`` relation on ``(BaseTy, Offset)``; tuples this way:. * If ``BaseTy`` is a scalar type then ``ImmediateParent",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:274554,load,load,274554,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,". They may touch on one boundaries or shape vertex. The daughter nodes of a volume can be also removed or replaced with; other nodes:. ~~~{.cpp}; void RemoveNode(TGeoNode* node); TGeoNode*ReplaceNode(TGeoNode* nodeorig, TGeoShape* newshape = 0,; TGeoMatrix* newpos = 0, TGeoMedium* newmed = 0); ~~~. The last method allows replacing an existing daughter of a volume with; another one. Providing only the node to be replaced will just create a; new volume for the node but having exactly the same parameters as the; old one. This helps in case of divisions for decoupling a node from the; logical hierarchy so getting new content/properties. For non-divided; volumes, one can change the shape and/or the position of the daughter. \anchor GP01bd; #### Virtual Containers and Assemblies of Volumes. Virtual containers are volumes that do not represent real objects, but; they are needed for grouping and positioning together other volumes.; Such grouping helps not only geometry creation, but also optimizes; tracking performance; therefore, it is highly recommended. Virtual; volumes need to inherit material/medium properties from the volume they; are placed into in order to be ""invisible"" at tracking time. Let us suppose that we need to group together two volumes `A` and `B`; into a structure and position this into several other volumes `D,E,` and; `F`. What we need to do is to create a virtual container volume `C`; holding `A` and `B`, then position `C` in the other volumes. Note that `C` is a volume having a determined medium. Since it is not a; real volume, we need to manually set its medium the same as that of; `D,E` or `F` in order to make it ""invisible"" (same physics properties).; In other words, the limitation in proceeding this way is that `D,E,` and; `F` must point to the same medium. If this was not the case, we would; have to define different virtual volumes for each placement: `C`, `C`'; and `C`\"", having the same shape but different media matching the; corresponding cont",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:30286,optimiz,optimizes,30286,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,2,"['optimiz', 'perform']","['optimizes', 'performance']"
Performance,". This is basically the same as ``switch`` statement in ``__builtin_expect``.; The probability that ``exp`` is equal to the expect value is given in; the third argument ``probability``, while the probability of other value is; the average of remaining probability(``1.0 - probability``). For example:. .. code-block:: c++. switch (__builtin_expect_with_probability(x, 5, 0.7)) {; default: break; // Take this case with probability 10%; case 0: break; // Take this case with probability 10%; case 3: break; // Take this case with probability 10%; case 5: break; // This case is likely to be taken with probability 70%; }. CFG Modifications; =================. Branch Weight Metatada is not proof against CFG changes. If terminator operands'; are changed some action should be taken. In other case some misoptimizations may; occur due to incorrect branch prediction information. Function Entry Counts; =====================. To allow comparing different functions during inter-procedural analysis and; optimization, ``MD_prof`` nodes can also be assigned to a function definition.; The first operand is a string indicating the name of the associated counter. Currently, one counter is supported: ""function_entry_count"". The second operand; is a 64-bit counter that indicates the number of times that this function was; invoked (in the case of instrumentation-based profiles). In the case of; sampling-based profiles, this operand is an approximation of how many times; the function was invoked. For example, in the code below, the instrumentation for function foo(); indicates that it was called 2,590 times at runtime. .. code-block:: llvm. define i32 @foo() !prof !1 {; ret i32 0; }; !1 = !{!""function_entry_count"", i64 2590}. If ""function_entry_count"" has more than 2 operands, the later operands are; the GUID of the functions that needs to be imported by ThinLTO. This is only; set by sampling based profile. It is needed because the sampling based profile; was collected on a binary that had alre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BranchWeightMetadata.rst:5731,optimiz,optimization,5731,interpreter/llvm-project/llvm/docs/BranchWeightMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BranchWeightMetadata.rst,1,['optimiz'],['optimization']
Performance,". Tree Libraries; Performance. Automatic support for multiple TTreeCache per TFile.; Multiple TTreeCache per TFile for reading are supported by using the existing TTree::SetCacheSize(Long64_t) interface.; In addition, a TTreeCache for a TTree can be added using TFile::SetCacheRead(TFileCacheRead*, TObject*), where the second (optional) argument is a pointer to the TTree. The cache can be removed by setting the pointer to 0. In that case the user will have to take ownership for the cache.; Similarily, a pointer to the TTreeCache for a TTree can be obtained using TFile::GetCacheRead(TObject*). In TBuffer::Expand, when shrinking the buffer do not shrink below the size of the; data already accumulated in the buffer (i. no less than the value of TBuffer::Length). In TBranch::SetBasketSize, instead of using the hard minimum of 100, use; 100 + the length of the branch name (as 100 is too small to hold the; basket's key information for any branch name larger than 30 characters). Reading form text file. Reworked TTree::ReadStream and TTree::ReadFile mainly to fix delimited reading of string columns:. TLeaf::ReadValue now takes an optional delimiter argument that is ignored for all but TLeafC. Here, input stops when reading this character, instead of at the first whitespace.; Use that in TTree::ReadStream() to delimit reading of TLeafC.; TTree::ReadStream now tokenizes the row itself, and passes a stringstream containing nothing but the current column to TLeaf::ReadValue.; Separate concepts of number of input line (for communication with user) and number of good lines (as returned).; Fix windows files leaving '\n' in branch names when reading them from the file.; Add error message for TLeaf::ReadValue(), i.e. if ReadValue() is called on a derived class that doesn't implement it.; Updated and clarified the documentation. TEntryList. Add new methods to find the base location of files and to modify it.; This allows to relocate the entry-lists to be able to use them of a; system w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html:378,cache,cache,378,tree/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html,2,['cache'],['cache']
Performance,". Tree Libraries; TEntryListArray: a list of entries and subentries in a TTree or TChain. TEntryListArray is an extension of TEntryList, used to hold selected entries and subentries (sublists) for when the user has a TTree with containers (vectors, arrays, ...). Usage with TTree::Draw to select entries and subentries . To fill a list elist :; ; tree->Draw("">> elist"", ""x > 0"", ""entrylistarray"");; . To use a list to select entries and subentries:. tree->SetEntryList(elist);; tree->Draw(""y"");; tree->Draw(""z"");; . Its main purpose is to improve the performance of a code that needs to apply complex cuts on TTree::Draw multiple times. After the first call above to TTree::Draw, a TEntryListArray is created and filled with the entries and the indices of the arrays that satisfied the selection cut (x > 0). In the subsequent calls to TTree::Draw, only these entries / subentries are used to fill histograms. About the class . The class derives from TEntryList and can be used basically in the same way. This same class is used to keep entries and subentries, so there are two types of TEntryListArray's:. The ones that only hold subentries; fEntry is set to the entry# for which the subentries correspond; fSubLists must be 0; The ones that hold entries and eventually lists with subentries in fSubLists.; fEntry = -1 for those; If there are no sublists for a given entry, all the subentries will be used in the selection. Additions with respect to TEntryList ; Data members:; fSubLists: a container to hold the sublists; fEntry: the entry number if the list is used to hold subentries; fLastSubListQueried and fSubListIter: a pointer to the last sublist queried and an iterator to resume the loop from the last sublist queried (to speed up selection and insertion in TTree::Draw); Public methods:; Contains, Enter and Remove with subentry as argument; GetSubListForEntry: to return the sublist corresponding to the given entry; Protected methods:; AddEntriesAndSubLists: called by Add when adding t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v532/index.html:551,perform,performance,551,tree/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v532/index.html,1,['perform'],['performance']
Performance,". Tree. Significantly improve performance of TTree Proxy.; Improve read performance of sub-branch containing vector of single types.; Fix TTree::LoadBasket to properly handle the (new) case where no basket is stored with the TTree object.; Fix the axis used for an histogram created by TTree::Draw for a branch of TString or std::string objects.; MakeProxy now correctly support branches that created with a leaflist with more than one leaf; (usually used for C-struct).; TTree::CloneTree and TChain::Merge in fast mode now can recover from some mismatch errors between; the input and output TTrees by falling back to using the 'slow' mode. In particular this allow; a 'fast cloning' to handle files that requires schema evolution (albeit it is of course much slower).; Make sure that the TTreeCache is not attempting to cache (wrongly) the content of branches that are in an auxiliary files.; Make sure that FillBuffer does it work when the learning phase is over even if the entry number is 'low' for the 'current' file of a chain.; If TTree::SetEventList is called, TTree::GetEntryList no longer relinquish ownership of the automatically created TEntryList; Add the ability to see the TTree UserInfo list from the TBrowser; Fix the case of reading a TTree containing an 'old' class layout that contained a std::vector that is no longer part of the current class layout; Implement direct interfaces from TTree to the result of TSelector::Draw; TTree:GetVal(int) and TTree::GetVar(int); In TTree::ReadFile add the possibility to read multiple input files and add support for large/wide Trees definition.; Added support for ""5-D"" plotting.; Added support for std::bitset; Reduce the memory used by the mechanism keeping track of the entry of variables sizes within a basket (fEntryOffset).; The memory used now automatically decrease if the number of entries in the basket is less than 1/4 oflength of fEntryOffset.; Also the default length fEntryOffset can be set via TTree::SetDefaultEntryOffsetLen ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v524/index.html:30,perform,performance,30,tree/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v524/index.html,3,"['cache', 'perform']","['cache', 'performance']"
Performance,". Tree; Performance. Introduce support for TTree with variable cluster size (i.e. value of fAutoFlush).; Iterating through the cluster should be done via the new class TTree::TClusterIterator (i.e. this replaces += fAutoFlush):. TTree::TClusterIterator clusterIter = tree->GetClusterIterator(which_entry_to_start_from);; Long64_t clusterStart;; while( (clusterStart = clusterIter()) < tree-<GetEntries()) {; printf(""The cluster starts at %lld and ends at %lld\n"",clusterStart,clusterIter.GetNextEntry()-1);; }; See TTreeCache::FillBuffer for a concrete usage example. Significant improvement of the performance of SetBranchAddress/SetAddress (by a factor 3 to 10 depending on the length/complexity of the classname).; Prevent the unlimited growth of the TBasket's buffer even if the basket is reused.; When the basket is Reset (this happens when it is written and will be reused),; if the TBuffer size is greater than. - twice the data in the current basket; and - twice the average data in each basket (of this branch); and - twice the requeste basket size (TBranch::GetBasketSize).; the size of the buffer is reduced to the max of; 'the data in the current basket' and 'the average' and the requested; buffer size and aligned to next highest multiple of 512.; In TBranchRef distinguish between the entry we need (now called RequestedEntry) and the; entry we have read (fReadEntry) so that we can avoid re-reading the same entry too many; times when executing TRef::GetObject.; Reduce by 40% the time taken GetEntry for a branch created using a leaflist (exclusive of the decompression time).; Introduce TVirtualPerfStats::FileUnzipEvent to be able to keep track of the cost of unzipping and use this in TTreePerfStats and TBasket ... This give a good picture of where the time in unzip or in unstreaming; Add more clusters to the TTreeCache buffer until fBufferMinSize is hit to avoid severely underfilled buffer when; a low number of branches is selected/used.; When reading backwards, make sure to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v530/index.html:599,perform,performance,599,tree/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v530/index.html,1,['perform'],['performance']
Performance,". Use TAxis attributes in lego plots - ticks/labels/title colors, sizes, offsets; 2. Correctly resize stats box when number of lines changes; 3. Support JSROOT usage with yarn and webpack; 4. Provide `FileProxy` class to let read ROOT files from arbitrary place; 5. Let 'hook' save file functionality to use alternative method to store image files; 6. Implement 'tabs' layout for objects display (#238); 7. Upgrade d3.js to version 7.6.1; 8. Fix - adjust pad margins when moving palette and frame. ## Changes in 7.1.1; 1. Fix - let modify node visibility bits via context menu; 2. Fix - menu position adjusting; 3. Fix - tree_draw.js example, export treeDraw function from main.mjs; 4. Fix - TH3 scatter plot with large number of bins converted to box2; 5. Fix - create geo css entries also when expand object in hierarchy (#240). ## Changes in 7.1.0; 1. Let change `settings` and `gStyle` parameters via ""Settings"" menu of the top hierarchy item; 2. Settings and gStyle can be stored as cookies, automatically read when next time loading webpage; 3. `settings.OnlyLastCycle` defines if only last object version shown in TFile (also as `&lastcycle` URL parameter); 4. `settings.DarkMode` configures dark mode for GUI and drawings (also as `&dark` URL parameter); 5. Support new `TGraph2DAsymmErrors` class; 6. Support `gStyle.fOptDate` and `gStyle.fOptFile` (also as `&optdate` and `&optfile` URL parameter); 7. Support `gStyle.fDateX` and `gStyle.fDateY` used for positioning date and file on canvas; 8. Support `gStyle.fHistTopMargin` (also as `&histmargin=value` URL parameter); 9. Let save frame, title and stats properties to `gStyle` via correspondent context menus; 10. Support majority of special symbols in TMathText; 11. Fix several issues with TPaveText. ## Changes in 7.0.2; 1. Fix - TH2 arrow drawing; 2. Fix - interactive change of fonts attributes; 3. Fix - proper draw results of TTree::Draw; 4. Fix - draw new histogram on same canvas. ## Changes in 7.0.1; 1. Fix problem with irregu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:17577,load,loading,17577,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,". Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; with :doc:`MemorySanitizer`. Use ``__has_feature(dataflow_sanitizer)`` to check if the code is being built; with :doc:`DataFlowSanitizer`. Use ``__has_feature(safe_stack)`` to check if the code is being built; with :doc:`SafeStack`. Extensions for selectively disabling optimization; =================================================. Clang provides a mechanism for selectively disabling optimizations in functions; and methods. To disable optimizations in a single function definition, the GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This function will be decorated with optnone.; int foo() {; // ... code; }. // optnone conflicts with always_inline, so bar() will not be decorated.; __attribute__((always_inline)) int bar() {; // ... code; }; #pragma clang optimize on. If no ``on`` is found to close an ``off`` region, the end of the region is the; end of the compilation unit. Note that a stray ``#pragma clang optimize on`` does not selectively enable; additional optimizations when compiling at low optimization levels. This feature; can only be used to selectively disable optimizations. The pragma has an effect on functions only at the point of their definition; for; functio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:159137,optimiz,optimize,159137,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimize']
Performance,". Whenever positioned inside; a mother volume, this will create a normal **`TGeoVolume`** object; having as shape a box with `dz` fitting the corresponding `dz `of the; mother shape. Generally, this type of parameterization is used when; positioning volumes in containers having a matching shape, but it works; also for most reasonable combinations. ## Geometry Creation. A given geometry can be built in various ways, but one has to follow; some mandatory steps. Even if we might use some terms that will be; explained later, here are few general rules:. - Volumes need media and shapes in order to be created.; - Both containers and contained volumes must be created before linking; them together, and the relative transformation matrix must be; provided.; - Any volume have to be positioned somewhere otherwise it will not be; considered as part of the geometry.; - Visibility or tracking properties of volumes can be provided both at; build time or after geometry is closed, but global visualization; settings (see section: ""The Drawing Package"") should not be provided; at build time, otherwise the drawing package will be loaded. There is also a list of specific rules:. - Positioned volumes should not extrude their container or intersect; with others within this unless it is specified (see section:; Overlapping Volumes).; - The top volume (containing all geometry trees) must be specified; before closing the geometry and must not be positioned - it; represents the global reference frame.; - After building the full geometry tree, the geometry must be closed; (see the method **`TGeoManager`**`::CloseGeometry()`). Voxelization; can be redone per volume after this process. The list is much bigger and we will describe in more detail the geometry; creation procedure in the following sections. Provided that geometry was; successfully built and closed, the **`TGeoManager`** class will register; itself to ROOT and the logical/physical structures will become; immediately browsable. ### Th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:57490,load,loaded,57490,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loaded']
Performance,". You can use ``llvm.uadd.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.uadd.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.uadd.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.uadd.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.uadd.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.uadd.with.overflow``' family of intrinsic functions perform; an unsigned addition of the two arguments, and indicate whether a carry; occurred during the unsigned summation. Arguments:; """""""""""""""""""". The arguments (%a and %b) and the first element of the result structure; may be of integer types of any bit width, but they must have the same; bit width. The second element of the result structure must be of type; ``i1``. ``%a`` and ``%b`` are the two values that will undergo unsigned; addition. Semantics:; """""""""""""""""""". The '``llvm.uadd.with.overflow``' family of intrinsic functions perform; an unsigned addition of the two arguments. They return a structure --- the; first element of which is the sum, and the second element of which is a; bit specifying if the unsigned summation resulted in a carry. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.uadd.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %carry, label %normal. '``llvm.ssub.with.overflow.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.ssub.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.ssub.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.ssub.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.ssub.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.ssub.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.ssu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:603626,perform,perform,603626,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,". _writing-an-llvm-pass-releaseMemory:. The ``releaseMemory`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual void releaseMemory();. The ``PassManager`` automatically determines when to compute analysis results,; and how long to keep them around for. Because the lifetime of the pass object; itself is effectively the entire duration of the compilation process, we need; some way to free analysis results when they are no longer useful. The; ``releaseMemory`` virtual method is the way to do this. If you are writing an analysis or any other pass that retains a significant; amount of state (for use by another pass which ""requires"" your pass and uses; the :ref:`getAnalysis <writing-an-llvm-pass-getAnalysis>` method) you should; implement ``releaseMemory`` to, well, release the memory allocated to maintain; this internal state. This method is called after the ``run*`` method for the; class, before the next call of ``run*`` in your pass. Registering dynamically loaded passes; =====================================. *Size matters* when constructing production quality tools using LLVM, both for; the purposes of distribution, and for regulating the resident code size when; running on the target system. Therefore, it becomes desirable to selectively; use some passes, while omitting others and maintain the flexibility to change; configurations later on. You want to be able to do all this, and, provide; feedback to the user. This is where pass registration comes into play. The fundamental mechanisms for pass registration are the; ``MachinePassRegistry`` class and subclasses of ``MachinePassRegistryNode``. An instance of ``MachinePassRegistry`` is used to maintain a list of; ``MachinePassRegistryNode`` objects. This instance maintains the list and; communicates additions and deletions to the command line interface. An instance of ``MachinePassRegistryNode`` subclass is used to maintain; information provided about a particular pass. This information includes the; c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:47823,load,loaded,47823,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['loaded']
Performance,". buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1. buffer_wbl2. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:278564,load,load,278564,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,". clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3534,optimiz,optimizations,3534,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,1,['optimiz'],['optimizations']
Performance,". clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. str",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:61371,optimiz,optimizing,61371,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimizing']
Performance,". eBPF maps are provided for sharing data between kernel and user-space.; Currently implemented types are hash and array, with potential extension to; support bloom filters, radix trees, etc. A map is defined by its type,; maximum number of elements, key size and value size in bytes. eBPF syscall; supports create, update, find and delete functions on maps. Function calls; ^^^^^^^^^^^^^^. Function call arguments are passed using up to five registers (R1 - R5).; The return value is passed in a dedicated register (R0). Four additional; registers (R6 - R9) are callee-saved, and the values in these registers; are preserved within kernel functions. R0 - R5 are scratch registers within; kernel functions, and eBPF programs must therefor store/restore values in; these registers if needed across function calls. The stack can be accessed; using the read-only frame pointer R10. eBPF registers map 1:1 to hardware; registers on x86_64 and other 64-bit architectures. For example, x86_64; in-kernel JIT maps them as. ::. R0 - rax; R1 - rdi; R2 - rsi; R3 - rdx; R4 - rcx; R5 - r8; R6 - rbx; R7 - r13; R8 - r14; R9 - r15; R10 - rbp. since x86_64 ABI mandates rdi, rsi, rdx, rcx, r8, r9 for argument passing; and rbx, r12 - r15 are callee saved. Program start; ^^^^^^^^^^^^^. An eBPF program receives a single argument and contains; a single eBPF main routine; the program does not contain eBPF functions.; Function calls are limited to a predefined set of kernel functions. The size; of a program is limited to 4K instructions: this ensures fast termination and; a limited number of kernel function calls. Prior to running an eBPF program,; a verifier performs static analysis to prevent loops in the code and; to ensure valid register usage and operand types. The AMDGPU backend; ------------------. The AMDGPU code generator lives in the ``lib/Target/AMDGPU``; directory. This code generator is capable of targeting a variety of; AMD GPU processors. Refer to :doc:`AMDGPUUsage` for more information.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:108144,perform,performs,108144,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['perform'],['performs']
Performance,". scan-build: running the analyzer from the command line. scan-build: running the analyzer from the command line. What is it?; scan-build is a command line utility that enables a user to run the; static analyzer over their codebase as part of performing a regular build (from; the command line).; How does it work?; During a project build, as source files are compiled they are also analyzed; in tandem by the static analyzer.; Upon completion of the build, results are then presented to the user within a; web browser.; Will it work with any build system?; scan-build has little or no knowledge about how you build your code.; It works by overriding the CC and CXX environment variables to; (hopefully) change your build to use a ""fake"" compiler instead of the; one that would normally build your project. This fake compiler executes either; clang or gcc (depending on the platform) to compile your; code and then executes the static analyzer to analyze your code.; This ""poor man's interposition"" works amazingly well in many cases; and falls down in others. Please consult the information on this page on making; the best use of scan-build, which includes getting it to work when the; aforementioned hack fails to work. Viewing static analyzer results in a web browser. Contents. Getting Started. Basic Usage; For Windows Users; Other Options; Output of scan-build. Recommended Usage Guidelines. Always Analyze a Project in its ""Debug"" Configuration; Use Verbose Output when Debugging scan-build; Run './configure' through scan-build. Analyzing iPhone Projects. Getting Started; The scan-build command can be used to analyze an entire project by; essentially interposing on a project's build process. This means that to run the; analyzer using scan-build, you will use scan-build to analyze; the source files compiled by gcc/clang during a project build.; This means that any files that are not compiled will also not be analyzed.; Basic Usage; Basic usage of scan-build is designed to be simple: j",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/scan-build.html:243,perform,performing,243,interpreter/llvm-project/clang/www/analyzer/scan-build.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/scan-build.html,1,['perform'],['performing']
Performance,".); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; hav",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:281765,load,load,281765,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,".. -*- mode: rst -*-. CPyCppyy: Python-C++ bindings interface based on Cling/LLVM; ===========================================================. CPyCppyy is the CPython equivalent of _cppyy in PyPy.; It provides dynamic Python-C++ bindings by leveraging the Cling C++; interpreter and LLVM.; Details and performance are described in; `this paper <http://conferences.computer.org/pyhpc/2016/papers/5220a027.pdf>`_. CPyCppyy is a CPython extension module built on top of the same backend API; as PyPy/_cppyy.; It thus requires the installation of the; `cppyy backend <https://pypi.python.org/pypi/cppyy-backend/>`_; for use, which will pull in Cling.; CPython/cppyy and PyPy/cppyy are designed to be compatible, although there; are differences due to the former being reference counted and the latter; being garbage collected, as well as temporary differences due to different; release cycles of the respective projects. ----. Find the cppyy documentation here:; http://cppyy.readthedocs.io. Change log:; https://cppyy.readthedocs.io/en/latest/changelog.html. Bug reports/feedback:; https://github.com/wlav/cppyy/issues; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/README.rst:303,perform,performance,303,bindings/pyroot/cppyy/CPyCppyy/README.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/README.rst,1,['perform'],['performance']
Performance,".. -*- mode: rst -*-. cppyy: Python-C++ bindings interface based on Cling/LLVM; ========================================================. cppyy provides fully automatic, dynamic Python-C++ bindings by leveraging; the Cling C++ interpreter and LLVM.; It supports both PyPy (natively), CPython, and C++ language standards; through C++17 (and parts of C++20). Details and performance are described in; `this paper <http://cern.ch/wlav/Cppyy_LavrijsenDutta_PyHPC16.pdf>`_,; originally presented at PyHPC'16, but since updated with improved performance; numbers. Full documentation: `cppyy.readthedocs.io <http://cppyy.readthedocs.io/>`_. Notebook-based tutorial: `Cppyy Tutorial <https://github.com/wlav/cppyy/blob/master/doc/tutorial/CppyyTutorial.ipynb>`_. For Anaconda/miniconda, install cppyy from `conda-forge <https://anaconda.org/conda-forge/cppyy>`_. ----. Change log:; https://cppyy.readthedocs.io/en/latest/changelog.html. Bug reports/feedback:; https://github.com/wlav/cppyy/issues; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/README.rst:369,perform,performance,369,bindings/pyroot/cppyy/cppyy/README.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/README.rst,2,['perform'],['performance']
Performance,".. _basic_types:. Basic types; ===========. C++ has a far richer set of builtin types than Python.; Most Python code can remain relatively agnostic to that, and ``cppyy``; provides automatic conversions as appropriate.; On the other hand, Python builtin types such as lists and maps are far; richer than any builtin types in C++.; These are mapped to their Standard Template Library equivalents instead. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded before running any of the example code snippets.; Download it, save it under the name ``features.h``, and simply include it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. `Builtins`; """""""""""""""""""". The selection of builtin data types varies greatly between Python and C++.; Where possible, builtin data types map onto the expected equivalent Python; types, with the caveats that there may be size differences, different; precision or rounding, etc.; For example, a C++ ``float`` is returned as a Python ``float``, which is in; fact a C++ ``double``.; If sizes allow, conversions are automatic.; For example, a C++ ``unsigned int`` becomes a Python2 ``long`` or Python3; ``int``, but unsigned-ness is still honored:. .. code-block:: python. >>> cppyy.gbl.gUint; 0L; >>> type(cppyy.gbl.gUint); <type 'long'>; >>> cppyy.gbl.gUint = -1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ValueError: cannot convert negative integer to unsigned; >>>. On some platforms, 8-bit integer types such as ``int8_t`` and ``uint8_t`` are; represented as `char` types.; For consistency, these are mapped onto Python `int`. Some types are builtin in Python, but (STL) classes in C++.; Examples are ``str`` vs. ``std::string`` (see also the; :doc:`Strings <strings>` section) and ``complex`` vs. ``std::complex``.; These classes have been pythonized to behave the same wherever possible.; For example, string comparison work",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/basic_types.rst:533,load,loaded,533,bindings/pyroot/cppyy/cppyy/doc/source/basic_types.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/basic_types.rst,1,['load'],['loaded']
Performance,".. _classes:. Classes; =======. Both Python and C++ support object-oriented code through classes and thus; it is logical to expose C++ classes as Python ones, including the full; inheritance hierarchy. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. `Basics`; --------. All bound C++ code starts off from the global C++ namespace, represented in; Python by ``gbl``.; This namespace, as any other namespace, is treated as a module after it has; been loaded.; Thus, we can import C++ classes that live underneath it:. .. code-block:: python. >>> from cppyy.gbl import Concrete; >>> Concrete; <class cppyy.gbl.Concrete at 0x2058e30>; >>>. Placing classes in the same structure as imposed by C++ guarantees identity,; even if multiple Python modules bind the same class.; There is, however, no necessity to expose that structure to end-users: when; developing a Python package that exposes C++ classes through ``cppyy``,; consider ``cppyy.gbl`` an ""internal"" module, and expose the classes in any; structure you see fit.; The C++ names will continue to follow the C++ structure, however, as is needed; for e.g. pickling:. .. code-block:: python. >>> from cppyy.gbl import Namespace; >>> Concrete == Namespace.Concrete; False; >>> n = Namespace.Concrete.NestedClass(); >>> type(n); <class cppyy.gbl.Namespace.Concrete.NestedClass at 0x22114c0>; >>> type(n).__name__; NestedClass; >>> type(n).__module__; cppyy.gbl.Namespace.Concrete; >>> type(n).__cpp_name__; Namespace::Concrete::NestedClass; >>>. `Constructors`; --------------. Python and C++ both make a distinction between allocation (``__new__`` in; Python, ``operator new`` in C++) and initialization (``__init__`` in Python,; the constructor call in C++).; When binding, however, ther",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/classes.rst:331,load,loaded,331,bindings/pyroot/cppyy/cppyy/doc/source/classes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/classes.rst,3,['load'],"['load', 'loaded']"
Performance,".. _cmake_interface:. CMake interface; ===============. CMake fragments are provided for an Automated generation of an end-user; bindings package from a CMake-based project build.; The bindings generated by rootcling, are 'raw' in the sense that:. * The .cpp file be compiled. The required compilation steps are; platform-dependent.; * The bindings are not packaged for distribution. Typically, users expect; to have a pip-compatible package.; * The binding are in the 'cppyy.gbl' namespace. This is an inconvenience at; best for users who might expect C++ code from KF5::Config to appear in; Python via ""import KF5.Config"".; * The bindings are loaded lazily, which limits the discoverability of the; content of the bindings.; * ``cppyy`` supports customization of the bindings via 'Pythonization' but; there is no automated way to load them. These issues are addressed by the CMake support. This is a blend of Python; packaging and CMake where CMake provides:. * Platform-independent scripting of the creation of a Python 'wheel' package; for the bindings.; * An facility for CMake-based projects to automate the entire bindings; generation process, including basic automated tests. .. note::. The JIT needs to resolve linker symbols in order to call them through; generated wrappers.; Thus, any classes, functions, and data that will be used in Python need; to be exported.; This is the default behavior on Mac and Linux, but not on Windows.; On that platform, use ``__declspec(dllexport)`` to explicitly export the; classes and function you expect to call.; CMake has simple `support for exporting all`_ C++ symbols. Python packaging; ----------------. Modern Python packaging usage is based on the 'wheel'. This is places the onus; on the creation of binary artifacts in the package on the distributor. In this; case, this includes the platform-dependent steps necessary to compile the .cpp; file. The generated package also takes advantage of the __init__.py load-time; mechanism to enhance the b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst:645,load,loaded,645,bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst,2,['load'],"['load', 'loaded']"
Performance,".. _debugging:; ; Debugging; =========. By default, the ``clang`` JIT as used by cppyy does not generate debugging; information.; This is first of all because it has proven to be not reliable in all cases,; but also because in a production setting this information, being internal to; the wrapper generation, goes unused.; However, that does mean that a debugger that starts from python will not be; able to step through JITed code into the C++ function that needs debugging,; even when such information is available for that C++ function. To enable debugging information in JITed code, set the ``EXTRA_CLING_ARGS``; envar to ``-g`` (and any further compiler options you need, e.g. add ``-O2``; to debug optimized code). On a crash in C++, the backend will attempt to provide a stack trace.; This works quite well on Linux (through ``gdb``) and decently on MacOS; (through ``unwind``), but is currently unreliable on MS Windows.; To prevent printing of this trace, which can be slow to produce, set the; envar ``CPPYY_CRASH_QUIET`` to '1'. It is even more useful to obtain a traceback through the Python code that led; up to the problem in C++.; Many modern debuggers allow mixed-mode C++/Python debugging (for example; `gdb`_ and `MSVC`_), but cppyy can also turn abortive C++ signals (such as a; segmentation violation) into Python exceptions, yielding a normal traceback.; This is particularly useful when working with cross-inheritance and other; cross-language callbacks. To enable the signals to exceptions conversion, import the lowlevel module; ``cppyy.ll`` and use:. .. code-block:: python. import cppyy.ll; cppyy.ll.set_signals_as_exception(True). Call ``set_signals_as_exception(False)`` to disable the conversion again.; It is recommended to only have the conversion enabled around the problematic; code, as it comes with a performance penalty.; If the problem can be localized to a specific function, you can use its; ``__sig2exc__`` flag to only have the conversion active in that functi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/debugging.rst:704,optimiz,optimized,704,bindings/pyroot/cppyy/cppyy/doc/source/debugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/debugging.rst,1,['optimiz'],['optimized']
Performance,".. _features:. Miscellaneous; =============. .. toctree::; :hidden:. cppyy_features_header. This is a collection of a few more features listed that do not have a proper; place yet in the rest of the documentation. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. `Special variables`; -------------------. There are several conventional ""special variables"" that control behavior of; functions or provide (internal) information.; Often, these can be set/used in pythonizations to handle memory management or; Global Interpreter Lock (GIL) release. * ``__python_owns__``: a flag that every bound instance carries and determines; whether Python or C++ owns the C++ instance (and associated memory).; If Python owns the instance, it will be destructed when the last Python; reference to the proxy disappears.; You can check/change the ownership with the __python_owns__ flag that every; bound instance carries.; Example:. .. code-block:: python. >>> from cppyy.gbl import Concrete; >>> c = Concrete(); >>> c.__python_owns__ # True: object created in Python; True; >>>. * ``__creates__``: a flag that every C++ overload carries and determines; whether the return value is owned by C++ or Python: if ``True``, Python owns; the return value, otherwise C++. * ``__set_lifeline__``: a flag that every C++ overload carries and determines; whether the return value should place a back-reference on ``self``, to; prevent the latter from going out of scope before the return value does.; The default is ``False``, but will be automatically set at run-time if a; return value's address is a C++ object pointing into the memory of ``this``,; or if ``self`` is a by-value return. * ``__release_gil__``: a flag that every C++ overload carries and determine",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/misc.rst:343,load,loaded,343,bindings/pyroot/cppyy/cppyy/doc/source/misc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/misc.rst,2,['load'],"['load', 'loaded']"
Performance,".. _functions:. Functions; =========. C++ functions are first-class objects in Python and can be used wherever; Python functions can be used, including for dynamically constructing; classes. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. Function argument type conversions follow the expected rules, with implicit; conversions allowed, including between Python builtin types and STL types,; but it is rather more efficient to make conversions explicit. `Free functions`; ----------------. All bound C++ code starts off from the global C++ namespace, represented in; Python by ``gbl``.; This namespace, as any other namespace, is treated as a module after it has; been loaded.; Thus, we can directly import C++ functions from it and other namespaces that; themselves may contain more functions.; All lookups on namespaces are done lazily, thus if loading more headers bring; in more functions (incl. new overloads), these become available dynamically. .. code-block:: python. >>> from cppyy.gbl import global_function, Namespace; >>> global_function == Namespace.global_function; False; >>> from cppyy.gbl.Namespace import global_function; >>> global_function == Namespace.global_function; True; >>> from cppyy.gbl import global_function; >>>. Free functions can be bound to a class, following the same rules as apply to; Python functions: unless marked as static, they will turn into member; functions when bound to an instance, but act as static functions when called; through the class.; Consider this example:. .. code-block:: python. >>> from cppyy.gbl import Concrete, call_abstract_method; >>> c = Concrete(); >>> Concrete.callit = call_abstract_method; >>> Concrete.callit(c); called Concrete::abstract_method; >>> c.callit(); ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst:320,load,loaded,320,bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,3,['load'],"['load', 'loaded']"
Performance,".. _history:. History; =======. .. toctree::; :hidden:. What is now called `cppyy` started life as `RootPython` from `CERN`_, but; cppyy is not associated with CERN (it is still used there, however,; underpinning `PyROOT`_). Back in late 2002, Pere Mato of CERN, had the idea of using the `CINT`_ C++; interpreter, which formed the interactive interface to `ROOT`_, to call from; Python into C++: this became RootPython.; This binder interfaced with Python through `boost.python`_ (v1), transpiling; Python code into C++ and interpreting the result with CINT.; In early 2003, I ported this code to boost.python v2, then recently released.; In practice, however, re-interpreting the transpiled code was unusably slow,; thus I modified the code to make direct use of CINT's internal reflection; system, gaining about 25x in performance.; I presented this work as `PyROOT` at the ROOT Users' Workshop in early 2004,; and, after removing the boost.python dependency by using the C-API directly; (gaining another factor 7 in speedup!), it was included in ROOT.; PyROOT was presented at the SciPy'06 conference, but was otherwise not; advocated outside of High Energy Physics (HEP). In 2010, the PyPy core developers and I held a `sprint at CERN`_ to use; `Reflex`, a standalone alternative to CINT's reflection of C++, to add; automatic C++ bindings, PyROOT-style, to `PyPy`_.; This is where the name ""cppyy"" originated.; Coined by Carl Friedrich Bolz, if you want to understand the meaning, just; pronounce it slowly: cpp-y-y. After the ROOT team replaced CINT with `Cling`_, PyROOT soon followed.; As part of Google's Summer of Code '16, Aditi Dutta moved PyPy/cppyy to Cling; as well, and packaged the code for use through `PyPI`_.; I continued this integration with the Python eco-system by forking PyROOT,; reducing its dependencies, and repackaging it as CPython/cppyy.; The combined result is the current cppyy project.; Mid 2018, version 1.0 was released. .. _`CERN`: https://cern.ch/; .. _`PyROOT`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/history.rst:822,perform,performance,822,bindings/pyroot/cppyy/cppyy/doc/source/history.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/history.rst,1,['perform'],['performance']
Performance,".. _loop-terminology:. ===========================================; LLVM Loop Terminology (and Canonical Forms); ===========================================. .. contents::; :local:. Loop Definition; ===============. Loops are an important concept for a code optimizer. In LLVM, detection; of loops in a control-flow graph is done by :ref:`loopinfo`. It is based; on the following definition. A loop is a subset of nodes from the control-flow graph (CFG; where; nodes represent basic blocks) with the following properties:. 1. The induced subgraph (which is the subgraph that contains all the; edges from the CFG within the loop) is strongly connected; (every node is reachable from all others). 2. All edges from outside the subset into the subset point to the same; node, called the **header**. As a consequence, the header dominates; all nodes in the loop (i.e. every execution path to any of the loop's; node will have to pass through the header). 3. The loop is the maximum subset with these properties. That is, no; additional nodes from the CFG can be added such that the induced; subgraph would still be strongly connected and the header would; remain the same. In computer science literature, this is often called a *natural loop*.; In LLVM, a more generalized definition is called a; :ref:`cycle <cycle-terminology>`. Terminology; -----------. The definition of a loop comes with some additional terminology:. * An **entering block** (or **loop predecessor**) is a non-loop node; that has an edge into the loop (necessarily the header). If there is; only one entering block, and its only edge is to the; header, it is also called the loop's **preheader**. The preheader; dominates the loop without itself being part of the loop. * A **latch** is a loop node that has an edge to the header. * A **backedge** is an edge from a latch to the header. * An **exiting edge** is an edge from inside the loop to a node outside; of the loop. The source of such an edge is called an **exiting block**, i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:258,optimiz,optimizer,258,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['optimiz'],['optimizer']
Performance,".. _milegalizer:. Legalizer; ---------. This pass transforms the generic machine instructions such that they are legal. A legal instruction is defined as:. * **selectable** --- the target will later be able to select it to a; target-specific (non-generic) instruction. This doesn't necessarily mean that; :doc:`InstructionSelect` has to handle it though. It just means that; **something** must handle it. * operating on **vregs that can be loaded and stored** -- if necessary, the; target can select a ``G_LOAD``/``G_STORE`` of each gvreg operand. As opposed to SelectionDAG, there are no legalization phases. In particular,; 'type' and 'operation' legalization are not separate. Legalization is iterative, and all state is contained in GMIR. To maintain the; validity of the intermediate code, instructions are introduced:. * ``G_MERGE_VALUES`` --- concatenate multiple registers of the same; size into a single wider register. * ``G_UNMERGE_VALUES`` --- extract multiple registers of the same size; from a single wider register. * ``G_EXTRACT`` --- extract a simple register (as contiguous sequences of bits); from a single wider register. As they are expected to be temporary byproducts of the legalization process,; they are combined at the end of the :ref:`milegalizer` pass.; If any remain, they are expected to always be selectable, using loads and stores; if necessary. The legality of an instruction may only depend on the instruction itself and; must not depend on any context in which the instruction is used. However, after; deciding that an instruction is not legal, using the context of the instruction; to decide how to legalize the instruction is permitted. As an example, if we; have a ``G_FOO`` instruction of the form::. %1:_(s32) = G_CONSTANT i32 1; %2:_(s32) = G_FOO %0:_(s32), %1:_(s32). it's impossible to say that G_FOO is legal iff %1 is a ``G_CONSTANT`` with; value ``1``. However, the following::. %2:_(s32) = G_FOO %0:_(s32), i32 1. can say that it's legal iff operand 2 is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst:440,load,loaded,440,interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst,1,['load'],['loaded']
Performance,".. _philosophy:. Philosophy; ==========. .. toctree::; :hidden:. As a Python-C++ language binder, cppyy has several unique features: it fills; gaps and covers use cases not available through other binders.; This document explains some of the design choices made and the thinking; behind the implementations of those features.; It's categorized as ""philosophy"" because a lot of it is open to; interpretation.; Its main purpose is simply to help you decide whether cppyy covers your use; cases and binding requirements, before committing any time to; :ref:`trying it out <starting>`. Run-time v.s. compile-time; --------------------------. What performs better, run-time or compile-time?; The obvious answer is compile-time: see the performance differences between; C++ and Python, for example.; Obvious, but completely wrong, however.; In fact, when it comes to Python, it is even the `wrong question.`. Everything in Python is run-time: modules, classes, functions, etc. are all; run-time constructs.; A Python module that defines a class is a set of instructions to the Python; interpreter that lead to the construction of the desired class object.; A C/C++ extension module that defines a class does the same thing by calling; a succession of Python interpreter Application Programming Interfaces (APIs;; the exact same that Python uses itself internally).; If you use a compile-time binder such as `SWIG`_ or `pybind11`_ to bind a C++; class, then what gets compiled is the series of API calls necessary to; construct a Python-side equivalent at `run-time` (when the module gets; loaded), not the Python class object.; In short, whether a binding is created at ""compile-time"" or at run-time has; no measurable bearing on performance. What does affect performance is the overhead to cross the language barrier.; This consists of unboxing Python objects to extract or convert the underlying; objects or data to something that matches what C++ expects; overload; resolution based on the unboxed argume",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst:643,perform,performs,643,bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,2,['perform'],"['performance', 'performs']"
Performance,".. _python:. Python; ======. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. `PyObject`; ----------. Arguments and return types of ``PyObject*`` can be used, and passed on to; CPython API calls (or through ``cpyext`` in PyPy). `Doc strings`; -------------. The documentation string of a method or function contains the C++; arguments and return types of all overloads of that name, as applicable.; Example:. .. code-block:: python. >>> from cppyy.gbl import Concrete; >>> print Concrete.array_method.__doc__; void Concrete::array_method(int* ad, int size); void Concrete::array_method(double* ad, int size); >>>. `Help`; ------. Bound C++ class is first-class Python and can thus be inspected like any; Python objects can.; For example, we can ask for ``help()``:. .. code-block:: python. >>> help(Concrete); Help on class Concrete in module gbl:. class Concrete(Abstract); | Method resolution order:; | Concrete; | Abstract; | CPPInstance; | __builtin__.object; |; | Methods defined here:; |; | __assign__(self, const Concrete&); | Concrete& Concrete::operator=(const Concrete&); |; | __init__(self, *args); | Concrete::Concrete(int n = 42); | Concrete::Concrete(const Concrete&); |; etc. .... ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/python.rst:158,load,loaded,158,bindings/pyroot/cppyy/cppyy/doc/source/python.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/python.rst,2,['load'],"['load', 'loaded']"
Performance,".. _starting:. Trying it out; =============. This is a basic guide to try cppyy and see whether it works for you.; Large code bases will benefit from more advanced features such as; :doc:`pythonizations <pythonizations>` for a cleaner interface to clients;; precompiled modules for faster parsing and reduced memory usage;; "":ref:`dictionaries <dictionaries>`"" to package locations and manage; dependencies; and mapping files for automatic, lazy, loading.; You can, however, get very far with just the basics and it may even be; completely sufficient for small packages with fewer classes. cppyy works by parsing C++ definitions through ``cling``, generating tiny; wrapper codes to honor compile-time features and create standardized; interfaces, then compiling/linking those wrappers with the ``clang`` JIT.; It thus requires only those two ingredients: *C++ definitions* and; *linker symbols*.; All cppyy uses, the basic and the more advanced, are variations on the; theme of bringing these two together at the point of use. Definitions typically live in header files and symbols in libraries.; Headers can be loaded with ``cppyy.include`` and libraries with the; ``cppyy.load_library`` call.; Loading the header is sufficient to start exploring, with ``cppyy.gbl`` the; starting point of all things C++, while the linker symbols are only needed at ; the point of first use. Here is an example using the `zlib`_ library, which is likely available on; your system:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('zlib.h') # bring in C++ definitions; >>> cppyy.load_library('libz') # load linker symbols; >>> cppyy.gbl.zlibVersion() # use a zlib API; '1.2.11'; >>>. Since header files can include other header files, it is easy to aggregate; all relevant ones into a single header to include.; If there are project-specific include paths, you can add those paths through; ``cppyy.add_include_path``.; If a header is C-only and not set for use with C++, use ``cppyy.c_include``,; which ad",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst:447,load,loading,447,bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,1,['load'],['loading']
Performance,".. _stl:. STL; ===. Parts of the Standard Template Library (STL), in particular its container; types, are the de facto equivalent of Python's builtin types.; STL is written in C++ and Python bindings of it are fully functional as-is,; but are much more useful when pluggable into idiomatic expressions where; Python builtin containers are expected (e.g. in list contractions). There are two extremes to achieve such drop-in behavior: copy into Python; builtins, so that the Python-side always deals with true Python objects; or; adjust the C++ interfaces to be the same as their Python equivalents.; Neither is very satisfactory: the former is not because of the existence of; global/static variables and return-by-reference.; If only a copy is available, then expected modifications do not propagate.; Copying is also either slow (when copying every time) or memory intensive (if; the results are cached).; Filling out the interfaces may look more appealing, but all operations then; involve C++ function calls, which can be slower than the Python equivalents,; and C++-style error handling. Given that neither choice will satisfy all cases, ``cppyy`` aims to maximize; functionality and minimum surprises based on common use.; Thus, for example, ``std::vector`` grows a pythonistic ``__len__`` method,; but does not lose its C++ ``size`` method.; Passing a Python container through a const reference to a ``std::vector``; will trigger automatic conversion, but such an attempt through a non-const; reference will fail since a non-temporary C++ object is required [#f1]_ to; return any updates/changes. ``std::string`` is almost always converted to Python's ``str`` on function; returns (the exception is return-by-reference when assigning), but not when; its direct use is more likely such as in the case of (global) variables or; when iterating over a ``std::vector<std::string>``. The rest of this section shows examples of how STL containers can be used in; a natural, pythonistic, way. `std::vec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/stl.rst:898,cache,cached,898,bindings/pyroot/cppyy/cppyy/doc/source/stl.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/stl.rst,1,['cache'],['cached']
Performance,".. _toplevel:. Top Level; =========. cppyy provides a couple of helper functions at the module level that provide; (direct) access to the Cling interpreter (any C++ code is always accessed; through the global namespace ``cppyy.gbl``).; The documentation makes use of these helpers throughout, so they are listed; here first, but their documentation is more conveniently accessible through; the Python interpreter itself, using the ``help()`` function::. $ python; >>> import cppyy; >>> help(cppyy). `Loading C++`; -------------. C++ code can be loaded as text to be JITed, or be compiled ahead of time and; supplied in the form of a shared library.; In the latter case, C++ headers need to be loaded as well to declare; classes, functions, and variables to Cling.; Instead of headers, pre-compiled code can be used; in particular all of the; standard C++ headers and several system headers are pre-compiled at startup.; cppyy provides the following helpers to load C++ code:. * ``cppdef``: direct access to the interpreter.; This function accepts C++ declarations as a string and JITs them (bindings; are not created until actual use).; The code is loaded into the global scope, thus any previously loaded code; is available from one ``cppdef`` call to the next, as are all standard; C++ headers that have been loaded through pre-compiled headers.; Example::. >>> cppyy.cppdef(r""""""\; ... void hello() {; ... std::cout << ""Hello, World!"" << std::endl;; ... }""""""); True; >>> cppyy.gbl.hello(); Hello, World!; >>> . * ``cppexec``: direct access to the interpreter.; This function accepts C++ statements as a string, JITs and executes them.; Just like ``cppdef``, execution is in the global scope and all previously; loaded code is available.; If the statements are declarations, the effect is the same as ``cppdef``,; but ``cppexec`` also accepts executable lines.; Example::. >>> cppyy.cppexec(r""""""std::string hello = ""Hello, World!"";""""""); True; >>> cppyy.cppexec(""std::cout << hello << std::endl;""); He",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:545,load,loaded,545,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,3,['load'],"['load', 'loaded']"
Performance,".. _transformation-metadata:. ============================; Code Transformation Metadata; ============================. .. contents::; :local:. Overview; ========. LLVM transformation passes can be controlled by attaching metadata to; the code to transform. By default, transformation passes use heuristics; to determine whether or not to perform transformations, and when doing; so, other details of how the transformations are applied (e.g., which; vectorization factor to select).; Unless the optimizer is otherwise directed, transformations are applied; conservatively. This conservatism generally allows the optimizer to; avoid unprofitable transformations, but in practice, this results in the; optimizer not applying transformations that would be highly profitable. Frontends can give additional hints to LLVM passes on which; transformations they should apply. This can be additional knowledge that; cannot be derived from the emitted IR, or directives passed from the; user/programmer. OpenMP pragmas are an example of the latter. If any such metadata is dropped from the program, the code's semantics; must not change. Metadata on Loops; =================. Attributes can be attached to loops as described in :ref:`llvm.loop`.; Attributes can describe properties of the loop, disable transformations,; force specific transformations and set transformation options. Because metadata nodes are immutable (with the exception of; ``MDNode::replaceOperandWith`` which is dangerous to use on uniqued; metadata), in order to add or remove a loop attributes, a new ``MDNode``; must be created and assigned as the new ``llvm.loop`` metadata. Any; connection between the old ``MDNode`` and the loop is lost. The; ``llvm.loop`` node is also used as LoopID (``Loop::getLoopID()``), i.e.; the loop effectively gets a new identifier. For instance,; ``llvm.mem.parallel_loop_access`` references the LoopID. Therefore, if; the parallel access property is to be preserved after adding/removing; loop attribut",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:339,perform,perform,339,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,4,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,".. _type_conversions:. Type conversions; ================. Most type conversions are done automatically, e.g. between Python ``str``; and C++ ``std::string`` and ``const char*``, but low-level APIs exist to; perform explicit conversions. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. .. _sec-auto-casting-label:. `Auto-casting`; --------------. Object pointer returns from functions provide the most derived class known; (i.e. exposed in header files) in the hierarchy of the object being returned.; This is important to preserve object identity as well as to make casting,; a pure C++ feature after all, superfluous.; Example:. .. code-block:: python. >>> from cppyy.gbl import Abstract, Concrete; >>> c = Concrete(); >>> Concrete.show_autocast.__doc__; 'Abstract* Concrete::show_autocast()'; >>> d = c.show_autocast(); >>> type(d); <class '__main__.Concrete'>; >>>. As a consequence, if your C++ classes should only be used through their; interfaces, then no bindings should be provided to the concrete classes; (e.g. by excluding them using a :ref:`selection file <selection-files>`).; Otherwise, more functionality will be available in Python than in C++. Sometimes, however, full control over a cast is needed.; For example, if the instance is bound by another tool or even a 3rd party,; hand-written, extension library.; Assuming the object supports the ``PyCapsule`` or ``CObject`` abstraction,; then a C++-style reinterpret_cast (i.e. without implicitly taking offsets; into account), can be done by taking and rebinding the address of an; object:. .. code-block:: python. >>> from cppyy import addressof, bind_object; >>> e = bind_object(addressof(d), Abstract); >>> type(e); <class '__main__.Abstract'>; >>>. `Operators`; ---",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst:208,perform,perform,208,bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst,3,"['load', 'perform']","['load', 'loaded', 'perform']"
Performance,".. _utilities:. Utilities; =========. The ``cppyy-backend`` package brings in the following utilities to help; with repackaging and redistribution:. * cling-config: for compile time flags; * rootcling and genreflex: for dictionary generation; * cppyy-generator: part of the :doc:`CMake interface <cmake_interface>`. Compiler/linker flags; ---------------------. ``cling-config`` is a small utility to provide access to the as-installed; configuration, such as compiler/linker flags and installation directories, of; other components.; Usage examples::. $ cling-config --help; Usage: cling-config [--cflags] [--cppflags] [--cmake]; $ cling-config --cmake; /usr/local/lib/python2.7/dist-packages/cppyy_backend/cmake. .. _dictionaries:. Dictionaries; ------------. Loading header files or code directly into ``cling`` is fine for interactive; work and smaller packages, but large scale applications benefit from; pre-compiling code, using the automatic class loader, and packaging; dependencies in so-called ""dictionaries."". A `dictionary` is a generated C++ source file containing references to the; header locations used when building (and any additional locations provided),; a set of forward declarations to reduce the need of loading header files, and; a few I/O helper functions.; The name ""dictionary"" is historic: before ``cling`` was used, it contained; the complete generated C++ reflection information, whereas now that is; derived at run-time from the header files.; It is still possible to fully embed header files rather than only storing; their names and search locations, to make the dictionary more self-contained. After generating the dictionary, it should be compiled into a shared library.; This provides additional dependency control: by linking it directly with any; further libraries needed, you can use standard mechanisms such as ``rpath``; to locate those library dependencies.; Alternatively, you can add the additional libraries to load to the mapping; files of the class load",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:956,load,loader,956,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loader']
Performance,".. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17554,load,load,17554,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['load'],['load']
Performance,".. cppyy documentation master file, created by; sphinx-quickstart on Wed Jul 12 14:35:45 2017.; You can adapt this file completely to your liking, but it should at least; contain the root `toctree` directive. .. meta::; :description: cppyy: Automatic Python-C++ bindings; :keywords: Python, C++, llvm, cling, binding, bindings, automatic bindings, bindings generator, cross-language inheritance, calling C++ from Python, calling Python from C++, high performance, data science. cppyy: Automatic Python-C++ bindings; ====================================. cppyy is an automatic, run-time, Python-C++ bindings generator, for calling; C++ from Python and Python from C++.; Run-time generation enables detailed specialization for higher performance,; lazy loading for reduced memory use in large scale projects, Python-side; cross-inheritance and callbacks for working with C++ frameworks, run-time; template instantiation, automatic object downcasting, exception mapping, and; interactive exploration of C++ libraries.; cppyy delivers this without any language extensions, intermediate languages,; or the need for boiler-plate hand-written code.; For design and performance, see this `PyHPC'16 paper`_, albeit that the; CPython/cppyy performance has been vastly improved since, as well as this; `CAAS presentation`_.; For a quick teaser, see `Jason Turner's`_ introduction video. cppyy is based on `Cling`_, the C++ interpreter, to match Python's dynamism,; interactivity, and run-time behavior.; Consider this session, showing dynamic, interactive, mixing of C++ and Python; features (there are more examples throughout the documentation and in the; `tutorial`_):. .. code-block:: python. >>> import cppyy; >>> cppyy.cppdef(""""""; ... class MyClass {; ... public:; ... MyClass(int i) : m_data(i) {}; ... virtual ~MyClass() {}; ... virtual int add_int(int i) { return m_data + i; }; ... int m_data;; ... };""""""); True; >>> from cppyy.gbl import MyClass; >>> m = MyClass(42); >>> cppyy.cppdef(""""""; ... void sa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/index.rst:451,perform,performance,451,bindings/pyroot/cppyy/cppyy/doc/source/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/index.rst,3,"['load', 'perform']","['loading', 'performance']"
Performance,".. role:: raw-html(raw); :format: html. ========================; LLVM Bitcode File Format; ========================. .. contents::; :local:. Abstract; ========. This document describes the LLVM bitstream file format and the encoding of the; LLVM IR into it. Overview; ========. What is commonly known as the LLVM bitcode file format (also, sometimes; anachronistically known as bytecode) is actually two things: a `bitstream; container format`_ and an `encoding of LLVM IR`_ into the container format. The bitstream format is an abstract encoding of structured data, very similar to; XML in some ways. Like XML, bitstream files contain tags, and nested; structures, and you can parse the file without having to understand the tags.; Unlike XML, the bitstream format is a binary encoding, and unlike XML it; provides a mechanism for the file to self-describe ""abbreviations"", which are; effectively size optimizations for the content. LLVM IR files may be optionally embedded into a `wrapper`_ structure, or in a; `native object file`_. Both of these mechanisms make it easy to embed extra; data along with LLVM IR files. This document first describes the LLVM bitstream format, describes the wrapper; format, then describes the record structure used by LLVM IR files. .. _bitstream container format:. Bitstream Format; ================. The bitstream format is literally a stream of bits, with a very simple; structure. This structure consists of the following concepts:. * A ""`magic number`_"" that identifies the contents of the stream. * Encoding `primitives`_ like variable bit-rate integers. * `Blocks`_, which define nested content. * `Data Records`_, which describe entities within the file. * Abbreviations, which specify compression optimizations for the file. Note that the :doc:`llvm-bcanalyzer <CommandGuide/llvm-bcanalyzer>` tool can be; used to dump and inspect arbitrary bitstreams, which is very useful for; understanding the encoding. .. _magic number:. Magic Numbers; -------------. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst:904,optimiz,optimizations,904,interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,1,['optimiz'],['optimizations']
Performance,".. role:: raw-html(raw); :format: html. =================================; LLVM Code Coverage Mapping Format; =================================. .. contents::; :local:. Introduction; ============. LLVM's code coverage mapping format is used to provide code coverage; analysis using LLVM's and Clang's instrumentation based profiling; (Clang's ``-fprofile-instr-generate`` option). This document is aimed at those who would like to know how LLVM's code coverage; mapping works under the hood. A prior knowledge of how Clang's profile guided; optimization works is useful, but not required. For those interested in using; LLVM to provide code coverage analysis for their own programs, see the `Clang; documentation <https://clang.llvm.org/docs/SourceBasedCodeCoverage.html>`. We start by briefly describing LLVM's code coverage mapping format and the; way that Clang and LLVM's code coverage tool work with this format. After; the basics are down, more advanced features of the coverage mapping format; are discussed - such as the data structures, LLVM IR representation and; the binary encoding. High Level Overview; ===================. LLVM's code coverage mapping format is designed to be a self contained; data format that can be embedded into the LLVM IR and into object files.; It's described in this document as a **mapping** format because its goal is; to store the data that is required for a code coverage tool to map between; the specific source ranges in a file and the execution counts obtained; after running the instrumented version of the program. The mapping data is used in two places in the code coverage process:. 1. When clang compiles a source file with ``-fcoverage-mapping``, it; generates the mapping information that describes the mapping between the; source ranges and the profiling instrumentation counters.; This information gets embedded into the LLVM IR and conveniently; ends up in the final executable file when the program is linked. 2. It is also used by *llvm-cov* -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CoverageMappingFormat.rst:541,optimiz,optimization,541,interpreter/llvm-project/llvm/docs/CoverageMappingFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CoverageMappingFormat.rst,1,['optimiz'],['optimization']
Performance,".. role:: raw-html(raw); :format: html. Libclang tutorial; =================; The C Interface to Clang provides a relatively small API that exposes facilities for parsing source code into an abstract syntax tree (AST), loading already-parsed ASTs, traversing the AST, associating physical source locations with elements within the AST, and other facilities that support Clang-based development tools.; This C interface to Clang will never provide all of the information representation stored in Clang's C++ AST, nor should it: the intent is to maintain an API that is relatively stable from one release to the next, providing only the basic functionality needed to support development tools.; The entire C interface of libclang is available in the file `Index.h`_. Essential types overview; -------------------------. All types of libclang are prefixed with ``CX``. CXIndex; ~~~~~~~; An Index that consists of a set of translation units that would typically be linked together into an executable or library. CXTranslationUnit; ~~~~~~~~~~~~~~~~~; A single translation unit, which resides in an index. CXCursor; ~~~~~~~~; A cursor representing a pointer to some element in the abstract syntax tree of a translation unit. Code example; """""""""""""""""""""""". .. code-block:: cpp. // file.cpp; struct foo{; int bar;; int* bar_pointer;; };. .. code-block:: cpp. #include <clang-c/Index.h>; #include <iostream>. int main(){; CXIndex index = clang_createIndex(0, 0); //Create index; CXTranslationUnit unit = clang_parseTranslationUnit(; index,; ""file.cpp"", nullptr, 0,; nullptr, 0,; CXTranslationUnit_None); //Parse ""file.cpp"". if (unit == nullptr){; std::cerr << ""Unable to parse translation unit. Quitting.\n"";; return 0;; }; CXCursor cursor = clang_getTranslationUnitCursor(unit); //Obtain a cursor at the root of the translation unit; }. Visiting elements of an AST; ~~~~~~~~~~~~~~~~~~~~~~~~~~~; The elements of an AST can be recursively visited with pre-order traversal with ``clang_visitChildren``. .. code-bloc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibClang.rst:219,load,loading,219,interpreter/llvm-project/clang/docs/LibClang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibClang.rst,1,['load'],['loading']
Performance,"... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, som",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5664,load,loading,5664,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,2,['load'],['loading']
Performance,"...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===-----------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2986,load,load,2986,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,2,['load'],['load']
Performance,"...; offsetN[.discriminator]: number_of_samples [fn5:num fn6:num ... ]; offsetA[.discriminator]: fnA:num_of_total_samples; offsetA1[.discriminator]: number_of_samples [fn7:num fn8:num ... ]; offsetA1[.discriminator]: number_of_samples [fn9:num fn10:num ... ]; offsetB[.discriminator]: fnB:num_of_total_samples; offsetB1[.discriminator]: number_of_samples [fn11:num fn12:num ... ]. This is a nested tree in which the indentation represents the nesting level; of the inline stack. There are no blank lines in the file. And the spacing; within a single line is fixed. Additional spaces will result in an error; while reading the file. Any line starting with the '#' character is completely ignored. Inlined calls are represented with indentation. The Inline stack is a; stack of source locations in which the top of the stack represents the; leaf function, and the bottom of the stack represents the actual; symbol to which the instruction belongs. Function names must be mangled in order for the profile loader to; match them in the current translation unit. The two numbers in the; function header specify how many total samples were accumulated in the; function (first number), and the total number of samples accumulated; in the prologue of the function (second number). This head sample; count provides an indicator of how frequently the function is invoked. There are two types of lines in the function body. - Sampled line represents the profile information of a source location.; ``offsetN[.discriminator]: number_of_samples [fn5:num fn6:num ... ]``. - Callsite line represents the profile information of an inlined callsite.; ``offsetA[.discriminator]: fnA:num_of_total_samples``. Each sampled line may contain several items. Some are optional (marked; below):. a. Source line offset. This number represents the line number; in the function where the sample was collected. The line number is; always relative to the line where symbol of the function is; defined. So, if the function has its head",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:98404,load,loader,98404,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['load'],['loader']
Performance,"...; };. template<class T, class U = AStruct<T>>; inline void freeFunction() { /* ... */ }; inline void do(unsigned N = 1) { /* ... */ }. ``` ; The associated with libA header files form libA's full descriptor. A.h,; potentially only part of the descriptor of libA, expands to more than 26000; lines of code. ```cpp; // Main.cpp; #include ""A.h""; int main() {; do();; return 0;; }. ```; Main.cpp, reuses code from libA by including libA's descriptor and links against; libA. The full descriptor can contain thousands of files expanding to millions; of lines of code -- a common case for framework libraries, for instance. ROOT goes further and enhances C++ by allowing the following code to work without; explicitly requiring to `#include <A.h>`. Currently, ROOT's lack of support of; line `#5` is a long-standing, known limitation that is lifted with modules. ```cpp; // ROOT prompt; root [] AStruct<float> S0; // #1: implicit loading of libA. Full descriptor required.; root [] AStruct<float>* S1; // #2: implicit loading of libA. No full descriptor required.; root [] if (gFile) S1->doIt(); // #3: implicit loading of libA. Full descriptor required.; root [] gSystem->Load(""libA""); // #4: explicit loading of libA. No full descriptor required.; root [] do(); // #5: error: implicit loading of libA is currently unsupported. ```. This pattern is not only used in the ROOT prompt but in I/O hotspots such as; `ShowMembers` and `TClass::IsA`. A naive implementation of this feature would require inclusion of all reachable; library descriptors (aka header files) at ROOT startup time. Of course this is; not feasible and ROOT inserts a set of optimizations to fence itself from the; costly full header inclusion. Unfortunately, several of them are home-grown and; in a few cases inaccurate (eg line #5) causing a noticeable technical debt. Here we will briefly describe the three common layers of optimizations: ROOT PCH,; ROOTMAP and RDICT. The ROOT precompiled header (PCH) reduces the CPU and memor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:4942,load,loading,4942,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['load'],['loading']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1013_vdst_9041ac:. vdst; ====. Image data to be loaded by an image instruction. *Size:* 4 dwords. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_9041ac.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_9041ac.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_9041ac.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1013_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_eae4c8.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_4d2300:. vdst; ====. Data loaded from memory. This is an optional operand. It must be used if and only if :ref:`lds<amdgpu_synid_lds>` is omitted. *Size:* 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_4d2300.rst:222,load,loaded,222,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_4d2300.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_4d2300.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_5ec176:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` affects operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_5ec176.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_5ec176.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_5ec176.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_9041ac:. vdst; ====. Image data to be loaded by an image instruction. *Size:* 4 dwords. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_9041ac.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_9041ac.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_9041ac.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_dfa6da.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_eae4c8.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx10_vdst_4d2300:. vdst; ====. Data loaded from memory. This is an optional operand. It must be used if and only if :ref:`lds<amdgpu_synid_lds>` is omitted. *Size:* 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_4d2300.rst:220,load,loaded,220,interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_4d2300.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_4d2300.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx10_vdst_5ec176:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` affects operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_5ec176.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_5ec176.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_5ec176.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx10_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_dfa6da.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx10_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_eae4c8.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_5ec176:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` affects operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_5ec176.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_5ec176.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_5ec176.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_9041ac:. vdst; ====. Image data to be loaded by an image instruction. *Size:* 4 dwords. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_9041ac.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_9041ac.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_9041ac.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_dfa6da.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_e2d005:. vdst; ====. Image data to be loaded by an image instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` and :ref:`tfe<amdgpu_synid_tfe>` affect operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_e2d005.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_e2d005.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_e2d005.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_eae4c8.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx7_vdst_1f3009:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies 1 dword. :ref:`tfe<amdgpu_synid_tfe>` adds one more dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_1f3009.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_1f3009.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_1f3009.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx7_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_dfa6da.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx8_vdst_4730df:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` has different meanings for GFX8.0 and GFX8.1:. * For GFX8.0, this modifier does not affect the size of data elements in registers. Values in registers are stored in low 16 bits, high 16 bits are unused. There is no packing.; * Starting from GFX8.1, this modifier specifies that values in registers are packed; each value occupies 16 bits. * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_4730df.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_4730df.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_4730df.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx8_vdst_829fc5:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` and :ref:`tfe<amdgpu_synid_tfe>` affect operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` has different meanings for GFX8.0 and GFX8.1:. * For GFX8.0, this modifier does not affect the size of data elements in registers. Values in registers are stored in low 16 bits, high 16 bits are unused. There is no packing.; * Starting from GFX8.1, this modifier specifies that values in registers are packed; each value occupies 16 bits. * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_829fc5.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_829fc5.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_829fc5.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx8_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_dfa6da.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx90a_vdst_92bb33:. vdst; ====. Data loaded from memory. This is an optional operand. It must be used if and only if :ref:`lds<amdgpu_synid_lds>` is omitted. *Size:* 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`, :ref:`a<amdgpu_synid_a>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_92bb33.rst:221,load,loaded,221,interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_92bb33.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_92bb33.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx90a_vdst_a9ee3f:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`, :ref:`a<amdgpu_synid_a>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_a9ee3f.rst:233,load,loaded,233,interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_a9ee3f.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_a9ee3f.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx90a_vdst_f5eb9d:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`, :ref:`a<amdgpu_synid_a>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_f5eb9d.rst:233,load,loaded,233,interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_f5eb9d.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_f5eb9d.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx9_vdst_4d2300:. vdst; ====. Data loaded from memory. This is an optional operand. It must be used if and only if :ref:`lds<amdgpu_synid_lds>` is omitted. *Size:* 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_4d2300.rst:219,load,loaded,219,interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_4d2300.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_4d2300.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx9_vdst_5ec176:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` affects operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_5ec176.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_5ec176.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_5ec176.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx9_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_dfa6da.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx9_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_eae4c8.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,".; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:367638,perform,performing,367638,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,".; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1. buffer_wbl2. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC globa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:278687,load,load,278687,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,".; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_inv.; - Ensures that the; acquire-fence-paired; atomic h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:325569,load,load,325569,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,".; - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; req",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:274734,load,load,274734,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,".; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; atomicrmw rele",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:260441,perform,performing,260441,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,".; Complex types and void views require additional memory copies to populate an object in memory from the column data. A view can iterate over the entry range, over the field range, and over the range of a collection within an entry.; For instance, for a field `std::vector<float> pt`, a view can iterate over all `pt` values of all entries, or over the `pt` values of a particular entry. A view can safely outlive its originating reader.; Once the reader is deconstructed, any attempt to read data will throw an exception, but the view is still properly destructed. Views that originate from the same reader _cannot_ be used concurrently by different threads. Internal Classes; ----------------. ### RNTupleDS; The `RNTupleDS` class is an internal class that provides an RNTuple data source for RDataFrame.; It is part of the `ROOTDataFrame` library.; The RNTuple data source supports chains with a constructor that takes a list of input files.; The RNTuple data source also supports multi-threaded dataframes, parallelized on the file and cluster level. The data source exposes inner fields of complex collections.; For instance, if the data model contains a vector of `Event` classes, where each `Event` has `pt` and `eta` floats,; the dataframe can use the event vector itself (`Event` column) as well as the `float` columns `Event.pt` and `Event.eta`. ### RClusterPool; The RClusterPool is an internal class owned be a page source.; The cluster pool maintains an I/O thread that asynchronously prefetches the next few clusters.; Through `RPageSource::SetEntryRange()`, the cluster pool is instructed to not read beyond the given limit.; This is used in the RNTuple data source when multiple threads work on different clusters of the same file. ### RMiniFile; The RMiniFile is an internal class used to read and write RNTuple data in a ROOT file.; It provides a minimal subset of the `TFile` functionality.; Its purpose is to reduce the coupling between RNTuple and the ROOT I/O library. For writ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:14705,multi-thread,multi-threaded,14705,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['multi-thread'],['multi-threaded']
Performance,".; Implement a new function in the MnUserTransformation class, FindIndex(name), which returns -1 when the parameter name does not exist.; Implement new methods in Minuit2Minimizer as requested by the Minimizer interface:; SetPrecision(double eps) to change the precision value used internally in Minuit2 (in MnPrecision), VariableName(index) to return the name of a variable (parameter) given an index, and VariableIndex(name) to return the index of a variable given a name.; Set a status code in Minuit2Minimizer according to the following convention:; status = minimizeStatus + 10 * minosStatus + 100 * hesseStatus.; See the Minuit2Minimizer reference documentation for the possible values of minimizeStatus , minosStatus and hesseStatus.; In MnHesse. when the inversion of the hessian matrix failed, return MnInvertFailed instead of MnHesseFailed. Mathcore Fitting classes. Fix the fitting with the integral option in multi-dimensions.; Force the gradient calculation when requested in the minimizer; classes and avoid to perform the check when using TMinuit. This was; already the case in Minuit2.; Add new class ROOT::Fit::SparseData for dealing with binned sparse data. This class automatically merges the empty region, so they can be considered, whenever possible as a larger single bin. This improves the performances when doing likelihood fits on the sparse data.; Fix the likelihood fits for variable bin histograms. Now a correct normalization is applied according to the bin volume.; Add new methods in Minimizer class :. Minimizer::SetPrecision(double eps) to change in the minimizer the precision on which the objective functions are evaluated. By default the numerical double precision is used inside the minimizers. This method should be used only if the precision in the function evaluation is worse than the double precision.; std::string Minimizer::VariableName (unsigned int index) to return a name of the minimizer variable (i.e. a fitting parameter) given the integer index. Ret",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v526/index.html:2823,perform,perform,2823,math/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v526/index.html,1,['perform'],['perform']
Performance,".; In the following we will focus on the non-declared overlaps of type A); and B) since this is the main source of errors during tracking. These; are generally non-intended overlaps due to coding mistakes or bad; geometry design. The checking package is loaded together with the; painter classes and contains an automated overlap checker.**. ![Overlap checking](pictures/030001DF.png). This can be activated both at volume level (checking for illegal; overlaps only one level inside a given volume) and from the geometry; manager level (checking full geometry):. ``` {.cpp}; myVolume->CheckOverlaps(precision, option);; gGeoManager->CheckOverlaps(precision);; myNode->CheckOverlaps(precision);; ```. Here precision represents the desired maximum accepted overlap value in; centimeters (default value is 0.1). This tool checks all possible; significant pairs of candidates inside a given volume (not declared as; overlapping or division volumes). The check is performed by verifying; the mesh representation of one candidate against the shape of the other.; This sort of check cannot identify all possible overlapping topologies,; but it works for more than 95% and is much faster than the usual; shape-to-shape comparison. For a 100% reliability, one can perform the; check at the level of a single volume by using `option`=""`d`"" or; `option`=""`d<number>`"" to perform overlap checking by sampling the; volume with \<`number`\> random points (default 1 million). This; produces also a picture showing in red the overlapping region and; estimates the volume of the overlaps. An extrusion A) is declared in any of the following cases:. - At least one of the vertices of the daughter mesh representation is; outside the mother volume (in fact its shape) and having a safety; distance to the mother greater than the desired value;; - At least one of the mother vertices is contained also by one of its; daughters, in the same conditions. An overlap B) is declared if:. - At least one vertex of a positione",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:132982,perform,performed,132982,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,".; When JSROOT is used with THttpServer, the address looks like:. ```javascript; <script type='module'>; import { httpRequest, draw } from 'http://your_root_server:8080/jsrootsys/modules/main.mjs';; let obj = await httpRequest('http://your_root_server:8080/Objects/hist/root.json','object');; await draw('drawing', obj, 'hist');; </script>; ```. Loading main module is enough to get public JSROOT functionality - reading files and drawing objects.; One also can load some special components directly like:. ```javascript; <script type='module'>; import { HierarchyPainter } from 'https://root.cern/js/latest/modules/gui.mjs';. let h = new HierarchyPainter(""example"", ""myTreeDiv"");. // configure 'simple' in provided <div> element; // one also can specify ""grid2x2"" or ""flex"" or ""tabs""; h.setDisplay(""simple"", ""myMainDiv"");. // open file and display element; await h.openRootFile('../../files/hsimple.root');; await h.display('hpxpy;1"",""colz');; </script>; ```. After script loading one can configure different parameters in `gStyle` object.; It is instance of the `TStyle` object and behaves like `gStyle` variable in ROOT. For instance,; to change stat format using to display value in stats box:. ```javascript; import { gStyle } from 'https://root.cern/js/latest/modules/main.mjs';; gStyle.fStatFormat = '7.5g';; ```. There is also `settings` object which contains all other JSROOT settings. For instance,; one can configure custom format for different axes:. ```javascript; import { settings } from 'https://root.cern/js/latest/modules/main.mjs';; settings.XValuesFormat = '4.2g';; settings.YValuesFormat = '6.1f';; ```. One also can use `build/jsroot.js` bundle to load all functionality at one and access it via `JSROOT` global handle:. ```javascript; <script src=""https://root.cern/js/latest/build/jsroot.js""></script>; <script>; // getting json string from somewhere; let obj = JSROOT.parse(root_json);; JSROOT.draw('plain', obj, 'colz');; </script>; ```. ### Use of JSON. It is strongly reco",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:35586,load,loading,35586,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['load'],['loading']
Performance,".bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64738,perform,performance,64738,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['perform'],['performance']
Performance,.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.h; clang-tools-extra/clang-tidy/readability/ConstReturnTypeCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.h; clang-tools-extra/clang-tidy/readability/ContainerDataPointerCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerDataPointe,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:66165,perform,performance,66165,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,".csail.mit.edu/vlk/spectre11.pdf. We will analyze these two variants independently. First, variant #1.1 works by; speculatively storing over the return address after a bounds check bypass. This; speculative store then ends up being used by the CPU during speculative; execution of the return, potentially directing speculative execution to; arbitrary gadgets in the binary. Let's look at an example.; ```; unsigned char local_buffer[4];; unsigned char *untrusted_data_from_caller = ...;; unsigned long untrusted_size_from_caller = ...;; if (untrusted_size_from_caller < sizeof(local_buffer)) {; // Speculative execution enters here with a too-large size.; memcpy(local_buffer, untrusted_data_from_caller,; untrusted_size_from_caller);; // The stack has now been smashed, writing an attacker-controlled; // address over the return address.; minor_processing(local_buffer);; return;; // Control will speculate to the attacker-written address.; }; ```. However, this can be mitigated by hardening the load of the return address just; like any other load. This is sometimes complicated because x86 for example; *implicitly* loads the return address off the stack. However, the; implementation technique below is specifically designed to mitigate this; implicit load by using the stack pointer to communicate misspeculation between; functions. This additionally causes a misspeculation to have an invalid stack; pointer and never be able to read the speculatively stored return address. See; the detailed discussion below. For variant #1.2, the attacker speculatively stores into the vtable or jump; table used to implement an indirect call or indirect jump. Because this is; speculative, this will often be possible even when these are stored in; read-only pages. For example:; ```; class FancyObject : public BaseObject {; public:; void DoSomething() override;; };; void f(unsigned long attacker_offset, unsigned long attacker_data) {; FancyObject object = getMyObject();; unsigned long *arr[4] = getFou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:12206,load,load,12206,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['load'],['load']
Performance,".f, do. RooAbsReal* dgdx = gauss.derivative(x,1) ;. A more complete example is available in the new tutorial macro rf111_derivatives.C. Improved handling of chi-squared fits; Chi-squared fits can now be performed through the same style of interface as likelihood fits,; through the newly added method RooAbsReal::chi2FitTo(const RooDataHist&,...). Functions that can be fitted with chi-squared minimization are any RooAbsReal based function; as well as RooAbsPdf based p.d.f.s. In case of non-extended p.d.f.s the probability density; calculated by the p.d.f. is multiplied with the number of events in the histogram to adjust; the scale of the function. In case of extended p.d.f.s, the adjustment is made with the expected; number of events, rather than the observed number of events. Tutorial macro rf602_chi2fit.C has been updated to use this new interface. Chi-squared fits to X-Y datasets now possible; In addition to the ability to perform chi-squared fits to histograms it is now also possible; to perform chi-squared fits to unbinned datasets containing a series of X and Y values; with associated errors on Y and optionally on X. These 'X-Y' chi-squared fits are interfaced through newly added method; RooAbsReal::chi2FitTo(const RooDataSet&,...). By default the event weight is; interpreted as the 'Y' value, but an YVar() argument can designate any other; dataset column as Y value. If X errors are defined, one can choose to integrate the fitted; function over the range of the X errors, rather than taking the central value by adding; an Integrate(true) argument to chi2FitTo(); Two new arguments, StoreError(const RooArgSet&) and StoreAsymError(const RooArgSet&); have been added to the RooDataSet constructor to simplify the process of storing the errors; of X and Y variables along with their values in a dataset. The newly added tutorial macro rf609_xychi2fit.C illustrates the use of all this; new functionality. Uniform interface for creation of (profile likelihoods) and chi-squa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html:8230,perform,perform,8230,roofit/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html,2,['perform'],['perform']
Performance,".fr = freeze <2 x i32> %v ; element-wise freeze; %d = extractelement <2 x i32> %v.fr, i32 0 ; not undef; %add.f = add i32 %d, %d ; even number. ; branching on frozen value; %poison = add nsw i1 %k, undef ; poison; %c = freeze i1 %poison; br i1 %c, label %foo, label %bar ; non-deterministic branch to %foo or %bar. .. _i_call:. '``call``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = [tail | musttail | notail ] call [fast-math flags] [cconv] [ret attrs] [addrspace(<num>)]; <ty>|<fnty> <fnptrval>(<function args>) [fn attrs] [ operand bundles ]. Overview:; """""""""""""""""". The '``call``' instruction represents a simple function call. Arguments:; """""""""""""""""""". This instruction requires several arguments:. #. The optional ``tail`` and ``musttail`` markers indicate that the optimizers; should perform tail call optimization. The ``tail`` marker is a hint that; `can be ignored <CodeGenerator.html#tail-call-optimization>`_. The; ``musttail`` marker means that the call must be tail call optimized in order; for the program to be correct. This is true even in the presence of; attributes like ""disable-tail-calls"". The ``musttail`` marker provides these; guarantees:. #. The call will not cause unbounded stack growth if it is part of a; recursive cycle in the call graph.; #. Arguments with the :ref:`inalloca <attr_inalloca>` or; :ref:`preallocated <attr_preallocated>` attribute are forwarded in place.; #. If the musttail call appears in a function with the ``""thunk""`` attribute; and the caller and callee both have varargs, then any unprototyped; arguments in register or memory are forwarded to the callee. Similarly,; the return value of the callee is returned to the caller's caller, even; if a void return type is in use. Both markers imply that the callee does not access allocas from the caller.; The ``tail`` marker additionally implies that the callee does not access; varargs from the caller. Calls marked ``musttail`` must obey the following; additional rules:. - T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:472831,optimiz,optimized,472831,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,".h""; #include ""TLine.h""; #include ""TEventList.h"". const Double_t dxbin = (0.17-0.13)/40; // Bin-width; const Double_t sigma = 0.0012;; TEventList *elist = 0;; Bool_t useList, fillList;; TH1F *hdmd;; TH2F *h2;. //_________________________________________________________; Double_t fdm5(Double_t *xx, Double_t *par); {; Double_t x = xx[0];; if (x <= 0.13957) return 0;; Double_t xp3 = (x-par[3])*(x-par[3]);; Double_t res = dxbin*(par[0]*TMath::Power(x-0.13957,par[1]); + par[2]/2.5066/par[4]*TMath::Exp(-xp3/2/par[4]/par[4]));; return res;; }. //_________________________________________________________; Double_t fdm2(Double_t *xx, Double_t *par); {; Double_t x = xx[0];; if (x <= 0.13957) return 0;; Double_t xp3 = (x-0.1454)*(x-0.1454);; Double_t res = dxbin*(par[0]*TMath::Power(x-0.13957,0.25); + par[1]/2.5066/sigma*TMath::Exp(-xp3/2/sigma/sigma));; return res;; }. //_________________________________________________________; void h1analysis::Begin(TTree *tree); {; // function called before starting the event loop; // -it performs some cleanup; // -it creates histograms; // -it sets some initialization for the event list. //initialize the Tree branch addresses; Init(tree);. //print the option specified in the Process function; TString option = GetOption();; printf(""Starting h1analysis with process option: %sn"",option.Data());. //Some cleanup in case this function had already been executed; //Delete any previously generated histograms or functions; gDirectory->Delete(""hdmd"");; gDirectory->Delete(""h2*"");; delete gROOT->GetFunction(""f5"");; delete gROOT->GetFunction(""f2"");. //create histograms; hdmd = new TH1F(""hdmd"",""dm_d"",40,0.13,0.17);; h2 = new TH2F(""h2"",""ptD0 vs dm_d"",30,0.135,0.165,30,-3,6);. //process cases with event list; fillList = kFALSE;; useList = kFALSE;; fChain->SetEventList(0);; delete gDirectory->GetList()->FindObject(""elist"");. // case when one creates/fills the event list; if (option.Contains(""fillList"")) {; fillList = kTRUE;; elist = new TEventList(""elist"",""s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ExampleAnalysis.md:6364,perform,performs,6364,documentation/users-guide/ExampleAnalysis.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ExampleAnalysis.md,1,['perform'],['performs']
Performance,".html); has shown that this approach is incomplete and only catches a small and limited; subset of attackable patterns which happen to resemble very closely the initial; proofs of concept. As such, while its performance is acceptable, it does not; appear to be an adequate systematic mitigation. ## Performance Overhead. The performance overhead of this style of comprehensive mitigation is very; high. However, it compares very favorably with previously recommended; approaches such as the `lfence` instruction. Just as users can restrict the; scope of `lfence` to control its performance impact, this mitigation technique; could be restricted in scope as well. However, it is important to understand what it would cost to get a fully; mitigated baseline. Here we assume targeting a Haswell (or newer) processor and; using all of the tricks to improve performance (so leaves the low 2gb; unprotected and +/- 2gb surrounding any PC in the program). We ran both; Google's microbenchmark suite and a large highly-tuned server built using; ThinLTO and PGO. All were built with `-march=haswell` to give access to BMI2; instructions, and benchmarks were run on large Haswell servers. We collected; data both with an `lfence`-based mitigation and load hardening as presented; here. The summary is that mitigating with load hardening is 1.77x faster than; mitigating with `lfence`, and the overhead of load hardening compared to a; normal program is likely between a 10% overhead and a 50% overhead with most; large applications seeing a 30% overhead or less. | Benchmark | `lfence` | Load Hardening | Mitigated Speedup |; | -------------------------------------- | -------: | -------------: | ----------------: |; | Google microbenchmark suite | -74.8% | -36.4% | **2.5x** |; | Large server QPS (using ThinLTO & PGO) | -62% | -29% | **1.8x** |. Below is a visualization of the microbenchmark suite results which helps show; the distribution of results that is somewhat lost in the summary. The y-axis is; a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:47167,tune,tuned,47167,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['tune'],['tuned']
Performance,".inc`` specifies enum values to identify the features,; arrays of constants to represent the CPU features and CPU subtypes, and the; ``ParseSubtargetFeatures`` method that parses the features string that sets; specified subtarget options. The generated ``SparcGenSubtarget.inc`` file; should be included in the ``SparcSubtarget.cpp``. The target-specific; implementation of the ``XXXSubtarget`` method should follow this pseudocode:. .. code-block:: c++. XXXSubtarget::XXXSubtarget(const Module &M, const std::string &FS) {; // Set the default features; // Determine default and user specified characteristics of the CPU; // Call ParseSubtargetFeatures(FS, CPU) to parse the features string; // Perform any additional operations; }. JIT Support; ===========. The implementation of a target machine optionally includes a Just-In-Time (JIT); code generator that emits machine code and auxiliary structures as binary; output that can be written directly to memory. To do this, implement JIT code; generation by performing the following steps:. * Write an ``XXXCodeEmitter.cpp`` file that contains a machine function pass; that transforms target-machine instructions into relocatable machine; code. * Write an ``XXXJITInfo.cpp`` file that implements the JIT interfaces for; target-specific code-generation activities, such as emitting machine code and; stubs. * Modify ``XXXTargetMachine`` so that it provides a ``TargetJITInfo`` object; through its ``getJITInfo`` method. There are several different approaches to writing the JIT support code. For; instance, TableGen and target descriptor files may be used for creating a JIT; code generator, but are not mandatory. For the Alpha and PowerPC target; machines, TableGen is used to generate ``XXXGenCodeEmitter.inc``, which; contains the binary coding of machine instructions and the; ``getBinaryCodeForInstr`` method to access those codes. Other JIT; implementations do not. Both ``XXXJITInfo.cpp`` and ``XXXCodeEmitter.cpp`` must include the; ``llvm/Cod",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:75710,perform,performing,75710,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['performing']
Performance,".org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitizer)`` to check if the code is being built; with :doc:`AddressSanitizer`. Use ``__has_feature(thread_sanitizer)`` to check if the code is being built; with :doc:`ThreadSanitizer`. Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; with :doc:`MemorySanitizer`. Use ``__has_feature(dataflow_sanitizer)`` to check if the code is being built; with :doc:`DataFlowSanitizer`. Use ``__has_feature(safe_stack)`` to check if the code is being built; with :doc:`SafeStack`. Extensions for selectively disabling optimization; =================================================. Clang provides a mechanism for selectively disabling optimizations in functions; and methods. To disable optimizations in a single function definition, the GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This function will be decorated with optnone.; int foo() {; // ... code; }. // optnone conflicts with always_inline, so bar() will not be decorated.; __attribute__((always_inline)) int bar() {; // ... code; }; #pragma cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:158661,optimiz,optimizations,158661,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizations']
Performance,".txt"")` or; `gStyle->SetPalette(""filename.txt"")`. The input file contains one color; per line in ""r g b"" float format. This function is useful to profit from; a full set of colour-vision deficiency friendly and perceptually uniform; colour maps that are available to; [download](https://doi.org/10.5281/zenodo.4491293). ## The Graphics Editor. A new graphics editor took place in ROOT v4.0. The editor can be; activated by selecting the Editor menu entry in the canvas View menu or; one of the context menu entries for setting line, fill, marker or text; attributes. The following object editors are available for the current; ROOT version. ### TAxisEditor. ![](pictures/030000D5.png). This user interface gives the possibility for changing the following; axis attributes:. - color of the selected axis, the axis' title and labels;. - the length of thick parameters and the possibility to set them on; both axis sides (if `+-` is selected);. - to set logarithmic or linear scale along the selected axis with a; choice for optimized or more logarithmic labels;. - primary, secondary and tertiary axis divisions can be set via the; three number fields;. - the axis title can be added or edited and the title's color,; position, offset, size and font can be set interactively;. - the color, size, and offset of axis labels can be set similarly. In; addition, there is a check box for no exponent choice, and another; one for setting the same decimal part for all labels. ### TPadEditor. ![](pictures/030000D6.png). - It provides the following user interface:. - Fixed aspect ratio - can be set for pad resizing. - Edit - sets pad or canvas as editable. - Cross-hair - sets a cross hair on the pad. - TickX - set ticks along the X axis. - TickY - set ticks along the Y axis. - GridX - set a grid along the X axis. - GridY - set a grid along the Y axis. - The pad or canvas border size can be set if a sunken or a raised; border mode is. - selected; no border mode can be set too. ## Copy and Paste. You ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:84737,optimiz,optimized,84737,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['optimiz'],['optimized']
Performance,".txt` files which may be required. - `TEST_SUITE_COLLECT_STATS`. Collect internal LLVM statistics. Appends `-save-stats=obj` when invoking the; compiler and makes the lit runner collect and merge the statistic files. - `TEST_SUITE_RUN_BENCHMARKS`. If this is set to `OFF` then lit will not actually run the tests but just; collect build statistics like compile time and code size. - `TEST_SUITE_USE_PERF`. Use the `perf` tool for time measurement instead of the `timeit` tool that; comes with the test-suite. The `perf` is usually available on linux systems. - `TEST_SUITE_SPEC2000_ROOT`, `TEST_SUITE_SPEC2006_ROOT`, `TEST_SUITE_SPEC2017_ROOT`, ... Specify installation directories of external benchmark suites. You can find; more information about expected versions or usage in the README files in the; `External` directory (such as `External/SPEC/README`). ### Common CMake Flags. - `-GNinja`. Generate build files for the ninja build tool. - `-Ctest-suite/cmake/caches/<cachefile.cmake>`. Use a CMake cache. The test-suite comes with several CMake caches which; predefine common or tricky build configurations. Displaying and Analyzing Results; --------------------------------. The `compare.py` script displays and compares result files. A result file is; produced when invoking lit with the `-o filename.json` flag. Example usage:. - Basic Usage:. ```text; % test-suite/utils/compare.py baseline.json; Warning: 'test-suite :: External/SPEC/CINT2006/403.gcc/403.gcc.test' has No metrics!; Tests: 508; Metric: exec_time. Program baseline. INT2006/456.hmmer/456.hmmer 1222.90; INT2006/464.h264ref/464.h264ref 928.70; ...; baseline; count 506.000000; mean 20.563098; std 111.423325; min 0.003400; 25% 0.011200; 50% 0.339450; 75% 4.067200; max 1222.896800; ```. - Show compile_time or text segment size metrics:. ```bash; % test-suite/utils/compare.py -m compile_time baseline.json; % test-suite/utils/compare.py -m size.__text baseline.json; ```. - Compare two result files and filter short running t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:7080,cache,cache,7080,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['cache'],['cache']
Performance,"// top doc Doxygen page for minuit2. /**. \page Minuit2Page Minuit2. The Minuit2 library is a new object-oriented implementation, written in C++, of the popular MINUIT minimization package.; These new version provides basically all the functionality present in the old Fortran version,; with almost equivalent numerical accuracy and computational performances.; Furthermore, it contains new functionality, like the possibility to set single side parameter limits or; the FUMILI algorithm, which is an optimized method for least square and log likelihood minimizations.; The package has been originally developed by M. Winkler and F. James.; More information on the new C++ version can be found on the; MINUIT Web Site. Minuit2, originally developed in the SEAL project, is now distributed within %ROOT.; The API has been then changed in this new version to follow the %ROOT coding convention (function names starting with capital letters) and the classes have been moved inside the namespace ROOT::Minuit2.; In addition, the %ROOT distribution contains classes needed to integrate Minuit2 in the %ROOT framework. A new class has been introduced, ROOT::Minuit2::Minuit2Minimizer, which implements the interface; ROOT::Math::Minimizer. Within %ROOT, it can be instantiates also using the %ROOT plug-in manager. This class provides a convenient entry point for using Minuit2. An example of using this interface is; the %ROOT tutorial tutorials/fit/NumericalMinimization.C or; the Minuit2 test program testMinimize.cxx. A standalone version of Minuit2 (independent of %ROOT) can be downloaded from here. It does not contain the %ROOT interface and it is therefore totally independent of external packages and can be simply build using the configure script and then make. Example tests are provided in the directory test/MnSim and test/MnTutorial and they can be built with the make check command. The Minuit2 User Guide provides all the information needed for using directly (without add-on packages like ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/Minuit2.html:347,perform,performances,347,math/minuit2/doc/Minuit2.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/Minuit2.html,2,"['optimiz', 'perform']","['optimized', 'performances']"
Performance,"//===- README.txt - Notes for improving PowerPC-specific code gen ---------===//. TODO:; * lmw/stmw pass a la arm load store optimizer for prolog/epilog. ===-------------------------------------------------------------------------===. This code:. unsigned add32carry(unsigned sum, unsigned x) {; unsigned z = sum + x;; if (sum + x < x); z++;; return z;; }. Should compile to something like:. 	addc r3,r3,r4; 	addze r3,r3. instead we get:. 	add r3, r4, r3; 	cmplw cr7, r3, r4; 	mfcr r4 ; 1; 	rlwinm r4, r4, 29, 31, 31; 	add r3, r3, r4. Ick. ===-------------------------------------------------------------------------===. We compile the hottest inner loop of viterbi to:. li r6, 0; b LBB1_84 ;bb432.i; LBB1_83: ;bb420.i; lbzx r8, r5, r7; addi r6, r7, 1; stbx r8, r4, r7; LBB1_84: ;bb432.i; mr r7, r6; cmplwi cr0, r7, 143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:114,load,load,114,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,2,"['load', 'optimiz']","['load', 'optimizer']"
Performance,"//===- README_X86_64.txt - Notes for X86-64 code gen ----------------------===//. AMD64 Optimization Manual 8.2 has some nice information about optimizing integer; multiplication by a constant. How much of it applies to Intel's X86-64; implementation? There are definite trade-offs to consider: latency vs. register; pressure vs. code size. //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:144,optimiz,optimizing,144,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,2,"['latency', 'optimiz']","['latency', 'optimizing']"
Performance,"//===----------------------------------------------------------------------===//; // Representing sign/zero extension of function results; //===----------------------------------------------------------------------===//. Mar 25, 2009 - Initial Revision. Most ABIs specify that functions which return small integers do so in a; specific integer GPR. This is an efficient way to go, but raises the question:; if the returned value is smaller than the register, what do the high bits hold?. There are three (interesting) possible answers: undefined, zero extended, or; sign extended. The number of bits in question depends on the data-type that; the front-end is referencing (typically i1/i8/i16/i32). Knowing the answer to this is important for two reasons: 1) we want to be able; to implement the ABI correctly. If we need to sign extend the result according; to the ABI, we really really do need to do this to preserve correctness. 2); this information is often useful for optimization purposes, and we want the; mid-level optimizers to be able to process this (e.g. eliminate redundant; extensions). For example, lets pretend that X86 requires the caller to properly extend the; result of a return (I'm not sure this is the case, but the argument doesn't; depend on this). Given this, we should compile this:. int a();; short b() { return a(); }. into:. _b:; 	subl	$12, %esp; 	call	L_a$stub; 	addl	$12, %esp; 	cwtl; 	ret. An optimization example is that we should be able to eliminate the explicit; sign extension in this example:. short y();; int z() {; return ((int)y() << 16) >> 16;; }. _z:; 	subl	$12, %esp; 	call	_y; 	;; movswl %ax, %eax -> not needed because eax is already sext'd; 	addl	$12, %esp; 	ret. //===----------------------------------------------------------------------===//; // What we have right now.; //===----------------------------------------------------------------------===//. Currently, these sorts of things are modelled by compiling a function to return; the small type a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:973,optimiz,optimization,973,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,2,['optimiz'],"['optimization', 'optimizers']"
Performance,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-22-using-mcjit-with-kaleidoscope-tutorial/. The source code in this directory demonstrates the initial working version of; the program before subsequent performance improvements are applied. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required, as mentioned in the blog posts.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt:634,perform,performance,634,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt,1,['perform'],['performance']
Performance,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The source code in this directory combines all previous versions, including the; old JIT-based implementation, into a single file for easy comparison with; command line options to select between the various possibilities. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt:969,perform,performance,969,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,1,['perform'],['performance']
Performance,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the second blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-29-kaleidoscope-performance-with-mcjit/. The source code in this directory demonstrates the second version of the; program, now modified to implement a sort of 'lazy' compilation. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to disable most stderr output for timing purposes. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt:498,perform,performance-with-mcjit,498,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,2,['perform'],"['performance', 'performance-with-mcjit']"
Performance,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the third blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-08-02-object-caching-with-kaleidoscope/. The source code in this directory demonstrates the third version of the; program, now modified to accept an input IR file on the command line and,; optionally, to use a basic caching mechanism to store generated object images. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to support the input IR file command line option. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt:1229,perform,performance,1229,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,1,['perform'],['performance']
Performance,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:600,load,load,600,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,2,['load'],['load']
Performance,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend.; //===---------------------------------------------------------------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===----------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:580,optimiz,optimization,580,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['optimiz'],['optimization']
Performance,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend: SSE-specific stuff.; //===---------------------------------------------------------------------===//. //===---------------------------------------------------------------------===//. SSE Variable shift can be custom lowered to something like this, which uses a; small table + unaligned load + shuffle instead of going through memory. __m128i_shift_right:; 	.byte	 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; 	.byte	 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1. ...; __m128i shift_right(__m128i value, unsigned long offset) {; return _mm_shuffle_epi8(value,; _mm_loadu_si128((__m128 *) (___m128i_shift_right + offset)));; }. //===---------------------------------------------------------------------===//. SSE has instructions for doing operations on complex numbers, we should pattern; match them. For example, this should turn into a horizontal add:. typedef float __attribute__((vector_size(16))) v4f32;; float f32(v4f32 A) {; return A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:404,load,load,404,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,1,['load'],['load']
Performance,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:674,load,load,674,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,2,"['load', 'perform']","['load', 'performance']"
Performance,"//root.cern/doc/v624/classRooCrystalBall.html), which implements some common generalizations of the Crystal Ball shape:. - symmetric or asymmetric power-law tails on both sides; - different width parameters for the left and right sides of the Gaussian core. The new `RooCrystalBall` class can substitute the `RooDSCBShape` and `RooSDSCBShape`, which were passed around in the community. ## 2D Graphics Libraries. - Add the method `AddPoint`to `TGraph(x,y)` and `TGraph2D(x,y,z)`, equivalent to `SetPoint(g->GetN(),x,y)`and `SetPoint(g->GetN(),x,y,z)`; - Option `E0` draws error bars and markers are drawn for bins with 0 contents. Now, combined; with options E1 and E2, it avoids error bars clipping. ## 3D Graphics Libraries. ## Geometry Libraries. ## Database Libraries. ## Networking Libraries. ### Multithreaded support for FastCGI. Now when THttpServer creates FastCGI engine, 10 worker threads used to process requests; received via FastCGI channel. This significantly increase a performance, especially when; several clients are connected. ### Better security for THttpServer with webgui. If THttpServer created for use with webgui widgets (RBrowser, RCanvas, REve), it only will; provide access to the widgets via websocket connection - any other kind of requests like root.json; or exe.json will be refused completely. Combined with connection tokens and https protocol,; this makes usage of webgui components in public networks more secure. ### Enabled WLCG Bearer Tokens support in RDavix. Bearer tokens are part of WLCG capability-based infrastructure with capability-based scheme which uses an infrastructure that describes what the bearer is allowed to do as opposed to who that bearer is. Token discovery procedure are developed according WLCG Bearer Token Discovery specification document (https://github.com/WLCG-AuthZ-WG/bearer-token-discovery/blob/master/specification.md). Short overview:. 1. If the `BEARER_TOKEN` environment variable is set, then the value is taken to be the tok",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:25204,perform,performance,25204,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['perform'],['performance']
Performance,"/C++ lookup then we create; redundant AST nodes during the merge! Also, having two instances of the same; node could result in false :ref:`structural in-equivalencies <structural-eq>`; of other nodes which depend on the duplicated node. Because of these reasons,; we created a lookup class which has the sole purpose to register all; declarations, so later they can be looked up by subsequent import requests.; This is the ``ASTImporterLookupTable`` class. This lookup table should be; shared amongst the different ``ASTImporter`` instances if they happen to import; to the very same ""to"" context. This is why we can use the importer specific; lookup only via the ``ASTImporterSharedState`` class. ExternalASTSource; ~~~~~~~~~~~~~~~~~. The ``ExternalASTSource`` is an abstract interface associated with the; ``ASTContext`` class. It provides the ability to read the declarations stored; within a declaration context either for iteration or for name lookup. A; declaration context with an external AST source may load its declarations; on-demand. This means that the list of declarations (represented as a linked; list, the head is ``DeclContext::FirstDecl``) could be empty. However, member; functions like ``DeclContext::lookup()`` may initiate a load. Usually, external sources are associated with precompiled headers. For example,; when we load a class from a PCH then the members are loaded only if we do want; to look up something in the class' context. In case of LLDB, an implementation of the ``ExternalASTSource`` interface is; attached to the AST context which is related to the parsed expression. This; implementation of the ``ExternalASTSource`` interface is realized with the help; of the ``ASTImporter`` class. This way, LLDB can reuse Clang's parsing; machinery while synthesizing the underlying AST from the debug data (e.g. from; DWARF). From the view of the ``ASTImporter`` this means both the ""to"" and the; ""from"" context may have declaration contexts with external lexical storage.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:106689,load,load,106689,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['load'],['load']
Performance,/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModul,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65474,perform,performance,65474,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"/F"").; - Allow reading of older version of TTreePerfStats (ROOT-8520); - In `TTree::OptimizeBaskets()` do not call GetBasket(0) to avoid disc reads; - It is now possible to define the precision of the default histogram created; by `TTree::Draw`. Three new parameters are available in `$ROOTSYS/etcsystem.rootrc`; ```{.cpp}; Hist.Precision.1D: float; Hist.Precision.2D: float; Hist.Precision.3D: float; ```; the default values are `float`. They can be set to `double`.; - Fix ROOT-8742: TTree::SetBranchAddress could not be invoked safely even when dealing with the same tree obtained from the same file opened in different threads.; - TTree::Branch() now complains if a ""name[size]/F"" branch specification is passed wrongly (e.g. as ""name/F[size]""). ### TDataFrame; - Creation of the TDataFrame class. The TDataFrame allows to interact with data; stored in columnar format in a functional and intuitive way in order to perform; data analysis. Parallelism is accessible simply by activating implicit; multi-threading with the ROOT::EnableImplicitMT() function.; In a nutshell, the functionality provided is:; - Create and fill histograms with one single method invocation; - Express filtering of entries with strings, lambdas or functions; - Easy creation of efficiencies of cut-flows; - Possibility to run on ranges of entries; - Creating columns not present in the original dataset; - Chain multiple actions to be executed on the same event loop; - Creation of events on-the-fly (e.g. via Pythia or user-define generator functors), with no need for an input TTree; - Snapshot on a rootfile the dataset after cuts and after augmentation with columns created by the user; - Run analyses expressed as chains of actions in parallel in a transparent way for the user; See [the online documentation](https://root.cern.ch/doc/master/classROOT_1_1Experimental_1_1TDF_1_1TDataFrame.html) for more details. ## 2D Graphics Libraries; - If one used ""col2"" or ""colz2"", the value of `TH1::fMaximum` got modified.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md:6341,multi-thread,multi-threading,6341,README/ReleaseNotes/v610/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md,1,['multi-thread'],['multi-threading']
Performance,/MissingHashCheck.cpp; clang-tools-extra/clang-tidy/objc/MissingHashCheck.h; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.cpp; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.h; clang-tools-extra/clang-tidy/objc/PropertyDeclarationCheck.h; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.cpp; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.h; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.cpp; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleChec,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:64979,perform,performance,64979,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"/WLCG-AuthZ-WG/bearer-token-discovery/blob/master/specification.md). Short overview:. 1. If the `BEARER_TOKEN` environment variable is set, then the value is taken to be the token contents.; 2. If the `BEARER_TOKEN_FILE` environment variable is set, then its value is interpreted as a filename. The contents of the specified file are taken to be the token contents.; 3. If the `XDG_RUNTIME_DIR` environment variable is set, then take the token from the contents of `$XDG_RUNTIME_DIR/bt_u$ID`(this additional location is intended to provide improved security for shared login environments as `$XDG_RUNTIME_DIR` is defined to be user-specific as opposed to a system-wide directory.).; 4. Otherwise, take the token from `/tmp/bt_u$ID`. ## GUI Libraries. ### RBrowser improvements. - central factory methods to handle browsing, editing and drawing of different classes; - simple possibility to extend RBrowser on user-defined classes; - support of web-based geometry viewer; - better support of TTree drawing; - server-side handling of code editor and image viewer widgets; - rbrowser content is fully recovered when web-browser is reloaded; - load of widgets code only when really required (shorter startup time for RBrowser). ## Montecarlo Libraries. ## PROOF Libraries. ## Language Bindings. ## JavaScript ROOT. ### Major JSROOT update to version 6. - update all used libraries `d3.js`, `three.js`, `MathJax.js`, openui5; - change to Promise based interface for all async methods, remove call-back arguments; - change scripts names, core scripts name now `JSRoot.core.js`; - unify function/methods naming conventions, many changes in method names; - provide central code loader via `JSROOT.require`, supporting 4 different loading engines; - many nice features and many bug fixes; see JSROOT v6 release notes. ## Tutorials. ## Class Reference Guide. ## Build, Configuration and Testing Infrastructure. - a new cmake variable, `CMAKE_INSTALL_PYTHONDIR`, has been added: it allows customization of the i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:27182,load,load,27182,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['load'],['load']
Performance,"/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:331427,load,load,331427,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:347143,load,load,347143,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before execut",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:330726,load,load,330726,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:365956,load,load,365956,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"/interleaved loop; Stmt(i:i+3);; }; for (; i < n; i+=1) // epilogue loop; Stmt(i);. where ``rtc`` is a generated runtime check. ``llvm.loop.vectorize.followup_vectorized`` will set the attributes for; the vectorized loop. If not specified, ``llvm.loop.isvectorized`` is; combined with the original loop's attributes to avoid it being; vectorized multiple times. ``llvm.loop.vectorize.followup_epilogue`` will set the attributes for; the remainder loop. If not specified, it will have the original loop's; attributes combined with ``llvm.loop.isvectorized`` and; ``llvm.loop.unroll.runtime.disable`` (unless the original loop already; has unroll metadata). The attributes specified by ``llvm.loop.vectorize.followup_all`` are; added to both loops. When using a follow-up attribute, it replaces any automatically deduced; attributes for the generated loop in question. Therefore it is; recommended to add ``llvm.loop.isvectorized`` to; ``llvm.loop.vectorize.followup_all`` which avoids that the loop; vectorizer tries to optimize the loops again. Loop Unrolling; --------------. Unrolling is interpreted as forced any ``!{!""llvm.loop.unroll.enable""}``; metadata or option (``llvm.loop.unroll.count``, ``llvm.loop.unroll.full``); is present. Unrolling can be full unrolling, partial unrolling of a loop; with constant trip count or runtime unrolling of a loop with a trip; count unknown at compile-time. If the loop has been unrolled fully, there is no followup-loop. For; partial/runtime unrolling, the original loop of. .. code-block:: c. for (int i = 0; i < n; i+=1) // original loop; Stmt(i);. is transformed into (using an unroll factor of 4):. .. code-block:: c. int i = 0;; for (; i + 3 < n; i+=4) { // unrolled loop; Stmt(i);; Stmt(i+1);; Stmt(i+2);; Stmt(i+3);; }; for (; i < n; i+=1) // remainder loop; Stmt(i);. ``llvm.loop.unroll.followup_unrolled`` will set the loop attributes of; the unrolled loop. If not specified, the attributes of the original loop; without the ``llvm.loop.unroll.*``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:7079,optimiz,optimize,7079,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,1,['optimiz'],['optimize']
Performance,"/pymva/src/RModelParser_PyTorch.cxx) parser for translating models in respective formats into SOFIE's internal representation. From ROOT command line, or in a ROOT macro, we can proceed with an ONNX model:. ```c++; using namespace TMVA::Experimental;; SOFIE::RModelParser_ONNX parser;; SOFIE::RModel model = parser.Parse(“./example_model.onnx”);; model.Generate();; model.OutputGenerated(“./example_output.hxx”);; ```. And an C++ header file and a `.dat` file containing the model weights will be generated. You can also use. ```c++; model.PrintRequiredInputTensors();; ```. to check the required size and type of input tensor for that particular model, and use. ```c++; model.PrintInitializedTensors();; ```. to check the tensors (weights) already included in the model. To use the generated inference code:. ```c++; #include ""example_output.hxx""; float input[INPUT_SIZE];; std::vector<float> out = TMVA_SOFIE_example_model::infer(input);. // Generated header file shall contain a Session class which requires initialization to load the corresponding weights.; TMVA_SOFIE_example_model::Session s(""example_model.dat""). // Once instantiated the session object's infer method can be used; std::vector<float> out = s.infer(input);; ```. With the default settings, the weights are contained in a separate binary file, but if the user instead wants them to be in the generated header file itself, they can use approproiate generation options. ```c++; model.Generate(Options::kNoWeightFile);; ```. Other such options includes `Options::kNoSession` (for not generating the Session class, and instead keeping the infer function independent).; SOFIE also supports generating inference code with RDataFrame as inputs, refer to the tutorials below for examples. ## Supported ONNX operators. Here is the updated list of supported ONNX operators. - [x] Add; - [x] AveragePool; - [x] BatchNormalization; - [x] Cast; - [x] Concat; - [x] Constant; - [x] ConstantOfShape; - [x] Conv; - [x] ConvTranspose; - [x] Elu; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/README.md:2124,load,load,2124,tmva/sofie/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/README.md,1,['load'],['load']
Performance,"/res; ${CMAKE_SOURCE_DIR}/core/meta/inc; ${CMAKE_SOURCE_DIR}/core/metacling/res; ${CMAKE_SOURCE_DIR}/core/thread/inc; ${CMAKE_SOURCE_DIR}/core/zip/inc; ${CMAKE_SOURCE_DIR}/io/io/inc; ${CMAKE_BINARY_DIR}/ginclude; ). set_target_properties(MetaCling PROPERTIES; COMPILE_FLAGS ""${CMAKE_CXX_FLAGS} ${CLING_CXXFLAGS}""; VISIBILITY_INLINES_HIDDEN ""ON""; ). if(MSVC); target_include_directories(MetaCling PRIVATE; ${CMAKE_SOURCE_DIR}/core/winnt/inc; ); set_source_files_properties(TCling.cxx COMPILE_FLAGS /bigobj); endif(). add_dependencies(MetaCling CLING). ##### libCling #############################################################. if(NOT builtin_clang); set(prefixed_link_libraries); foreach(dep ${CLING_DEPEND_LIBS}); if(""${dep}"" MATCHES ""^clang""); set(dep ""${LLVM_LIBRARY_DIR}/lib${dep}.a""); endif(); list(APPEND prefixed_link_libraries ""${dep}""); endforeach(); set(LINK_LIBS ""${prefixed_link_libraries}""); link_directories(""${LLVM_LIBRARY_DIR}""); endif(). # We need to paste the content of the cling plugins disabling link symbol optimizations.; set(CLING_PLUGIN_LINK_LIBS); if (clad); if (APPLE); set(CLING_PLUGIN_LINK_LIBS -Wl,-force_load cladPlugin -Wl,-force_load cladDifferentiator); elseif(MSVC); set(CLING_PLUGIN_LINK_LIBS cladPlugin cladDifferentiator); set(CLAD_LIBS ""-WHOLEARCHIVE:cladPlugin.lib -WHOLEARCHIVE:cladDifferentiator.lib""); else(); set(CLING_PLUGIN_LINK_LIBS -Wl,--whole-archive cladPlugin cladDifferentiator -Wl,--no-whole-archive); endif(); if(TARGET clang); # Link our clad libraries to clang. If users use the clang from ROOT they will; # also be able to use clad out of the box.; add_dependencies(clang clad); target_link_libraries(clang PUBLIC ${CLING_PLUGIN_LINK_LIBS}); endif(); endif(). ROOT_LINKER_LIBRARY(Cling; $<TARGET_OBJECTS:ClingUtils>; $<TARGET_OBJECTS:Dictgen>; $<TARGET_OBJECTS:MetaCling>; LIBRARIES ${CLING_LIBRARIES} ${LINK_LIBS} ${CLING_PLUGIN_LINK_LIBS}). # When these two link at the same time, they can exhaust the RAM on many machines, since they both ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/CMakeLists.txt:3019,optimiz,optimizations,3019,core/metacling/src/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/CMakeLists.txt,1,['optimiz'],['optimizations']
Performance,"/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Must happen after; any preceding; local/generic; load/load; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:226652,load,load,226652,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"/test: TMVAMulticlass.C; and TMVAMulticlassApplication.C. New TMVA event vector building. The code; for splitting the input data into training and test samples for; all classes and the mixing of those samples to one training and; one test sample has been rewritten completely. The new code is; more performant and has a clearer structure. This fixes several; bugs which have been reported by some users of TMVA.; Code and performance test framework: A unit; test framework for daily software and method performance; validation has been implemented.; . Methods. BDT Automatic parameter optimisation for building the; tree architecture: The optimisation procedure uses the; performance of the trained classifier on the ""test sample"" for; finding the set of optimal parameters. Two different methods to; traverse the parameter space are available (scanning, genetic; algorithm). Currently parameter optimization is implemented only; for these three parameters that influence the tree architectur:; the maximum depth of a tree, MaxDepth, the minimum; number of events in each node, NodeMinEvents, and; the number of tress, NTrees. Optimization can; is invoked by calling; factory->OptimizeAllMethods(); prior to the call; factory->TrainAllMethods();. Automated and configurable parameter optimization is soon to; be enabled for all methods (for those parameters where; optimization is applicable).; . BDT node splitting: While Decision Trees; typically have only univariate splits, in TMVA one can now; also opt for multivariate splits that use a ""Fisher; Discriminant"" (option: UseFisherCuts), built from all; observables that show correlations larger than some threshold; (MinLinCorrForFisher). The training will then test at each; split a cut on this fisher discriminant in addition to all; univariate cuts on the variables (or only on those variables; that have not been used in the Fisher discriminant, option; UseExcusiveVars). No obvious improvement betwen very simple; decision trees after boostin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html:1892,optimiz,optimization,1892,tmva/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html,1,['optimiz'],['optimization']
Performance,"/tmp/cc-Sn4RKF.s""], output: ""/tmp/cc-gvSnbS.o""; # ""i386-apple-darwin9"" - ""darwin::Link"", inputs: [""/tmp/cc-gvSnbS.o""], output: ""/tmp/cc-jgHQxi.out""; # ""ppc-apple-darwin9"" - ""gcc::Compile"", inputs: [""t0.c""], output: ""/tmp/cc-Q0bTox.s""; # ""ppc-apple-darwin9"" - ""gcc::Assemble"", inputs: [""/tmp/cc-Q0bTox.s""], output: ""/tmp/cc-WCdicw.o""; # ""ppc-apple-darwin9"" - ""gcc::Link"", inputs: [""/tmp/cc-WCdicw.o""], output: ""/tmp/cc-HHBEBh.out""; # ""i386-apple-darwin9"" - ""darwin::Lipo"", inputs: [""/tmp/cc-jgHQxi.out"", ""/tmp/cc-HHBEBh.out""], output: ""a.out"". This shows the tool chain, tool, inputs and outputs which have been; bound for this compilation sequence. Here clang is being used to; compile t0.c on the i386 architecture and darwin specific versions of; the tools are being used to assemble and link the result, but generic; gcc versions of the tools are being used on PowerPC. #. **Translate: Tool Specific Argument Translation**. Once a Tool has been selected to perform a particular Action, the; Tool must construct concrete Commands which will be executed during; compilation. The main work is in translating from the gcc style; command line options to whatever options the subprocess expects. Some tools, such as the assembler, only interact with a handful of; arguments and just determine the path of the executable to call and; pass on their input and output arguments. Others, like the compiler; or the linker, may translate a large number of arguments in addition. The ArgList class provides a number of simple helper methods to; assist with translating arguments; for example, to pass on only the; last of arguments corresponding to some option, or all arguments for; an option. The result of this stage is a list of Commands (executable paths and; argument strings) to execute. #. **Execute**. Finally, the compilation pipeline is executed. This is mostly; straightforward, although there is some interaction with options like; ``-pipe``, ``-pass-exit-codes`` and ``-time``. Additional Notes; --",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst:11017,perform,perform,11017,interpreter/llvm-project/clang/docs/DriverInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst,1,['perform'],['perform']
Performance,"/wiki/Curiously_recurring_template_pattern. E.g. .. code-block:: c++. class Shape : public RTTIExtends<Shape, RTTIRoot> {; public:; static char ID;; virtual double computeArea() = 0;; };. class Square : public RTTIExtends<Square, Shape> {; double SideLength;; public:; static char ID;. Square(double S) : SideLength(S) {}; double computeArea() override;; };. class Circle : public RTTIExtends<Circle, Shape> {; double Radius;; public:; static char ID;. Circle(double R) : Radius(R) {}; double computeArea() override;; };. char Shape::ID = 0;; char Square::ID = 0;; char Circle::ID = 0;. Advanced Use Cases; ==================. The underlying implementation of isa/cast/dyn_cast is all controlled through a; struct called ``CastInfo``. ``CastInfo`` provides 4 methods, ``isPossible``,; ``doCast``, ``castFailed``, and ``doCastIfPossible``. These are for ``isa``,; ``cast``, and ``dyn_cast``, in order. You can control the way your cast is; performed by creating a specialization of the ``CastInfo`` struct (to your; desired types) that provides the same static methods as the base ``CastInfo``; struct. This can be a lot of boilerplate, so we also have what we call Cast Traits.; These are structs that provide one or more of the above methods so you can; factor out common casting patterns in your project. We provide a few in the; header file ready to be used, and we'll show a few examples motivating their; usage. These examples are not exhaustive, and adding new cast traits is easy; so users should feel free to add them to their project, or contribute them if; they're particularly useful!. Value to value casting; ----------------------; In this case, we have a struct that is what we call 'nullable' - i.e. it is; constructible from ``nullptr`` and that results in a value you can tell is; invalid. .. code-block:: c++. class SomeValue {; public:; SomeValue(void *ptr) : ptr(ptr) {}; void *getPointer() const { return ptr; }; bool isValid() const { return ptr != nullptr; }; private:; void *pt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst:14885,perform,performed,14885,interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,1,['perform'],['performed']
Performance,"0 then the collection; will be written in split mode. Ie. if it contains objects of any; types deriving from TTrack this function will sort the objects; basing on their type and store them in separate branches in split; mode.; The ROOT test example in ROOTSYS/test/bench.cxx shows many examples of collections; and storage in a TTree when using split mode or not. This program illustrates the important; gain in space and time when using this new facility. Parallel unzipping. Introducing a parallel unzipping algorithm for pre-fetched buffers. Since we already know what buffers are going to be read, we can decompress a few of them in advance in an additional thread and give the impression that the data decompression comes for free (we gain up to 30% in reading intensive jobs). The size of this unzipping cache is 20% the size of the TTreeCache and can be modified with TTreeCache::SetUnzipBufferSize(Long64_t bufferSize). Theoretically, we only need one buffer in advance but in practice we might fall short if the unzipping cache is too small (synchronization costs). This experimental feature is disabled by default, to activate it use the static function TTreeCache::SetParallelUnzip(TTreeCacheUnzip::EParUnzipMode option = TTreeCacheUnzip::kEnable). The possible values to pass are: TTreeCacheUnzip::kEnable to enable itTTreeCacheUnzip::kDisable to disable itTTreeCacheUnzip::kForce to force it.The TTreeCacheUnzip is actived; only if you have more than one core. To activate it with only one core useTTreeCacheUnzip::kForce option (for example to measure the overhead). Disk and Memory Space Gain. In ROOT older than v5.20/00, the branches' last basket, also known as the write basket, was always saved in the same ""key"" as the TTree object and was always present in memory when reading or writing.; When reading this write basket was always present in memory even if the branch was never accessed. Starting in v5.20/00, TTree::Write closes out, compresses (when requested) and writes to di",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html:5229,cache,cache,5229,tree/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html,1,['cache'],['cache']
Performance,"0' bits, these operations are identical to :ref:`llvm.masked.store <int_mstore>` and :ref:`llvm.masked.load <int_mload>`. .. _int_expandload:. '``llvm.masked.expandload.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. Several values of integer, floating point or pointer data type are loaded from consecutive memory addresses and stored into the elements of a vector according to the mask. ::. declare <16 x float> @llvm.masked.expandload.v16f32 (ptr <ptr>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x i64> @llvm.masked.expandload.v2i64 (ptr <ptr>, <2 x i1> <mask>, <2 x i64> <passthru>). Overview:; """""""""""""""""". Reads a number of scalar values sequentially from memory location provided in '``ptr``' and spreads them in a vector. The '``mask``' holds a bit for each vector lane. The number of elements read from memory is equal to the number of '1' bits in the mask. The loaded elements are positioned in the destination vector according to the sequence of '1' and '0' bits in the mask. E.g., if the mask vector is '10010001', ""expandload"" reads 3 values from memory addresses ptr, ptr+1, ptr+2 and places them in lanes 0, 3 and 7 accordingly. The masked-off lanes are filled by elements from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. It has the same underlying type as the element of the returned vector. The second operand, mask, is a vector of boolean values with the same number of elements as the return type. The third is a pass-through value that is used to fill the masked-off lanes of the result. The return type and the type of the '``passthru``' operand have the same vector type. Semantics:; """""""""""""""""""". The '``llvm.masked.expandload``' intrinsic is designed for reading multiple scalar values from adjacent memory addresses into possibly non-adjacent vector lanes. It is useful for targets that support vector expanding load",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:855911,load,loaded,855911,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following glob",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:249266,cache,cache,249266,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:319498,load,loads,319498,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic sc1=1; 4. s_waitcnt vmcnt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:323407,load,loads,323407,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"0, 0);; works/worked fine). Add type information to the result of TTree::Print in the case of; TBranchElement:; *Br 17 :fH : TH1F* *; *Entries : 20 : Total Size= 19334 bytes File Size = 1671 *; *Baskets : 2 : Basket Size= 16000 bytes Compression= 11.29 *; *............................................................................*; *Br 18 :fTriggerBits : TBits *; *Entries : 20 : Total Size= 1398 bytes File Size = 400 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 2.23 *; *............................................................................*; *Br 19 :fIsValid : Bool_t *; *Entries : 20 : Total Size= 582 bytes File Size = 92 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 1.00 *. Add a new function TBranch::SetStatus It is much faster to call this function in case of a Tree with many branches; instead of calling TTree::SetBranchStatus.; Implement TTreeCache::Print that shows information like:; // ******TreeCache statistics for file: cms2.root ******; // Number of branches in the cache ...: 1093; // Cache Efficiency ..................: 0.997372; // Cache Efficiency Rel...............: 1.000000; // Learn entries......................: 100; // Reading............................: 72761843 bytes in 7 transactions; // Readahead..........................: 256000 bytes with overhead = 0 bytes; // Average transaction................: 10394.549000 Kbytes; // Number of blocks in current cache..: 210, total size: 6280352; This function can be called directly from TTree: T->PrintCacheStats();. Add support for variable size array of object in a TTree (when the owner of the array is split.); And many other bug fixes, security fixes, thread safety and performance improvements ; see the svn log for details. TTree Scan and Draw. Insured that the generated histogram as an integral bin width when plotting a string or integer.; Improved the output of TTree::Scan by inserting a blank space whenever a value is not available because there is no proper row in a frien",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html:5787,cache,cache,5787,tree/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html,1,['cache'],['cache']
Performance,"0, i32 2; store i32 %inc, ptr %inc.spill.addr; call void @print(i32 %n). ret ptr %frame; }. Outlined resume part of the coroutine will reside in function `f.resume`:. .. code-block:: llvm. define internal fastcc void @f.resume(ptr %frame.ptr.resume) {; entry:; %inc.spill.addr = getelementptr %f.frame, ptr %frame.ptr.resume, i64 0, i32 2; %inc.spill = load i32, ptr %inc.spill.addr, align 4; %inc = add i32 %inc.spill, 1; store i32 %inc, ptr %inc.spill.addr, align 4; tail call void @print(i32 %inc); ret void; }. Whereas function `f.destroy` will contain the cleanup code for the coroutine:. .. code-block:: llvm. define internal fastcc void @f.destroy(ptr %frame.ptr.destroy) {; entry:; tail call void @free(ptr %frame.ptr.destroy); ret void; }. Avoiding Heap Allocations; -------------------------. A particular coroutine usage pattern, which is illustrated by the `main`; function in the overview section, where a coroutine is created, manipulated and; destroyed by the same calling function, is common for coroutines implementing; RAII idiom and is suitable for allocation elision optimization which avoid; dynamic allocation by storing the coroutine frame as a static `alloca` in its; caller. In the entry block, we will call `coro.alloc`_ intrinsic that will return `true`; when dynamic allocation is required, and `false` if dynamic allocation is; elided. .. code-block:: llvm. entry:; %id = call token @llvm.coro.id(i32 0, ptr null, ptr null, ptr null); %need.dyn.alloc = call i1 @llvm.coro.alloc(token %id); br i1 %need.dyn.alloc, label %dyn.alloc, label %coro.begin; dyn.alloc:; %size = call i32 @llvm.coro.size.i32(); %alloc = call ptr @CustomAlloc(i32 %size); br label %coro.begin; coro.begin:; %phi = phi ptr [ null, %entry ], [ %alloc, %dyn.alloc ]; %hdl = call noalias ptr @llvm.coro.begin(token %id, ptr %phi). In the cleanup block, we will make freeing the coroutine frame conditional on; `coro.free`_ intrinsic. If allocation is elided, `coro.free`_ returns `null`; thus skipping t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:16479,optimiz,optimization,16479,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['optimiz'],['optimization']
Performance,"0.; GFX11; Number of instruction bytes to prefetch, starting at the kernel's entry; point instruction, before wavefront starts execution. The value is 0..63; with a granularity of 128 bytes.; 10 1 bit TRAP_ON_START GFX10; Reserved, must be 0.; GFX11; Must be 0. If 1, wavefront starts execution by trapping into the trap handler. CP is responsible for filling in the trap on start bit in; ``COMPUTE_PGM_RSRC3.TRAP_ON_START`` according to what the runtime; requests.; 11 1 bit TRAP_ON_END GFX10; Reserved, must be 0.; GFX11; Must be 0. If 1, wavefront execution terminates by trapping into the trap handler. CP is responsible for filling in the trap on end bit in; ``COMPUTE_PGM_RSRC3.TRAP_ON_END`` according to what the runtime requests.; 30:12 19 bits Reserved, must be 0.; 31 1 bit IMAGE_OP GFX10; Reserved, must be 0.; GFX11; If 1, the kernel execution contains image instructions. If executed as; part of a graphics pipeline, image read instructions will stall waiting; for any necessary ``WAIT_SYNC`` fence to be performed in order to; indicate that earlier pipeline stages have completed writing to the; image. Not used for compute kernels that are not part of a graphics pipeline and; must be 0.; 32 **Total size 4 bytes.**; ======= ===================================================================================================================. .. .. table:: compute_pgm_rsrc3 for GFX12; :name: amdgpu-amdhsa-compute_pgm_rsrc3-gfx12-table. ======= ======= =============================== ===========================================================================; Bits Size Field Name Description; ======= ======= =============================== ===========================================================================; 3:0 4 bits RESERVED Reserved, must be 0.; 11:4 8 bits INST_PREF_SIZE Number of instruction bytes to prefetch, starting at the kernel's entry; point instruction, before wavefront starts execution. The value is 0..255; with a granularity of 128 bytes.; 12 1 bit RESE",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:177992,perform,performed,177992,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"000000 (-1.7976922776554302e308). Examples of disabled conversions:. .. parsed-literal::. // GFX9. v_add_u16 v0, 0x1ff00, v0 // truncated bits are not all 0 or 1; v_add_u16 v0, 0xffffffffffff00ff, v0 // truncated bits do not match MSB of the result. .. _amdgpu_synid_fp_conv:. Conversion of Floating-Point Values; -----------------------------------. Instruction operands may be specified as 64-bit; :ref:`floating-point numbers<amdgpu_synid_floating-point_number>`.; These values are converted to the; :ref:`expected operand type<amdgpu_syn_instruction_type>`; using the following steps:. 1. *Validation*. Assembler checks if the input f64 number can be converted; to the *required floating-point type* (see the table below) without overflow; or underflow. Precision lost is allowed. If this conversion is not possible,; the assembler triggers an error. 2. *Conversion*. The input value is converted to the expected type; as described in the table below. Depending on operand kind, this is; performed by either assembler or AMDGPU H/W (or both). ============== ================ ================= =================================================================; Expected type Required FP Type Conversion Description; ============== ================ ================= =================================================================; i16, u16, b16 f16 f16(num) Convert to f16 and use bits of the result as an integer value.; The value has to be encoded as a literal, or an error occurs.; Note that the value cannot be encoded as an inline constant.; i32, u32, b32 f32 f32(num) Convert to f32 and use bits of the result as an integer value.; i64, u64, b64 \- \- Conversion disabled.; f16 f16 f16(num) Convert to f16.; f32 f32 f32(num) Convert to f32.; f64 f64 {num.u32.hi,0} Use high 32 bits of the number as high 32 bits of the result;; zero-fill low 32 bits of the result. Note that the result may differ from the original number.; ============== ================ ================= ================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst:37901,perform,performed,37901,interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,1,['perform'],['performed']
Performance,"00; ev++) {; Float_t sigmat, sigmas;; gRandom->Rannor(sigmat,sigmas);; Int_t ntrack = Int_t(600 + 600 *sigmat/120.);; Float_t random = gRandom->Rndm(1);; sprintf(etype,""type%d"",ev%5);; event->SetType(etype);; event->SetHeader(ev, 200, 960312, random);; event->SetNseg(Int_t(10*ntrack+20*sigmas));; event->SetNvertex(Int_t(1+20*gRandom->Rndm()));; event->SetFlag(UInt_t(random+0.5));; event->SetTemperature(random+20.);; for(UChar_t m = 0; m < 10; m++) {; event->SetMeasure(m, Int_t(gRandom->Gaus(m,m+1)));; }; // continued...; // fill the matrix; for(UChar_t i0 = 0; i0 < 4; i0++) {; for(UChar_t i1 = 0; i1 < 4; i1++) {; event->SetMatrix(i0,i1,gRandom->Gaus(i0*i1,1));; }; }; // create and fill the Track objects; for (Int_t t = 0; t < ntrack; t++) event->AddTrack(random);; t4.Fill(); // Fill the tree; event->Clear(); // Clear before reloading event; }; f.Write(); // Write the file header; t4.Print(); // Print the tree contents; }; ```. ### Reading the Tree. First, we check if the shared library with the class definitions is; loaded. If not we load it. Then we read two branches, one for the number; of tracks and one for the entire event. We check the number of tracks; first, and if it meets our condition, we read the entire event. We show; the fist entry that meets the condition. ``` {.cpp}; void tree4r() {; // check if the event class is in the dictionary; // if it is not load the definition in libEvent.so; if (!TClassTable::GetDict(""Event"")) {; gSystem->Load(""$ROOTSYS/test/libEvent.so"");; }; // read the tree generated with tree4w. // note that we use ""new"" to create the TFile and TTree objects, because we; // want to keep these objects alive when we leave this function.; TFile *f = new TFile(""tree4.root"");; TTree *t4 = (TTree*)f->Get(""t4"");. // create a pointer to an event object for reading the branch values.; Event *event = new Event();; // get two branches and set the branch address; TBranch *bntrack = t4->GetBranch(""fNtrack"");; TBranch *branch = t4->GetBranch(""event_spli",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:63936,load,loaded,63936,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loaded']
Performance,"0324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could be added to ``git blame`` itself. Minimising cost of downstream merges; ************************************. There are many forks of LLVM with downstream changes. Merging a large-scale; renaming change could be difficult for the fork maintainers. **Mitigation**: A large-scale renaming would be automated. A fork maintainer can; merge from the commit immediately before the renaming, then apply the renaming; script to their own branch. They can then merge again from the renaming commit,; resolving all conflicts by choosing their own version. This could be tested on; the [SVE]_ fork. Provisional Plan; ================. This is a provisional plan for the `Big bang`_ approach. It has not been agreed. #. Investigate improving ``git blame``. The extent to which it can be made to; ""look through"" commits may impact how big a change can be made. #. Write a script to expand acronyms. #. Experiment and perform dry runs of the various refactoring options.; Results can be published in forks of the LLVM Git repository. #. Consider the evidence and agree on the new policy. #. Agree & announce a date for the renaming of the starter project (LLD). #. Update the `policy page <../CodingStandards.html>`_. This will explain the; old and new rules and which projects each applies to. #. Refactor the starter project in two commits:. 1. Add or change the project's .clang-tidy to reflect the agreed rules.; (This is in a separate commit to enable the merging process described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy page.; 2. Apply ``clang-tidy`` to the project's files, with only the; ``readability-identifier-naming`` rules enabled. ``clang-tidy`` will also; reformat the affected lines according to the rules in ``.clang-format``.; It is anticipated that this will be a good dog-fooding opportunity for; clang-tidy, and bugs should be fixed in the process, likely includin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:10728,perform,perform,10728,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,1,['perform'],['perform']
Performance,"08:xnack+``; Enable the ``xnack`` feature.; ``-mcpu=gfx908:xnack-``; Disable the ``xnack`` feature.; ``-mcumode``; Enable the ``cumode`` feature.; ``-mno-cumode``; Disable the ``cumode`` feature. .. table:: AMDGPU Target Features; :name: amdgpu-target-features-table. =============== ============================ ==================================================; Target Feature Clang Option to Control Description; Name; =============== ============================ ==================================================; cumode - ``-m[no-]cumode`` Control the wavefront execution mode used; when generating code for kernels. When disabled; native WGP wavefront execution mode is used,; when enabled CU wavefront execution mode is used; (see :ref:`amdgpu-amdhsa-memory-model`). sramecc - ``-mcpu`` If specified, generate code that can only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for SRAMECC. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with SRAMECC enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a process; with either setting of SRAMECC. tgsplit ``-m[no-]tgsplit`` Enable/disable generating code that assumes; work-groups are launched in threadgroup split mode.; When enabled the waves of a work-group may be; launched in different CUs. wavefrontsize64 - ``-m[no-]wavefrontsize64`` Control the wavefront size used when; generating code for kernels. When disabled; native wavefront size 32 is used, when enabled; wavefront size 64 is used. xnack - ``-mcpu`` If specified, generate code that can only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for XNACK replay. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with XNACK replay enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:17405,load,loaded,17405,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"0],s[3])"") ;; w.factory(""Chebychev::g(x[-10,10],{0,1,2})"") ; // Now OK, x has identical spec, existing x will be used. Improvements to functions and pdfs. Addition to, reorganization of morphing operator classes. The existing class RooLinearMorph which; implements 'Alex Read' morphing has been renamed RooIntegralMorph. A new class RooMomentMorph; has been added (contribution from Max Baak and Stefan Gadatsch) that implements a different morphing algorithm ; based on shifting the mean and variance of the input pdfs. The new moment morphing class can also interpolate ; between multiple input templates and works with multi-dimensional input pdfs. One of the appealing features; is that no expensive calculations are required to calculate in the interpolated pdfs shapes after the pdf; initialization. An extension that allows morphing in two parameters is foreseen for the next root release.; Progress indication in plot projections; The RooAbsReal::plotOn() now accepts a new argument ShowProgress() that will print a dot for every; function evaluation performed in the process of creating the plot. This can be useful when plotting very expensive; functions such as profile likelihoods; Automatic handling of constraint terms; It is no longer necessary to add a Constrain() argument to fitTo() calls to have internal constraints; applied. Any pdf term appearing in a product that does not contain an observable and shares one or more parameters; with another pdf term in the same product that does contain an observable is automatically picked up as a constraint term.; For example given a dataset D(x) which defines variable x as observable, the default logic works out as follows. F(x,a,b)*G(a,a0,a1) --> G is constraint term (a also appears in F(x)); F(x,a,b)*G(y,c,d) --> G is dropped (factorizing term). A Constrain(y) term in the above example will still force term G(y,c,d) to be interpreted as constraint term; Automatic caching of numeric integral calculations; Integrals that require ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html:6808,perform,performed,6808,roofit/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html,1,['perform'],['performed']
Performance,"0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:348448,cache,caches,348448,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:347459,load,load,347459,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"1, 0, 0, 0, 0). // Reverse 4-element vector V1.; __builtin_shufflevector(V1, V1, 3, 2, 1, 0). // Concatenate every other element of 4-element vectors V1 and V2.; __builtin_shufflevector(V1, V2, 0, 2, 4, 6). // Concatenate every other element of 8-element vectors V1 and V2.; __builtin_shufflevector(V1, V2, 0, 2, 4, 6, 8, 10, 12, 14). // Shuffle v1 with some elements being undefined; __builtin_shufflevector(v1, v1, 3, -1, 1, -1). **Description**:. The first two arguments to ``__builtin_shufflevector`` are vectors that have; the same element type. The remaining arguments are a list of integers that; specify the elements indices of the first two vectors that should be extracted; and returned in a new vector. These element indices are numbered sequentially; starting with the first vector, continuing into the second vector. Thus, if; ``vec1`` is a 4-element vector, index 5 would refer to the second element of; ``vec2``. An index of -1 can be used to indicate that the corresponding element; in the returned vector is a don't care and can be optimized by the backend. The result of ``__builtin_shufflevector`` is a vector with the same element; type as ``vec1``/``vec2`` but that has an element count equal to the number of; indices specified. Query for this feature with ``__has_builtin(__builtin_shufflevector)``. .. _langext-__builtin_convertvector:. ``__builtin_convertvector``; ---------------------------. ``__builtin_convertvector`` is used to express generic vector; type-conversion operations. The input vector and the output vector; type must have the same number of elements. **Syntax**:. .. code-block:: c++. __builtin_convertvector(src_vec, dst_vec_type). **Examples**:. .. code-block:: c++. typedef double vector4double __attribute__((__vector_size__(32)));; typedef float vector4float __attribute__((__vector_size__(16)));; typedef short vector4short __attribute__((__vector_size__(8)));; vector4float vf; vector4short vs;. // convert from a vector of 4 floats to a vector of 4 d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:107868,optimiz,optimized,107868,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimized']
Performance,"1, <2 x double> %vec2, i32 %imm); declare <vscale x 4 x i32> @llvm.experimental.vector.splice.nxv4i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2, i32 %imm). Overview:; """""""""""""""""". The '``llvm.experimental.vector.splice.*``' intrinsics construct a vector by; concatenating elements from the first input vector with elements of the second; input vector, returning a vector of the same type as the input vectors. The; signed immediate, modulo the number of elements in the vector, is the index; into the first vector from which to extract the result value. This means; conceptually that for a positive immediate, a vector is extracted from; ``concat(%vec1, %vec2)`` starting at index ``imm``, whereas for a negative; immediate, it extracts ``-imm`` trailing elements from the first vector, and; the remaining elements from ``%vec2``. These intrinsics work for both fixed and scalable vectors. While this intrinsic; is marked as experimental, the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. llvm.experimental.vector.splice(<A,B,C,D>, <E,F,G,H>, 1); ==> <B, C, D, E> index; llvm.experimental.vector.splice(<A,B,C,D>, <E,F,G,H>, -3); ==> <B, C, D, E> trailing elements. Arguments:; """""""""""""""""""". The first two operands are vectors with the same type. The start index is imm; modulo the runtime number of elements in the source vector. For a fixed-width; vector <N x eltty>, imm is a signed integer constant in the range; -N <= imm < N. For a scalable vector <vscale x N x eltty>, imm is a signed; integer constant in the range -X <= imm < X where X=vscale_range_min * N. '``llvm.experimental.stepvector``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This is an overloaded intrinsic. You can use ``llvm.experimental.stepvector``; to generate a vector whose lane values comprise the linear sequence; <0, 1, 2, ...>. It is primarily intended fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:671696,optimiz,optimization,671696,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"1, any corpus inputs from the 2nd, 3rd etc. corpus directories; that trigger new code coverage will be merged into the first corpus; directory. Defaults to 0. This flag can be used to minimize a corpus.; ``-merge_control_file``; Specify a control file used for the merge process.; If a merge process gets killed it tries to leave this file in a state; suitable for resuming the merge. By default a temporary file will be used.; ``-minimize_crash``; If 1, minimizes the provided crash input.; Use with -runs=N or -max_total_time=N to limit the number of attempts.; ``-reload``; If set to 1 (the default), the corpus directory is re-read periodically to; check for new inputs; this allows detection of new inputs that were discovered; by other fuzzing processes.; ``-jobs``; Number of fuzzing jobs to run to completion. Default value is 0, which runs a; single fuzzing process until completion. If the value is >= 1, then this; number of jobs performing fuzzing are run, in a collection of parallel; separate worker processes; each such worker process has its; ``stdout``/``stderr`` redirected to ``fuzz-<JOB>.log``.; ``-workers``; Number of simultaneous worker processes to run the fuzzing jobs to completion; in. If 0 (the default), ``min(jobs, NumberOfCpuCores()/2)`` is used.; ``-dict``; Provide a dictionary of input keywords; see Dictionaries_.; ``-use_counters``; Use `coverage counters`_ to generate approximate counts of how often code; blocks are hit; defaults to 1.; ``-reduce_inputs``; Try to reduce the size of inputs while preserving their full feature sets;; defaults to 1.; ``-use_value_profile``; Use `value profile`_ to guide corpus expansion; defaults to 0.; ``-only_ascii``; If 1, generate only ASCII (``isprint``+``isspace``) inputs. Defaults to 0.; ``-artifact_prefix``; Provide a prefix to use when saving fuzzing artifacts (crash, timeout, or; slow inputs) as ``$(artifact_prefix)file``. Defaults to empty.; ``-exact_artifact_path``; Ignored if empty (the default). If non-empty",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LibFuzzer.rst:12641,perform,performing,12641,interpreter/llvm-project/llvm/docs/LibFuzzer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LibFuzzer.rst,1,['perform'],['performing']
Performance,"1-bit significand; (counting the implicit leading 1). ``__bf16`` uses the `bfloat16; <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>`_ format,; which provides an 8-bit exponent and an 8-bit significand; this is the same; exponent range as `float`, just with greatly reduced precision. ``_Float16`` and ``__bf16`` follow the usual rules for arithmetic; floating-point types. Most importantly, this means that arithmetic operations; on operands of these types are formally performed in the type and produce; values of the type. ``__fp16`` does not follow those rules: most operations; immediately promote operands of type ``__fp16`` to ``float``, and so; arithmetic operations are defined to be performed in ``float`` and so result in; a value of type ``float`` (unless further promoted because of other operands).; See below for more information on the exact specifications of these types. When compiling arithmetic on ``_Float16`` and ``__bf16`` for a target without; native support, Clang will perform the arithmetic in ``float``, inserting; extensions and truncations as necessary. This can be done in a way that; exactly matches the operation-by-operation behavior of native support,; but that can require many extra truncations and extensions. By default,; when emulating ``_Float16`` and ``__bf16`` arithmetic using ``float``, Clang; does not truncate intermediate operands back to their true type unless the; operand is the result of an explicit cast or assignment. This is generally; much faster but can generate different results from strict operation-by-operation; emulation. Usually the results are more precise. This is permitted by the; C and C++ standards under the rules for excess precision in intermediate operands;; see the discussion of evaluation formats in the C standard and [expr.pre] in; the C++ standard. The use of excess precision can be independently controlled for these two; types with the ``-ffloat16-excess-precision=`` and; ``-fbfloat16-excess-precision=",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:32317,perform,perform,32317,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['perform']
Performance,"1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/sto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:321859,cache,caches,321859,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"1. Cross-process and cross-architecture linking of single relocatable objects; into a target *executor* process. 2. Support for all object format features. 3. Open linker data structures (``LinkGraph``) and pass system. JITLink and ObjectLinkingLayer; ==============================. ``ObjectLinkingLayer`` is ORCs wrapper for JITLink. It is an ORC layer that; allows objects to be added to a ``JITDylib``, or emitted from some higher level; program representation. When an object is emitted, ``ObjectLinkingLayer`` uses; JITLink to construct a ``LinkGraph`` (see :ref:`constructing_linkgraphs`) and; calls JITLink's ``link`` function to link the graph into the executor process. The ``ObjectLinkingLayer`` class provides a plugin API,; ``ObjectLinkingLayer::Plugin``, which users can subclass in order to inspect and; modify ``LinkGraph`` instances at link time, and react to important JIT events; (such as an object being emitted into target memory). This enables many features; and optimizations that were not possible under MCJIT or RuntimeDyld. ObjectLinkingLayer Plugins; --------------------------. The ``ObjectLinkingLayer::Plugin`` class provides the following methods:. * ``modifyPassConfig`` is called each time a LinkGraph is about to be linked. It; can be overridden to install JITLink *Passes* to run during the link process. .. code-block:: c++. void modifyPassConfig(MaterializationResponsibility &MR,; const Triple &TT,; jitlink::PassConfiguration &Config). * ``notifyLoaded`` is called before the link begins, and can be overridden to; set up any initial state for the given ``MaterializationResponsibility`` if; needed. .. code-block:: c++. void notifyLoaded(MaterializationResponsibility &MR). * ``notifyEmitted`` is called after the link is complete and code has been; emitted to the executor process. It can be overridden to finalize state; for the ``MaterializationResponsibility`` if needed. .. code-block:: c++. Error notifyEmitted(MaterializationResponsibility &MR). * ``noti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:2564,optimiz,optimizations,2564,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['optimiz'],['optimizations']
Performance,"1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; followi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:348166,load,load,348166,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"1.0). If set to; non zero auto cache creation is enabled and both auto created and; default sized caches will use the configured factor: 0.0 no automatic cache; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previously learnt) nor restart the learning phase.; The learning phase is restarted when a new cache is created, e.g. after having; removed a cache with SetCacheSize(0). ### TSelectorDraw. The axis titles in case of a `x:y:z` plot with the option `COLZ` were not correct. ### TParallelCoordVar. Change the format used to print the variables limit for ||-Coord to `%g`. It was; `%6.4f` before. ## Histogram Libraries. ### TFormula. - New version of the TFormula class based on Cling. Formula expressions are now used to create functions which are passed to Cling to be Just In Time compiled.; The expression is therefore compiled using Clang/LLVVM which will give execution time as compiled code and in addition correctness of the result obtained.; - This class is not 100% backward compatible with the old TFormula cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:14547,cache,cache,14547,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cache']
Performance,"1150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA first since the LEA matching code does some; estimate to determine whether the match is profitable. However, if we care more about code size, then imull is better. It's two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) {; entry:; 	br label %cond_true. cond_true:		; preds = %cond_true, %entry; 	%x.0.0 = phi i32 [ 0, %entry ], [ %tmp9, %cond_true ]		; <i32> [#uses=3]; 	%t_addr.0.0 = phi i32 [ %t, %entry ], [ %tmp7, %cond_true ]		; <i32> [#uses=1]; 	%tmp2 = getelementptr i32* %a, i32 %x.0.0		; <i32*> [#uses=1]; 	%tmp3 = load i32* %tmp2		; <i32> [#uses=1]; 	%tmp5 = add i32 %t_addr.0.0, %x.0.0		; <i32> [#uses=1]; 	%tmp7 = add i32 %tmp5, %tmp3		; <i32> [#uses=2]; 	%tmp9 = ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:7542,throughput,throughput,7542,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,"['latency', 'throughput']","['latency', 'throughput']"
Performance,"16 overflow generates; +/-INF values.; - If 1, fp16 overflow that is the; result of an +/-INF input value; or divide by 0 produces a +/-INF,; otherwise clamps computed; overflow to +/-MAX_FP16 as; appropriate. Used by CP to set up; ``COMPUTE_PGM_RSRC1.FP16_OVFL``.; 28:27 2 bits Reserved, must be 0.; 29 1 bit WGP_MODE GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; - If 0 execute work-groups in; CU wavefront execution mode.; - If 1 execute work-groups on; in WGP wavefront execution mode. See :ref:`amdgpu-amdhsa-memory-model`. Used by CP to set up; ``COMPUTE_PGM_RSRC1.WGP_MODE``.; 30 1 bit MEM_ORDERED GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; Controls the behavior of the; s_waitcnt's vmcnt and vscnt; counters. - If 0 vmcnt reports completion; of load and atomic with return; out of order with sample; instructions, and the vscnt; reports the completion of; store and atomic without; return in order.; - If 1 vmcnt reports completion; of load, atomic with return; and sample instructions in; order, and the vscnt reports; the completion of store and; atomic without return in order. Used by CP to set up; ``COMPUTE_PGM_RSRC1.MEM_ORDERED``.; 31 1 bit FWD_PROGRESS GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; - If 0 execute SIMD wavefronts; using oldest first policy.; - If 1 execute SIMD wavefronts to; ensure wavefronts will make some; forward progress. Used by CP to set up; ``COMPUTE_PGM_RSRC1.FWD_PROGRESS``.; 32 **Total size 4 bytes**; ======= ===================================================================================================================. .. .. table:: compute_pgm_rsrc2 for GFX6-GFX12; :name: amdgpu-amdhsa-compute_pgm_rsrc2-gfx6-gfx12-table. ======= ======= =============================== ===========================================================================; Bits Size Field Name Description; ======= ======= =============================== ===========================================================================; 0 1 bit ENABLE_PRIVATE_SEGMENT * En",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:169789,load,load,169789,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>, i1 <is_int_min_poison>); declare <vscale x 4 x i32> @llvm.vp.abs.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>, i1 <is_int_min_poison>); declare <256 x i64> @llvm.vp.abs.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>, i1 <is_int_min_poison>). Overview:; """""""""""""""""". Predicated abs of a vector of integers. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of integer type. The; second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. The fourth argument must be a constant and is a flag to indicate; whether the result value of the '``llvm.vp.abs``' intrinsic is a; :ref:`poison value <poisonvalues>` if the argument is statically or dynamically; an ``INT_MIN`` value. Semantics:; """""""""""""""""""". The '``llvm.vp.abs``' intrinsic performs abs (:ref:`abs <int_abs>`) of the first operand on each; enabled lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.abs.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl, i1 false); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.abs.v4i32(<4 x i32> %a, i1 false); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_smax:. '``llvm.vp.smax.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.smax.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.smax.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.smax.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_leng",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:717080,perform,performs,717080,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64015,perform,performance,64015,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['perform'],['performance']
Performance,"17. One could use demo.htm directly with THttpServer providing address like:; <http://localhost:8080/jsrootsys/demo/demo.htm?addr=../../Files/job1.root/hpx/root.json.gz&layout=3x3>; 18. Also for online server process url options like 'item', 'items', 'layout'; 19. Possibility to generate URL, which reproduces opened page with layout and drawn items. ### August 2014; 1. All communication between server and browser done with JSON format.; 2. Fix small error in dtree.js - one should always set; last sibling (_ls) property while tree can be dynamically changed.; 3. In JSRootCore.js provide central function, which handles different kinds; of XMLHttpRequest. Use only async requests, also when getting file header.; 4. Fully reorganize data management in file/tree/directory/collection hierarchical; display. Now complete description collected in HPainter class and decoupled from; visualization, performed with dTree.js.; 5. Remove all global variables from the code.; 6. Automatic scripts/style loading handled via JSROOT.loadScript() function.; One can specify arbitrary scripts list, which asynchronously loaded by browser.; 7. Method to build simple GUI changed and more simplified :). The example in index.htm.; While loadScript and AssertPrerequisites functions moved to JSROOT, one; can easily build many different kinds of GUIs, reusing provided JSRootCore.js functions.; 8. In example.htm also use AssertPrerequisites to load necessary scripts.; This helps to keep code up-to-date even by big changes in JavaScript code.; 9. Provide monitoring of online THttpServer with similar interface as for ROOT files.; 10. Fix several errors in TKey Streamer, use member names as in ROOT itself.; 11. Keep the only version identifier JSROOT.version for JS code; 12. One can specify in JSROOT.AssertPrerequisites functionality which is required.; One could specify '2d', 'io' (default) or '3d'.; 13. Use new AssertPrerequisites functionality to load only required functionality.; 14. When displaying",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:74398,load,loading,74398,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"1; %Y = bitcast i32* %x to i16* ; yields i16*:%x; %Z = bitcast <2 x i32> %V to i64; ; yields i64: %V (depends on endianness); %Z = bitcast <2 x i32*> %V to <2 x i64*> ; yields <2 x i64*>. .. _i_addrspacecast:. '``addrspacecast .. to``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = addrspacecast <pty> <ptrval> to <pty2> ; yields pty2. Overview:; """""""""""""""""". The '``addrspacecast``' instruction converts ``ptrval`` from ``pty`` in; address space ``n`` to type ``pty2`` in address space ``m``. Arguments:; """""""""""""""""""". The '``addrspacecast``' instruction takes a pointer or vector of pointer value; to cast and a pointer type to cast it to, which must have a different; address space. Semantics:; """""""""""""""""""". The '``addrspacecast``' instruction converts the pointer value; ``ptrval`` to type ``pty2``. It can be a *no-op cast* or a complex; value modification, depending on the target and the address space; pair. Pointer conversions within the same address space must be; performed with the ``bitcast`` instruction. Note that if the address; space conversion produces a dereferenceable result then both result; and operand refer to the same memory location. The conversion must; have no side effects, and must not capture the value of the pointer. If the source is :ref:`poison <poisonvalues>`, the result is; :ref:`poison <poisonvalues>`. If the source is not :ref:`poison <poisonvalues>`, and both source and; destination are :ref:`integral pointers <nointptrtype>`, and the; result pointer is dereferenceable, the cast is assumed to be; reversible (i.e. casting the result back to the original address space; should yield the original bit pattern). Example:; """""""""""""""". .. code-block:: llvm. %X = addrspacecast ptr %x to ptr addrspace(1); %Y = addrspacecast ptr addrspace(1) %y to ptr addrspace(2); %Z = addrspacecast <4 x ptr> %z to <4 x ptr addrspace(3)>. .. _otherops:. Other Operations; ----------------. The instructions in this category are the ""miscellane",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:458412,perform,performed,458412,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> <left_op>, <vscale x 2 x i32> <right_op>, metadata <condition code>, <vscale x 2 x i1> <mask>, i32 <vector_length>); declare <128 x i1> @llvm.vp.icmp.v128i8(<128 x i8> <left_op>, <128 x i8> <right_op>, metadata <condition code>, <128 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.icmp``' intrinsic returns a vector of boolean values based on; the comparison of its operands. The operation has a mask and an explicit vector; length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.icmp``' intrinsic takes the two values to compare as its first; and second operands. These two values must be vectors of :ref:`integer; <t_integer>` types.; The return type is the result of the comparison. The return type must be a; vector of :ref:`i1 <t_integer>` type. The fourth operand is the vector mask.; The return type, the values to compare, and the vector mask have the same; number of elements. The third operand is the condition code indicating the kind; of comparison to perform. It must be a metadata string with :ref:`one of the; supported integer condition code values <icmp_md_cc>`. The fifth operand is the; explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.icmp``' compares its first two operands according to the; condition code given as the third operand. The operands are compared element by; element on each enabled lane, where the semantics of the comparison are; defined :ref:`according to the condition code <icmp_md_cc_sem>`. Masked-off; lanes are ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i1> @llvm.vp.icmp.v4i32(<4 x i32> %a, <4 x i32> %b, metadata !""ne"", <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = icmp ne <4 x i32> %a, %b; %also.r = select <4 x i1> %mask, <4 x i1> %t, <4 x i1> poison. .. _int_vp_ceil:. '``llvm.vp.ceil.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:820436,perform,perform,820436,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"1F * h2 = new TH1F(""h2"", ""h2"", 100, -3., 3.);; h1->FillRandom(""gaus"", 5000);; h2->FillRandom(""gaus"", 4000);; h1->SetMaximum(100);; h1->Draw();; h2->Draw(""same"");; }; . In some case, when a graph had some vertical parts, the exclusion; zone was not drawn correctly. The following small example shows the; problem:; ; {; TCanvas *c1 = new TCanvas();; gPad->DrawFrame(-1,-1,3,3);. TGraph * graph=new TGraph(3);; graph->SetFillColor(3);; graph->SetFillStyle(3001);; graph->SetLineWidth(2000);. graph->SetPoint(0,1.,1.);; graph->SetPoint(1,1.,0);; graph->SetPoint(2,0.,0.);; graph->Draw(""*L"");; }; . TUnfold. Add a new version. A new class TUnfoldSys provides support for the propagation of systematic errors.; Some bugs were also fixed due to multiplication of addition of sparse matrices. Fitting Methods. Introduce a better treatment of the step size used when fitting an object with a TF1. Use now by default is not zero the error provided by TF1. In case of limits use an appropriate step size to avoid Minuit to go over the limits.; Fix bug https://savannah.cern.ch/bugs/?45909 when fitting with bad range values (outside the histogram range).; detect the case when the data set is empty and don't perform any minimizationin this case but exits from fitting and produce a warning message; Fix a bug when fitting histograms with option W and the bin errors are = 0.; Fix a bug in the InitGaus function when having only one data point (see https://savannah.cern.ch/bugs/?48936); Fix a bug in calculating the error on the integral after having fitted when fix parameters were present; Fix a bug in calculating the confidence intervas when the number of bins for the given object is different from the number of bins of the fitted object.; ; FitPanel. Add support for drawing the fit function confidence levels.; Make gaus the default function when fitting 1D objects.; Add GSL minimizer and use now a new widget for showing and selecting the list of available algorithms according to the minimizer.; ; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v524/index.html:8418,perform,perform,8418,hist/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v524/index.html,1,['perform'],['perform']
Performance,1`; - `3`; - :part:`25%`; * - clang-tools-extra/clang-move/tool; - `1`; - `1`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-query; - `5`; - `4`; - `1`; - :part:`80%`; * - clang-tools-extra/clang-query/tool; - `1`; - `0`; - `1`; - :none:`0%`; * - clang-tools-extra/clang-reorder-fields; - `2`; - `1`; - `1`; - :part:`50%`; * - clang-tools-extra/clang-reorder-fields/tool; - `1`; - `0`; - `1`; - :none:`0%`; * - clang-tools-extra/clang-tidy; - `20`; - `14`; - `6`; - :part:`70%`; * - clang-tools-extra/clang-tidy/abseil; - `42`; - `31`; - `11`; - :part:`73%`; * - clang-tools-extra/clang-tidy/altera; - `11`; - `9`; - `2`; - :part:`81%`; * - clang-tools-extra/clang-tidy/android; - `33`; - `23`; - `10`; - :part:`69%`; * - clang-tools-extra/clang-tidy/boost; - `3`; - `3`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/bugprone; - `125`; - `106`; - `19`; - :part:`84%`; * - clang-tools-extra/clang-tidy/cert; - `29`; - `28`; - `1`; - :part:`96%`; * - clang-tools-extra/clang-tidy/concurrency; - `5`; - `4`; - `1`; - :part:`80%`; * - clang-tools-extra/clang-tidy/cppcoreguidelines; - `45`; - `42`; - `3`; - :part:`93%`; * - clang-tools-extra/clang-tidy/darwin; - `5`; - `2`; - `3`; - :part:`40%`; * - clang-tools-extra/clang-tidy/fuchsia; - `15`; - `10`; - `5`; - :part:`66%`; * - clang-tools-extra/clang-tidy/google; - `33`; - `22`; - `11`; - :part:`66%`; * - clang-tools-extra/clang-tidy/hicpp; - `9`; - `7`; - `2`; - :part:`77%`; * - clang-tools-extra/clang-tidy/linuxkernel; - `3`; - `2`; - `1`; - :part:`66%`; * - clang-tools-extra/clang-tidy/llvm; - `11`; - `10`; - `1`; - :part:`90%`; * - clang-tools-extra/clang-tidy/llvmlibc; - `7`; - `7`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/misc; - `33`; - `30`; - `3`; - :part:`90%`; * - clang-tools-extra/clang-tidy/modernize; - `67`; - `48`; - `19`; - :part:`71%`; * - clang-tools-extra/clang-tidy/mpi; - `5`; - `5`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/objc; - `17`; - `12`; - `5`; - :part:`70%`; * -,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst:17040,concurren,concurrency,17040,interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,1,['concurren'],['concurrency']
Performance,"1minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *target &= ~(1 << bit);; }. //===---------------------------------------------------------------------===//. Instead of the following for memset char*, 1, 10:. 	movl $16843009, 4(%edx); 	movl $",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2902,load,load,2902,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['load'],['load']
Performance,"2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13212,load,load,13212,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['load']
Performance,"2 = INC32r %0, implicit-def dead $eflags, debug-location !5; DBG_VALUE %1, $noreg, !3, !DIExpression(), debug-location !5; %6:gr32 = ADD32rm %4, %2, 4, killed %5, 0, $noreg, implicit-def dead $eflags :: (load 4 from %ir.addr2); %7:gr32 = SUB32rr %6, %0, implicit-def $eflags, debug-location !5; JB_1 %bb.1, implicit $eflags, debug-location !5; JMP_1 %bb.2, debug-location !5. bb.2.bb2:; %8:gr32 = MOV32r0 implicit-def dead $eflags; $eax = COPY %8, debug-location !5; RET 0, $eax, debug-location !5. Observe first that there is a DBG_VALUE instruction for every ``llvm.dbg.value``; intrinsic in the source IR, ensuring no source level assignments go missing.; Then consider the different ways in which variable locations have been recorded:. * For the first dbg.value an immediate operand is used to record a zero value.; * The dbg.value of the PHI instruction leads to a DBG_VALUE of virtual register; ``%0``.; * The first GEP has its effect folded into the first load instruction; (as a 4-byte offset), but the variable location is salvaged by folding; the GEPs effect into the DIExpression.; * The second GEP is also folded into the corresponding load. However, it is; insufficiently simple to be salvaged, and is emitted as a ``$noreg``; DBG_VALUE, indicating that the variable takes on an undefined location.; * The final dbg.value has its Value placed in virtual register ``%1``. Instruction Scheduling; ----------------------. A number of passes can reschedule instructions, notably instruction selection; and the pre-and-post RA machine schedulers. Instruction scheduling can; significantly change the nature of the program -- in the (very unlikely) worst; case the instruction sequence could be completely reversed. In such; circumstances LLVM follows the principle applied to optimizations, that it is; better for the debugger not to display any state than a misleading state.; Thus, whenever instructions are advanced in order of execution, any; corresponding DBG_VALUE is kept in its origin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:32223,load,load,32223,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['load'],['load']
Performance,"2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - E",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:249390,load,load,249390,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"2();; virtual void f3();; };. struct B : A {; virtual void f1();; virtual void f2();; virtual void f3();; };. struct C : A {; virtual void f1();; virtual void f2();; virtual void f3();; };. The scheme will cause the virtual tables for A, B and C to be laid out; consecutively:. .. csv-table:: Virtual Table Layout for A, B, C; :header: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A::offset-to-top, &A::rtti, &A::f1, &A::f2, &A::f3, B::offset-to-top, &B::rtti, &B::f1, &B::f2, &B::f3, C::offset-to-top, &C::rtti, &C::f1, &C::f2, &C::f3. The bit vector for static types A, B and C will look like this:. .. csv-table:: Bit Vectors for A, B, C; :header: Class, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0; B, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0; C, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0. Bit vectors are represented in the object file as byte arrays. By loading; from indexed offsets into the byte array and applying a mask, a program can; test bits from the bit set with a relatively short instruction sequence. Bit; vectors may overlap so long as they use different bits. For the full details,; see the `ByteArrayBuilder`_ class. In this case, assuming A is laid out at offset 0 in bit 0, B at offset 0 in; bit 1 and C at offset 0 in bit 2, the byte array would look like this:. .. code-block:: c++. char bits[] = { 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 5, 0, 0 };. To emit a virtual call, the compiler will assemble code that checks that; the object's virtual table pointer is in-bounds and aligned and that the; relevant bit is set in the bit vector. For example on x86 a typical virtual call may look like this:. .. code-block:: none. ca7fbb: 48 8b 0f mov (%rdi),%rcx; ca7fbe: 48 8d 15 c3 42 fb 07 lea 0x7fb42c3(%rip),%rdx; ca7fc5: 48 89 c8 mov %rcx,%rax; ca7fc8: 48 29 d0 sub %rdx,%rax; ca7fcb: 48 c1 c0 3d rol $0x3d,%rax; ca7fcf: 48 3d 7f 01 00 00 cmp $0x17f,%rax; ca7fd5: 0f 87 36 05 00 00 ja ca8511; ca7fdb: 48 8d 15 c0 0b f7 0",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:1736,load,loading,1736,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['load'],['loading']
Performance,"2); %g2 = call i32 @gazonk(); br label %exit; exit:; %merge = phi [ %tval, %truebr ], [ %fval, %falsebr ]; %g = phi [ %g1, %truebr ], [ %g2, %falsebr ]; call @llvm.dbg.value(metadata i32 %merge, metadata !1, metadata !2); call @llvm.dbg.value(metadata i32 %g, metadata !3, metadata !2); %plusten = add i32 %merge, 10; %toret = add i32 %plusten, %g; call @llvm.dbg.value(metadata i32 %toret, metadata !1, metadata !2); ret i32 %toret; }. Containing two source-level variables in ``!1`` and ``!3``. The function could,; perhaps, be optimized into the following code:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; %g = call i32 @gazonk(); %addoper = select i1 %cond, i32 11, i32 12; %plusten = add i32 %bar, %addoper; %toret = add i32 %plusten, %g; ret i32 %toret; }. What ``llvm.dbg.value`` intrinsics should be placed to represent the original variable; locations in this code? Unfortunately the second, third and fourth; dbg.values for ``!1`` in the source function have had their operands; (%tval, %fval, %merge) optimized out. Assuming we cannot recover them, we; might consider this placement of dbg.values:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg.value(metadata i32 0, metadata !1, metadata !2); %g = call i32 @gazonk(); call @llvm.dbg.value(metadata i32 %g, metadata !3, metadata !2); %addoper = select i1 %cond, i32 11, i32 12; %plusten = add i32 %bar, %addoper; %toret = add i32 %plusten, %g; call @llvm.dbg.value(metadata i32 %toret, metadata !1, metadata !2); ret i32 %toret; }. However, this will cause ``!3`` to have the return value of ``@gazonk()`` at; the same time as ``!1`` has the constant value zero -- a pair of assignments; that never occurred in the unoptimized program. To avoid this, we must terminate; the range that ``!1`` has the constant value assignment by inserting a poison; dbg.value before the dbg.value for ``!3``:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:21378,optimiz,optimized,21378,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"2**; Depends on stage2-instrumented-generate-profdata and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. **stage2-check-llvm**; Depends on stage2 and runs check-llvm using the stage2 compiler. **stage2-check-clang**; Depends on stage2 and runs check-clang using the stage2 compiler. **stage2-check-all**; Depends on stage2 and runs check-all using the stage2 compiler. **stage2-test-suite**; Depends on stage2 and runs the test-suite using the stage2 compiler (requires; in-tree test-suite). BOLT; ====. `BOLT <https://github.com/llvm/llvm-project/blob/main/bolt/README.md>`_; (Binary Optimization and Layout Tool) is a tool that optimizes binaries; post-link by profiling them at runtime and then using that information to; optimize the layout of the final binary among other optimizations performed; at the binary level. There are also CMake caches available to build; LLVM/Clang with BOLT. To configure a single-stage build that builds LLVM/Clang and then optimizes; it with BOLT, use the following CMake configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. code-block:: console. $ cmake -G Ninja <path to source>/llvm \; -C <path to source>/clang/cmake/caches/BOLT-PGO.cmake \; -DBOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DBOOTSTRAP_BOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DPGO_INSTRUMENT_LTO=Thin. Then, to build the final optimized b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:10278,optimiz,optimizes,10278,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimizes']
Performance,"2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2787,load,load,2787,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,3,['load'],['load']
Performance,"2,0.2,0.8,0.3); root[] b->SetFillColor(5); root[] b->Draw(); ```. ![A rectangle with a border](pictures/020000AF.jpg). A **`TWbox`** is a rectangle (**`TBox`**) with a border size and a; border mode. The attributes of the outline line and of the fill area are; described in ""Graphical Objects Attributes"". ### Markers. A marker is a point with a fancy shape! The possible markers are shown; in the next figure. ![Markers](pictures/030000B0.png). The marker constructor is:. ``` {.cpp}; TMarker(Double_t x,Double_t y,Int_t marker); ```. The parameters `x` and `y` are the marker coordinates and `marker` is; the marker type, shown in the previous figure. Suppose the pointer `ma`; is a valid marker. The marker size is set via `ma->SetMarkerSize(size)`,; where `size` is the desired size. Note, that the marker types 1, 6 and 7; (the dots) cannot be scaled. They are always drawn with the same number; of pixels. `SetMarkerSize` does not apply on them. To have a ""scalable; dot"" a circle shape should be used instead, for example, the marker type; 20. The default marker type is 1, if `SetMarkerStyle` is not specified.; It is the most common one to draw scatter plots. ![Different marker sizes](pictures/030000B1.png). ![Different marker sizes](pictures/030000B2.png). The user interface for changing the marker color, style and size looks; like shown in this picture. It takes place in the editor frame anytime; the selected object inherits the class **`TAttMarker`**. Non-symmetric symbols should be used carefully in plotting. The next two; graphs show how the misleading a careless use of symbols can be. The two; plots represent the same data sets but because of a bad symbol choice,; the two on the top appear further apart from the next example. ![The use of non-symmetric markers](pictures/030000B3.png). A **`TPolyMaker`** is defined by an array on N points in a 2D space. At; each point `x[i]`, `y[i]` a marker is drawn. The list of marker types is; shown in the previous paragraph. The mark",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:36707,scalab,scalable,36707,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['scalab'],['scalable']
Performance,"2-bit; integer). ``` {.cpp}; ""ntrack/I2""; ```. With this Branch method, you can also add a leaf that holds an entire; array of variables. To add an array of floats use the `f[n]` notation; when describing the leaf. ``` {.cpp}; Float_t f[10];; tree->Branch(""fBranch"",f,""f[10]/F"");; ```. You can also add an array of variable length:. ``` {.cpp}; {; TFile *f = new TFile(""peter.root"",""recreate"");; Int_t nPhot;; Float_t E[500];; TTree* nEmcPhotons = new TTree(""nEmcPhotons"",""EMC Photons"");; nEmcPhotons->Branch(""nPhot"",&nPhot,""nPhot/I"");; nEmcPhotons->Branch(""E"",E,""E[nPhot]/F"");; }; ```. See ""Example 2: A Tree with a C Structure"" below; (`$ROOTSYS/tutorials/tree/tree2.C`) and `staff.C` at the beginning of; this chapter. ## Adding a TBranch to Hold an Object. To write a branch to hold an event object, we need to load the; definition of the `Event` class, which is in `$ROOTSYS/test/libEvent.so`; (if it doesn't exist type make in `$ROOTSYS/test`). An object can be; saved in a tree if a ROOT dictionary for its class has been generated; and loaded. ``` {.cpp}; root[] .L libEvent.so; ```. First, we need to open a file and create a tree. ``` {.cpp}; root[] TFile *f = new TFile(""AFile.root"",""RECREATE""); root[] TTree *tree = new TTree(""T"",""A Root Tree""); ```. We need to create a pointer to an `Event` object that will be used as a; reference in the `TTree::Branch` method. Then we create a branch; with the `TTree::Branch` method. ``` {.cpp}; root[] Event *event = new Event(); root[] tree->Branch(""EventBranch"",""Event"",&event,32000,99); ```. To add a branch to hold an object we use the signature above. The first; parameter is the name of the branch. The second parameter is the name of; the class of the object to be stored. The third parameter is the address; of a pointer to the object to be stored. Note that it is an address of a pointer to the object, not just a; pointer to the object. The fourth parameter is the buffer size and is by default 32000 bytes.; It is the number of bytes of d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:23406,load,loaded,23406,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loaded']
Performance,"2.nxv8i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2). Overview:; """""""""""""""""". The '``llvm.experimental.vector.interleave2``' intrinsic constructs a vector; by interleaving two input vectors. This intrinsic works for both fixed and scalable vectors. While this intrinsic; supports all vector types the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. <4 x i64> llvm.experimental.vector.interleave2.v4i64(<2 x i64> <i64 0, i64 2>, <2 x i64> <i64 1, i64 3>); ==> <4 x i64> <i64 0, i64 1, i64 2, i64 3>. Arguments:; """"""""""""""""""""; Both arguments must be vectors of the same type whereby their logical; concatenation matches the result type. '``llvm.experimental.cttz.elts``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ```llvm.experimental.cttz.elts```; on any vector of integer elements, both fixed width and scalable. ::. declare i8 @llvm.experimental.cttz.elts.i8.v8i1(<8 x i1> <src>, i1 <is_zero_poison>). Overview:; """""""""""""""""". The '``llvm.experimental.cttz.elts``' intrinsic counts the number of trailing; zero elements of a vector. Arguments:; """""""""""""""""""". The first argument is the vector to be counted. This argument must be a vector; with integer element type. The return type must also be an integer type which is; wide enough to hold the maximum number of elements of the source vector. The; behaviour of this intrinsic is undefined if the return type is not wide enough; for the number of elements in the input vector. The second argument is a constant flag that indicates whether the intrinsic; returns a valid result if the first argument is all zero. If the first argument; is all zero and the second argument is true, the result is poison. Semantics:; """""""""""""""""""". The '``llvm.experimental.cttz.elts``' intrinsic counts the trailing (least; significant) zero elements ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:669328,scalab,scalable,669328,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"20 : Total Size= 582 bytes File Size = 92 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 1.00 *. Add a new function TBranch::SetStatus It is much faster to call this function in case of a Tree with many branches; instead of calling TTree::SetBranchStatus.; Implement TTreeCache::Print that shows information like:; // ******TreeCache statistics for file: cms2.root ******; // Number of branches in the cache ...: 1093; // Cache Efficiency ..................: 0.997372; // Cache Efficiency Rel...............: 1.000000; // Learn entries......................: 100; // Reading............................: 72761843 bytes in 7 transactions; // Readahead..........................: 256000 bytes with overhead = 0 bytes; // Average transaction................: 10394.549000 Kbytes; // Number of blocks in current cache..: 210, total size: 6280352; This function can be called directly from TTree: T->PrintCacheStats();. Add support for variable size array of object in a TTree (when the owner of the array is split.); And many other bug fixes, security fixes, thread safety and performance improvements ; see the svn log for details. TTree Scan and Draw. Insured that the generated histogram as an integral bin width when plotting a string or integer.; Improved the output of TTree::Scan by inserting a blank space whenever a value is not available because there is no proper row in a friend.; (Previously it was re-printing the previous value). This required changes in ; When the draw option to TTree::Draw contains ""norm"" the output histogram is normalized to 1.; Improve the selection of the leaf used for size of an array in a leaflist by giving preference; for the leaf inside the same branch and by adding support for explicit full path name. For example the following now works properly:; tree->Branch(""JET1"", &JET1, ""njets/I:et[njets]/F:pt[njets]/F"");; tree->BranchBranch(""JET2"", &JET2, ""njets/I:et[njets]/F:pt[njets]/F"");; ...; tree->Scan(""njets/I:et[JETS1.njets]/F:pt[JETS1.njets]"");. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html:6458,perform,performance,6458,tree/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html,1,['perform'],['performance']
Performance,"2> @llvm.vp.sext.v16i32.v16i16 (<16 x i16> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.sext.nxv4i32.nxv4i16 (<vscale x 4 x i16> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.sext``' intrinsic sign extends its first operand to the return; type. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.sext``' intrinsic takes a value to cast as its first operand.; The return type is the type to cast the value to. Both types must be vectors of; :ref:`integer <t_integer>` type. The bit size of the value must be smaller than; the bit size of the return type. The second operand is the vector mask. The; return type, the value to cast, and the vector mask have the same number of; elements. The third operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.sext``' intrinsic performs a sign extension by copying the sign; bit (highest order bit) of the value until it reaches the size of the return; type. When sign extending from i1, the result will always be either -1 or 0.; The conversion is performed on lane positions below the explicit vector length; and where the vector mask is true. Masked-off lanes are ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.sext.v4i32.v4i16(<4 x i16> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = sext <4 x i16> %a to <4 x i32>; %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_fptrunc:. '``llvm.vp.fptrunc.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fptrunc.v16f32.v16f64 (<16 x double> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.trunc.nxv4f32.nxv4f64 (<vscale x 4 x double> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>). Overv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:800378,perform,performs,800378,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"2^32 bytes and the base is; aligned to 2^32 which makes it easier to convert from flat to segment or; segment to flat. A global address space address has the same value when used as a flat address; so no conversion is needed. **Global and Constant**; The global and constant address spaces both use global virtual addresses,; which are the same virtual address space used by the CPU. However, some; virtual addresses may only be accessible to the CPU, some only accessible; by the GPU, and some by both. Using the constant address space indicates that the data will not change; during the execution of the kernel. This allows scalar read instructions to; be used. As the constant address space could only be modified on the host; side, a generic pointer loaded from the constant address space is safe to be; assumed as a global pointer since only the device global memory is visible; and managed on the host side. The vector and scalar L1 caches are invalidated; of volatile data before each kernel dispatch execution to allow constant; memory to change values between kernel dispatches. **Region**; The region address space uses the hardware Global Data Store (GDS). All; wavefronts executing on the same device will access the same memory for any; given region address. However, the same region address accessed by wavefronts; executing on different devices will access different memory. It is higher; performance than global memory. It is allocated by the runtime. The data; store (DS) instructions can be used to access it. **Local**; The local address space uses the hardware Local Data Store (LDS) which is; automatically allocated when the hardware creates the wavefronts of a; work-group, and freed when all the wavefronts of a work-group have; terminated. All wavefronts belonging to the same work-group will access the; same memory for any given local address. However, the same local address; accessed by wavefronts belonging to different work-groups will access; different memory. It is h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:25339,cache,caches,25339,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"3), !dbg !19; ; [debug line = 5:9] [debug variable = Z]. The third intrinsic ``%llvm.dbg.declare`` encodes debugging information for; variable ``Z``. The metadata ``!dbg !19`` attached to the intrinsic provides; scope information for the variable ``Z``. .. code-block:: text. !18 = distinct !DILexicalBlock(scope: !4, file: !1, line: 4, column: 5); !19 = !DILocation(line: 5, column: 11, scope: !18). Here ``!19`` indicates that ``Z`` is declared at line number 5 and column; number 11 inside of lexical scope ``!18``. The lexical scope itself resides; inside of subprogram ``!4`` described above. The scope information attached with each instruction provides a straightforward; way to find instructions covered by a scope. Object lifetime in optimized code; =================================. In the example above, every variable assignment uniquely corresponds to a; memory store to the variable's position on the stack. However in heavily; optimized code LLVM promotes most variables into SSA values, which can; eventually be placed in physical registers or memory locations. To track SSA; values through compilation, when objects are promoted to SSA values an; ``llvm.dbg.value`` intrinsic is created for each assignment, recording the; variable's new location. Compared with the ``llvm.dbg.declare`` intrinsic:. * A dbg.value terminates the effect of any preceding dbg.values for (any; overlapping fragments of) the specified variable.; * The dbg.value's position in the IR defines where in the instruction stream; the variable's value changes.; * Operands can be constants, indicating the variable is assigned a; constant value. Care must be taken to update ``llvm.dbg.value`` intrinsics when optimization; passes alter or move instructions and blocks -- the developer could observe such; changes reflected in the value of variables when debugging the program. For any; execution of the optimized program, the set of variable values presented to the; developer by the debugger should not show a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:18012,optimiz,optimized,18012,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"30. Several improvements for touch devices or devices with small displays; 31. Remove settings.FrameNDC, use Style.fPadLeft/Right/Top/BottomMargin values instead; 32. Fix - rescan sumw2 when update TH1; 33. Fix - correct placing for TLegend header; 34. Fix - correctly align sub/super scripts in complex TLatex; 35. Fix - correctly set visibility level for geo drawing (#258); 36. Fix - use more factor for number of nodes in geo drawing (#258). ## Changes in 7.3.4; 1. Fix - failure in normal_cdf calculation; 2. Fix - check in TTree::Draw for null buffer; 3. Fix - do not rise exception in treeProcess; 4. Fix - RH1 zero line drawing only when required; 5. Fix - do not allow move float browser too far left/top. ## Changes in 7.3.2; 1. Fix - undefined graph in TGraphPainter; 2. Fix - error in showing info in the geo painter; 3. Fix - stack limitation with Math.min.apply in tree draw. ## Changes in 7.3.1; 1. Fix - TGeo update in the TWebCanvas; 2. Fix - several tutorials with three.js modules loading; 3. Fix - redraw pad when change text align attributes; 4. Fix - pad ranges for TWebCanvas, handle log2 scales; 5. Fix - support candle and violin options when creating string draw option; 6. Fix - labels and tooltips on reversed axes; 7. Fix - zooming on TRatioPlot; 8. Fix - pad ranges calculations for TWebCanvas; 9. Fix - set proper background for geo drawing. ## Changes in 7.3.0; 1. Mark methods returning `Promise` as **async**; 2. Upgrade three.js to r146; 3. Fix several bugs in `csg.mjs`, improve geometry clipping; 4. Provide `settings.PreferSavedPoints` to exclude function evaluation when there are saved points; 5. Add more interactive features with `TWebCanvas`; 6. 3-dimensional `TTree::Draw()` now produces `TPolyMarker3D` by default; 7. Force MathJax rendering when `\` symbol is found (#243); 8. Support `TButton` class; 9. Remove `localfile` url option, only interactively one can open file selection dialog; 10. Fix - show correct bin index in `TH2` tooltips; 11. Fix - i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:15041,load,loading,15041,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"386.c) has a ton of interesting; simplifications for integer ""x cmp y ? a : b"". //===---------------------------------------------------------------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===----------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:10741,cache,cacheline,10741,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['cache'],['cacheline']
Performance,3`; - :part:`93%`; * - clang-tools-extra/clang-tidy/darwin; - `5`; - `2`; - `3`; - :part:`40%`; * - clang-tools-extra/clang-tidy/fuchsia; - `15`; - `10`; - `5`; - :part:`66%`; * - clang-tools-extra/clang-tidy/google; - `33`; - `22`; - `11`; - :part:`66%`; * - clang-tools-extra/clang-tidy/hicpp; - `9`; - `7`; - `2`; - :part:`77%`; * - clang-tools-extra/clang-tidy/linuxkernel; - `3`; - `2`; - `1`; - :part:`66%`; * - clang-tools-extra/clang-tidy/llvm; - `11`; - `10`; - `1`; - :part:`90%`; * - clang-tools-extra/clang-tidy/llvmlibc; - `7`; - `7`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/misc; - `33`; - `30`; - `3`; - :part:`90%`; * - clang-tools-extra/clang-tidy/modernize; - `67`; - `48`; - `19`; - :part:`71%`; * - clang-tools-extra/clang-tidy/mpi; - `5`; - `5`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/objc; - `17`; - `12`; - `5`; - :part:`70%`; * - clang-tools-extra/clang-tidy/openmp; - `5`; - `5`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/performance; - `31`; - `24`; - `7`; - :part:`77%`; * - clang-tools-extra/clang-tidy/plugin; - `1`; - `1`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/portability; - `5`; - `3`; - `2`; - :part:`60%`; * - clang-tools-extra/clang-tidy/readability; - `88`; - `76`; - `12`; - :part:`86%`; * - clang-tools-extra/clang-tidy/tool; - `3`; - `2`; - `1`; - :part:`66%`; * - clang-tools-extra/clang-tidy/utils; - `35`; - `31`; - `4`; - :part:`88%`; * - clang-tools-extra/clang-tidy/zircon; - `3`; - `3`; - `0`; - :good:`100%`; * - clang-tools-extra/clangd; - `97`; - `81`; - `16`; - :part:`83%`; * - clang-tools-extra/clangd/benchmarks; - `1`; - `1`; - `0`; - :good:`100%`; * - clang-tools-extra/clangd/benchmarks/CompletionModel; - `1`; - `0`; - `1`; - :none:`0%`; * - clang-tools-extra/clangd/fuzzer; - `2`; - `2`; - `0`; - :good:`100%`; * - clang-tools-extra/clangd/index; - `39`; - `36`; - `3`; - :part:`92%`; * - clang-tools-extra/clangd/index/dex; - `9`; - `7`; - `2`; - :part:`77%`; * - clang-tools-,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst:18154,perform,performance,18154,interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,1,['perform'],['performance']
Performance,"3e3d81e041a804481df228fe0081c) to the `RDataFrame` interface, which allows to get useful information, e.g. the columns and their types.; - Add [`DescribeDataset`](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html#a1bc5b86a2a834bb06711fb535451146d) to the `RDataFrame` interface, which allows to get information about the dataset (subset of the output of Describe()).; - Add [`DefinePerSample`](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html#a29d77593e95c0f84e359a802e6836a0e), a method which makes it possible to define columns based on the sample and entry range being processed. It is also a useful way to register callbacks that should only be called when the input dataset/TTree changes.; - Add [`HistoND`](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html#a0c9956a0f48c26f8e4294e17376c7fea) action that fills a N-dimensional histogram.; - `Book` now supports just-in-time compilation, i.e. it can be called without passing the column types as template parameters (with some performance penalty, as usual).; - As an aid to `RDataSource` implementations with which collection sizes can be retrieved more efficiently than the full collection, `#var` can now be used as a short-hand notation for column name `R_rdf_sizeof_var`.; - Helpers have been added to export data from `RDataFrame` to RooFit datasets. See the ""RooFit Libraries"" section below for more details, or see [the tutorial](https://root.cern/doc/master/rf408__RDataFrameToRooFit_8C.html). ### Notable changes in behavior. - Using `Alias`, it is now possible to register homonymous aliases (alternative column names) in different branches of the computation graph, in line with the behavior of `Define` (until now, aliases were required to be unique in the whole computaton graph).; - The `Histo*D` methods now support the combination of scalar values and vector-like weight values. For each entry, the histogram is filled once for each weight, always with the same scalar value.; - The ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md:8497,perform,performance,8497,README/ReleaseNotes/v626/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md,1,['perform'],['performance']
Performance,"4 (or 5) (1.5 x 1.5 = 2.25). ; Saturation; %res = call i4 @llvm.umul.fix.sat.i4(i4 8, i4 2, i32 0) ; %res = 15 (8 x 2 -> clamped to 15); %res = call i4 @llvm.umul.fix.sat.i4(i4 8, i4 8, i32 2) ; %res = 15 (2 x 2 -> clamped to 3.75). ; Scale can affect the saturation result; %res = call i4 @llvm.umul.fix.sat.i4(i4 2, i4 4, i32 0) ; %res = 7 (2 x 4 -> clamped to 7); %res = call i4 @llvm.umul.fix.sat.i4(i4 2, i4 4, i32 1) ; %res = 4 (1 x 2 = 2). '``llvm.sdiv.fix.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.sdiv.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.sdiv.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.sdiv.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.sdiv.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.sdiv.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.sdiv.fix``' family of intrinsic functions perform signed; fixed point division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type, or if the second argument is zero. Examples; """""""""""""""""". .. co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:630442,perform,perform,630442,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"4 x float> %t, <4 x float> poison. .. _int_vp_roundtozero:. '``llvm.vp.roundtozero.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.roundtozero.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.roundtozero.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.roundtozero.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point round-to-zero of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of floating-point type.; The second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.roundtozero``' intrinsic performs floating-point roundeven; (:ref:`llvm.trunc <int_llvm_trunc>`) of the first vector operand on each enabled lane. The; result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.roundtozero.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.trunc.v4f32(<4 x float> %a); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_bitreverse:. '``llvm.vp.bitreverse.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.bitreverse.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.bitreverse.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.bitreverse.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:830785,perform,performs,830785,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"4* only up to 4 32-bit floating-point parameters,; 4 64-bit floating-point parameters, and 10 bit type parameters; are supported.; - *RISCV64* only supports up to 11 bit type parameters, 4; 32-bit floating-point parameters, and 4 64-bit floating-point; parameters. This calling convention supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; both the caller and callee are using it.; ""``cc 11``"" - The HiPE calling convention; This calling convention has been implemented specifically for use by; the `High-Performance Erlang; (HiPE) <http://www.it.uu.se/research/group/hipe/>`_ compiler, *the*; native code compiler of the `Ericsson's Open Source Erlang/OTP; system <http://www.erlang.org/download.shtml>`_. It uses more; registers for argument passing than the ordinary C calling; convention and defines no callee-saved registers. The calling; convention properly supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; that both the caller and the callee use it. It uses a *register pinning*; mechanism, similar to GHC's convention, for keeping frequently; accessed runtime components pinned to specific hardware registers.; At the moment only X86 supports this convention (both 32 and 64; bit).; ""``anyregcc``"" - Dynamic calling convention for code patching; This is a special convention that supports patching an arbitrary code; sequence in place of a call site. This convention forces the call; arguments into registers but allows them to be dynamically; allocated. This can currently only be used with calls to; llvm.experimental.patchpoint because only this intrinsic records; the location of its arguments in a side table. See :doc:`StackMaps`.; ""``preserve_mostcc``"" - The `PreserveMost` calling convention; This calling convention attempts to make the code in the caller as; unintrusive as possible. This convention behaves identically to the `C`; calling convention on how arguments and return values are pass",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:15401,optimiz,optimization,15401,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"4.txt; CODE test.elf 0x4004a0; DATA inlined.elf 0x601028. $ llvm-symbolizer < addr4.txt; main; /tmp/test.cpp:15:0. bar; 6295592 4. Example 6 - path-style options:. This example uses the same source file as above, but the source file's; full path is /tmp/foo/test.cpp and is compiled as follows. The first case; shows the default absolute path, the second --basenames, and the third; shows --relativenames. .. code-block:: console. $ pwd; /tmp; $ clang -g foo/test.cpp -o test.elf; $ llvm-symbolizer --obj=test.elf 0x4004a0; main; /tmp/foo/test.cpp:15:0; $ llvm-symbolizer --obj=test.elf 0x4004a0 --basenames; main; test.cpp:15:0; $ llvm-symbolizer --obj=test.elf 0x4004a0 --relativenames; main; foo/test.cpp:15:0. Example 7 - Addresses as symbol names:. .. code-block:: console. $ llvm-symbolizer --obj=test.elf main; main; /tmp/test.cpp:14:0; $ llvm-symbolizer --obj=test.elf ""CODE foz""; foz; /tmp/test.h:1:0. OPTIONS; -------. .. option:: --adjust-vma <offset>. Add the specified offset to object file addresses when performing lookups.; This can be used to perform lookups as if the object were relocated by the; offset. .. option:: --basenames, -s. Print just the file's name without any directories, instead of the; absolute path. .. option:: --build-id. Look up the object using the given build ID, specified as a hexadecimal; string. Mutually exclusive with :option:`--obj`. .. option:: --color [=<always|auto|never>]. Specify whether to use color in :option:`--filter-markup` mode. Defaults to; ``auto``, which detects whether standard output supports color. Specifying; ``--color`` alone is equivalent to ``--color=always``. .. option:: --debug-file-directory <path>. Provide a path to a directory with a `.build-id` subdirectory to search for; debug information for stripped binaries. Multiple instances of this argument; are searched in the order given. .. option:: --debuginfod, --no-debuginfod. Whether or not to try debuginfod lookups for debug binaries. Unless specified,; debuginfod is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst:4874,perform,performing,4874,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst,1,['perform'],['performing']
Performance,"483648 by -1. If the ``exact`` keyword is present, the result value of the ``sdiv`` is; a :ref:`poison value <poisonvalues>` if the result would be rounded. Example:; """""""""""""""". .. code-block:: text. <result> = sdiv i32 4, %var ; yields i32:result = 4 / %var. .. _i_fdiv:. '``fdiv``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = fdiv [fast-math flags]* <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``fdiv``' instruction returns the quotient of its two operands. Arguments:; """""""""""""""""""". The two arguments to the '``fdiv``' instruction must be; :ref:`floating-point <t_floating>` or :ref:`vector <t_vector>` of; floating-point values. Both arguments must have identical types. Semantics:; """""""""""""""""""". The value produced is the floating-point quotient of the two operands.; This instruction is assumed to execute in the default :ref:`floating-point; environment <floatenv>`.; This instruction can also take any number of :ref:`fast-math; flags <fastmath>`, which are optimization hints to enable otherwise; unsafe floating-point optimizations:. Example:; """""""""""""""". .. code-block:: text. <result> = fdiv float 4.0, %var ; yields float:result = 4.0 / %var. .. _i_urem:. '``urem``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = urem <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``urem``' instruction returns the remainder from the unsigned; division of its two arguments. Arguments:; """""""""""""""""""". The two arguments to the '``urem``' instruction must be; :ref:`integer <t_integer>` or :ref:`vector <t_vector>` of integer values. Both; arguments must have identical types. Semantics:; """""""""""""""""""". This instruction returns the unsigned integer *remainder* of a division.; This instruction always performs an unsigned division to get the; remainder. Note that unsigned integer remainder and signed integer remainder are; distinct operations; for signed integer remainder, use '``srem``'. Taking the remainder of a division ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:387572,optimiz,optimization,387572,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"4:$XT, (fminnum f64:$XA, f64:$XB)). . xsmaxjdp xsminjdp; (set f64:$XT, (int_ppc_vsx_xsmaxjdp f64:$XA, f64:$XB)); (set f64:$XT, (int_ppc_vsx_xsminjdp f64:$XA, f64:$XB)). - Vector Byte-Reverse H/W/D/Q Word: xxbrh xxbrw xxbrd xxbrq; . Use intrinsic; (set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17365,load,load,17365,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,1,['load'],['load']
Performance,"4; Total number of mappings created: 0; Max number of mappings used: 0. If we look at the *Dynamic Dispatch Stall Cycles* table, we see the counter for; SCHEDQ reports 272 cycles. This counter is incremented every time the dispatch; logic is unable to dispatch a full group because the scheduler's queue is full. Looking at the *Dispatch Logic* table, we see that the pipeline was only able to; dispatch two micro opcodes 51.5% of the time. The dispatch group was limited to; one micro opcode 44.6% of the cycles, which corresponds to 272 cycles. The; dispatch statistics are displayed by either using the command option; ``-all-stats`` or ``-dispatch-stats``. The next table, *Schedulers*, presents a histogram displaying a count,; representing the number of micro opcodes issued on some number of cycles. In; this case, of the 610 simulated cycles, single opcodes were issued 306 times; (50.2%) and there were 7 cycles where no opcodes were issued. The *Scheduler's queue usage* table shows that the average and maximum number of; buffer entries (i.e., scheduler queue entries) used at runtime. Resource JFPU01; reached its maximum (18 of 18 queue entries). Note that AMD Jaguar implements; three schedulers:. * JALU01 - A scheduler for ALU instructions.; * JFPU01 - A scheduler floating point operations.; * JLSAGU - A scheduler for address generation. The dot-product is a kernel of three floating point instructions (a vector; multiply followed by two horizontal adds). That explains why only the floating; point scheduler appears to be used. A full scheduler queue is either caused by data dependency chains or by a; sub-optimal usage of hardware resources. Sometimes, resource pressure can be; mitigated by rewriting the kernel using different instructions that consume; different scheduler resources. Schedulers with a small queue are less resilient; to bottlenecks caused by the presence of long data dependencies. The scheduler; statistics are displayed by using the command option ``-all-st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:31313,queue,queue,31313,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"5]; | 0x........ | HashData[6]; | 0x........ | HashData[7]; | 0x........ | HashData[8]; | 0x00000000 | String offset into .debug_str (terminate data for hash); `------------'. So we still have all of the same data, we just organize it more efficiently for; debugger lookup. If we repeat the same ""``printf``"" lookup from above, we; would hash ""``printf``"" and find it matches ``BUCKETS[3]`` by taking the 32 bit; hash value and modulo it by ``n_buckets``. ``BUCKETS[3]`` contains ""6"" which; is the index into the ``HASHES`` table. We would then compare any consecutive; 32 bit hashes values in the ``HASHES`` array as long as the hashes would be in; ``BUCKETS[3]``. We do this by verifying that each subsequent hash value modulo; ``n_buckets`` is still 3. In the case of a failed lookup we would access the; memory for ``BUCKETS[3]``, and then compare a few consecutive 32 bit hashes; before we know that we have no match. We don't end up marching through; multiple words of memory and we really keep the number of processor data cache; lines being accessed as small as possible. The string hash that is used for these lookup tables is the Daniel J.; Bernstein hash which is also used in the ELF ``GNU_HASH`` sections. It is a; very good hash for all kinds of names in programs with very few hash; collisions. Empty buckets are designated by using an invalid hash index of ``UINT32_MAX``. Details; ^^^^^^^. These name hash tables are designed to be generic where specializations of the; table get to define additional data that goes into the header (""``HeaderData``""),; how the string value is stored (""``KeyType``"") and the content of the data for each; hash value. Header Layout; """""""""""""""""""""""""". The header has a fixed part, and the specialized part. The exact format of the; header is:. .. code-block:: c. struct Header; {; uint32_t magic; // 'HASH' magic value to allow endian detection; uint16_t version; // Version number; uint16_t hash_function; // The hash function enumeration that was used; u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:67896,cache,cache,67896,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['cache'],['cache']
Performance,"5][003] {Code} 'movl	-0x4(%rbp), %eax'; [0x0000000038][003] {Code} 'popq	%rbp'; [0x0000000039][003] {Code} 'retq'; [0x000000003a][003] 9 {Line} {NewStatement} {EndSequence} '/test.cpp'. -----------------------------; Element Total Printed; -----------------------------; Scopes 3 3; Symbols 4 4; Types 5 5; Lines 25 25; -----------------------------; Total 37 37. Scope Sizes:; 189 (100.00%) : [0x000000000b][001] {CompileUnit} 'test.cpp'; 110 ( 58.20%) : [0x000000002a][002] 2 {Function} extern not_inlined 'foo' -> [0x0000000099]'int'; 27 ( 14.29%) : [0x0000000071][003] {Block}. Totals by lexical level:; [001]: 189 (100.00%); [002]: 110 ( 58.20%); [003]: 27 ( 14.29%). The **Scope Sizes** table shows the contribution in bytes to the debug; information by each scope, which can be used to determine unexpected; size changes in the DWARF sections between different versions of the; same toolchain. .. code-block:: none. [0x000000002a][002] 2 {Function} extern not_inlined 'foo' -> [0x0000000099]'int'; [0x000000002a][003] {Range} Lines 2:9 [0x0000000000:0x000000003a]; [0x000000002a][003] {Linkage} 0x2 '_Z3fooPKijb'; [0x0000000071][003] {Block}; [0x0000000071][004] {Range} Lines 5:8 [0x000000001c:0x000000002f]; [0x000000007e][004] 5 {Variable} 'CONSTANT' -> [0x00000000c3]'const INTEGER'; [0x000000007e][005] {Coverage} 100.00%; [0x000000007f][005] {Location}; [0x000000007f][006] {Entry} Stack Offset: -28 (0xffffffffffffffe4) [DW_OP_fbreg]. The **{Range}** attribute describe the line ranges for a logical scope.; For this case, the function **foo** is within the lines **2** and **9**. The **{Coverage}** and **{Location}** attributes describe the debug; location and coverage for logical symbols. For optimized code, the; coverage value decreases and it affects the program debuggability. EXIT STATUS; -----------; :program:`llvm-debuginfo-analyzer` returns 0 if the input files were; parsed and printed successfully. Otherwise, it returns 1. SEE ALSO; --------; :manpage:`llvm-dwarfdump`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst:59798,optimiz,optimized,59798,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,1,['optimiz'],['optimized']
Performance,"6 x float>, align 16; 	store <16 x float> %A, <16 x float>* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%esp); 	movaps	%xmm0, 64(%esp); 	movl	60(%esp), %eax; 	movl	%eax, 124(%esp); 	movl	56(%esp), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit copies>; 	movaps	(%esp), %xmm0; 	movaps	16(%esp), %xmm1; 	movaps	32(%esp), %xmm2; 	movaps	48(%esp), %xmm3; 	addl	$140, %esp; 	ret. On Nehalem, it may even be cheaper to just use movups when unaligned than to; fall back to lower-granularity chunks. //===---------------------------------------------------------------------===//. Implement processor-specific optimizations for parity with GCC on these; processors. GCC does two optimizations:. 1. ix86_pad_returns inserts a noop before ret instructions if immediately; preceded by a conditional branch or is the target of a jump.; 2. ix86_avoid_jump_misspredicts inserts noops in cases where a 16-byte block of; code contains more than 3 branches.; ; The first one is done for all AMDs, Core2, and ""Generic""; The second one is done for: Atom, Pentium Pro, all AMDs, Pentium 4, Nocona,; Core 2, and ""Generic"". //===---------------------------------------------------------------------===//; Testcase:; int x(int a) { return (a&0xf0)>>4; }. Current output:; 	movl	4(%esp), %eax; 	shrl	$4, %eax; 	andl	$15, %eax; 	ret. Ideal output:; 	movzbl	4(%esp), %eax; 	shrl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. Re-implement atomic builtins __sync_add_and_fetch() and __sync_sub_and_fetch; properly. When the return value is not used (i.e. only care about the value in the; mem",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:32762,optimiz,optimizations,32762,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['optimiz'],['optimizations']
Performance,"6 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.frem.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.frem.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point remainder of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.frem``' intrinsic performs floating-point remainder (:ref:`frem <i_frem>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.frem.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = frem <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fneg:. '``llvm.vp.fneg.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fneg.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fneg.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fneg.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point negation of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:739803,perform,performed,739803,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.wave.reduce.umax Performs an arithmetic unsigned max reduction on the unsigned values; provided by each lane in the wavefront.; Intrinsic takes a hint for reduction strategy using second operand; 0: Target default preference,; 1: `Iterative strategy`, and; 2: `DPP`.; If target does not support the DPP operations (e.g. gfx6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.udot2 Provides direct access to v_dot2_u32_u16 across targets which; support such instructions. This performs unsigned dot product; with two v2i16 operands, summed with the third i32 operand. The; i1 fourth operand is used to clamp the output. llvm.amdgcn.udot4 Provides direct access to v_dot4_u32_u8 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 4 8bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output. llvm.amdgcn.udot8 Provides direct access to v_dot8_u32_u4 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 8 4bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output. llvm.amdgcn.sdot2 Provides direct access to v_dot2_i32_i16 across targets which; support such instructions. This performs signed dot product; with two v2i16 operands, summed with the third i32 operand. The; i1 fourth operand is used to clamp the output.; When applicable (e.g. no clamping), this is lowered into; v_dot2c_i32_i16 for targets which support it. llvm.amdgcn.sdot4 Provides direct access to v_dot4_i32_i8 across targets which; support such instructions. This performs signed dot product; with two i32 operands (holding a vector of 4 8bit values), summed; with the third i32 operan",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:39504,perform,performs,39504,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performs']
Performance,"64 or BPF_JMP,; 4th bit encodes source operand. ::. BPF_X 0x1 use src_reg register as source operand; BPF_K 0x0 use 32 bit immediate as source operand. and four MSB bits store operation code. ::. BPF_ADD 0x0 add; BPF_SUB 0x1 subtract; BPF_MUL 0x2 multiply; BPF_DIV 0x3 divide; BPF_OR 0x4 bitwise logical OR; BPF_AND 0x5 bitwise logical AND; BPF_LSH 0x6 left shift; BPF_RSH 0x7 right shift (zero extended); BPF_NEG 0x8 arithmetic negation; BPF_MOD 0x9 modulo; BPF_XOR 0xa bitwise logical XOR; BPF_MOV 0xb move register to register; BPF_ARSH 0xc right shift (sign extended); BPF_END 0xd endianness conversion. If BPF_CLASS(code) == BPF_JMP, BPF_OP(code) is one of. ::. BPF_JA 0x0 unconditional jump; BPF_JEQ 0x1 jump ==; BPF_JGT 0x2 jump >; BPF_JGE 0x3 jump >=; BPF_JSET 0x4 jump if (DST & SRC); BPF_JNE 0x5 jump !=; BPF_JSGT 0x6 jump signed >; BPF_JSGE 0x7 jump signed >=; BPF_CALL 0x8 function call; BPF_EXIT 0x9 function return. Instruction encoding (load, store); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; For load and store instructions the 8-bit 'code' field is divided as:. ::. +--------+--------+-------------------+; | 3 bits | 2 bits | 3 bits |; | mode | size | instruction class |; +--------+--------+-------------------+; (MSB) (LSB). Size modifier is one of. ::. BPF_W 0x0 word; BPF_H 0x1 half word; BPF_B 0x2 byte; BPF_DW 0x3 double word. Mode modifier is one of. ::. BPF_IMM 0x0 immediate; BPF_ABS 0x1 used to access packet data; BPF_IND 0x2 used to access packet data; BPF_MEM 0x3 memory; (reserved) 0x4; (reserved) 0x5; BPF_XADD 0x6 exclusive add. Packet data access (BPF_ABS, BPF_IND); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Two non-generic instructions: (BPF_ABS | <size> | BPF_LD) and; (BPF_IND | <size> | BPF_LD) which are used to access packet data.; Register R6 is an implicit input that must contain pointer to sk_buff.; Register R0 is an implicit output which contains the data fetched; from the packet. Registers R1-R5 are scratch registers and must not; be used to store the data ac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:105051,load,load,105051,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,['load'],['load']
Performance,"6U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20837,perform,perform,20837,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['perform'],['perform']
Performance,"7-GFX9 ``flat_load/store/atomic`` instructions can report out of; vector memory order if they access LDS memory, and out of LDS operation order; if they access global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore, no special action is required for coherence between the; lanes of a single wavefront, or for coherence between wavefronts in the same; work-group. A ``buffer_wbinvl1_vol`` is required for coherence between; wavefronts executing in different work-groups as they may be executing on; different CUs.; * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0)`` is required to; ensure synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` varia",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:208478,cache,cache,208478,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"8 89 c8 mov %rcx,%rax; ca7fc8: 48 29 d0 sub %rdx,%rax; ca7fcb: 48 c1 c0 3d rol $0x3d,%rax; ca7fcf: 48 3d 7f 01 00 00 cmp $0x17f,%rax; ca7fd5: 0f 87 36 05 00 00 ja ca8511; ca7fdb: 48 8d 15 c0 0b f7 06 lea 0x6f70bc0(%rip),%rdx; ca7fe2: f6 04 10 10 testb $0x10,(%rax,%rdx,1); ca7fe6: 0f 84 25 05 00 00 je ca8511; ca7fec: ff 91 98 00 00 00 callq *0x98(%rcx); [...]; ca8511: 0f 0b ud2. The compiler relies on co-operation from the linker in order to assemble; the bit vectors for the whole program. It currently does this using LLVM's; `type metadata`_ mechanism together with link-time optimization. .. _address point: https://itanium-cxx-abi.github.io/cxx-abi/abi.html#vtable-general; .. _type metadata: https://llvm.org/docs/TypeMetadata.html; .. _ByteArrayBuilder: https://llvm.org/docs/doxygen/html/structllvm_1_1ByteArrayBuilder.html. Optimizations; -------------. The scheme as described above is the fully general variant of the scheme.; Most of the time we are able to apply one or more of the following; optimizations to improve binary size or performance. In fact, if you try the above example with the current version of the; compiler, you will probably find that it will not use the described virtual; table layout or machine instructions. Some of the optimizations we are about; to introduce cause the compiler to use a different layout or a different; sequence of machine instructions. Stripping Leading/Trailing Zeros in Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If a bit vector contains leading or trailing zeros, we can strip them from; the vector. The compiler will emit code to check if the pointer is in range; of the region covered by ones, and perform the bit vector check using a; truncated version of the bit vector. For example, the bit vectors for our; example class hierarchy will be emitted like this:. .. csv-table:: Bit Vectors for A, B, C; :header: Class, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A, , , 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, ,; B, ,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:3624,optimiz,optimizations,3624,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"8b032d0, ID#5):; Predecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4); %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>. Note ADDri is not a two-address instruction. However, its result %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retain",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:1768,load,load,1768,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['load'],['load']
Performance,8e9/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. ``XCVelw``; LLVM implements `version 1.0.0 of the CORE-V Event load custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/master/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. These instructions are only available for riscv32 at this time. ``XCVmac``; LLVM implements `version 1.0.0 of the CORE-V Multiply-Accumulate (MAC) custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/4f024fe4b15a68b76615b0630c07a6745c620da7/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.mac` as described in the specification. These instructions are only available for riscv32 at this time. ``XCVmem``; LLVM implements `version 1.0.0 of the CORE-V Post-Increment load and stores custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/master/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. These instructions are only available for riscv32 at this time. ``XCValu``; LLVM implements `version 1.0.0 of the Core-V ALU custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/4f024fe4b15a68b76615b0630c07a6745c620da7/docs/source/instruction_set_extensions.rst>`_ by Core-V. All instructions are prefixed with `cv.` as described in the specification. These instructions are only available for riscv32 at this time. ``XCVsimd``; LLVM implements `version 1.0.0 of the CORE-V SIMD custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/cv32e40p_v1.3.2/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. ``XCVbi``; LLVM implements `version,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/RISCVUsage.rst:18895,load,load,18895,interpreter/llvm-project/llvm/docs/RISCVUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/RISCVUsage.rst,1,['load'],['load']
Performance,"91 98 00 00 00 callq *0x98(%rcx); [...]; ca8511: 0f 0b ud2. The compiler relies on co-operation from the linker in order to assemble; the bit vectors for the whole program. It currently does this using LLVM's; `type metadata`_ mechanism together with link-time optimization. .. _address point: https://itanium-cxx-abi.github.io/cxx-abi/abi.html#vtable-general; .. _type metadata: https://llvm.org/docs/TypeMetadata.html; .. _ByteArrayBuilder: https://llvm.org/docs/doxygen/html/structllvm_1_1ByteArrayBuilder.html. Optimizations; -------------. The scheme as described above is the fully general variant of the scheme.; Most of the time we are able to apply one or more of the following; optimizations to improve binary size or performance. In fact, if you try the above example with the current version of the; compiler, you will probably find that it will not use the described virtual; table layout or machine instructions. Some of the optimizations we are about; to introduce cause the compiler to use a different layout or a different; sequence of machine instructions. Stripping Leading/Trailing Zeros in Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If a bit vector contains leading or trailing zeros, we can strip them from; the vector. The compiler will emit code to check if the pointer is in range; of the region covered by ones, and perform the bit vector check using a; truncated version of the bit vector. For example, the bit vectors for our; example class hierarchy will be emitted like this:. .. csv-table:: Bit Vectors for A, B, C; :header: Class, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A, , , 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, ,; B, , , , , , , , 1, , , , , , ,; C, , , , , , , , , , , , , 1, ,. Short Inline Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~. If the vector is sufficiently short, we can represent it as an inline constant; on x86. This saves us a few instructions when reading the correct element; of the bit vector. If the bit vector fits in 32 bits",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:3875,optimiz,optimizations,3875,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['optimiz'],['optimizations']
Performance,": ""; <<min.userState().value(1); <<"" ""<<e1.first<<"" ""<<e1.second<<std::endl;; std::cout<<""par2: ""; <<min.userState().value(""area""); <<"" ""<<e2.first<<"" ""<<e2.second<<std::endl;; }; }. {; // demonstrate how to use the CONTOURs. // create Minuit parameters with names; MnUserParameters upar;; upar.add(""mean"", mean, 0.1);; upar.add(""sigma"", rms, 0.1);; upar.add(""area"", area, 0.1);. // create Migrad minimizer; MnMigrad migrad(theFCN, upar);. // minimize; FunctionMinimum min = migrad();. // create contours factory with FCN and minimum; MnContours contours(theFCN, min);. // 70% confidence level for 2 parameters contour; // around the minimum; theFCN.setErrorDef(2.41);; std::vector<std::pair<double,double> > cont =; contours(0, 1, 20);. // 95% confidence level for 2 parameters contour; theFCN.setErrorDef(5.99);; std::vector<std::pair<double,double> > cont4 =; contours(0, 1, 20);. // plot the contours; MnPlot plot;; cont4.insert(cont4.end(), cont.begin(), cont.end());; plot(min.userState().value(""mean""),; min.userState().value(""sigma""),; cont4);; }. return 0;; };; ```. [^1]: ROOT @bib-ROOT uses its own version of the Fortran M when this; manual was written. However an interface for this version exists and; the library can be loaded dynamically on demand. [^2]: The *internal error matrix* maintained by M is transformed for the; user into *external coordinates*, but the numbering of rows and; columns is of course still according to internal parameter; numbering, since one does not want rows and columns corresponding to; parameters which are not variable. The transformation therefore; affects only parameters with limits; if there are no limits,; internal and external error matrices are the same. [^3]: For example, if $\mbox{a}$ and $\mbox{b}$ are double; precision variables, the statement $\mbox{a = 2*b}$ is not good; programming, but happens to do what the user probably intended,; whereas the statement $\mbox{a = b + 2/3}$ almost certainly will; not do what the user intended.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:83552,load,loaded,83552,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['load'],['loaded']
Performance,": FaultingPCOffset; uint32 : HandlerPCOffset; }; }. FailtKind describes the reason of expected fault. Currently three kind; of faults are supported:. 1. ``FaultMaps::FaultingLoad`` - fault due to load from memory.; 2. ``FaultMaps::FaultingLoadStore`` - fault due to instruction load and store.; 3. ``FaultMaps::FaultingStore`` - fault due to store to memory. The ``ImplicitNullChecks`` pass; ===============================. The ``ImplicitNullChecks`` pass transforms explicit control flow for; checking if a pointer is ``null``, like:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %ptr_is_null = icmp i32* %ptr, null; br i1 %ptr_is_null, label %is_null, label %not_null, !make.implicit !0. not_null:; %t = load i32, i32* %ptr; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. !0 = !{}. to control flow implicit in the instruction loading or storing through; the pointer being null checked:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %t = load i32, i32* %ptr ;; handler-pc = label %is_null; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. This transform happens at the ``MachineInstr`` level, not the LLVM IR; level (so the above example is only representative, not literal). The; ``ImplicitNullChecks`` pass runs during codegen, if; ``-enable-implicit-null-checks`` is passed to ``llc``. The ``ImplicitNullChecks`` pass adds entries to the; ``__llvm_faultmaps`` section described above as needed. ``make.implicit`` metadata; --------------------------. Making null checks implicit is an aggressive optimization, and it can; be a net performance pessimization if too many memory operations end; up faulting because of it. A language runtime typically needs to; ensure that only a negligible number of implicit null checks actually; fault once the application has reached a steady state. A standard way; of doing this is by healing failed implicit null checks into explicit; null checks via code patching or recompilation. It follows that t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst:2614,load,load,2614,interpreter/llvm-project/llvm/docs/FaultMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst,1,['load'],['load']
Performance,": LLVM's SmallVectorBase; is templated over the type of fSize and fCapacity. It contains the parts of `RVec` that do not depend on the value; type.; No other classes in the hierarchy can contain data members! We expect the memory after `SmallVectorBase` to be; occupied by the small inline buffer. `SmallVectorTemplateCommon<T>`; - `getFirstEl()`: returns the address of the beginning of the small buffer; - `begin()`, `end()`, `front()`, `back()`, etc. Basically the same as the corresponding LLVM class.; It contains the parts that are independent of whether T is a POD or not. `SmallVectorTemplateBase<T, bool TriviallyCopiable>` and the specialization `SmallVectorTemplateBase<T, true>`; - `grow()`, `uninitialized_copy`, `uninitialized_move`, `push_back()`, `pop_back()`. This class contains the parts of `RVec` that can be optimized for trivially copiable types.; In particular, destruction can be skipped and memcpy can be used in place of copy/move construction.; These optimizations are inherited from LLVM's SmallVector. `RVecImpl<T>`; The analogous of LLVM's `SmallVectorImpl`, it factors out of `RVec` the parts that are independent of; the small buffer size, to limit the amount of code generated and provide a way to slice the small buffer; size when passing around `RVec` objects. `RVecN<T, N>`; It aggregates `RVecImpl` and `SmallVectorStorage` (see below) through public inheritance.; `N` is the small buffer size. `RVec<T>`; Inherits from `RVecN` and fixes the small buffer size `N` to a reasonable default.; We expect most users to use this type and only very rarely switch to `RVecN` to tweak the small buffer size. ### Helper types. - `SmallVectorAlignmentAndSize`: used to figure out the offset of the first small-buffer element in; `SmallVectorTemplateCommon::getFirstEl`; - `SmallVectorStorage`: properly aligned ""small buffer"" storage. It's a separate type so that it can be specialized to; be properly aligned also for the case of small buffer size = 0; - `RVecInlineStorage",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/vecops/ARCHITECTURE.md:2987,optimiz,optimizations,2987,math/vecops/ARCHITECTURE.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/vecops/ARCHITECTURE.md,1,['optimiz'],['optimizations']
Performance,": Loading from 0x[[#%x,ADDR:]]; ; CHECK-SAME: to 0x[[#ADDR + 7]]. The above example would match the text:. .. code-block:: gas. load r5, [r0]; load r6, [r1]; Loading from 0xa0463440 to 0xa0463447. but would not match the text:. .. code-block:: gas. load r5, [r0]; load r7, [r1]; Loading from 0xa0463440 to 0xa0463443. Due to ``7`` being unequal to ``5 + 1`` and ``a0463443`` being unequal to; ``a0463440 + 7``. A numeric variable can also be defined to the result of a numeric expression,; in which case the numeric expression constraint is checked and if verified the; variable is assigned to the value. The unified syntax for both checking a; numeric expression and capturing its value into a numeric variable is thus; ``[[#%<fmtspec>,<NUMVAR>: <constraint> <expr>]]`` with each element as; described previously. One can use this syntax to make a testcase more; self-describing by using variables instead of values:. .. code-block:: gas. ; CHECK: mov r[[#REG_OFFSET:]], 0x[[#%X,FIELD_OFFSET:12]]; ; CHECK-NEXT: load r[[#]], [r[[#REG_BASE:]], r[[#REG_OFFSET]]]. which would match:. .. code-block:: gas. mov r4, 0xC; load r6, [r5, r4]. The ``--enable-var-scope`` option has the same effect on numeric variables as; on string variables. Important note: In its current implementation, an expression cannot use a; numeric variable defined earlier in the same CHECK directive. FileCheck Pseudo Numeric Variables; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Sometimes there's a need to verify output that contains line numbers of the; match file, e.g. when testing compiler diagnostics. This introduces a certain; fragility of the match file structure, as ""``CHECK:``"" lines contain absolute; line numbers in the same file, which have to be updated whenever line numbers; change due to text addition or deletion. To support this case, FileCheck expressions understand the ``@LINE`` pseudo; numeric variable which evaluates to the line number of the CHECK pattern where; it is found. This way match patterns can be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:33201,load,load,33201,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['load'],['load']
Performance,": Rationale. Mismatches with returned results will cause over-retains or over-releases,; depending on the direction. Again, the rule about function calls is really; just an application of the existing C/C++ rule about calling functions; through an incompatible function type. .. _arc.objects.operands.unretained-returns:. Unretained return values; ^^^^^^^^^^^^^^^^^^^^^^^^. A method or function which returns a retainable object type but does not return; a retained value must ensure that the object is still valid across the return; boundary. When returning from such a function or method, ARC retains the value at the; point of evaluation of the return statement, then leaves all local scopes, and; then balances out the retain while ensuring that the value lives across the; call boundary. In the worst case, this may involve an ``autorelease``, but; callers must not assume that the value is actually in the autorelease pool. ARC performs no extra mandatory work on the caller side, although it may elect; to do something to shorten the lifetime of the returned value. .. admonition:: Rationale. It is common in non-ARC code to not return an autoreleased value; therefore; the convention does not force either path. It is convenient to not be; required to do unnecessary retains and autoreleases; this permits; optimizations such as eliding retain/autoreleases when it can be shown that; the original pointer will still be valid at the point of return. A method or function may be marked with; ``__attribute__((ns_returns_autoreleased))`` to indicate that it returns a; pointer which is guaranteed to be valid at least as long as the innermost; autorelease pool. There are no additional semantics enforced in the definition; of such a method; it merely enables optimizations in callers. .. _arc.objects.operands.casts:. Bridged casts; ^^^^^^^^^^^^^. A :arc-term:`bridged cast` is a C-style cast annotated with one of three; keywords:. * ``(__bridge T) op`` casts the operand to the destination typ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:21899,perform,performs,21899,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performs']
Performance,": llvm. %res = call i4 @llvm.udiv.fix.i4(i4 6, i4 2, i32 0) ; %res = 3 (6 / 2 = 3); %res = call i4 @llvm.udiv.fix.i4(i4 6, i4 4, i32 1) ; %res = 3 (3 / 2 = 1.5); %res = call i4 @llvm.udiv.fix.i4(i4 1, i4 -8, i32 4) ; %res = 2 (0.0625 / 0.5 = 0.125). ; The result in the following could be rounded up to 1 or down to 0.5; %res = call i4 @llvm.udiv.fix.i4(i4 3, i4 4, i32 1) ; %res = 2 (or 1) (1.5 / 2 = 0.75). '``llvm.sdiv.fix.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.sdiv.fix.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.sdiv.fix.sat.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.sdiv.fix.sat.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.sdiv.fix.sat.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.sdiv.fix.sat.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.sdiv.fix.sat``' family of intrinsic functions perform signed; fixed point saturating division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the first 2 arguments. The minimum value is the; smallest signed value representable by this bit width. It is undefined behavi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:634418,perform,perform,634418,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,":. '``llvm.vp.mul.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.mul.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.mul.nxv46i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.mul.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer multiplication of two vectors of integers. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of integer type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """"""""""""""""""""; The '``llvm.vp.mul``' intrinsic performs integer multiplication; (:ref:`mul <i_mul>`) of the first and second vector operand on each enabled; lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.mul.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = mul <4 x i32> %a, %b; %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_sdiv:. '``llvm.vp.sdiv.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.sdiv.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.sdiv.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.sdiv.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overvi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:701130,perform,performs,701130,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,":. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:235058,perform,performed,235058,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['perform'],['performed']
Performance,":. ::. foo not inlined into bar because it should never be inlined; (cost=never): noinline function attribute. ``Analysis``. Remarks that describe the result of an analysis, that can bring more; information to the user regarding the generated code. :Example:. ::. 16 stack bytes in function. ::. 10 instructions in function. Enabling optimization remarks; =============================. There are two modes that are supported for enabling optimization remarks in; LLVM: through remark diagnostics, or through serialized remarks. Remark diagnostics; ------------------. Optimization remarks can be emitted as diagnostics. These diagnostics will be; propagated to front-ends if desired, or emitted by tools like :doc:`llc; <CommandGuide/llc>` or :doc:`opt <CommandGuide/opt>`. .. option:: -pass-remarks=<regex>. Enables optimization remarks from passes whose name match the given (POSIX); regular expression. .. option:: -pass-remarks-missed=<regex>. Enables missed optimization remarks from passes whose name match the given; (POSIX) regular expression. .. option:: -pass-remarks-analysis=<regex>. Enables optimization analysis remarks from passes whose name match the given; (POSIX) regular expression. Serialized remarks; ------------------. While diagnostics are useful during development, it is often more useful to; refer to optimization remarks post-compilation, typically during performance; analysis. For that, LLVM can serialize the remarks produced for each compilation unit to; a file that can be consumed later. By default, the format of the serialized remarks is :ref:`YAML; <yamlremarks>`, and it can be accompanied by a :ref:`section <remarkssection>`; in the object files to easily retrieve it. :doc:`llc <CommandGuide/llc>` and :doc:`opt <CommandGuide/opt>` support the; following options:. ``Basic options``. .. option:: -pass-remarks-output=<filename>. Enables the serialization of remarks to a file specified in <filename>. By default, the output is serialized to :ref:`YAML <yamlr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:1664,optimiz,optimization,1664,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimization']
Performance,":. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8833,load,load,8833,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,2,['load'],['load']
Performance,":. declare float @llvm.vp.reduce.fadd.v4f32(float <start_value>, <4 x float> <val>, <4 x i1> <mask>, i32 <vector_length>); declare double @llvm.vp.reduce.fadd.nxv8f64(double <start_value>, <vscale x 8 x double> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point ``ADD`` reduction of a vector and a scalar starting; value, returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; floating-point type equal to the result type. The second operand is the vector; on which the reduction is performed and must be a vector of floating-point; values whose element type is the result/start type. The third operand is the; vector mask and is a vector of boolean values with the same number of elements; as the vector operand. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.fadd``' intrinsic performs the floating-point ``ADD``; reduction (:ref:`llvm.vector.reduce.fadd <int_vector_reduce_fadd>`) of the; vector operand ``val`` on each enabled lane, adding it to the scalar; ``start_value``. Disabled lanes are treated as containing the neutral value; ``-0.0`` (i.e. having no effect on the reduction operation). If no lanes are; enabled, the resulting value will be equal to ``start_value``. To ignore the start value, the neutral value can be used. See the unpredicated version (:ref:`llvm.vector.reduce.fadd; <int_vector_reduce_fadd>`) for more detail on the semantics of the reduction. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call float @llvm.vp.reduce.fadd.v4f32(float %start, <4 x float> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x float> %a, <4 x float> <float -0.0, float -0.0, float -0.0, float -0.0>; %also.r = call float @llvm.vector.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:751365,perform,performs,751365,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,":. declare float @llvm.vp.reduce.fmul.v4f32(float <start_value>, <4 x float> <val>, <4 x i1> <mask>, i32 <vector_length>); declare double @llvm.vp.reduce.fmul.nxv8f64(double <start_value>, <vscale x 8 x double> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point ``MUL`` reduction of a vector and a scalar starting; value, returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; floating-point type equal to the result type. The second operand is the vector; on which the reduction is performed and must be a vector of floating-point; values whose element type is the result/start type. The third operand is the; vector mask and is a vector of boolean values with the same number of elements; as the vector operand. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.fmul``' intrinsic performs the floating-point ``MUL``; reduction (:ref:`llvm.vector.reduce.fmul <int_vector_reduce_fmul>`) of the; vector operand ``val`` on each enabled lane, multiplying it by the scalar; `start_value``. Disabled lanes are treated as containing the neutral value; ``1.0`` (i.e. having no effect on the reduction operation). If no lanes are; enabled, the resulting value will be equal to the starting value. To ignore the start value, the neutral value can be used. See the unpredicated version (:ref:`llvm.vector.reduce.fmul; <int_vector_reduce_fmul>`) for more detail on the semantics. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call float @llvm.vp.reduce.fmul.v4f32(float %start, <4 x float> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x float> %a, <4 x float> <float 1.0, float 1.0, float 1.0, float 1.0>; %also.r = call float @llvm.vector.reduce.fmul.v4f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:755577,perform,performs,755577,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"://www.sphinx-doc.org/en/stable/ and published to; http://cppyy.readthedocs.io/en/latest/index.html using a webhook. To create; the docs::. $ pip install sphinx_rtd_theme; Collecting sphinx_rtd_theme; ...; Successfully installed sphinx-rtd-theme-0.2.4; $ cd docs; $ make html. The Python code in this module supports:. * Interfacing to the correct backend for CPython or PyPy.; * Pythonizations (TBD). Cppyy-backend; -------------. The ``cppyy-backend`` module contains two areas:. * A patched copy of cling; * Wrapper code. Package structure; -----------------; .. _package-structure:. There are four PyPA packages involved in a full installation, with the; following structure::. (A) _cppyy (PyPy); / \; (1) cppyy (3) cppyy-backend -- (4) cppyy-cling; \ /; (2) CPyCppyy (CPython). The user-facing package is always ``cppyy`` (1).; It is used to select the other (versioned) required packages, based on the; python interpreter for which it is being installed. Below (1) follows a bifurcation based on interpreter.; This is needed for functionality and performance: for CPython, there is the; CPyCppyy package (2).; It is written in C++, makes use of the Python C-API, and installs as a Python; extension module.; For PyPy, there is the builtin module ``_cppyy`` (A).; This is not a PyPA package.; It is written in RPython as it needs access to low-level pointers, JIT hints,; and the ``_cffi_backend`` backend module (itself builtin). Shared again across interpreters is the backend, which is split in a small; wrapper (3) and a large package that contains Cling/LLVM (4).; The former is still under development and expected to be updated frequently.; It is small enough to download and build very quickly.; The latter, however, takes a long time to build, but since it is very stable,; splitting it off allows the creation of binary wheels that need updating; only infrequently (expected about twice a year). All code is publicly available; see the; :doc:`section on repositories <repositories>`.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/packages.rst:1329,perform,performance,1329,bindings/pyroot/cppyy/cppyy/doc/source/packages.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/packages.rst,1,['perform'],['performance']
Performance,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3921,load,loadObject,3921,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,3,"['load', 'perform']","['loadObject', 'loading', 'perform']"
Performance,":: c++. typedef float m4x4_t __attribute__((matrix_type(4, 4)));. m4x4_t f(m4x4_t a, m4x4_t b) {; a += b;; a -= b;; a *= b;; a += 23;; a -= 12;; return a;; }. The matrix type extension supports explicit casts. Implicit type conversion between matrix types is not allowed. .. code-block:: c++. typedef int ix5x5 __attribute__((matrix_type(5, 5)));; typedef float fx5x5 __attribute__((matrix_type(5, 5)));. fx5x5 f1(ix5x5 i, fx5x5 f) {; return (fx5x5) i;; }. template <typename X>; using matrix_4_4 = X __attribute__((matrix_type(4, 4)));. void f2() {; matrix_5_5<double> d;; matrix_5_5<int> i;; i = (matrix_5_5<int>)d;; i = static_cast<matrix_5_5<int>>(d);; }. Half-Precision Floating Point; =============================. Clang supports three half-precision (16-bit) floating point types:; ``__fp16``, ``_Float16`` and ``__bf16``. These types are supported; in all language modes, but their support differs between targets.; A target is said to have ""native support"" for a type if the target; processor offers instructions for directly performing basic arithmetic; on that type. In the absence of native support, a type can still be; supported if the compiler can emulate arithmetic on the type by promoting; to ``float``; see below for more information on this emulation. * ``__fp16`` is supported on all targets. The special semantics of this; type mean that no arithmetic is ever performed directly on ``__fp16`` values;; see below. * ``_Float16`` is supported on the following targets:. * 32-bit ARM (natively on some architecture versions); * 64-bit ARM (AArch64) (natively on ARMv8.2a and above); * AMDGPU (natively); * SPIR (natively); * X86 (if SSE2 is available; natively if AVX512-FP16 is also available); * RISC-V (natively if Zfh or Zhinx is available). * ``__bf16`` is supported on the following targets (currently never natively):. * 32-bit ARM; * 64-bit ARM (AArch64); * RISC-V; * X86 (when SSE2 is available). (For X86, SSE2 is available on 64-bit and all recent 32-bit processors.). `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:30225,perform,performing,30225,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['performing']
Performance,":: console. $ clang -mllvm -force-vector-interleave=2 ...; $ opt -loop-vectorize -force-vector-interleave=2 ... Pragma loop hint directives; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``#pragma clang loop`` directive allows loop vectorization hints to be; specified for the subsequent for, while, do-while, or c++11 range-based for; loop. The directive allows vectorization and interleaving to be enabled or; disabled. Vector width as well as interleave count can also be manually; specified. The following example explicitly enables vectorization and; interleaving:. .. code-block:: c++. #pragma clang loop vectorize(enable) interleave(enable); while(...) {; ...; }. The following example implicitly enables vectorization and interleaving by; specifying a vector width and interleaving count:. .. code-block:: c++. #pragma clang loop vectorize_width(2) interleave_count(2); for(...) {; ...; }. See the Clang; `language extensions; <https://clang.llvm.org/docs/LanguageExtensions.html#extensions-for-loop-hint-optimizations>`_; for details. Diagnostics; -----------. Many loops cannot be vectorized including loops with complicated control flow,; unvectorizable types, and unvectorizable calls. The loop vectorizer generates; optimization remarks which can be queried using command line options to identify; and diagnose loops that are skipped by the loop-vectorizer. Optimization remarks are enabled using:. ``-Rpass=loop-vectorize`` identifies loops that were successfully vectorized. ``-Rpass-missed=loop-vectorize`` identifies loops that failed vectorization and; indicates if vectorization was specified. ``-Rpass-analysis=loop-vectorize`` identifies the statements that caused; vectorization to fail. If in addition ``-fsave-optimization-record`` is; provided, multiple causes of vectorization failure may be listed (this behavior; might change in the future). Consider the following loop:. .. code-block:: c++. #pragma clang loop vectorize(enable); for (int i = 0; i < Length; i++) {; switch(A[i]) {; ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:2452,optimiz,optimizations,2452,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['optimiz'],['optimizations']
Performance,"::. ;; This instruction unconditionally stores data vector in multiple addresses; call @llvm.masked.scatter.v8i32.v8p0(<8 x i32> %value, <8 x ptr> %ptrs, i32 4, <8 x i1> <true, true, .. true>). ;; It is equivalent to a list of scalar stores; %val0 = extractelement <8 x i32> %value, i32 0; %val1 = extractelement <8 x i32> %value, i32 1; ..; %val7 = extractelement <8 x i32> %value, i32 7; %ptr0 = extractelement <8 x ptr> %ptrs, i32 0; %ptr1 = extractelement <8 x ptr> %ptrs, i32 1; ..; %ptr7 = extractelement <8 x ptr> %ptrs, i32 7; ;; Note: the order of the following stores is important when they overlap:; store i32 %val0, ptr %ptr0, align 4; store i32 %val1, ptr %ptr1, align 4; ..; store i32 %val7, ptr %ptr7, align 4. Masked Vector Expanding Load and Compressing Store Intrinsics; -------------------------------------------------------------. LLVM provides intrinsics for expanding load and compressing store operations. Data selected from a vector according to a mask is stored in consecutive memory addresses (compressed store), and vice-versa (expanding load). These operations effective map to ""if (cond.i) a[j++] = v.i"" and ""if (cond.i) v.i = a[j++]"" patterns, respectively. Note that when the mask starts with '1' bits followed by '0' bits, these operations are identical to :ref:`llvm.masked.store <int_mstore>` and :ref:`llvm.masked.load <int_mload>`. .. _int_expandload:. '``llvm.masked.expandload.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. Several values of integer, floating point or pointer data type are loaded from consecutive memory addresses and stored into the elements of a vector according to the mask. ::. declare <16 x float> @llvm.masked.expandload.v16f32 (ptr <ptr>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x i64> @llvm.masked.expandload.v2i64 (ptr <ptr>, <2 x i1> <mask>, <2 x i64> <passthru>). Overview:; """""""""""""""""". Reads a number of scalar values sequentially from memory location pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:854785,load,load,854785,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"::Interface getInterface(Ast &ast) {; SymbolFlagsMap Symbols;; // Find all the symbols in the AST and for each of them; // add it to the Symbols map.; Symbols[mangler(someNameFromAST)] =; JITSymbolFlags(JITSymbolFlags::Exported | JITSymbolFlags::Callable);; return MaterializationUnit::Interface(std::move(Symbols), nullptr);; }; };. Take look at the source code of `Building A JIT's Chapter 4 <tutorial/BuildingAJIT4.html>`_ for a complete example. How to use ThreadSafeModule and ThreadSafeContext; -------------------------------------------------. ThreadSafeModule and ThreadSafeContext are wrappers around Modules and; LLVMContexts respectively. A ThreadSafeModule is a pair of a; std::unique_ptr<Module> and a (possibly shared) ThreadSafeContext value. A; ThreadSafeContext is a pair of a std::unique_ptr<LLVMContext> and a lock.; This design serves two purposes: providing a locking scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:28293,concurren,concurrent,28293,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['concurren'],['concurrent']
Performance,"::MinimizerOptions::SetDefaultMinimizer()`, or alternatively in the `.rootrc` file by adding for example `Root.Fitter: Minuit2` to select Minuit2. ### Code modernization by using `std::string` in RooFit interfaces. The following lesser-used RooFit functions now return a `std::string` instead of a `const char*`, potentially requiring the update of your code:. - [std::string RooCmdConfig::missingArgs() const](https://root.cern/doc/v628/classRooCmdConfig.html#aec50335293c45a507d347c604bf9651f); ### Uniquely identifying RooArgSet and RooDataSet objects. Before v6.28, it was ensured that no `RooArgSet` and `RooDataSet` objects on the heap were located at an address that had already been used for an instance of the same class before.; With v6.28, this is not guaranteed anymore.; Hence, if your code uses pointer comparisons to uniquely identify RooArgSet or RooDataSet instances, please consider using the new `RooArgSet::uniqueId()` or `RooAbsData::uniqueId()`. ### Introducing binned likelihood fit optimization in HistFactory. In a binned likelihood fit, it is possible to skip the PDF normalization when; the unnormalized binned PDF can be interpreted directly in terms of event; yields. This is now done by default for HistFactory models, which; results in great speedups for binned fits with many channels. Some RooFit users; like ATLAS were already using this for a long time. To disable this optimization when using the `hist2workspace` executable, add the `-disable_binned_fit_optimization` command line argument.; Directly in C++, you can also set the `binnedFitOptimization` to `false` in the; HistFactory configuration as follows:; ```C++; RooStats::HistFactory::MakeModelAndMeasurementFast(measurement, {.binnedFitOptimization=false});; ```; If your compiler doesn't support aggregate initialization with designators, you; need to create and edit the configuration struct explicitely:; ```C++; RooStats::HistFactory::HistoToWorkspaceFactoryFast::Configuration hfCfg;; hfCfg.binnedFit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md:20122,optimiz,optimization,20122,README/ReleaseNotes/v628/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md,1,['optimiz'],['optimization']
Performance,"::get(opcode)::ImplicitUses``. Pre-colored registers impose; constraints on any register allocation algorithm. The register allocator must; make sure that none of them are overwritten by the values of virtual registers; while still alive. Mapping virtual registers to physical registers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. There are two ways to map virtual registers to physical registers (or to memory; slots). The first way, that we will call *direct mapping*, is based on the use; of methods of the classes ``TargetRegisterInfo``, and ``MachineOperand``. The; second way, that we will call *indirect mapping*, relies on the ``VirtRegMap``; class in order to insert loads and stores sending and getting values to and from; memory. The direct mapping provides more flexibility to the developer of the register; allocator; however, it is more error prone, and demands more implementation; work. Basically, the programmer will have to specify where load and store; instructions should be inserted in the target function being compiled in order; to get and store values in memory. To assign a physical register to a virtual; register present in a given operand, use ``MachineOperand::setReg(p_reg)``. To; insert a store instruction, use ``TargetInstrInfo::storeRegToStackSlot(...)``,; and to insert a load instruction, use ``TargetInstrInfo::loadRegFromStackSlot``. The indirect mapping shields the application developer from the complexities of; inserting load and store instructions. In order to map a virtual register to a; physical one, use ``VirtRegMap::assignVirt2Phys(vreg, preg)``. In order to map; a certain virtual register to memory, use; ``VirtRegMap::assignVirt2StackSlot(vreg)``. This method will return the stack; slot where ``vreg``'s value will be located. If it is necessary to map another; virtual register to the same stack slot, use; ``VirtRegMap::assignVirt2StackSlot(vreg, stack_location)``. One important point; to consider when using the indirect mapping, is that",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:64516,load,load,64516,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['load']
Performance,"::mapSectionAddress is called, MCJIT passes the call on to; RuntimeDyldImpl (via its Dyld member). RuntimeDyldImpl stores the new; address in an internal data structure but does not update the code at this; time, since other sections are likely to change. When the client is finished remapping section addresses, it will call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations iterates through the list of; sections and for each section iterates through a list of relocations that; have been saved which reference that symbol and call resolveRelocation for; each entry in this list. The relocation list here is a list of; relocations for which the symbol associated with the relocation is located; in the section associated with the list. Each of these locations will; have a target location at which the relocation will be applied that is; likely located in a different section. .. image:: MCJIT-resolve-relocations.png. Once relocations have been applied as described above, MCJIT calls; RuntimeDyld::getEHFrameSection, and if a non-zero result is returned; passes the section data to the memory manager's registerEHFrames method.; This allows the memory manager to call any desired target-specific; func",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:7514,load,loaded,7514,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['loaded']
Performance,"::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29775,load,loadModuleOnContext,29775,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['load'],['loadModuleOnContext']
Performance,":; """""""""""""""""""". The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the arguments. The minimum value is the; smallest signed value representable by this bit width. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.sshl.sat.i4(i4 2, i4 1) ; %res = 4; %res = call i4 @llvm.sshl.sat.i4(i4 2, i4 2) ; %res = 7; %res = call i4 @llvm.sshl.sat.i4(i4 -5, i4 1) ; %res = -8; %res = call i4 @llvm.sshl.sat.i4(i4 -1, i4 1) ; %res = -2. '``llvm.ushl.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.ushl.sat``; on integers or vectors of integers of any bit width. ::. declare i16 @llvm.ushl.sat.i16(i16 %a, i16 %b); declare i32 @llvm.ushl.sat.i32(i32 %a, i32 %b); declare i64 @llvm.ushl.sat.i64(i64 %a, i64 %b); declare <4 x i32> @llvm.ushl.sat.v4i32(<4 x i32> %a, <4 x i32> %b). Overview; """""""""""""""""". The '``llvm.ushl.sat``' family of intrinsic functions perform unsigned; saturating left shift on the first argument. Arguments; """""""""""""""""""". The arguments (``%a`` and ``%b``) and the result may be of integer types of any; bit width, but they must have the same bit width. ``%a`` is the value to be; shifted, and ``%b`` is the amount to shift by. If ``b`` is (statically or; dynamically) equal to or larger than the integer bit width of the arguments,; the result is a :ref:`poison value <poisonvalues>`. If the arguments are; vectors, each vector element of ``a`` is shifted by the corresponding shift; amount in ``b``. Semantics:; """""""""""""""""""". The maximum value this operation can clamp to is the largest unsigned value; representable by the bit width of the arguments. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.ushl.sat.i4(i4 2, i4 1) ; %res = 4; %res = call i4 @llvm.ushl.sat.i4(i4 3, i4 3) ; %res = 15. Fixed Point Arithmetic Intrinsics; ---------------------------------. A fixed point number represents a real data type for a number that has",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:618006,perform,perform,618006,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,":; """""""""""""""""". The '``llvm.vp.trunc``' intrinsic truncates its first operand to the return; type. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.trunc``' intrinsic takes a value to cast as its first operand.; The return type is the type to cast the value to. Both types must be vector of; :ref:`integer <t_integer>` type. The bit size of the value must be larger than; the bit size of the return type. The second operand is the vector mask. The; return type, the value to cast, and the vector mask have the same number of; elements. The third operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.trunc``' intrinsic truncates the high order bits in value and; converts the remaining bits to return type. Since the source size must be larger; than the destination size, '``llvm.vp.trunc``' cannot be a *no-op cast*. It will; always truncate bits. The conversion is performed on lane positions below the; explicit vector length and where the vector mask is true. Masked-off lanes are; ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i16> @llvm.vp.trunc.v4i16.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = trunc <4 x i32> %a to <4 x i16>; %also.r = select <4 x i1> %mask, <4 x i16> %t, <4 x i16> poison. .. _int_vp_zext:. '``llvm.vp.zext.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.zext.v16i32.v16i16 (<16 x i16> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.zext.nxv4i32.nxv4i16 (<vscale x 4 x i16> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.zext``' intrinsic zero extends its first operand to the return; type. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.zext``' intrin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:797117,perform,performed,797117,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,":; """""""""""""". ::. declare void @llvm.ubsantrap(i8 immarg) cold noreturn nounwind. Overview:; """""""""""""""""". The '``llvm.ubsantrap``' intrinsic. Arguments:; """""""""""""""""""". An integer describing the kind of failure detected. Semantics:; """""""""""""""""""". This intrinsic is lowered to code which is intended to cause an execution trap,; embedding the argument into encoding of that trap somehow to discriminate; crashes if possible. Equivalent to ``@llvm.trap`` for targets that do not support this behaviour. '``llvm.stackprotector``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.stackprotector(ptr <guard>, ptr <slot>). Overview:; """""""""""""""""". The ``llvm.stackprotector`` intrinsic takes the ``guard`` and stores it; onto the stack at ``slot``. The stack slot is adjusted to ensure that it; is placed on the stack before local variables. Arguments:; """""""""""""""""""". The ``llvm.stackprotector`` intrinsic requires two pointer arguments.; The first argument is the value loaded from the stack guard; ``@__stack_chk_guard``. The second variable is an ``alloca`` that has; enough space to hold the value of the guard. Semantics:; """""""""""""""""""". This intrinsic causes the prologue/epilogue inserter to force the position of; the ``AllocaInst`` stack slot to be before local variables on the stack. This is; to ensure that if a local variable on the stack is overwritten, it will destroy; the value of the guard. When the function exits, the guard on the stack is; checked against the original guard by ``llvm.stackprotectorcheck``. If they are; different, then ``llvm.stackprotectorcheck`` causes the program to abort by; calling the ``__stack_chk_fail()`` function. '``llvm.stackguard``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare ptr @llvm.stackguard(). Overview:; """""""""""""""""". The ``llvm.stackguard`` intrinsic returns the system stack guard value. It should not be generated by frontends, since it is only for internal usage.; The reason why we crea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:930061,load,loaded,930061,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,":; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), %eax; 	orl	$255, %eax; 	ret. //===---------------------------------------------------------------------===//. We're codegen'ing multiply of long longs inefficiently:. unsigned long long LLM(unsigned long long arg1, unsigned long long arg2) {; return arg1 * arg2;; }. We compile to (fomit-frame-pointer):. _LLM:; 	pushl	%esi; 	movl	8(%esp), %ecx; 	movl	16(%esp), %esi; 	movl	%esi, %eax; 	mull	%ecx; 	imull	12(%esp), %esi; 	addl	%edx, %esi; 	imull	20(%esp), %ecx; 	movl	%esi, %edx; 	addl	%ecx, %edx; 	popl	%esi; 	ret. This looks like a scheduling deficiency and lack of remat of the load from; the argument area. ICC apparently produces:. movl 8(%esp), %ecx; imull 12(%esp), %ecx; movl 16(%esp), %eax; imull 4(%esp), %eax ; addl %eax, %ecx ; movl 4(%esp), %eax; mull 12(%esp) ; addl %ecx, %edx; ret. Note that it remat'd loads from 4(esp) and 12(esp). See this GCC PR:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17236. //===---------------------------------------------------------------------===//. We can fold a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:. 	movsd	(%esp), %xmm0; 	xorpd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp). Since we 'know' that this is a 'neg', we can actually ""fold"" the spill into; the neg/abs instruction, turning it into an *integer* operation, like this:. 	xorl 2147483648, [mem+4] ## 214748364",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:21737,load,loads,21737,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['load'],['loads']
Performance,":CreateModelUpdater()`), the following steps are taken:. 1. On calling `RUpdater::BeginUpdate()`, all `REntry` instances belonging to the underlying RNTupleModel are invalidated.; 2. After adding the desired additional fields, calling `RUpdater::CommitUpdate()` will add the relevant fields to the footer's [schema extension record frame](./BinaryFormatSpecification.md#schema-extensions-record-frame).; 1. The principal columns of top-level fields and record subfields will have a non-zero first element index.; These columns are referred to as ""deferred columns"".; In particular, columns in a subfield tree of collections or variants are _not_ stored as deferred columns (see next point).; 2. All other columns belonging to the added (sub)fields will be written as usual.; 3. `RNTuple(Writer|Model)::CreateEntry()` or `RNTupleModel::CreateBareEntry()` must be used to create an `REntry` matching the new model.; 4. Writing continues as described in steps 2-5 above. ### Reading Case; The reverse process is performed on reading (e.g. `RNTupleReader::LoadEntry()`, `RNTupleView` call operator). By default, the page source uses an `RClusterPool` to asynchronously read-ahead data.; When a page of a certain cluster is required, the cluster pool reads pages of _active_ columns.; For instance, if only certain fields are used (e.g., through an imposed model), only the pages of columns connected to those fields are read.; Columns can be dynamically added (e.g. during event iteration, a new field view is created in a reader).; The cluster pool reads ahead a limited number of clusters given by the _cluster bunch size_ option (default = 1).; The read-ahead uses vector reads.; For the file backend, it additionally coalesces close read requests and uses uring reads when available. The page source can be restricted to a certain entry range.; This allows for optimizing the page lists that are being read.; Additionally, it allows for optimizing the cluster pool to not read-ahead beyond the limits",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:21949,perform,performed,21949,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['perform'],['performed']
Performance,":Math:::GSLIntegrator` |. #### ROOT::Math:::GaussIntegrator. It uses the most basic Gaussian integration algorithm, it uses the 8-point and the 16-point Gaussian; quadrature approximations. It is derived from the `DGAUSS` routine of the *CERNLIB* by S. Kolbig.; This class; Here is an example of using directly the `GaussIntegrator` class. ```{.cpp}; #include ""TF1.h""; #include ""Math/WrappedTF1.h""; #include ""Math/GaussIntegrator.h"". int main(); {; TF1 f(""Sin Function"", ""sin(x)"", 0, TMath::Pi());; ROOT::Math::WrappedTF1 wf1(f);. ROOT::Math::GaussIntegrator ig;. ig.SetFunction(wf1, false);; ig.SetRelTolerance(0.001);. cout << ig.Integral(0, TMath::PiOver2()) << endl;. return 0;; }; ```; #### ROOT::Math::GaussLegendreIntegrator. This class implementes the Gauss-Legendre quadrature formulas. This sort of numerical methods requieres that the user specifies the number of intermediate function points; used in the calculation of the integral. It will automatically determine the coordinates and weights of such points before performing the integration.; We can use the example above, but replacing the creation of a `ROOT::Math::GaussIntegrator` object with `ROOT::Math::GaussLegendreIntegrator`. #### ROOT::Math::GSLIntegrator. This is a wrapper for the *QUADPACK* integrator implemented in the GSL library. It supports several integration methods that can be chosen in construction time.; The default type is adaptive integration with singularity applying a Gauss-Kronrod 21-point integration rule. For a detail description of the GSL methods visit the GSL user guide; This class implements the best algorithms for numerical integration for one dimensional functions. We encourage the use it as the main option, bearing in mind that it uses code from the; GSL library, wich is provided in the *MathMore* library of ROOT. The interface to use is the same as above. We have now the possibility to specify a different integration algorithm in the constructor of the `ROOT::Math::GSLIntegrator` clas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:54377,perform,performing,54377,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['performing']
Performance,":Math::SVector`** classes for describing matrices and vectors of; arbitrary dimensions and of arbitrary type. The classes are templated on; the scalar type and on the size, like number of rows and columns for a; matrix . Therefore, the matrix/vector dimension has to be known at; compile time. An advantage of using the dimension as template parameters; is that the correctness of dimension in the matrix/vector operations can; be checked at compile time. `SMatrix` supports, since ROOT v5.10, symmetric matrices using a storage; class (**`ROOT::Math::MatRepSym`**) which contains only the `N*(N+1)/2`; independent element of a `NxN` symmetric matrix. It is not in the; mandate of this package to provide complete linear algebra; functionality. It provides basic matrix and vector functions such as; matrix-matrix, matrix-vector, vector-vector operations, plus some extra; functionality for square matrices, like inversion and determinant; calculation. The inversion is based on the optimized Cramer method for; squared matrices of size up to `6x6`. The `SMatrix` package contains only header files. Normally one does not; need to build any library. In the ROOT distribution a library,; `libSmatrix` is produced with the C++ dictionary information for squared; and symmetric matrices and vectors up to dimension 7 and based on; **`Double_t`**, **`Float_t`** and **`Double32_t`**. The following; paragraphs describe the main characteristics of the matrix and vector; classes. More detailed information about the `SMatrix` classes API is; available in the; [online reference documentation](online reference documentation). ### Example: Vector Class (SVector). The template class **`ROOT::Math::SVector`** represents `n`-dimensional; vectors for objects of arbitrary type. This class has 2 template; parameters, which define at compile time, its properties: 1) type of the; contained elements (for example *float* or *double*); 2) size of the; vector. The use of this dictionary is mandatory if one want ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:101241,optimiz,optimized,101241,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['optimiz'],['optimized']
Performance,":arc-term:`bridged cast` is a C-style cast annotated with one of three; keywords:. * ``(__bridge T) op`` casts the operand to the destination type ``T``. If; ``T`` is a retainable object pointer type, then ``op`` must have a; non-retainable pointer type. If ``T`` is a non-retainable pointer type,; then ``op`` must have a retainable object pointer type. Otherwise the cast; is ill-formed. There is no transfer of ownership, and ARC inserts no retain; operations.; * ``(__bridge_retained T) op`` casts the operand, which must have retainable; object pointer type, to the destination type, which must be a non-retainable; pointer type. ARC retains the value, subject to the usual optimizations on; local values, and the recipient is responsible for balancing that +1.; * ``(__bridge_transfer T) op`` casts the operand, which must have; non-retainable pointer type, to the destination type, which must be a; retainable object pointer type. ARC will release the value at the end of; the enclosing full-expression, subject to the usual optimizations on local; values. These casts are required in order to transfer objects in and out of ARC; control; see the rationale in the section on :ref:`conversion of retainable; object pointers <arc.objects.restrictions.conversion>`. Using a ``__bridge_retained`` or ``__bridge_transfer`` cast purely to convince; ARC to emit an unbalanced retain or release, respectively, is poor form. .. _arc.objects.restrictions:. Restrictions; ------------. .. _arc.objects.restrictions.conversion:. Conversion of retainable object pointers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. In general, a program which attempts to implicitly or explicitly convert a; value of retainable object pointer type to any non-retainable type, or; vice-versa, is ill-formed. For example, an Objective-C object pointer shall; not be converted to ``void*``. As an exception, cast to ``intptr_t`` is; allowed because such casts are not transferring ownership. The :ref:`bridged; casts <arc.objec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:23853,optimiz,optimizations,23853,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizations']
Performance,":function<std::shared_ptr<Module>(std::shared_ptr<Module>)>;. IRTransformLayer<decltype(CompileLayer), OptimizeFunction> OptimizeLayer;. std::unique_ptr<JITCompileCallbackManager> CompileCallbackManager;; CompileOnDemandLayer<decltype(OptimizeLayer)> CODLayer;. public:; using ModuleHandle = decltype(CODLayer)::ModuleHandleT;. First we need to include the CompileOnDemandLayer.h header, then add two new; members: a std::unique_ptr<JITCompileCallbackManager> and a CompileOnDemandLayer,; to our class. The CompileCallbackManager member is used by the CompileOnDemandLayer; to create the compile callback needed for each function. .. code-block:: c++. KaleidoscopeJIT(); : TM(EngineBuilder().selectTarget()), DL(TM->createDataLayout()),; ObjectLayer([]() { return std::make_shared<SectionMemoryManager>(); }),; CompileLayer(ObjectLayer, SimpleCompiler(*TM)),; OptimizeLayer(CompileLayer,; [this](std::shared_ptr<Module> M) {; return optimizeModule(std::move(M));; }),; CompileCallbackManager(; orc::createLocalCompileCallbackManager(TM->getTargetTriple(), 0)),; CODLayer(OptimizeLayer,; [this](Function &F) { return std::set<Function*>({&F}); },; *CompileCallbackManager,; orc::createLocalIndirectStubsManagerBuilder(; TM->getTargetTriple())) {; llvm::sys::DynamicLibrary::LoadLibraryPermanently(nullptr);; }. Next we have to update our constructor to initialize the new members. To create; an appropriate compile callback manager we use the; createLocalCompileCallbackManager function, which takes a TargetMachine and an; ExecutorAddr to call if it receives a request to compile an unknown; function. In our simple JIT this situation is unlikely to come up, so we'll; cheat and just pass '0' here. In a production quality JIT you could give the; address of a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:5178,optimiz,optimizeModule,5178,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,1,['optimiz'],['optimizeModule']
Performance,":sup:`1` Visual Studio; Windows x64 x86-64 Visual Studio; ================== ===================== =============. .. note::. #. Code generation supported for Pentium processors and up; #. Code generation supported for 32-bit ABI only; #. To use LLVM modules on Win32-based system, you may configure LLVM; with ``-DBUILD_SHARED_LIBS=On``. Note that Debug builds require a lot of time and disk space. An LLVM-only build; will need about 1-3 GB of space. A full build of LLVM and Clang will need around; 15-20 GB of disk space. The exact space requirements will vary by system. (It; is so large because of all the debugging information and the fact that the; libraries are statically linked into multiple tools). If you are space-constrained, you can build only selected tools or only; selected targets. The Release build requires considerably less space. The LLVM suite *may* compile on other platforms, but it is not guaranteed to do; so. If compilation is successful, the LLVM utilities should be able to; assemble, disassemble, analyze, and optimize LLVM bitcode. Code generation; should work as well, although the generated native code may not work on your; platform. Software; --------. Compiling LLVM requires that you have several software packages installed. The; table below lists those required packages. The Package column is the usual name; for the software package that LLVM depends on. The Version column provides; ""known to work"" versions of the package. The Notes column describes how LLVM; uses the package and provides other details. =========================================================== ============ ==========================================; Package Version Notes; =========================================================== ============ ==========================================; `CMake <http://cmake.org/>`__ >=3.20.0 Makefile/workspace generator; `python <http://www.python.org/>`_ >=3.6 Automated test suite\ :sup:`1`; `zlib <http://zlib.net>`_ >=1.2.3.4 Compression libr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:9920,optimiz,optimize,9920,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimize']
Performance,"; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.smul.fix.i4(i4 3, i4 2, i32 0) ; %res = 6 (2 x 3 = 6); %res = call i4 @llvm.smul.fix.i4(i4 3, i4 2, i32 1) ; %res = 3 (1.5 x 1 = 1.5); %res = call i4 @llvm.smul.fix.i4(i4 3, i4 -2, i32 1) ; %res = -3 (1.5 x -1 = -1.5). ; The result in the following could be rounded up to -2 or down to -2.5; %res = call i4 @llvm.smul.fix.i4(i4 3, i4 -3, i32 1) ; %res = -5 (or -4) (1.5 x -1.5 = -2.25). '``llvm.umul.fix.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.umul.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.umul.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.umul.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.umul.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.umul.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.umul.fix``' family of intrinsic functions perform unsigned; fixed point multiplication on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo unsigned fixed point multiplication. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs unsigned fixed point multiplication on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type. Examples; """""""""""""""""". .. code-block",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:623607,perform,perform,623607,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"; # prebuilt/A.pcm prebuilt/B.pcm. Note that with explicit or prebuilt modules, we are responsible for, and should be particularly careful about the compatibility of our modules.; Using mismatching compilation options and modules may lead to issues. .. code-block:: sh. clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -DENABLE_A; # use.c:4:10: warning: implicit declaration of function 'a' is invalid in C99 [-Wimplicit-function-declaration]; # return a(x);; # ^; # 1 warning generated. So we need to maintain multiple versions of prebuilt modules. We can do so using a manual module mapping, or pointing to a different prebuilt module cache path. For example:. .. code-block:: sh. rm -rf prebuilt ; mkdir prebuilt ; rm -rf prebuilt_a ; mkdir prebuilt_a; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -fdisable-module-hash; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt_a -fdisable-module-hash -DENABLE_A; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt_a -DENABLE_A. Instead of managing the different module versions manually, we can build implicit modules in a given cache path (using ``-fmodules-cache-path``), and reuse them as prebuilt implicit modules by passing ``-fprebuilt-module-path`` and ``-fprebuilt-implicit-modules``. .. code-block:: sh. rm -rf prebuilt; mkdir prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -DENABLE_A; find prebuilt -name ""*.pcm""; # prebuilt/1AYBIGPM8R2GA/A-3L1K4LUA6O31.pcm; # prebuilt/1AYBIGPM8R2GA/B-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/A-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:21890,cache,cache-path,21890,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['cache'],['cache-path']
Performance,"; $$. Remarkable feature of algorithm is the technique for step restriction.; For an initial value of parameter ${\vec\theta}^0$ a parallelepiped; $P_0$ is built with the center at ${\vec\theta}^0$ and axes parallel; to coordinate axes $\theta_i$. The lengths of parallelepiped sides; along i-th axis is $2b_i$, where $b_i$ is such a value that the; functions $f_j(\vec\theta)$ are quasi-linear all over the; parallelepiped. FUMILI takes into account simple linear inequalities in the form:. $$ \theta_i^{min}\le\theta_i\le\theta^{max}_i$$. They form parallelepiped $P$ ($P_0$ may be deformed by $P$). Very; similar step formulae are used in FUMILI for negative logarithm of; the likelihood function with the same idea - linearization of function; argument. ## Neural Networks. ### Introduction. Neural Networks are used in various fields for data analysis and; classification, both for research and commercial institutions. Some; randomly chosen examples are image analysis, financial movements'; predictions and analysis, or sales forecast and product shipping; optimization. In particles physics neural networks are mainly used for; classification tasks (signal over background discrimination). A vast; majority of commonly used neural networks are multilayer perceptrons.; This implementation of multilayer perceptrons is inspired from the; `MLPfit` package, which remains one of the fastest tools for neural; networks studies. ### The MLP. The multilayer perceptron is a simple feed-forward network with the; following structure showed on the left. ![](pictures/0300008D.png). It is made of neurons characterized by a bias and weighted links in; between - let's call those links synapses. The input neurons receive; the inputs, normalize them and forward them to the first hidden layer.; Each neuron in any subsequent layer first computes a linear; combination of the outputs of the previous layer. The output of the; neuron is then function of that combination with f being linear for; output ne",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:70114,optimiz,optimization,70114,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['optimiz'],['optimization']
Performance,"; % llvm-lit .; # Use the profile data for compilation and actual benchmark run:; % cmake -DTEST_SUITE_PROFILE_GENERATE=OFF \; -DTEST_SUITE_PROFILE_USE=ON \; -DTEST_SUITE_RUN_TYPE=ref \; .; % make; % llvm-lit -o result.json .; ```. To use Clang frontend's PGO instead of LLVM IR PGO, set `-DTEST_SUITE_USE_IR_PGO=OFF`. The `TEST_SUITE_RUN_TYPE` setting only affects the SPEC benchmark suites. Cross Compilation and External Devices; --------------------------------------. ### Compilation. CMake allows to cross compile to a different target via toolchain files. More; information can be found here:. - [https://llvm.org/docs/lnt/tests.html#cross-compiling](https://llvm.org/docs/lnt/tests.html#cross-compiling). - [https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html](https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html). Cross compilation from macOS to iOS is possible with the; `test-suite/cmake/caches/target-target-*-iphoneos-internal.cmake` CMake cache; files; this requires an internal iOS SDK. ### Running. There are two ways to run the tests in a cross compilation setting:. - Via SSH connection to an external device: The `TEST_SUITE_REMOTE_HOST` option; should be set to the SSH hostname. The executables and data files need to be; transferred to the device after compilation. This is typically done via the; `rsync` make target. After this, the lit runner can be used on the host; machine. It will prefix the benchmark and verification command lines with an; `ssh` command. Example:. ```bash; % cmake -G Ninja -D CMAKE_C_COMPILER=path/to/clang \; -C ../test-suite/cmake/caches/target-arm64-iphoneos-internal.cmake \; -D CMAKE_BUILD_TYPE=Release \; -D TEST_SUITE_REMOTE_HOST=mydevice \; ../test-suite; % ninja; % ninja rsync; % llvm-lit -j1 -o result.json .; ```. - You can specify a simulator for the target machine with the; `TEST_SUITE_RUN_UNDER` setting. The lit runner will prefix all benchmark; invocations with it. Running the test-suite via LNT; ------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:11462,cache,cache,11462,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['cache'],['cache']
Performance,"; * - `Compiler Research - Calling C++ libraries from a D-written DSL: A cling/cppyy-based approach <https://www.youtube.com/watch?v=7teqrCNzrD8>`_; - *Alexandru Militaru* 2021 Compiler-Research Meeting; - This video presents D and C++ interoperability through SIL-Cling architecture. .. list-table:: Interactive CUDA C++ with Cling:; :widths: 25 25 50; :header-rows: 1. * - Link; - Info ; - Description; * - `Adding CUDA® Support to Cling: JIT Compile to GPUs <https://www.youtube.com/watch?v=XjjZRhiFDVs>`_; - *Simeon Ehrig* 2020 LLVM Developer Meeting; - Interactive CUDA-C++ through Cling is presented. Cling-CUDA architecture is discussed in detail, and an example of interactive simulation for laser plasma applications is shown. . .. list-table:: C++ in Jupyter Notebook - Xeus Cling:; :widths: 25 25 50; :header-rows: 1; ; * - Link; - Info ; - Description; * - `Interactive C++ code development using C++Explorer and GitHub Classroom for educational purposes <https://www.youtube.com/watch?v=HBgF2Yr0foA>`_; - *Patrick Diehl* 2020 Youtube; - C++Explorer is a novel teaching environment based on Jupyterhub and Cling, adapted to teaching C++ programming and source code management.; * - `Deep dive into the Xeus-based Cling kernel for Jupyter <https://www.youtube.com/watch?v=kx3wvKk4Qss>`_; - *Vassil Vassilev* 2021 Youtube; - Xeus-Cling is a Cling-based notebook kernel which delivers interactive C++. ; * - `Xeus-Cling: Run C++ code in Jupyter Notebook <https://www.youtube.com/watch?v=4fcKlJ_5QQk>`_ ; - *LearnOpenCV* 2019 Youtube; - In this demo, you will learn an example of C++ code in Jupyter Notebook using Xeus-Cling kernel. . .. list-table:: Clad:; :widths: 25 25 50; :header-rows: 1; ; * - Link; - Info ; - Description; * - `Clad: Automatic differentiation plugin for C++ <https://clad.readthedocs.io/en/latest/index.html>`_ ; - Read The Docs webpage; - Clad is a plugin for Cling. It allows to perform Automatic Differentiation (AD) on multivariate functions and functor objects. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/references.rst:7765,perform,perform,7765,interpreter/cling/docs/chapters/references.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/references.rst,1,['perform'],['perform']
Performance,"; * Do not automatically setup read cache during `TTree::Fill()`. This fixes [ROOT-8031].; * Make sure the option ""PARA"" in `TTree::Draw` is used with at least tow variables [ROOT-8196].; * The with `goff` option one can use as many variables as needed. There no more; limitation, like with the options `para`and `candle`.; * Fix detection of errors that appears in nested TTreeFormula [ROOT-8218]; * Better basket size optimization by taking into account meta data and rounding up to next 512 bytes, ensuring a complete cluster fits into a single basket. ### Fast Cloning. We added a cache specifically for the fast option of the TTreeCloner to significantly reduce the run-time when fast-cloning remote files to address [ROOT-5078]. It can be controlled from the `TTreeCloner`, `TTree::CopyEntries` or `hadd` interfaces. The new cache is enabled by default, to update the size of the cache or disable it from `TTreeCloner` use: `TTreeCloner::SetCacheSize`. To do the same from `TTree::CopyEntries` add to the option string ""cachesize=SIZE"". To update the size of the cache or disable it from `hadd`, use the command line option `-cachesize SIZE`. `SIZE` shouyld be given in number bytes and can be expressed in 'human readable form' (number followed by size unit like MB, MiB, GB or GiB, etc. or SIZE can be set zero to disable the cache. ### Other Changes. * Update `TChain::LoadTree` so that the user call back routine is actually called for each input file even those containing `TTree` objects with no entries.; * Repair setting the branch address of a leaflist style branch taking directly the address of the struct. (Note that leaflist is nonetheless still deprecated and declaring the struct to the interpreter and passing the object directly to create the branch is much better).; * Provide an implicitly parallel implementation of `TTree::GetEntry`. The approach is based on creating a task per top-level branch in order to do the reading, unzipping and deserialisation in parallel. In add",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:9934,cache,cachesize,9934,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['cache'],['cachesize']
Performance,"; - 1:; Use a single thread only (disable multi-threading); - all:; Use one thread per logical core (uses all hyper-threads). Incremental; -----------; .. _incremental:. ThinLTO supports fast incremental builds through the use of a cache,; which currently must be enabled through a linker option. - gold (as of LLVM 4.0):; ``-Wl,-plugin-opt,cache-dir=/path/to/cache``; - ld64 (supported since clang 3.9 and Xcode 8) and Mach-O ld64.lld (as of LLVM; 15.0):; ``-Wl,-cache_path_lto,/path/to/cache``; - ELF ld.lld (as of LLVM 5.0):; ``-Wl,--thinlto-cache-dir=/path/to/cache``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocache:/path/to/cache``. Cache Pruning; -------------. To help keep the size of the cache under control, ThinLTO supports cache; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value pairs separated by ``:`` characters.; Possible key-value pairs are:. - ``cache_size=X%``: The maximum size for the cache directory is ``X`` percent; of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is invalid. A value of 0 disables the percentage; size-based pruning. The default is 75%. - ``cache_size_bytes=X``, ``cache_size_bytes=Xk``, ``cache_size_bytes=Xm``,; ``cache_size_bytes=Xg``:; Sets the maximum size for the cache directory to ``X`` bytes (or KB, MB,; GB respectively). A value over the amount of available space on the disk; will be reduced to the amount of available space. A value of 0 disables; the byte size-based pruning. T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:5398,cache,cache-policy,5398,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache-policy']
Performance,"; - 32-bit floating-point value. * - ``double``; - 64-bit floating-point value. * - ``fp128``; - 128-bit floating-point value (113-bit significand). * - ``x86_fp80``; - 80-bit floating-point value (X87). * - ``ppc_fp128``; - 128-bit floating-point value (two 64-bits). The binary format of half, float, double, and fp128 correspond to the; IEEE-754-2008 specifications for binary16, binary32, binary64, and binary128; respectively. X86_amx Type; """""""""""""""""""""""". :Overview:. The x86_amx type represents a value held in an AMX tile register on an x86; machine. The operations allowed on it are quite limited. Only few intrinsics; are allowed: stride load and store, zero and dot product. No instruction is; allowed for this type. There are no arguments, arrays, pointers, vectors; or constants of this type. :Syntax:. ::. x86_amx. X86_mmx Type; """""""""""""""""""""""". :Overview:. The x86_mmx type represents a value held in an MMX register on an x86; machine. The operations allowed on it are quite limited: parameters and; return values, load and store, and bitcast. User-specified MMX; instructions are represented as intrinsic or asm calls with arguments; and/or results of this type. There are no arrays, vectors or constants; of this type. :Syntax:. ::. x86_mmx. .. _t_pointer:. Pointer Type; """""""""""""""""""""""". :Overview:. The pointer type ``ptr`` is used to specify memory locations. Pointers are; commonly used to reference objects in memory. Pointer types may have an optional address space attribute defining; the numbered address space where the pointed-to object resides. For; example, ``ptr addrspace(5)`` is a pointer to address space 5.; In addition to integer constants, ``addrspace`` can also reference one of the; address spaces defined in the :ref:`datalayout string<langref_datalayout>`.; ``addrspace(""A"")`` will use the alloca address space, ``addrspace(""G"")``; the default globals address space and ``addrspace(""P"")`` the program address; space. The default address space is number zero. The sema",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:170468,load,load,170468,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic sc1=1; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:324768,load,loads,324768,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkmcnt(0) &; vm/vscnt(0). - If CU wavefront execution; mode, omit vm/vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vm/vscnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:350402,cache,caches,350402,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vmcnt(0) &; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:215369,load,loads,215369,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:263166,load,load,263166,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; - Must happen before; the following; buffer_gl0_inv.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_gl*_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the caches. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_gl0_inv;;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:372318,load,load,372318,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be spli",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:263041,load,load,263041,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``mayWriteToMemory()``: Existing predicate, but note; that they return true for any operation which is volatile or at least; Monotonic. * ``isStrongerThan`` / ``isAtLeastOrStrongerThan``: ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15359,load,loads,15359,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"; ---------. Monotonic is the weakest level of atomicity that can be used in synchronization; primitives, although it does not provide any general synchronization. It; essentially guarantees that if you take all the operations affecting a specific; address, a consistent ordering exists. Relevant standard; This corresponds to the C++/C ``memory_order_relaxed``; see those; standards for the exact definition. Notes for frontends; If you are writing a frontend which uses this directly, use with caution. The; guarantees in terms of synchronization are very weak, so make sure these are; only used in a pattern which you know is correct. Generally, these would; either be used for atomic operations which do not protect other memory (like; an atomic counter), or along with a ``fence``. Notes for optimizers; In terms of the optimizer, this can be treated as a read+write on the relevant; memory location (and alias analysis will take advantage of that). In addition,; it is legal to reorder non-atomic and Unordered loads around Monotonic; loads. CSE/DSE and a few other optimizations are allowed, but Monotonic; operations are unlikely to be used in ways which would make those; optimizations useful. Notes for code generation; Code generation is essentially the same as that for unordered for loads and; stores. No fences are required. ``cmpxchg`` and ``atomicrmw`` are required; to appear as a single operation. Acquire; -------. Acquire provides a barrier of the sort necessary to acquire a lock to access; other memory with normal loads and stores. Relevant standard; This corresponds to the C++/C ``memory_order_acquire``. It should also be; used for C++/C ``memory_order_consume``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. It is; also possible to move stores from be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:11053,load,loads,11053,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],['loads']
Performance,"; ... float x;; ... float y;; ... float z;; ... };; ...; ... std::vector<Atom> atoms = {{1, 2, 3}, {2, 3, 4}, {3, 4, 5}, {4, 5, 6}, {5, 6, 7}};; ... """"""); ...; >>> @numba.njit; >>> def lj_numba_scalar(r):; ... sr6 = (1./r)**6; ... pot = 4.*(sr6*sr6 - sr6); ... return pot. >>> @numba.njit; >>> def distance_numba_scalar(atom1, atom2):; ... dx = atom2.x - atom1.x; ... dy = atom2.y - atom1.y; ... dz = atom2.z - atom1.z; ...; ... r = (dx * dx + dy * dy + dz * dz) ** 0.5; ...; ... return r; ...; >>> def potential_numba_scalar(cluster):; ... energy = 0.0; ... for i in range(cluster.size() - 1):; ... for j in range(i + 1, cluster.size()):; ... r = distance_numba_scalar(cluster[i], cluster[j]); ... e = lj_numba_scalar(r); ... energy += e; ...; ... return energy; ...; >>> print(""Total lennard jones potential ="", potential_numba_scalar(cppyy.gbl.atoms)); Total lennard jones potential = -0.5780277345740283. Overhead; --------. The main overhead of JITing Numba traces is in the type annotation in Numba; itself, optimization of the IR and assembly by the backend less so.; (There is also a non-negligible cost to Numba initialization, which is why; ``cppyy`` does not provide automatic extension hooks.); The use of ``cppyy`` bound C++, which relies on the same Numba machinery,; does not change that, since the reflection-based lookups are in C++ and; comparatively very fast.; For example, there is no appreciable difference in wall clock time to JIT a; trace using Numba's included math functions (from module ``math`` or; ``numpy``) or one that uses C++ bound ones whether from the standard library; or a templated versions from e.g. Eigen.; Use of very complex template expressions may change this balance, but in; principle, wherever it makes sense in the first place to use Numba JITing, it; is also fine, performance-wise, to use ``cppyy`` bound C++ inside the trace. A second important overhead is in unboxing Python proxies of C++ objects,; in particular when passed as an argument to a Nu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst:9339,optimiz,optimization,9339,bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,1,['optimiz'],['optimization']
Performance,"; ...; }. define i32 @never_instrument() uwtable ""function-instrument""=""xray-never"" {; ; ...; }. You can also set the ``xray-instruction-threshold`` attribute and provide a; numeric string value for how many instructions should be in the function before; it gets instrumented. .. code-block:: llvm. define i32 @maybe_instrument() uwtable ""xray-instruction-threshold""=""2"" {; ; ...; }. Special Case File; -----------------. Attributes can be imbued through the use of special case files instead of; adding them to the original source files. You can use this to mark certain; functions and classes to be never, always, or instrumented with first-argument; logging from a file. The file's format is described below:. .. code-block:: bash. # Comments are supported; [always]; fun:always_instrument; fun:log_arg1=arg1 # Log the first argument for the function. [never]; fun:never_instrument. These files can be provided through the ``-fxray-attr-list=`` flag to clang.; You may have multiple files loaded through multiple instances of the flag. XRay Runtime Library; --------------------. The XRay Runtime Library is part of the compiler-rt project, which implements; the runtime components that perform the patching and unpatching of inserted; instrumentation points. When you use ``clang`` to link your binaries and the; ``-fxray-instrument`` flag, it will automatically link in the XRay runtime. The default implementation of the XRay runtime will enable XRay instrumentation; before ``main`` starts, which works for applications that have a short; lifetime. This implementation also records all function entry and exit events; which may result in a lot of records in the resulting trace. Also by default the filename of the XRay trace is ``xray-log.XXXXXX`` where the; ``XXXXXX`` part is randomly generated. These options can be controlled through the ``XRAY_OPTIONS`` environment; variable, where we list down the options and their defaults below. +-------------------+-----------------+--------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRay.rst:4494,load,loaded,4494,interpreter/llvm-project/llvm/docs/XRay.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRay.rst,1,['load'],['loaded']
Performance,"; // #5: error: implicit loading of libA is currently unsupported. ```. This pattern is not only used in the ROOT prompt but in I/O hotspots such as; `ShowMembers` and `TClass::IsA`. A naive implementation of this feature would require inclusion of all reachable; library descriptors (aka header files) at ROOT startup time. Of course this is; not feasible and ROOT inserts a set of optimizations to fence itself from the; costly full header inclusion. Unfortunately, several of them are home-grown and; in a few cases inaccurate (eg line #5) causing a noticeable technical debt. Here we will briefly describe the three common layers of optimizations: ROOT PCH,; ROOTMAP and RDICT. The ROOT precompiled header (PCH) reduces the CPU and memory cost for ROOT's; most used libraries. The precompiled header technology is well-understood since; decades [[4]]. It is an efficient on-disk representation of the state of the; compiler after parsing a set of headers. It can be loaded before starting the; next instance to avoid doing redundant work. At build time, rootcling (ROOT's; dictionary generator) creates such PCH file which is attached at ROOT startup; time. Its major drawback is the fact that if third-party users want to include; their libraries, they have to recompile it every time there is a change. RDICT files store some useful information (in particular about class offsets) in; ROOT files to avoid the potentially expensive call to the interpreter if the; information is not the PCH. For example, ROOT's libGeom and other third-party; code. This is done to circumvent the costly call to `ShowMembers` which will; require parsing. ROOTMAP files reduce parsing for code which is not in the PCH. Consider; `foo::bar` and `S` are defined in `libFoo`'s `Foo.h`:; ```cpp; // Foo.h; namespace foo { struct bar{}; }; struct S{};; ```. ```bash; # libFoo.rootmap; { decls }; namespace foo { }; struct S;; ; [ libFoo.so ]; # List of selected classes; class bar; struct S; ```. ```cpp; // G__Foo.cxx ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:6156,load,loaded,6156,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['load'],['loaded']
Performance,"; 15. Support direct reading of objects from sub-sub-directories.; 16. Introduce demo.htm, which demonstrates online usage of JSROOT.; 17. One could use demo.htm directly with THttpServer providing address like:; <http://localhost:8080/jsrootsys/demo/demo.htm?addr=../../Files/job1.root/hpx/root.json.gz&layout=3x3>; 18. Also for online server process url options like 'item', 'items', 'layout'; 19. Possibility to generate URL, which reproduces opened page with layout and drawn items. ### August 2014; 1. All communication between server and browser done with JSON format.; 2. Fix small error in dtree.js - one should always set; last sibling (_ls) property while tree can be dynamically changed.; 3. In JSRootCore.js provide central function, which handles different kinds; of XMLHttpRequest. Use only async requests, also when getting file header.; 4. Fully reorganize data management in file/tree/directory/collection hierarchical; display. Now complete description collected in HPainter class and decoupled from; visualization, performed with dTree.js.; 5. Remove all global variables from the code.; 6. Automatic scripts/style loading handled via JSROOT.loadScript() function.; One can specify arbitrary scripts list, which asynchronously loaded by browser.; 7. Method to build simple GUI changed and more simplified :). The example in index.htm.; While loadScript and AssertPrerequisites functions moved to JSROOT, one; can easily build many different kinds of GUIs, reusing provided JSRootCore.js functions.; 8. In example.htm also use AssertPrerequisites to load necessary scripts.; This helps to keep code up-to-date even by big changes in JavaScript code.; 9. Provide monitoring of online THttpServer with similar interface as for ROOT files.; 10. Fix several errors in TKey Streamer, use member names as in ROOT itself.; 11. Keep the only version identifier JSROOT.version for JS code; 12. One can specify in JSROOT.AssertPrerequisites functionality which is required.; One could specify",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:74298,perform,performed,74298,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['perform'],['performed']
Performance,"; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``-noalias`` is the; only option). Essentially, there is no option to specify a different memory; type (e.g., write-back, write-combining, write-through; etc.) and consequently; to weaken, or strengthen, the memory model. Other limitations are:. * The LSUnit does not know when store-to-load forwarding may occur.; * The LSUnit does not know anything about cache hierarchy and memory types.; * The LSUnit does not know how to identify serializing operations and memory; fences. The LSUnit does not attempt to predict if a load or store hits or misses the L1; cache. It only knows if an instruction ""MayLoad"" and/or ""MayStore."" For; loads, the scheduling model provides an ""optimistic"" load-to-use latency (which; usually matches the load-to-use latency for when there is a hit in the L1D). :program:`llvm-mca` does not (on its own) know about serializing operations or; memory-barrier like instructions. The LSUnit used to conservatively use an; instruction's ""MayLoad"", ""MayStore"", and unmodeled side effects flags to; determine whether an instruction should be treated as a memory-barrier. This was; inaccurate in general and was changed so that now each instruction has an; IsAStoreBarrier and IsALoadBarrier flag. These flags are mca specific and; default to false for every instruction. If any instruction should have either of; these flags set, i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:41034,cache,cache,41034,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['cache'],['cache']
Performance,"; 8. Provide example of custom entries in histogram context menu; 9. Provide alternative external location for zstd-codec, let use zstd even when not found locally; 10. Let skip HEAD requests when reading files, adding ""^"" symbol to file name (#223); 11. Show long histogram names in stats box when possible; 12. Fix logic how ""ndiv"" parameter of TAxis is handled, showing really the configured number of ticks; 13. Fix problem with curved TGraph drawings (#218); 14. Fix problems with TGraph drawing updates; 15. Base version for ROOT 6.26 release. ## Changes in 6.2.2; 1. Fix - proper fill TH1 which drawn with line option; 2. Fix - object drawing from inspector; 3. Fix - error with filling data of TGeoTrack in ""extract tracks"" example; 4. Fix - error in pad items context menu; 5. Fix - assigned dropped item name only when new painter created. ## Changes in 6.2.1; 1. Fix logy and logz handling on lego plots; 2. Fix error in statistic calculations for projections; 3. Fix zstd-codec loading with minified jsroot scripts. ## Changes in 6.2.0; 1. Support fully interactive second X/Y axis for histograms, graphs, functions and spline; 2. Support X+, Y+, RX, RY draw options for TF1; 3. Remove deprecated JSRootCore.js script, one have to use JSRoot.core.js; 4. Upgrade three.js to r127; 5. Upgrade d3.js to 6.7.0; 6. Implement ""nozoomx"" and ""nozoomy"" draw options for TPad; 7. Implement ""frame"" draw option for TGaxis - fix position of axis relative to the frame; 8. Preserve position of TPaletteAxis, if provided with histogram; make default position like in ROOT; 9. Support basic TLatex symbols in lego plos axis title; 10. Use frame margins when create 3D lego drawings; 11. Implement ""nomargins"" draw option for pad/canvas; 12. Support custom mouse click/dblcklick handlers in lego plots; 13. Implement marker styles 35 - 49; 14. Let switch orthographic camera in geometry via control gui (#217); 15. Fix drawing of custom markers on 3D, also in node.js (#205). ## Changes in 6.1.1; 1. Fix b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:22541,load,loading,22541,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"; 9. Fix problem of simultaneous move TGeo drawings and canvas in flexible layout. ## Changes in 4.2; 1. Significant performance improvements in 3D drawings - TGeo/TH2/TH3; 2. Implement TGeoPara, TGeoGtra, TGeoXtru and TGeoEltu shapes; 3. Optimize (reduce vertices number) for others TGeo shapes; 4. Correct rotation/translation/scaling of TGeo nodes; 5. Workaround for axis reflection (not directly supported in three.js); 6. Support array of objects in I/O (like in TAxis3D); 7. Correct reading of multi-dim arrays like Double_t fXY[8][2];; 8. Provide canvas toolbar for actions like savepng or unzoom; 9. Implement JSROOT.resize() function to let resize drawing after changes in page layout; 10. Fix error with title display/update. ## Changes in 4.1; 1. Introduce object inspector - one could browse object members of any class; 2. Let draw sub-items from TCanvas list of primitives like sub-pad or TLatex; 3. Provide possibility to save drawn SVG canvas as PNG; 4. TGraph drawing optimization - limit number of drawn points; 5. Implement painter for TPolyMarker3D; 6. Improve drawing and update of TMultiGraph; 7. Reorganize 3D drawing of TH2/TH3 histograms, allow to mix 2D and 3D display together; 8. Support overlay of 3D graphic over SVG canvas (used for IE); 9. Fix problems and improve flex(ible) layout. ## Changes in 4.0; 1. New TGeo classes support:; - browsing through volumes hierarchy; - changing visibility flags; - drawing of selected volumes; 2. New 'flex' layout:; - create frames like in Multi Document Interface; - one could move/resize/minimize/maximize such frames; 3. Significant (factor 4) I/O performance improvement:; - use ArrayBuffer class in HTTP requests instead of String; - use native arrays (like Int32Array) for array data members; - highly optimize streamer infos handling; 4. TH2 drawing optimization:; - if there are too many non-empty bins, combine them together; - when zoom-in, all original bins will be displayed separately; - let draw big TH2 histogram fas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:59104,optimiz,optimization,59104,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['optimiz'],['optimization']
Performance,"; :ref:`expression<amdgpu_synid_expression>`; is used for an operand which has a different type or size. .. _amdgpu_synid_int_conv:. Conversion of Integer Values; ----------------------------. Instruction operands may be specified as 64-bit; :ref:`integer numbers<amdgpu_synid_integer_number>` or; :ref:`absolute expressions<amdgpu_synid_absolute_expression>`.; These values are converted to the; :ref:`expected operand type<amdgpu_syn_instruction_type>`; using the following steps:. 1. *Validation*. Assembler checks if the input value may be truncated; without loss to the required *truncation width* (see the table below).; There are two cases when this operation is enabled:. * The truncated bits are all 0.; * The truncated bits are all 1 and the value after truncation has its MSB bit set. In all other cases, the assembler triggers an error. 2. *Conversion*. The input value is converted to the expected type; as described in the table below. Depending on operand kind, this conversion; is performed by either assembler or AMDGPU H/W (or both). ============== ================= =============== ====================================================================; Expected type Truncation Width Conversion Description; ============== ================= =============== ====================================================================; i16, u16, b16 16 num.u16 Truncate to 16 bits.; i32, u32, b32 32 num.u32 Truncate to 32 bits.; i64 32 {-1,num.i32} Truncate to 32 bits and then sign-extend the result to 64 bits.; u64, b64 32 {0,num.u32} Truncate to 32 bits and then zero-extend the result to 64 bits.; f16 16 num.u16 Use low 16 bits as an f16 value.; f32 32 num.u32 Use low 32 bits as an f32 value.; f64 32 {num.u32,0} Use low 32 bits of the number as high 32 bits; of the result; low 32 bits of the result are zeroed.; ============== ================= =============== ====================================================================. Examples of enabled conversions:. .. parsed-litera",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst:35175,perform,performed,35175,interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,1,['perform'],['performed']
Performance,"; ;; ; [%ptr + 0]: 00100001 (0x21); ; [%ptr + 1]: 01010011 (0x53). When ``<N*M>`` isn't evenly divisible by the byte size the exact memory layout; is unspecified (just like it is for an integral type of the same size). This; is because different targets could put the padding at different positions when; the type size is smaller than the type's store size. :Syntax:. ::. < <# elements> x <elementtype> > ; Fixed-length vector; < vscale x <# elements> x <elementtype> > ; Scalable vector. The number of elements is a constant integer value larger than 0;; elementtype may be any integer, floating-point or pointer type. Vectors; of size zero are not allowed. For scalable vectors, the total number of; elements is a constant multiple (called vscale) of the specified number; of elements; vscale is a positive integer that is unknown at compile time; and the same hardware-dependent constant for all scalable vectors at run; time. The size of a specific scalable vector type is thus constant within; IR, even if the exact size in bytes cannot be determined until run time. :Examples:. +------------------------+----------------------------------------------------+; | ``<4 x i32>`` | Vector of 4 32-bit integer values. |; +------------------------+----------------------------------------------------+; | ``<8 x float>`` | Vector of 8 32-bit floating-point values. |; +------------------------+----------------------------------------------------+; | ``<2 x i64>`` | Vector of 2 64-bit integer values. |; +------------------------+----------------------------------------------------+; | ``<4 x ptr>`` | Vector of 4 pointers |; +------------------------+----------------------------------------------------+; | ``<vscale x 4 x i32>`` | Vector with a multiple of 4 32-bit integer values. |; +------------------------+----------------------------------------------------+. .. _t_label:. Label Type; ^^^^^^^^^^. :Overview:. The label type represents code labels. :Syntax:. ::. label. .. _t_token:. Token ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:177461,scalab,scalable,177461,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"; ======================; Thread Safety Analysis; ======================. Introduction; ============. Clang Thread Safety Analysis is a C++ language extension which warns about; potential race conditions in code. The analysis is completely static (i.e.; compile-time); there is no run-time overhead. The analysis is still; under active development, but it is mature enough to be deployed in an; industrial setting. It is being developed by Google, in collaboration with; CERT/SEI, and is used extensively in Google's internal code base. Thread safety analysis works very much like a type system for multi-threaded; programs. In addition to declaring the *type* of data (e.g. ``int``, ``float``,; etc.), the programmer can (optionally) declare how access to that data is; controlled in a multi-threaded environment. For example, if ``foo`` is; *guarded by* the mutex ``mu``, then the analysis will issue a warning whenever; a piece of code reads or writes to ``foo`` without first locking ``mu``.; Similarly, if there are particular routines that should only be called by; the GUI thread, then the analysis will warn if other threads call those; routines. Getting Started; ----------------. .. code-block:: c++. #include ""mutex.h"". class BankAccount {; private:; Mutex mu;; int balance GUARDED_BY(mu);. void depositImpl(int amount) {; balance += amount; // WARNING! Cannot write balance without locking mu.; }. void withdrawImpl(int amount) REQUIRES(mu) {; balance -= amount; // OK. Caller must have locked mu.; }. public:; void withdraw(int amount) {; mu.Lock();; withdrawImpl(amount); // OK. We've locked mu.; } // WARNING! Failed to unlock mu. void transferFrom(BankAccount& b, int amount) {; mu.Lock();; b.withdrawImpl(amount); // WARNING! Calling withdrawImpl() requires locking b.mu.; depositImpl(amount); // OK. depositImpl() has no requirements.; mu.Unlock();; }; };. This example demonstrates the basic concepts behind the analysis. The; ``GUARDED_BY`` attribute declares that a thread must lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst:188,race condition,race conditions,188,interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,3,"['multi-thread', 'race condition']","['multi-threaded', 'race conditions']"
Performance,"; ==========================================. Introduction; ============. The :ref:`inalloca <attr_inalloca>` attribute is designed to allow; taking the address of an aggregate argument that is being passed by; value through memory. Primarily, this feature is required for; compatibility with the Microsoft C++ ABI. Under that ABI, class; instances that are passed by value are constructed directly into; argument stack memory. Prior to the addition of inalloca, calls in LLVM; were indivisible instructions. There was no way to perform intermediate; work, such as object construction, between the first stack adjustment; and the final control transfer. With inalloca, all arguments passed in; memory are modelled as a single alloca, which can be stored to prior to; the call. Unfortunately, this complicated feature comes with a large; set of restrictions designed to bound the lifetime of the argument; memory around the call. For now, it is recommended that frontends and optimizers avoid producing; this construct, primarily because it forces the use of a base pointer.; This feature may grow in the future to allow general mid-level; optimization, but for now, it should be regarded as less efficient than; passing by value with a copy. Intended Usage; ==============. The example below is the intended LLVM IR lowering for some C++ code; that passes two default-constructed ``Foo`` objects to ``g`` in the; 32-bit Microsoft C++ ABI. .. code-block:: c++. // Foo is non-trivial.; struct Foo { int a, b; Foo(); ~Foo(); Foo(const Foo &); };; void g(Foo a, Foo b);; void f() {; g(Foo(), Foo());; }. .. code-block:: text. %struct.Foo = type { i32, i32 }; declare void @Foo_ctor(%struct.Foo* %this); declare void @Foo_dtor(%struct.Foo* %this); declare void @g(<{ %struct.Foo, %struct.Foo }>* inalloca %memargs). define void @f() {; entry:; %base = call i8* @llvm.stacksave(); %memargs = alloca <{ %struct.Foo, %struct.Foo }>; %b = getelementptr <{ %struct.Foo, %struct.Foo }>* %memargs, i32 1; call voi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst:1061,optimiz,optimizers,1061,interpreter/llvm-project/llvm/docs/InAlloca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst,1,['optimiz'],['optimizers']
Performance,"; >>> for i in range(0,10):; ... v.push_back(i); ...; >>> for i in v:; ... print(i, end=' '); 1 2 3 4 5 6 7 8 9; >>>; >>> list(v); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; >>>; ```. The parameters to the template instantiation can either be an actual; type or value (as is used here, ""int""), or a string representation of; the parameters (e.g. ""'double'""), or a mixture of both (e.g. ""'TCanvas,; 0'"" or ""'double', 0"" ). The ""std::vector\<int\>"" class is one of the; classes builtin by default into the Cling extension dlls. You will get a; non-functional class (instances of which can still be passed around to; C++) if the corresponding dictionary doesn't exist. #### Access to ROOT Globals. Most globals and global functions can be imported directly from the; ROOT.py module, but some common ones (most notably **`gMinuit`**,; although that variable now exists at startup from release 5.08 onward); do not exist yet at program startup, as they exist in modules that are; loaded later (e.g. through the auto-loading mechanism). An example; session should make this clear:. ``` {.cpp}; >>> from ROOT import *; >>> gROOT # directly available; <ROOT.TROOT object at 0x399c30>; >>> gMinuit # library not yet loaded: not available; Traceback (most recent call last):; File ""<stdin>"", line 1, in ?; NameError: name 'gMinuit' is not defined; >>> TMinuit # use of TMinuit class forces auto-loading; <class '__main__.TMinuit'>; >>> gMinuit # now gMinuit is available; <__main__.TMinuit object at 0x1458c70>; >>> not not gMinuit # but it is the null pointer, until set; False; >>> g = TMinuit(); >>> not not gMinuit; True; ```. It is also possible to create globals interactively, either by executing; a Cling macro, or by a call to `gROOT.ProcessLine()`. These globals are; made available in the same way: either use them directly after creation; in 'from ROOT import \*' more, or get them from the ROOT namespace after; an 'import ROOT'. As of 5.08, the behaviour of ROOT globals is the same as python globals,; which",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:13124,load,loading,13124,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['loading']
Performance,"; BEGIN_HTML; <h2>Minuit2 Package</h2>. <b>Minuit2</b> is a new object-oriented implementation, written in C++, of the popular MINUIT minimization package.<p>; These new version provides basically all the functionality present in the old Fortran version, with almost equivalent numerical accuracy and computational performances. Furthermore, it contains new functionality, like the possibility to set single side parameter limits or the FUMILI algorithm, which is an optimized method for least square and log likelihood minimizations. The package has been originally developed by M. Winkler and F. James. More information on the new C++ version can be found on the <a href=""http://www.cern.ch/minuit"">MINUIT Web Site</a> and in particular the online doc can be found here <a href=""http://www.cern.ch/mathlibs/sw/html/Minuit2.html"">here</a>.<p>; Minuit2, originally developed in the SEAL project, is now distributed within ROOT. The API has been then changed in this new version to follow the ROOT coding convention (function names starting with capital letters) and the classes have been moved inside the namespace <em>ROOT::Minuit2</em>. In addition, the ROOT distribution contains classes like TFitterMinuit and TFitterFumili needed to integrate Minuit2 in the ROOT framework. <p>; In the latest version (from 5.17.08) a new class has been introduced, ROOT::Minuit2::Minuit2Minimizer, which implements the interface; ROOT::Math::Minimizer from <a href=""http://www.cern.ch/mathlibs/sw/html/MathCore.html"">MathCore</a>.; . It can be instantiates also using the ROOT plug-in manager. It is a convenient entry point for using Minuit2. <h3>References</h3>; <p>; <ol>; <li>; F. James, <em>Fortran MINUIT Reference Manual</em> (<a href=""https://cern-tex.web.cern.ch/cern-tex/minuit/minmain.html"">html</a>); </li>; <li>; F. James and M. Winkler, <em>C++ MINUIT User's Guide</em> (<a href=""http://seal.cern.ch/documents/minuit/mnusersguide.pdf"">pdf</a>); </li>; <li>; F. James, <em>Minuit Tutorial on Functio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/index.txt:315,perform,performances,315,math/minuit2/doc/index.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/index.txt,2,"['optimiz', 'perform']","['optimized', 'performances']"
Performance,"; DistSampler::Generate(n, data). The data set; generation can be unbinned or binned in the; given range (only equidistant bins are currently supported); Sampling of 1D or multi-dim distributions is supported via; the same interface; Derived classes; implementing this interface are not provided by MathCore but by; other libraries and they can be instantiated using the plugin; manager. Implementations based on Unuran and Foam exist.; The tutorial math/multidimSampling.C is an example on; how to use this class. New class ROOT::Math::GoFTest for goodness of fit; tests of unbinned data; ; The class implements the Kolmogorov-Smirnov and; Anderson-Darling tests for two samples (data vs data ) and; one sample (data vs distribution); For the data vs distribution test, the user can compare using a; predefined distributions (Gaussian, LogNormal or Exponential) or; by passing a user defined PDF or CDF.; Example 1: perform a 2 sample GoF test from two arrays,; sample1[n1] and sample2[n2] containing the data; ; ROOT::Math::GoFTest goftest(n1, sample1, n2, sample2);; double pValueAD = goftest.AndersonDarling2SamplesTest();; double pValueKS = goftest.KolmogorovSmirnov2SamplesTest();; ; The class can return optionally also the test statistics instead of; the p value.; Example 2: perform a 1 sample test with a pre-defined; distribution starting from a data set sample[n]. ROOT::Math::GoFTest goftest(n, sample, ROOT::Math::GoFTest::kGaussian);; double pValueAD = goftest.AndersonDarlingTest();; double pValueKS = goftest.KolmogorovSmirnovTest();; . Example 3: perform a 1 sample test with a user-defined; distribution provided as cdf; ; ROOT::Math::Functor1D cdf_func(&ROOT::Math::landau_cdf);; ROOT::Math::GofTest goftest(n, sample, cdf_func, ROOT::Math::GoFTest::kCDF);; double pValueAD = goftest.AndersonDarlingTest();; . Example 4: perform a 1 sample test with a user-defined; distribution provided as pdf. Note that in this case to avoid; integration problems is sometimes recommended to gi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v528/index.html:1151,perform,perform,1151,math/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v528/index.html,1,['perform'],['perform']
Performance,"; LangRef; LibFuzzer; MarkedUpDisassembly; MIRLangRef; OptBisect; PCSectionsMetadata; PDB/index; PointerAuth; ScudoHardenedAllocator; MemTagSanitizer; Security; SecurityTransparencyReports; SegmentedStacks; StackMaps; SpeculativeLoadHardening; Statepoints; SymbolizerMarkupFormat; SystemLibrary; TestingGuide; TransformMetadata; TypeMetadata; XRay; XRayExample; XRayFDRFormat; YamlIO. API Reference; -------------. `Doxygen generated documentation <https://llvm.org/doxygen/>`_; (`classes <https://llvm.org/doxygen/inherits.html>`_). :doc:`HowToUseAttributes`; Answers some questions about the new Attributes infrastructure. LLVM Reference; --------------. ======================; Command Line Utilities; ======================. :doc:`LLVM Command Guide <CommandGuide/index>`; A reference manual for the LLVM command line utilities (""man"" pages for LLVM; tools). :doc:`Bugpoint`; Automatic bug finder and test-case reducer description and usage; information. :doc:`OptBisect`; A command line option for debugging optimization-induced failures. :doc:`SymbolizerMarkupFormat`; A reference for the log symbolizer markup accepted by ``llvm-symbolizer``. :doc:`The Microsoft PDB File Format <PDB/index>`; A detailed description of the Microsoft PDB (Program Database) file format. ==================; Garbage Collection; ==================. :doc:`GarbageCollection`; The interfaces source-language compilers should use for compiling GC'd; programs. :doc:`Statepoints`; This describes a set of experimental extensions for garbage; collection support. =========; LibFuzzer; =========. :doc:`LibFuzzer`; A library for writing in-process guided fuzzers. :doc:`FuzzingLLVM`; Information on writing and using Fuzzers to find bugs in LLVM. ========; LLVM IR; ========. :doc:`LLVM Language Reference Manual <LangRef>`; Defines the LLVM intermediate representation and the assembly form of the; different nodes. :doc:`InAlloca`; Description of the ``inalloca`` argument attribute. :doc:`BitCodeFormat`; This describ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Reference.rst:1477,optimiz,optimization-induced,1477,interpreter/llvm-project/llvm/docs/Reference.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Reference.rst,1,['optimiz'],['optimization-induced']
Performance,"; Large code bases will benefit from more advanced features such as; :doc:`pythonizations <pythonizations>` for a cleaner interface to clients;; precompiled modules for faster parsing and reduced memory usage;; "":ref:`dictionaries <dictionaries>`"" to package locations and manage; dependencies; and mapping files for automatic, lazy, loading.; You can, however, get very far with just the basics and it may even be; completely sufficient for small packages with fewer classes. cppyy works by parsing C++ definitions through ``cling``, generating tiny; wrapper codes to honor compile-time features and create standardized; interfaces, then compiling/linking those wrappers with the ``clang`` JIT.; It thus requires only those two ingredients: *C++ definitions* and; *linker symbols*.; All cppyy uses, the basic and the more advanced, are variations on the; theme of bringing these two together at the point of use. Definitions typically live in header files and symbols in libraries.; Headers can be loaded with ``cppyy.include`` and libraries with the; ``cppyy.load_library`` call.; Loading the header is sufficient to start exploring, with ``cppyy.gbl`` the; starting point of all things C++, while the linker symbols are only needed at ; the point of first use. Here is an example using the `zlib`_ library, which is likely available on; your system:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('zlib.h') # bring in C++ definitions; >>> cppyy.load_library('libz') # load linker symbols; >>> cppyy.gbl.zlibVersion() # use a zlib API; '1.2.11'; >>>. Since header files can include other header files, it is easy to aggregate; all relevant ones into a single header to include.; If there are project-specific include paths, you can add those paths through; ``cppyy.add_include_path``.; If a header is C-only and not set for use with C++, use ``cppyy.c_include``,; which adds ``extern ""C""`` around the header. Library files can be aggregated by linking all relevant ones to a single; li",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst:1112,load,loaded,1112,bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,1,['load'],['loaded']
Performance,"; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:303244,load,load,303244,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,; NAD; Digit separators following non-octal prefix; Clang 3.5. 1948; NAD; exception-specification of replacement global new; Clang 3.5. 1949; CD4; “sequenced after” instead of “sequenced before”; Unknown. 1950; NAD; Restructuring description of ranks of conversion sequences; Unknown. 1951; CD4; Cv-qualification and literal types; Unknown. 1952; CD4; Constant expressions and library undefined behavior; Unknown. 1953; open; Data races and common initial sequence; Not resolved. 1954; tentatively ready; typeid null dereference check in subexpressions; Unknown. 1955; CD4; #elif with invalid controlling expression; Unknown. 1956; CD4; Reuse of storage of automatic variables; Unknown. 1957; NAD; decltype(auto) with direct-list-initialization; Unknown. 1958; CD4; decltype(auto) with parenthesized initializer; Unknown. 1959; CD4; Inadvertently inherited copy constructor; Clang 3.9. 1960; NAD; Visibility of entity named in class-scope using-declaration; No. 1961; C++17; Potentially-concurrent actions within a signal handler; Unknown. 1962; open; Type of __func__; Not resolved. 1963; CD4; Implementation-defined identifier characters; Unknown. 1964; NAD; opaque-enum-declaration in alias-declaration?; Unknown. 1965; drafting; Explicit casts to reference types; Not resolved. 1966; CD4; Colon following enumeration elaborated-type-specifier; Clang 11. 1967; CD4; Temporary lifetime and move-elision; Unknown. 1968; NAD; Address of typeid in constant expressions; No. 1969; CD6; Missing exclusion of ~S as an ordinary function name; Unknown. 1970; NAD; Ambiguity resolution for (T())*x; Unknown. 1971; CD4; Unclear disambiguation of destructor and operator~; Unknown. 1972; CD6; Identifier character restrictions in non-identifiers; Unknown. 1973; DRWP; Which parameter-declaration-clause in a lambda-expression?; Unknown. 1974; NAD; Redundant specification of non-type typename-specifier; Unknown. 1975; CD4; Permissible declarations for exception-specifications; Unknown. 1976; NAD; Ambiguity ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html:133193,concurren,concurrent,133193,interpreter/llvm-project/clang/www/cxx_dr_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html,1,['concurren'],['concurrent']
Performance,"; Python functions can be used, including for dynamically constructing; classes. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. Function argument type conversions follow the expected rules, with implicit; conversions allowed, including between Python builtin types and STL types,; but it is rather more efficient to make conversions explicit. `Free functions`; ----------------. All bound C++ code starts off from the global C++ namespace, represented in; Python by ``gbl``.; This namespace, as any other namespace, is treated as a module after it has; been loaded.; Thus, we can directly import C++ functions from it and other namespaces that; themselves may contain more functions.; All lookups on namespaces are done lazily, thus if loading more headers bring; in more functions (incl. new overloads), these become available dynamically. .. code-block:: python. >>> from cppyy.gbl import global_function, Namespace; >>> global_function == Namespace.global_function; False; >>> from cppyy.gbl.Namespace import global_function; >>> global_function == Namespace.global_function; True; >>> from cppyy.gbl import global_function; >>>. Free functions can be bound to a class, following the same rules as apply to; Python functions: unless marked as static, they will turn into member; functions when bound to an instance, but act as static functions when called; through the class.; Consider this example:. .. code-block:: python. >>> from cppyy.gbl import Concrete, call_abstract_method; >>> c = Concrete(); >>> Concrete.callit = call_abstract_method; >>> Concrete.callit(c); called Concrete::abstract_method; >>> c.callit(); called Concrete::abstract_method; >>> Concrete.callit = staticmethod(call_abstract_method); >>> c.callit(); Trac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst:1111,load,loading,1111,bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,1,['load'],['loading']
Performance,"; TGeoVolume *module2 = TGeoVolume::Import(""file.root"", ""MOD2"");; top->AddNode(module1, 1, new TGeoTranslation(0,0,100));; top->AddNode(module2, 1, new TGeoTranslation(0,0,-100));; // One should close oneself the geometry; geom->CloseGeometry();; ```. ### GDML. Few lines above word GDML was used. GDML stands for **G**eometry; **D**escription **M**arkup **L**anguage. It is an application-independent; geometry description format based on XML. It is mainly used for geometry; interchange between ROOT and Geant4 framework. More details about this; project can be found http://gdml.web.cern.ch. This feature; (importing/exporting from/to gdml file format) is disabled by default in; ROOT installation. To enable this feature add `--enable-gdml` option to; `./configure` script call. ## Navigation Algorithms. This section will describe the main methods and algorithms used for; implementing the navigation features within the geometrical modeller.; This includes navigation queries at shape level, global geometrical; queries and optimization mechanisms. ### Finding the State Corresponding to a Location (x,y,z). For reminder, a geometry state is a ‘touchable' object in the geometry; hierarchy. It is represented by a path like: **/TOP\_1/A\_1/B\_3/C\_1**,; where **B\_3** for instance is a copy of volume **B** positioned inside; volume **A**. A state is always associated to a transformation matrix; **M** of the touchable with respect to the global reference frame; (obtained by piling-up all local transformations of nodes in the branch; with respect to their containers). The current state and the; corresponding global matrix are updated whenever the geometry depth is; modified. The global transformations corresponding to all nodes in the; current branch are kept in an array: (**MTOP\_1, MA\_1, MB\_3, ...**). ![Navigation in the geometry hierarchy](pictures/080001E6.png). The elementary operations for changing the state are:. ``` {.cpp}; TGeoManager::CdUp();; TGeoManager::CdDown(i);; TG",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:154763,optimiz,optimization,154763,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['optimiz'],['optimization']
Performance,"; The high 32 bits of the flat addressing shared aperture base.; Only used by GFX8 to allow conversion between shared segment; and flat addresses. See :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`. ""hidden_queue_ptr""; A global memory address space pointer to the ROCm runtime; ``struct amd_queue_t`` structure for the HSA queue of the; associated dispatch AQL packet. It is only required for pre-GFX9; devices for the trap handler ABI (see :ref:`amdgpu-amdhsa-trap-handler-abi`). ====================== ============== ========= ================================. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149274,queue,queue,149274,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"; The result of :ref:`freeze instruction <i_freeze>` is well defined regardless; of its operand. .. _blockaddress:. Addresses of Basic Blocks; -------------------------. ``blockaddress(@function, %block)``. The '``blockaddress``' constant computes the address of the specified; basic block in the specified function. It always has an ``ptr addrspace(P)`` type, where ``P`` is the address space; of the function containing ``%block`` (usually ``addrspace(0)``). Taking the address of the entry block is illegal. This value only has defined behavior when used as an operand to the; ':ref:`indirectbr <i_indirectbr>`' or for comparisons against null. Pointer; equality tests between labels addresses results in undefined behavior ---; though, again, comparison against null is ok, and no label is equal to the null; pointer. This may be passed around as an opaque pointer sized value as long as; the bits are not inspected. This allows ``ptrtoint`` and arithmetic to be; performed on these values so long as the original value is reconstituted before; the ``indirectbr`` instruction. Finally, some targets may provide defined semantics when using the value; as the operand to an inline assembly, but that is target specific. .. _dso_local_equivalent:. DSO Local Equivalent; --------------------. ``dso_local_equivalent @func``. A '``dso_local_equivalent``' constant represents a function which is; functionally equivalent to a given function, but is always defined in the; current linkage unit. The resulting pointer has the same type as the underlying; function. The resulting pointer is permitted, but not required, to be different; from a pointer to the function, and it may have different values in different; translation units. The target function may not have ``extern_weak`` linkage. ``dso_local_equivalent`` can be implemented as such:. - If the function has local linkage, hidden visibility, or is; ``dso_local``, ``dso_local_equivalent`` can be implemented as simply a pointer; to the function",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:200689,perform,performed,200689,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"; This operator produces a DAG node with the same arguments as *dag*, but with its; operator replaced with *op*. Example: ``!setdagop((foo 1, 2), bar)`` results in ``(bar 1, 2)``. ``!shl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* left logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!size(``\ *a*\ ``)``; This operator produces the size of the string, list, or dag *a*.; The size of a DAG is the number of arguments; the operator does not count. ``!sra(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right arithmetically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!srl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!strconcat(``\ *str1*\ ``,`` *str2*\ ``, ...)``; This operator concatenates the string arguments *str1*, *str2*, etc., and; produces the resulting string. ``!sub(``\ *a*\ ``,`` *b*\ ``)``; This operator subtracts *b* from *a* and produces the arithmetic difference. ``!subst(``\ *target*\ ``,`` *repl*\ ``,`` *value*\ ``)``; This operator replaces all occurrences of the *target* in the *value* with; the *repl* and produces the resulting value. The *value* can; be a string, in which case substring substitution is performed. The *value* can be a record name, in which case the operator produces the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *value*. ``!substr(``\ *string*\ ``,`` *start*\ [``,`` *length*]\ ``)``; This operator extracts a substring of the given *string*. The starting; position of the substring is specified by *start*, which can range; between 0 and the length o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:72085,perform,performed,72085,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performed']
Performance,"; [5] - JFPU0; [6] - JFPU1; [7] - JLAGU; [8] - JMul; [9] - JSAGU; [10] - JSTC; [11] - JVALU0; [12] - JVALU1; [13] - JVIMUL. Resource pressure per iteration:; [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13]; - - - 2.00 1.00 2.00 1.00 - - - - - - -. Resource pressure by instruction:; [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] Instructions:; - - - - 1.00 - 1.00 - - - - - - - vmulps	%xmm0, %xmm1, %xmm2; - - - 1.00 - 1.00 - - - - - - - - vhaddps	%xmm2, %xmm2, %xmm3; - - - 1.00 - 1.00 - - - - - - - - vhaddps	%xmm3, %xmm3, %xmm4. According to this report, the dot-product kernel has been executed 300 times,; for a total of 900 simulated instructions. The total number of simulated micro; opcodes (uOps) is also 900. The report is structured in three main sections. The first section collects a; few performance numbers; the goal of this section is to give a very quick; overview of the performance throughput. Important performance indicators are; **IPC**, **uOps Per Cycle**, and **Block RThroughput** (Block Reciprocal; Throughput). Field *DispatchWidth* is the maximum number of micro opcodes that are dispatched; to the out-of-order backend every simulated cycle. For processors with an; in-order backend, *DispatchWidth* is the maximum number of micro opcodes issued; to the backend every simulated cycle. IPC is computed dividing the total number of simulated instructions by the total; number of cycles. Field *Block RThroughput* is the reciprocal of the block throughput. Block; throughput is a theoretical quantity computed as the maximum number of blocks; (i.e. iterations) that can be executed per simulated clock cycle in the absence; of loop carried dependencies. Block throughput is superiorly limited by the; dispatch rate, and the availability of hardware resources. In the absence of loop-carried data dependencies, the observed IPC tends to a; theoretical maximum which can be computed by dividing the number of instructions; of a single iteration by the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:15786,perform,performance,15786,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance,"; ``CHECK:`` directives only accept lines corresponding to the body of the; ``@C_ctor_base`` function, even if the patterns match lines found later in; the file. Furthermore, if one of these three ``CHECK:`` directives fail,; FileCheck will recover by continuing to the next block, allowing multiple test; failures to be detected in a single invocation. There is no requirement that ``CHECK-LABEL:`` directives contain strings that; correspond to actual syntactic labels in a source or output language: they must; simply uniquely match a single line in the file being verified. ``CHECK-LABEL:`` directives cannot contain variable definitions or uses. Directive modifiers; ~~~~~~~~~~~~~~~~~~~. A directive modifier can be append to a directive by following the directive; with ``{<modifier>}`` where the only supported value for ``<modifier>`` is; ``LITERAL``. The ``LITERAL`` directive modifier can be used to perform a literal match. The; modifier results in the directive not recognizing any syntax to perform regex; matching, variable capture or any substitutions. This is useful when the text; to match would require excessive escaping otherwise. For example, the; following will perform literal matches rather than considering these as; regular expressions:. .. code-block:: text. Input: [[[10, 20]], [[30, 40]]]; Output %r10: [[10, 20]]; Output %r10: [[30, 40]]. ; CHECK{LITERAL}: [[[10, 20]], [[30, 40]]]; ; CHECK-DAG{LITERAL}: [[30, 40]]; ; CHECK-DAG{LITERAL}: [[10, 20]]. FileCheck Regex Matching Syntax; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. All FileCheck directives take a pattern to match.; For most uses of FileCheck, fixed string matching is perfectly sufficient. For; some things, a more flexible form of matching is desired. To support this,; FileCheck allows you to specify regular expressions in matching strings,; surrounded by double braces: ``{{yourregex}}``. FileCheck implements a POSIX; regular expression matcher; it supports Extended POSIX regular expressions; (ERE). Because we w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:24070,perform,perform,24070,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['perform'],['perform']
Performance,"; ``RegisterRegAlloc::FunctionPassCtor``. In the same file add the ""installing""; declaration, in the form:. .. code-block:: c++. static RegisterRegAlloc myRegAlloc(""myregalloc"",; ""my register allocator help string"",; createMyRegisterAllocator);. Note the two spaces prior to the help string produces a tidy result on the; :option:`-help` query. .. code-block:: console. $ llc -help; ...; -regalloc - Register allocator to use (default=linearscan); =linearscan - linear scan register allocator; =local - local register allocator; =simple - simple register allocator; =myregalloc - my register allocator help string; ... And that's it. The user is now free to use ``-regalloc=myregalloc`` as an; option. Registering instruction schedulers is similar except use the; ``RegisterScheduler`` class. Note that the; ``RegisterScheduler::FunctionPassCtor`` is significantly different from; ``RegisterRegAlloc::FunctionPassCtor``. To force the load/linking of your register allocator into the; :program:`llc`/:program:`lli` tools, add your creator function's global; declaration to ``Passes.h`` and add a ""pseudo"" call line to; ``llvm/Codegen/LinkAllCodegenComponents.h``. Creating new registries; -----------------------. The easiest way to get started is to clone one of the existing registries; we; recommend ``llvm/CodeGen/RegAllocRegistry.h``. The key things to modify are; the class name and the ``FunctionPassCtor`` type. Then you need to declare the registry. Example: if your pass registry is; ``RegisterMyPasses`` then define:. .. code-block:: c++. MachinePassRegistry<RegisterMyPasses::FunctionPassCtor> RegisterMyPasses::Registry;. And finally, declare the command line option for your passes. Example:. .. code-block:: c++. cl::opt<RegisterMyPasses::FunctionPassCtor, false,; RegisterPassParser<RegisterMyPasses> >; MyPassOpt(""mypass"",; cl::init(&createDefaultMyPass),; cl::desc(""my pass option help""));. Here the command option is ""``mypass``"", with ``createDefaultMyPass`` as the; default creator",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:50918,load,load,50918,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['load']
Performance,"; ```. This will collapse the register to zero or one, and everything but the offset; in the addressing mode to be less than or equal to 9. This means the full; address can only be guaranteed to be less than `(1 << 31) + 9`. The OS may wish; to protect an extra page of the low address space to account for this. ##### Optimizations. A very large portion of the cost for this approach comes from checking loads in; this way, so it is important to work to optimize this. However, beyond making; the instruction sequences to *apply* the checks efficient (for example by; avoiding `pushfq` and `popfq` sequences), the only significant optimization is; to check fewer loads without introducing a vulnerability. We apply several; techniques to accomplish that. ###### Don't check loads from compile-time constant stack offsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:35125,optimiz,optimization,35125,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['optimiz'],['optimization']
Performance,"; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:269923,cache,cache,269923,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"; and flat addresses. See :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`. ""hidden_queue_ptr""; A global memory address space pointer to the ROCm runtime; ``struct amd_queue_t`` structure for the HSA queue of the; associated dispatch AQL packet. It is only required for pre-GFX9; devices for the trap handler ABI (see :ref:`amdgpu-amdhsa-trap-handler-abi`). ====================== ============== ========= ================================. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149420,queue,queues,149420,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previously learnt) nor restart the learning phase.; The learning phase is restarted when a new cache is created, e.g. after having; removed a cache with SetCacheSize(0). ### TSelectorDraw. The axis titles in case of a `x:y:z` plot with the option `COLZ` were not correct. ### TParallelCoordVar. Change the format used to print the variables limit for ||-Coord to `%g`. It was; `%6.4f` before. ## Histogram Libraries. ### TFormula. - New version of the TFormula class based on Cling. Formula expressions are now used to create functions which are passed to Cling to be Just In Time compiled.; The expression is therefore compiled using Clang/LLVVM which will give execution time as compiled code and in addition correctness of the result obtained.; - This class is not 100% backward compatible with the old TFormula class, which is still available in ROOT as =ROOT::v5::TFormula=.; Some of the TFormula member funtions available in version 5, such as =Analyze= and =AnalyzeFun",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:14739,cache,cached,14739,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cached']
Performance,"; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local. 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); Must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:375875,load,load,375875,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"; are, above all, a structuring technique for compiler code. All LLVM passes are subclasses of the `Pass; <https://llvm.org/doxygen/classllvm_1_1Pass.html>`_ class, which implement; functionality by overriding virtual methods inherited from ``Pass``. Depending; on how your pass works, you should inherit from the :ref:`ModulePass; <writing-an-llvm-pass-ModulePass>` , :ref:`CallGraphSCCPass; <writing-an-llvm-pass-CallGraphSCCPass>`, :ref:`FunctionPass; <writing-an-llvm-pass-FunctionPass>` , or :ref:`LoopPass; <writing-an-llvm-pass-LoopPass>`, or :ref:`RegionPass; <writing-an-llvm-pass-RegionPass>` classes, which gives the system more; information about what your pass does, and how it can be combined with other; passes. One of the main features of the LLVM Pass Framework is that it; schedules passes to run in an efficient way based on the constraints that your; pass meets (which are indicated by which class they derive from). We start by showing you how to construct a pass, everything from setting up the; code, to compiling, loading, and executing it. After the basics are down, more; advanced features are discussed. .. warning::; This document deals with the legacy pass manager. LLVM uses the new pass; manager for the optimization pipeline (the codegen pipeline; still uses the legacy pass manager), which has its own way of defining; passes. For more details, see :doc:`WritingAnLLVMNewPMPass` and; :doc:`NewPassManager`. Quick Start --- Writing hello world; ===================================. Here we describe how to write the ""hello world"" of passes. The ""Hello"" pass is; designed to simply print out the name of non-external functions that exist in; the program being compiled. It does not modify the program at all, it just; inspects it. The source code and files for this pass are available in the LLVM; source tree in the ``lib/Transforms/Hello`` directory. .. _writing-an-llvm-pass-makefile:. Setting up the build environment; --------------------------------. First, confi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:1525,load,loading,1525,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['loading']
Performance,"; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after any; preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; sc0=1 sc1=1; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic sc0=1; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:310408,load,load,310408,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic sc0=1; load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic sc1=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic monotonic - workgroup - global 1. buffer/global/flat_store; - generic sc0=1; store atomic monotonic - agent - global 1. buffer/global/flat_store; - generic sc1=1; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic sc1=1;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:294247,load,load,294247,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:299526,load,loads,299526,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<Memo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12594,cache,cache,12594,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['cache'],['cache']
Performance,"; check at the level of a single volume by using `option`=""`d`"" or; `option`=""`d<number>`"" to perform overlap checking by sampling the; volume with \<`number`\> random points (default 1 million). This; produces also a picture showing in red the overlapping region and; estimates the volume of the overlaps. An extrusion A) is declared in any of the following cases:. - At least one of the vertices of the daughter mesh representation is; outside the mother volume (in fact its shape) and having a safety; distance to the mother greater than the desired value;; - At least one of the mother vertices is contained also by one of its; daughters, in the same conditions. An overlap B) is declared if:. - At least one vertex of a positioned volume mesh is contained (having; a safety bigger than the accepted maximum value) by other positioned; volume inside the same container. The check is performed also by; inverting the candidates. The code is highly optimized to avoid checking candidates that are far; away in space by performing a fast check on their bounding boxes. Once; the checking tool is fired-up inside a volume or at top level, the list; of overlaps (visible as Illegal overlaps inside a **`TBrowser`**) held; by the manager class will be filled with **`TGeoOverlap`** objects; containing a full description of the detected overlaps. The list is; sorted in the decreasing order of the overlapping distance, extrusions; coming first. An overlap object name represents the full description of; the overlap, containing both candidate node names and a letter; (x-extrusion, o-overlap) representing the type. Double-clicking an; overlap item in a **`TBrowser`** produces a picture of the overlap; containing only the two overlapping nodes (one in blue and one in green); and having the critical vertices represented by red points. The picture; can be rotated/zoomed or drawn in X3d as any other view. Calling; `gGeoManager->PrintOverlaps()` prints the list of overlaps. ### Graphical Checking M",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:134240,optimiz,optimized,134240,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,2,"['optimiz', 'perform']","['optimized', 'performing']"
Performance,"; completed before; performing the; store that is being; released. 2. GFX940, GFX941; buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store; sc0=1; store atomic release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_store; store atomic release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 3. GFX940, GFX941; buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store; sc1=1; store atomic release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after any; preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:308454,load,load,308454,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; details.**. #### Creating / Obtaining Viewer Handle. External viewers are bound to a **`TPad`** object (this may be removed; as a requirement in the future). You can create or obtain the current; viewer handle via the method:. ``` {.cpp}; TVirtualViewer3D * v = gPad->GetViewer3D(""type"");; ```. Here the ""type"" string defines the viewer type - currently one of:. - ""`ogl`"" : External GL viewer. - ""`x3d`"": External X3D viewer. - ""`pad`"": Pad viewer. If no type is passed (null string), and there is no current viewer, then; the type is defaulted to ""`pad`"". If no type is passed and there is a; current viewer, then this is returned - hence once a viewer is created; it can be obtained elsewhere by:. ``` {.cpp}; TVirtualViewer3D * v = gPad->GetViewer3D();; ```. #### Opening / Closing Scenes. Objects must be added to viewer between `BeginScene()` and `EndScene()`; calls e.g. ``` {.cpp}; viewer->BeginScene();; // Add objects; viewer ->EndScene();; ```. These calls enable the viewer to suspend redraws, and perform internal; caching/setup. If the object you attach to the pad derives from; **`TAtt3D`**, then the pad will take responsibility for calling; `BeginScene()` and `EndScene()` for you. You can always test if the; scene is already open for object addition with:. ``` {.cpp}; viewer->BuildingScene();; ```. ![Overview of 3D viewer architecture](pictures/030000DF.png). Note: the x3d viewer does not support rebuilding of scenes - objects; added after the first Open/Close Scene pair will be ignored. #### Describing Objects - Filling TBuffer3D. The viewers behind the **`TVirtualViewer3D`** interface differ greatly; in their capabilities e.g. - Some support native shape (e.g. spheres/tubes in OpenGL) and can; draw these based on an abstract description. Others always require a; tessellation description based on **`TBuffer3D`**'s `kRaw` /; `kRawSizes` points/lines/segments sections. - Some need the 3D object positions in the master (world) frame,; others can cope with local frames",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:125702,perform,perform,125702,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['perform'],['perform']
Performance,"; each; level includes all the guarantees of the previous level except for; Acquire/Release. (See also `LangRef Ordering <LangRef.html#ordering>`_.). .. _NotAtomic:. NotAtomic; ---------. NotAtomic is the obvious, a load or store which is not atomic. (This isn't; really a level of atomicity, but is listed here for comparison.) This is; essentially a regular load or store. If there is a race on a given memory; location, loads from that location return undef. Relevant standard; This is intended to match shared variables in C/C++, and to be used in any; other context where memory access is necessary, and a race is impossible. (The; precise definition is in `LangRef Memory Model <LangRef.html#memmodel>`_.). Notes for frontends; The rule is essentially that all memory accessed with basic loads and stores; by multiple threads should be protected by a lock or other synchronization;; otherwise, you are likely to run into undefined behavior. If your frontend is; for a ""safe"" language like Java, use Unordered to load and store any shared; variable. Note that NotAtomic volatile loads and stores are not properly; atomic; do not try to use them as a substitute. (Per the C/C++ standards,; volatile does provide some limited guarantees around asynchronous signals, but; atomics are generally a better solution.). Notes for optimizers; Introducing loads to shared variables along a codepath where they would not; otherwise exist is allowed; introducing stores to shared variables is not. See; `Optimization outside atomic`_. Notes for code generation; The one interesting restriction here is that it is not allowed to write to; bytes outside of the bytes relevant to a store. This is mostly relevant to; unaligned stores: it is not allowed in general to convert an unaligned store; into two aligned stores of the same width as the unaligned store. Backends are; also expected to generate an i8 store as an i8 store, and not an instruction; which writes to surrounding bytes. (If you are writing a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:6808,load,load,6808,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['load']
Performance,"; editor. Then, it scans all object base classes searching the; corresponding object editors. When it finds one, it makes an instance of; the base class editor too. Once the object editor is in place, it sets the user interface elements; according to the object's status. After that, it is ready to interact; with the object following the user actions. The graphics editor gives an intuitive way to edit objects in a canvas; with immediate feedback. Complexity of some object editors is reduced by; hiding GUI elements and revealing them only on users' requests. An object in the canvas is selected by clicking on it with the left; mouse button. Its name is displayed on the top of the editor frame in; red color. If the editor frame needs more space than the canvas window,; a vertical scroll bar appears for easy navigation. \image html ged.png width=800px. **Histogram, pad and axis editors**. ### Editor Design Elements. The next rules describe the path to follow when creating your own object; editor that will be recognized and loaded by the graphics editor in; ROOT, i.e. it will be included as a part of it. (a) Derive the code of your object editor from the base editor class; **`TGedFrame`**. (b) Keep the correct naming convention: the name of the object editor; should be the object class name concatenated with the word `‘Editor'`. (c) Provide a default constructor. (d) Use the signals/slots communication mechanism for event processing. (e) Implement the virtual method `SetModel(TObject *obj)` where all; widgets are set with the current object's attributes. This method is; called when the editor receives a signal from the canvas saying that an; object is the selected. (f) Implement all necessary slots and connect them to appropriate; signals that GUI widgets send out. The GUI classes in ROOT are developed; to emit signals whenever they change a state that others might be; interested. As we noted already, the signals/slots communication; mechanism allows total independence of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/ged/doc/index.md:3472,load,loaded,3472,gui/ged/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/ged/doc/index.md,1,['load'],['loaded']
Performance,"; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symbolic resolution; using the JIT symbol tables should be preferred: it keeps the IR and objects; readable and reusable in subsequent JIT sessions. Hardcoded addresses are; difficult to read, and usually only good for one session. Roadmap; =======. ORC is still undergoing active development. Some current and future works are; listed below. Current Work; ------------. 1. **TargetProcessControl: Improvements to in-tree support for out-of-process; execution**. The ``TargetProcessControl`` API provides various operations on the JIT; target process (the one which will execute the JIT'd code), including; memory allocation, memory writes, function execution, and process queries; (e.g. for the target triple). By targeting this API new components can be; developed which will work equally well for in-process and out-of-process; JITing. 2. **OR",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:34383,load,loadModule,34383,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['load'],['loadModule']
Performance,"; endif(). # if CMAKE_CXX_STANDARD is still set after the cache unset above it means that the user requested it; # and we allow it to be set to something newer than the required standard but otherwise we fail.; if(DEFINED CMAKE_CXX_STANDARD AND CMAKE_CXX_STANDARD LESS ${LLVM_REQUIRED_CXX_STANDARD}); message(FATAL_ERROR ""Requested CMAKE_CXX_STANDARD=${CMAKE_CXX_STANDARD} which is less than the required ${LLVM_REQUIRED_CXX_STANDARD}.""); endif(). set(CMAKE_CXX_STANDARD ${LLVM_REQUIRED_CXX_STANDARD} CACHE STRING ""C++ standard to conform to""); set(CMAKE_CXX_STANDARD_REQUIRED YES). if (CYGWIN); # Cygwin is a bit stricter and lack things like 'strdup', 'stricmp', etc in; # c++xx mode.; set(CMAKE_CXX_EXTENSIONS YES); else(); set(CMAKE_CXX_EXTENSIONS NO); endif(). if (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES); message(FATAL_ERROR ""; No build type selected. You need to pass -DCMAKE_BUILD_TYPE=<type> in order to configure LLVM.; Available options are:; * -DCMAKE_BUILD_TYPE=Release - For an optimized build with no assertions or debug info.; * -DCMAKE_BUILD_TYPE=Debug - For an unoptimized build with assertions and debug info.; * -DCMAKE_BUILD_TYPE=RelWithDebInfo - For an optimized build with no assertions but with debug info.; * -DCMAKE_BUILD_TYPE=MinSizeRel - For a build optimized for size instead of speed.; Learn more about these options in our documentation at https://llvm.org/docs/CMake.html#cmake-build-type; ""); endif(). # Set default build type for cmake's try_compile module.; # CMake 3.17 or newer sets CMAKE_DEFAULT_BUILD_TYPE to one of the; # items from CMAKE_CONFIGURATION_TYPES. Logic below can be further; # simplified once LLVM's minimum CMake version is updated to 3.17.; if(CMAKE_DEFAULT_BUILD_TYPE); set(CMAKE_TRY_COMPILE_CONFIGURATION ${CMAKE_DEFAULT_BUILD_TYPE}); else(); if(CMAKE_CONFIGURATION_TYPES); list(GET CMAKE_CONFIGURATION_TYPES 0 CMAKE_TRY_COMPILE_CONFIGURATION); elseif(CMAKE_BUILD_TYPE); set(CMAKE_TRY_COMPILE_CONFIGURATION ${CMAKE_BUILD_TYPE});",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:4021,optimiz,optimized,4021,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['optimiz'],['optimized']
Performance,"; filling in; ``COMPUTE_PGM_RSRC1.CDBG_USER``.; 26 1 bit FP16_OVFL GFX6-GFX8; Reserved, must be 0.; GFX9-GFX11; Wavefront starts execution; with specified fp16 overflow; mode. - If 0, fp16 overflow generates; +/-INF values.; - If 1, fp16 overflow that is the; result of an +/-INF input value; or divide by 0 produces a +/-INF,; otherwise clamps computed; overflow to +/-MAX_FP16 as; appropriate. Used by CP to set up; ``COMPUTE_PGM_RSRC1.FP16_OVFL``.; 28:27 2 bits Reserved, must be 0.; 29 1 bit WGP_MODE GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; - If 0 execute work-groups in; CU wavefront execution mode.; - If 1 execute work-groups on; in WGP wavefront execution mode. See :ref:`amdgpu-amdhsa-memory-model`. Used by CP to set up; ``COMPUTE_PGM_RSRC1.WGP_MODE``.; 30 1 bit MEM_ORDERED GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; Controls the behavior of the; s_waitcnt's vmcnt and vscnt; counters. - If 0 vmcnt reports completion; of load and atomic with return; out of order with sample; instructions, and the vscnt; reports the completion of; store and atomic without; return in order.; - If 1 vmcnt reports completion; of load, atomic with return; and sample instructions in; order, and the vscnt reports; the completion of store and; atomic without return in order. Used by CP to set up; ``COMPUTE_PGM_RSRC1.MEM_ORDERED``.; 31 1 bit FWD_PROGRESS GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; - If 0 execute SIMD wavefronts; using oldest first policy.; - If 1 execute SIMD wavefronts to; ensure wavefronts will make some; forward progress. Used by CP to set up; ``COMPUTE_PGM_RSRC1.FWD_PROGRESS``.; 32 **Total size 4 bytes**; ======= ===================================================================================================================. .. .. table:: compute_pgm_rsrc2 for GFX6-GFX12; :name: amdgpu-amdhsa-compute_pgm_rsrc2-gfx6-gfx12-table. ======= ======= =============================== ===========================================================================; Bits Siz",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:169598,load,load,169598,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"; find prebuilt -name ""*.pcm""; # prebuilt/1AYBIGPM8R2GA/A-3L1K4LUA6O31.pcm; # prebuilt/1AYBIGPM8R2GA/B-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/A-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -DENABLE_A. Finally we want to allow implicit modules for configurations that were not prebuilt. When using the clang driver a module cache path is implicitly selected. Using ``-cc1``, we simply add use the ``-fmodules-cache-path`` option. .. code-block:: sh. clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A -DOTHER_OPTIONS. This way, a single directory containing multiple variants of modules can be prepared and reused. The options configuring the module cache are independent of other options. Module Semantics; ================. Modules are modeled as if each submodule were a separate translation unit, and a module import makes names from the other translation unit visible. Each submodule starts with a new preprocessor state and an empty translation unit. .. note::. This behavior is currently only approximated when building a module with submodules. Entities within a submodule that has already been built are visible when building later submodules in that module. This can lead to fragile modules that depend on the build order used for the submodules of the module, and should",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:23696,cache,cache-path,23696,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,2,['cache'],"['cache', 'cache-path']"
Performance,"; global, which is known to be somewhere outside the module. Globals; with ``available_externally`` linkage are allowed to be discarded at; will, and allow inlining and other optimizations. This linkage type is; only allowed on definitions, not declarations.; ``linkonce``; Globals with ""``linkonce``"" linkage are merged with other globals of; the same name when linkage occurs. This can be used to implement; some forms of inline functions, templates, or other code which must; be generated in each translation unit that uses it, but where the; body may be overridden with a more definitive definition later.; Unreferenced ``linkonce`` globals are allowed to be discarded. Note; that ``linkonce`` linkage does not actually allow the optimizer to; inline the body of this function into callers because it doesn't; know if this definition of the function is the definitive definition; within the program or whether it will be overridden by a stronger; definition. To enable inlining and other optimizations, use; ""``linkonce_odr``"" linkage.; ``weak``; ""``weak``"" linkage has the same merging semantics as ``linkonce``; linkage, except that unreferenced globals with ``weak`` linkage may; not be discarded. This is used for globals that are declared ""weak""; in C source code.; ``common``; ""``common``"" linkage is most similar to ""``weak``"" linkage, but they; are used for tentative definitions in C, such as ""``int X;``"" at; global scope. Symbols with ""``common``"" linkage are merged in the; same way as ``weak symbols``, and they may not be deleted if; unreferenced. ``common`` symbols may not have an explicit section,; must have a zero initializer, and may not be marked; ':ref:`constant <globalvars>`'. Functions and aliases may not have; common linkage. .. _linkage_appending:. ``appending``; ""``appending``"" linkage may only be applied to global variables of; pointer to array type. When two global variables with appending; linkage are linked together, the two global arrays are appended; togethe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:9319,optimiz,optimizations,9319,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:300779,load,load,300779,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:302089,cache,caches,302089,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"; invocation of ""loadHTMLString:baseURL:"".; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>isClassMessage; Returns true when the Objective-C message is sent to a class. Example; matcher = objcMessageExpr(isClassMessage()); matches; [NSString stringWithFormat:@""format""];; but not; NSString *x = @""hello"";; [x containsString:@""h""];. Matcher<ObjCMessageExpr>isInstanceMessage; Returns true when the Objective-C message is sent to an instance. Example; matcher = objcMessageExpr(isInstanceMessage()); matches; NSString *x = @""hello"";; [x containsString:@""h""];; but not; [NSString stringWithFormat:@""format""];. Matcher<ObjCMessageExpr>matchesSelectorStringRef RegExp, Regex::RegexFlags Flags = NoFlags; Matches ObjC selectors whose name contains; a substring matched by the given RegExp.; matcher = objCMessageExpr(matchesSelector(""loadHTMLStringmatches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTMLString:html baseURL:NULL];. If the matcher is used in clang-query, RegexFlags parameter; should be passed as a quoted string. e.g: ""NoFlags"".; Flags can be combined with '|' example ""IgnoreCase | BasicRegex"". Matcher<ObjCMessageExpr>numSelectorArgsunsigned N; Matches when the selector has the specified number of arguments. matcher = objCMessageExpr(numSelectorArgs(0));; matches self.bodyView in the code below. matcher = objCMessageExpr(numSelectorArgs(2));; matches the invocation of ""loadHTMLString:baseURL:"" but not that; of self.bodyView; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMethodDecl>isClassMethod; Returns true when the Objective-C method declaration is a class method. Example; matcher = objcMethodDecl(isClassMethod()); matches; @interface I + (void)foo; @end; but not; @interface I - (void)bar; @end. Matcher<ObjCMethodDecl>isDefinition; Matches if a declaration has a body attached. Example matches A, va, fa; class A {};; class B; // Doesn't match, as it has no b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:108123,load,loadHTMLString,108123,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,1,['load'],['loadHTMLString']
Performance,"; know that ``foo`` and ``test`` were not mutually recursive for the poll to be; redundant. In practice, you'd probably want to your poll definition to contain; a conditional branch of some form. At the moment, PlaceSafepoints can insert safepoint polls at method entry and; loop backedges locations. Extending this to work with return polls would be; straight forward if desired. PlaceSafepoints includes a number of optimizations to avoid placing safepoint; polls at particular sites unless needed to ensure timely execution of a poll; under normal conditions. PlaceSafepoints does not attempt to ensure timely; execution of a poll under worst case conditions such as heavy system paging. The implementation of a safepoint poll action is specified by looking up a; function of the name ``gc.safepoint_poll`` in the containing Module. The body; of this function is inserted at each poll site desired. While calls or invokes; inside this method are transformed to a ``gc.statepoints``, recursive poll; insertion is not performed. This pass is useful for any language frontend which only has to support; garbage collection semantics at safepoints. If you need other abstract; frame information at safepoints (e.g. for deoptimization or introspection),; you can insert safepoint polls in the frontend. If you have the later case,; please ask on llvm-dev for suggestions. There's been a good amount of work; done on making such a scheme work well in practice which is not yet documented; here. Supported Architectures; =======================. Support for statepoint generation requires some code for each backend.; Today, only Aarch64 and X86_64 are supported. .. _OpenWork:. Limitations and Half Baked Ideas; ================================. Mixing References and Raw Pointers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Support for languages which allow unmanaged pointers to garbage collected; objects (i.e. pass a pointer to an object to a C routine) in the abstract; machine model. At the moment, the be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:33334,perform,performed,33334,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['perform'],['performed']
Performance,"; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local. 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); Must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not nee",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:375350,load,load,375350,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-at",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:302177,load,load,302177,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw tha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:270006,load,load,270006,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atom",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:365117,perform,performing,365117,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"; number, and each offset, interpreted as a signed number, does not wrap the; unsigned address space and remains *in bounds* of the allocated object.; As a corollary, if the added offset is non-negative, the addition does not; wrap in an unsigned sense (``nuw``).; * In cases where the base is a vector of pointers, the ``inbounds`` keyword; applies to each of the computations element-wise. Note that ``getelementptr`` with all-zero indices is always considered to be; ``inbounds``, even if the base pointer does not point to an allocated object.; As a corollary, the only pointer in bounds of the null pointer in the default; address space is the null pointer itself. These rules are based on the assumption that no allocated object may cross; the unsigned address space boundary, and no allocated object may be larger; than half the pointer index type space. If the ``inrange`` keyword is present before any index, loading from or; storing to any pointer derived from the ``getelementptr`` has undefined; behavior if the load or store would access memory outside of the bounds of; the element selected by the index marked as ``inrange``. The result of a; pointer comparison or ``ptrtoint`` (including ``ptrtoint``-like operations; involving memory) involving a pointer derived from a ``getelementptr`` with; the ``inrange`` keyword is undefined, with the exception of comparisons; in the case where both operands are in the range of the element selected; by the ``inrange`` keyword, inclusive of the address one past the end of; that element. Note that the ``inrange`` keyword is currently only allowed; in constant ``getelementptr`` expressions. The getelementptr instruction is often confusing. For some more insight; into how it works, see :doc:`the getelementptr FAQ <GetElementPtr>`. Example:; """""""""""""""". .. code-block:: llvm. %aptr = getelementptr {i32, [12 x i8]}, ptr %saptr, i64 0, i32 1; %vptr = getelementptr {i32, <2 x i8>}, ptr %svptr, i64 0, i32 1, i32 1; %eptr = getelementptr [12 x i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:438913,load,loading,438913,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],"['load', 'loading']"
Performance,"; numeric string value for how many instructions should be in the function before; it gets instrumented. .. code-block:: llvm. define i32 @maybe_instrument() uwtable ""xray-instruction-threshold""=""2"" {; ; ...; }. Special Case File; -----------------. Attributes can be imbued through the use of special case files instead of; adding them to the original source files. You can use this to mark certain; functions and classes to be never, always, or instrumented with first-argument; logging from a file. The file's format is described below:. .. code-block:: bash. # Comments are supported; [always]; fun:always_instrument; fun:log_arg1=arg1 # Log the first argument for the function. [never]; fun:never_instrument. These files can be provided through the ``-fxray-attr-list=`` flag to clang.; You may have multiple files loaded through multiple instances of the flag. XRay Runtime Library; --------------------. The XRay Runtime Library is part of the compiler-rt project, which implements; the runtime components that perform the patching and unpatching of inserted; instrumentation points. When you use ``clang`` to link your binaries and the; ``-fxray-instrument`` flag, it will automatically link in the XRay runtime. The default implementation of the XRay runtime will enable XRay instrumentation; before ``main`` starts, which works for applications that have a short; lifetime. This implementation also records all function entry and exit events; which may result in a lot of records in the resulting trace. Also by default the filename of the XRay trace is ``xray-log.XXXXXX`` where the; ``XXXXXX`` part is randomly generated. These options can be controlled through the ``XRAY_OPTIONS`` environment; variable, where we list down the options and their defaults below. +-------------------+-----------------+---------------+------------------------+; | Option | Type | Default | Description |; +===================+=================+===============+========================+; | patch_premain | ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRay.rst:4692,perform,perform,4692,interpreter/llvm-project/llvm/docs/XRay.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRay.rst,1,['perform'],['perform']
Performance,"; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also does something similar to constpool load:; LPCRELL0:; ldr r0, LCPI1_0; =>; ldr r0, pc, #((LCPI1_0-(LPCRELL0+4))&0xfffffffc). //===---------------------------------------------------------------------===//. We compile the following:. define i16 @func_entry_2E_ce(i32 %i) {; switch i32 %i, label %bb12.exitStub [; i32 0, label %bb4.exitStub; i32 1, label %bb9.exitStub; i32 2, label %bb4.exitStub; i32 3, label %bb4.exitStub; i32 7, label %bb9.exitStub; i32 8, label %bb.exitStub; i32 9, label %bb9.exitStub; ]. bb12.exitStub:; ret i16 0. bb4.exitStub:; ret i16 1. bb9.exitStub:; ret i16 2. bb.exitStub:; ret i16 3; }. into:. _func_entry_2E_ce:; mov r2, #1; lsl r2, r0; cmp r0, #9; bhi LBB1_4 @bb12.exitStub; LBB1_1: @newFuncRoot; mov r1, #13; tst r2, r1; bne LBB1_5 @bb4.exitStub; LBB1_2: @newFuncRoot; ldr r1, LCPI1_0; tst r2, r1; bne LBB1_6 @bb9.exitStub; LBB1_3: @newFuncRoot; mov r1, #1; lsl r1, r1, #8; tst r2, r1; bne LBB1_7 @bb.exitStub; LBB1_4: @bb12.exitStub; mov r0, #0; bx lr; LBB1_5: @bb4.exitStub; mov r0, #1; bx lr; LBB1_6: @bb9.exitStub",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:2034,load,load,2034,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,1,['load'],['load']
Performance,"; option(LLVM_ENABLE_LLVM_LIBC ""Set to on to link all LLVM executables against LLVM libc, assuming it is accessible by the host compiler."" OFF); option(LLVM_STATIC_LINK_CXX_STDLIB ""Statically link the standard library."" OFF); option(LLVM_ENABLE_LLD ""Use lld as C and C++ linker."" OFF); option(LLVM_ENABLE_PEDANTIC ""Compile with pedantic enabled."" ON); option(LLVM_ENABLE_WERROR ""Fail and stop if a warning is triggered."" OFF). option(LLVM_ENABLE_DUMP ""Enable dump functions even when assertions are disabled"" OFF); option(LLVM_UNREACHABLE_OPTIMIZE ""Optimize llvm_unreachable() as undefined behavior (default), guaranteed trap when OFF"" ON). if( NOT uppercase_CMAKE_BUILD_TYPE STREQUAL ""DEBUG"" ); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" OFF); else(); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" ON); endif(). option(LLVM_ENABLE_EXPENSIVE_CHECKS ""Enable expensive checks"" OFF). # While adding scalable vector support to LLVM, we temporarily want to; # allow an implicit conversion of TypeSize to uint64_t, and to allow; # code to get the fixed number of elements from a possibly scalable vector.; # This CMake flag enables a more strict mode where it asserts that the type; # is not a scalable vector type.; #; # Enabling this flag makes it easier to find cases where the compiler makes; # assumptions on the size being 'fixed size', when building tests for; # SVE/SVE2 or other scalable vector architectures.; option(LLVM_ENABLE_STRICT_FIXED_SIZE_VECTORS; ""Enable assertions that type is not scalable in implicit conversion from TypeSize to uint64_t and calls to getNumElements"" OFF). set(LLVM_ABI_BREAKING_CHECKS ""WITH_ASSERTS"" CACHE STRING; ""Enable abi-breaking checks. Can be WITH_ASSERTS, FORCE_ON or FORCE_OFF.""). option(LLVM_FORCE_USE_OLD_TOOLCHAIN; ""Set to ON to force using an old, unsupported host toolchain."" OFF). set(LLVM_LOCAL_RPATH """" CACHE FILEPATH; ""If set, an absolute path added as rpath on binaries that do not already contain an executable-relative rpath.""). option",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:25491,scalab,scalable,25491,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,2,['scalab'],['scalable']
Performance,"; particular some code that used to run with CINT will either issue new; warnings or new compilation errors. For example when CINT was parsing; Namespace::%Symbol it would not only apply the C++ search rules but also; search in the outer scopes and for this example could actually return; ::%Symbol instead of (as Cling now does) issuing a compilation error. #### Template class names; Cling no longer supports refering to a class template instantiation of a; class template that has all default template parameter without the \<\>.; With:. ``` {.cpp}; template <typename T = int> class templt {};; ```. With Cling (and any standard compliant compiler), using `*templt<>*` is; allowed (but `*templt*` is not). #### Namespace prefix of template parameters; Given `namespace N { class A; template <typename T> class B;}`, the name; `N::B<N::A>` is no longer ""shortened"" to `N::B<A>`. This affects the forward; and backward compatibility of files. #### Implicit dynamic up-casts; CINT would perform automatic upcasts to derived classes under certain contexts:. ``` {.cpp}; TH1* h1 = hpx; TH1F* h1f = h1;; ```. Cling does not allow this anymore. We might add this feature later if demand exists ([ROOT-4802](https://sft.its.cern.ch/jira/browse/ROOT-4802)). #### Using symbols that are only available at runtime: load libFoo; foo(); CINT was processing macros line by line; Cling compiles code.; When calling a function (or in general using a symbol) that is provided by a library loaded at runtime,; Cling will in some cases report an unresolved symbol:. ``` {.cpp}; #include ""Event.h""; void dynload() {; gSystem->Load(""libEvent"");; new Event();; }; ```. You will currently have to provide a rootmap file for libEvent (which also requires include; guards for Event.h). This might get fixed in a later version ([ROOT-4691](https://sft.its.cern.ch/jira/browse/ROOT-4691)). #### Using identifiers that are only available at runtime: gROOT->LoadMacro(""foo.h""); foo(); CINT was processing macros line by line; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v600/index.md:3426,perform,perform,3426,core/doc/v600/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v600/index.md,1,['perform'],['perform']
Performance,"; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value pairs separated by ``:`` characters.; Possible key-value pairs are:. - ``cache_size=X%``: The maximum size for the cache directory is ``X`` percent; of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is invalid. A value of 0 disables the percentage; size-based pruning. The default is 75%. - ``cache_size_bytes=X``, ``cache_size_bytes=Xk``, ``cache_size_bytes=Xm``,; ``cache_size_bytes=Xg``:; Sets the maximum size for the cache directory to ``X`` bytes (or KB, MB,; GB respectively). A value over the amount of available space on the disk; will be reduced to the amount of available space. A value of 0 disables; the byte size-based pruning. The default is no byte size-based pruning. Note that ThinLTO will apply both size-based pruning policies simultaneously,; and changing one does not affect the other. For example, a policy of; ``cache_size_bytes=1g`` on its own will cause both the 1GB and default 75%; policies to be applied unless the default ``cache_size`` is overridden. - ``cache_size_files=X``:; Set the maximum number of files in the cache directory. Set to 0 to indicate; no limit. The default is 1000000 files. - ``prune_after=Xs``, ``prune_after=Xm``, ``prune_after=Xh``: Sets the; expiration time for cache files to ``X`` seconds (or minutes, hours; respectively). When a file hasn't been accessed for ``prune_after`` seconds,; it is removed from the cache. A value of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:6182,cache,cache,6182,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache']
Performance,"; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To ensure coherence of local memory reads of CUs in different agents a; ``buffer_inv sc0 sc1`` is required. It will invalidate non-local L2 cache; lines if configured to have multiple L2 caches. * PCIe access from the GPU to the CPU can be kept coherent by using the MTYPE; UC (uncached) which bypasses the L2. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:289183,cache,cache,289183,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:223968,load,load,223968,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; sandbox file removal; file upload, download. TProofDraw. Allow to set a color, size, size, width for lines,; area, markers; the attributes are transmitted via the input list and; automatically derived from the ones of the chain. Support automatic creation of a dataset out of files; created on the worker nodes by worker processes. The implementation is; an extension of the functionality of the class TProofOutputFile used; for merging via file.; Add the possibility to enable/disable the tree cache and; to change its size on per-query base; two new parameters are available:. PROOF_UseTreeCache   ; Int_t       ; Enable (0) or Disable (1) the tree cache (default 1); PROOF_CacheSize      ; Long64_t     Cache size in bytes; (default 10000000). Examples:;        ; a) to disable the cache for the next run enter:;                                 ; proof->SetParameter(""PROOF_UseTreeCache"", 0);        ; b) to set the cache size to 20M;                                 ; proof->SetParameter(""PROOF_CacheSize"", 20000000);  Add the parameter; PROOF_UseParallelUnzip to toggle the use of the parallel unzip; (default off for now); to enable it add the following call;            ;            ;        ;  proof->SetParameter(""PROOF_UseParallelUnzip"", 1).  Add the possibility to give indications about; the number of workers at startup.;  E.g.;        1. To; start max 5 workers;             ; TProof::Open(""<master>"",""workers=5"");        2. To; start max 2 workers per physical machine;             ; TProof::Open(""<master>"",""workers=2x"");      This is useful in general when; running tests (equivalent but quicker then full startup;      followed by; TProof::SetParallel(n) or TProof::DeactivateWorker(...)).; Add support for the worker SysInfo_t in TSlaveInfo; (obtained via TProof::GetListOfSlaveInfos()); Add new submerger functionality to speed up the merging; phase. At the end of the query, a set of workers are promoted; submergers and assigned a sub-set of workers to merge. Once each; sub-me",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:907,cache,cache,907,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,2,['cache'],['cache']
Performance,"; sequence of loads, shifts and masks (or load-right + load-left on MIPS) for; all cases where the load / store does not have a sufficiently high alignment; in the IR. The alignment is used to guarantee the alignment on allocas and globals,; though in most cases this is unnecessary (most targets have a sufficiently; high default alignment that they’ll be fine). It is also used to provide a; contract to the back end saying ‘either this load/store has this alignment, or; it is undefined behavior’. This means that the back end is free to emit; instructions that rely on that alignment (and mid-level optimizers are free to; perform transforms that require that alignment). For x86, it doesn’t make; much difference, as almost all instructions are alignment-independent. For; MIPS, it can make a big difference. Note that if your loads and stores are atomic, the backend will be unable to; lower an under aligned access into a sequence of natively aligned accesses.; As a result, alignment is mandatory for atomic loads and stores. Other Things to Consider; ^^^^^^^^^^^^^^^^^^^^^^^^. #. Use ptrtoint/inttoptr sparingly (they interfere with pointer aliasing; analysis), prefer GEPs. #. Prefer globals over inttoptr of a constant address - this gives you; dereferencability information. In MCJIT, use getSymbolAddress to provide; actual address. #. Be wary of ordered and atomic memory operations. They are hard to optimize; and may not be well optimized by the current optimizer. Depending on your; source language, you may consider using fences instead. #. If calling a function which is known to throw an exception (unwind), use; an invoke with a normal destination which contains an unreachable; instruction. This form conveys to the optimizer that the call returns; abnormally. For an invoke which neither returns normally or requires unwind; code in the current function, you can use a noreturn call instruction if; desired. This is generally not required because the optimizer will convert; an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:5958,load,loads,5958,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['load'],['loads']
Performance,"; serv->SetItemField(""/"", ""_drawopt"", ""colz"");; ```. In such case URL parameters are not required - specified item will be displayed automatically when web page is opened.; One also can configure to display several items at once. For that one also can configure layout of the drawing area:. ```cpp; serv->SetItemField(""/"", ""_layout"", ""grid2x2""); // layout for drawing area; serv->SetItemField(""/"", ""_drawitem"", ""[Files/job1.root/hpxpy,Files/job1.root/hpx]""); // items; serv->SetItemField(""/"", ""_drawopt"", ""[colz,hist]""); // options; ```. One also can change appearance of hierarchy browser on the left side of the web page:. ```cpp; serv->SetItemField(""/"", ""_browser"", ""off""); // allowed ""fix"" (default), ""float"", ""no"", ""off""; serv->SetItemField(""/"", ""_toptitle"", ""Custom title""); // title of web page, shown when browser off; ```. If necessary, one also can automatically open ROOT file when web page is opened:. ```cpp; serv->SetItemField(""/"", ""_loadfile"", ""currentdir/hsimple.root""); // name of ROOT file to load; ```. ## Configuring user access. By default, the http server is open for anonymous access. One could restrict the access to the server for authenticated users only. First of all, one should create a password file, using the **htdigest** utility. ```bash; [shell] htdigest -c .htdigest domain_name user_name; ```. It is recommended not to use special symbols in domain or user names. Several users can be add to the "".htdigest"" file. When starting the server, the following arguments should be specified:. ```cpp; auto serv = new THttpServer(""http:8080?auth_file=.htdigest&auth_domain=domain_name"");; ```. After that, the web browser will automatically request to input a name/password for the domain ""domain_name"". Based on authorized accounts, one could restrict or enable access to some elements in the server objects hierarchy, using `THttpServer::Restrict()` method. For instance, one could hide complete folder from 'guest' account:. ```cpp; serv->Restrict(""/Folder"", ""hidden=gu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md:8086,load,load,8086,documentation/HttpServer/HttpServer.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md,1,['load'],['load']
Performance,"; small overhead for each allocation. The checksum is computed using a CRC32 (made faster with hardware support); of the global secret, the chunk pointer itself, and the 8 bytes of header with; the checksum field zeroed out. It is not intended to be cryptographically; strong. The header is atomically loaded and stored to prevent races. This is important; as two consecutive chunks could belong to different threads. We work on local; copies and use compare-exchange primitives to update the headers in the heap; memory, and avoid any type of double-fetching. Randomness; ----------; Randomness is a critical factor to the additional security provided by the; allocator. The allocator trusts the memory mapping primitives of the OS to; provide pages at (mostly) non-predictable locations in memory, as well as the; binaries to be compiled with ASLR. In the event one of those assumptions is; incorrect, the security will be greatly reduced. Scudo further randomizes how; blocks are allocated in the Primary, can randomize how caches are assigned to; threads. Memory reclaiming; -----------------; Primary and Secondary allocators have different behaviors with regard to; reclaiming. While Secondary mapped allocations can be unmapped on deallocation,; it isn't the case for the Primary, which could lead to a steady growth of the; RSS of a process. To counteract this, if the underlying OS allows it, pages; that are covered by contiguous free memory blocks in the Primary can be; released: this generally means they won't count towards the RSS of a process and; be zero filled on subsequent accesses). This is done in the deallocation path,; and several options exist to tune this behavior. Usage; =====. Platform; --------; If using Fuchsia or an Android version greater than 11, your memory allocations; are already service by Scudo (note that Android Svelte configurations still use; jemalloc). Library; -------; The allocator static library can be built from the LLVM tree thanks to the; ``scud",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:4856,cache,caches,4856,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,1,['cache'],['caches']
Performance,"; src/TStreamerInfoReadBuffer.cxx; src/TStreamerInfoWriteBuffer.cxx; src/TZIPFile.cxx; $<TARGET_OBJECTS:RootPcmObjs>; LIBRARIES; ${CMAKE_DL_LIBS}; DEPENDENCIES; Core; Thread; ). target_include_directories(RIO PRIVATE ${CMAKE_SOURCE_DIR}/core/clib/res); target_link_libraries(RIO PUBLIC ${ROOT_ATOMIC_LIBS}). if(builtin_nlohmannjson); target_include_directories(RIO PRIVATE ${CMAKE_SOURCE_DIR}/builtins); else(); target_link_libraries(RIO PRIVATE nlohmann_json::nlohmann_json); endif(). if(root7); set(RIO_EXTRA_HEADERS ROOT/RFile.hxx); target_sources(RIO PRIVATE v7/src/RFile.cxx); endif(). if(uring); target_link_libraries(RIO PUBLIC ${LIBURING_LIBRARY}); target_include_directories(RIO PRIVATE ${LIBURING_INCLUDE_DIR}); endif(). ROOT_GENERATE_DICTIONARY(G__RIO; ROOT/RRawFile.hxx; ROOT/RRawFileTFile.hxx; ${rawfile_local_headers}; ROOT/TBufferMerger.hxx; TArchiveFile.h; TBufferFile.h; TBufferText.h; TBufferIO.h; TBufferJSON.h; TCollectionProxyFactory.h; TContainerConverters.h; TEmulatedMapProxy.h; TEmulatedCollectionProxy.h; TDirectoryFile.h; TFileCacheRead.h; TFileMerger.h; TFree.h; TFileCacheWrite.h; TFilePrefetch.h; TFile.h; TFPBlock.h; TGenCollectionStreamer.h; TGenCollectionProxy.h; TKey.h; TKeyMapFile.h; TLockFile.h; TMemFile.h; TMapFile.h; TMakeProject.h; TStreamerInfoActions.h; TVirtualCollectionIterators.h; TStreamerInfo.h; TZIPFile.h; ${RIO_EXTRA_HEADERS}; STAGE1; MODULE; RIO; LINKDEF; LinkDef.h; OPTIONS; -writeEmptyRootPCM; DEPENDENCIES; Core; Thread; ). # TStreamerInfoReadBuffer in O0 needs 6k on the stack. It is called; # recursively, quickly exhausting the stack. Prevent that by forcing; # the many scope-local vars to share their stack space / become; # registers, thanks to the optimizer.; if(MSVC); set_source_files_properties(src/TStreamerInfoReadBuffer.cxx COMPILE_FLAGS $<IF:$<CONFIG:Debug>,""/Od"",""/O2"">); else(); set_source_files_properties(src/TStreamerInfoReadBuffer.cxx COMPILE_FLAGS ""-O3""); endif(). ROOT_INSTALL_HEADERS(). ROOT_ADD_TEST_SUBDIRECTORY(test); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/CMakeLists.txt:3156,optimiz,optimizer,3156,io/io/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/CMakeLists.txt,1,['optimiz'],['optimizer']
Performance,"; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:321947,load,load,321947,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:317488,load,load,317488,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:266965,load,load,266965,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; those of :doc:`llc <CommandGuide/llc>` and the triple is required. For example,; the following command would fuzz AArch64 with :doc:`GlobalISel/index`:. .. code-block:: shell. % bin/llvm-isel-fuzzer <corpus-dir> -ignore_remaining_args=1 -mtriple aarch64 -global-isel -O0. Some flags can also be specified in the binary name itself in order to support; OSS Fuzz, which has trouble with required arguments. To do this, you can copy; or move ``llvm-isel-fuzzer`` to ``llvm-isel-fuzzer--x-y-z``, separating options; from the binary name using ""--"". The valid options are architecture names; (``aarch64``, ``x86_64``), optimization levels (``O0``, ``O2``), or specific; keywords, like ``gisel`` for enabling global instruction selection. In this; mode, the same example could be run like so:. .. code-block:: shell. % bin/llvm-isel-fuzzer--aarch64-O0-gisel <corpus-dir>. llvm-opt-fuzzer; ---------------. A |LLVM IR fuzzer| aimed at finding bugs in optimization passes. It receives optimization pipeline and runs it for each fuzzer input. Interface of this fuzzer almost directly mirrors ``llvm-isel-fuzzer``. Both; ``mtriple`` and ``passes`` arguments are required. Passes are specified in a; format suitable for the new pass manager. You can find some documentation about; this format in the doxygen for ``PassBuilder::parsePassPipeline``. .. code-block:: shell. % bin/llvm-opt-fuzzer <corpus-dir> -ignore_remaining_args=1 -mtriple x86_64 -passes instcombine. Similarly to the ``llvm-isel-fuzzer`` arguments in some predefined configurations; might be embedded directly into the binary file name:. .. code-block:: shell. % bin/llvm-opt-fuzzer--x86_64-instcombine <corpus-dir>. llvm-mc-assemble-fuzzer; -----------------------. A |generic fuzzer| that fuzzes the MC layer's assemblers by treating inputs as; target specific assembly. Note that this fuzzer has an unusual command line interface which is not fully; compatible with all of libFuzzer's features. Fuzzer arguments must be passed; after ``--f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst:3419,optimiz,optimization,3419,interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,1,['optimiz'],['optimization']
Performance,"; type; for an indirect function call it is the function signature. The; mapping from a type to an identifier is an ABI detail. In the current,; experimental, implementation the identifier of type T is calculated as; follows:. - Obtain the mangled name for ""typeinfo name for T"".; - Calculate MD5 hash of the name as a string.; - Reinterpret the first 8 bytes of the hash as a little-endian; 64-bit integer. It is possible, but unlikely, that collisions in the; ``CallSiteTypeId`` hashing will result in weaker CFI checks that would; still be conservatively correct. CFI_Check; ---------. In the general case, only the target DSO knows whether the call to; function ``f`` with type ``CallSiteTypeId`` is valid or not. To; export this information, every DSO implements. .. code-block:: none. void __cfi_check(uint64 CallSiteTypeId, void *TargetAddr, void *DiagData). This function provides external modules with access to CFI checks for; the targets inside this DSO. For each known ``CallSiteTypeId``, this; function performs an ``llvm.type.test`` with the corresponding type; identifier. It reports an error if the type is unknown, or if the; check fails. Depending on the values of compiler flags; ``-fsanitize-trap`` and ``-fsanitize-recover``, this function may; print an error, abort and/or return to the caller. ``DiagData`` is an; opaque pointer to the diagnostic information about the error, or; ``null`` if the caller does not provide this information. The basic implementation is a large switch statement over all values; of CallSiteTypeId supported by this DSO, and each case is similar to; the InlinedFastCheck() in the basic CFI mode. CFI Shadow; ----------. To route CFI checks to the target DSO's __cfi_check function, a; mapping from possible virtual / indirect call targets to the; corresponding __cfi_check functions is maintained. This mapping is; implemented as a sparse array of 2 bytes for every possible page (4096; bytes) of memory. The table is kept readonly most of the time.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:21134,perform,performs,21134,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['perform'],['performs']
Performance,"; use ``__has_feature((objc_arc_weak))``; to test whether they are enabled. Unsafe references are enabled; unconditionally. ARC-style weak and unsafe references cannot be used; when Objective-C garbage collection is enabled. Except as noted below, the language rules for the ``__weak`` and; ``__unsafe_unretained`` qualifiers (and the ``weak`` and; ``unsafe_unretained`` property attributes) are just as laid out; in the :doc:`ARC specification <AutomaticReferenceCounting>`.; In particular, note that some classes do not support forming weak; references to their instances, and note that special care must be; taken when storing weak references in memory where initialization; and deinitialization are outside the responsibility of the compiler; (such as in ``malloc``-ed memory). Loading from a ``__weak`` variable always implicitly retains the; loaded value. In non-ARC modes, this retain is normally balanced; by an implicit autorelease. This autorelease can be suppressed; by performing the load in the receiver position of a ``-retain``; message send (e.g. ``[weakReference retain]``); note that this performs; only a single retain (the retain done when primitively loading from; the weak reference). For the most part, ``__unsafe_unretained`` in non-ARC modes is just the; default behavior of variables and therefore is not needed. However,; it does have an effect on the semantics of block captures: normally,; copying a block which captures an Objective-C object or block pointer; causes the captured pointer to be retained or copied, respectively,; but that behavior is suppressed when the captured variable is qualified; with ``__unsafe_unretained``. Note that the ``__weak`` qualifier formerly meant the GC qualifier in; all non-ARC modes and was silently ignored outside of GC modes. It now; means the ARC-style qualifier in all non-GC modes and is no longer; allowed if not enabled by either ``-fobjc-arc`` or ``-fobjc-weak``.; It is expected that ``-fobjc-weak`` will eventually be enab",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:72694,perform,performing,72694,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,"['load', 'perform']","['load', 'performing']"
Performance,"; warranty; and distribute a copy of this License along with the; Library. You may charge a fee for the physical act of transferring a copy,; and you may at your option offer warranty protection in exchange for a; fee. 2. You may modify your copy or copies of the Library or any portion; of it, thus forming a work based on the Library, and copy and; distribute such modifications or work under the terms of Section 1; above, provided that you also meet all of these conditions:. a) The modified work must itself be a software library. b) You must cause the files modified to carry prominent notices; stating that you changed the files and the date of any change. c) You must cause the whole of the work to be licensed at no; charge to all third parties under the terms of this License. d) If a facility in the modified Library refers to a function or a; table of data to be supplied by an application program that uses; the facility, other than as an argument passed when the facility; is invoked, then you must make a good faith effort to ensure that,; in the event an application does not supply such function or; table, the facility still operates, and performs whatever part of; its purpose remains meaningful. (For example, a function in a library to compute square roots has; a purpose that is entirely well-defined independent of the; application. Therefore, Subsection 2d requires that any; application-supplied function or table used by this function must; be optional: if the application does not supply it, the square; root function must still compute square roots.). These requirements apply to the modified work as a whole. If; identifiable sections of that work are not derived from the Library,; and can be reasonably considered independent and separate works in; themselves, then this License, and its terms, do not apply to those; sections when you distribute them as separate works. But when you; distribute the same sections as part of a whole which is a work based; on the Library",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/LICENSE.TXT:11573,perform,performs,11573,interpreter/cling/LICENSE.TXT,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/LICENSE.TXT,1,['perform'],['performs']
Performance,"; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:227163,load,load,227163,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; with the following CMake arguments:. - ``-DCMAKE_C_COMPILER=/path/to/stage2/clang`` - Use the Clang we built in; step 2.; - ``-DCMAKE_CXX_COMPILER=/path/to/stage2/clang++`` - Same as above. If your users are fans of debug info, you may want to consider using; ``-DCMAKE_BUILD_TYPE=RelWithDebInfo`` instead of; ``-DCMAKE_BUILD_TYPE=Release``. This will grant better coverage of; debug info pieces of clang, but will take longer to complete and will; result in a much larger build directory. It's recommended to build the ``all`` target with your instrumented Clang,; since more coverage is often better. b. You should now have a few ``*.profraw`` files in; ``path/to/stage2/profiles/``. You need to merge these using; ``llvm-profdata`` (even if you only have one! The profile merge transforms; profraw into actual profile data, as well). This can be done with; ``/path/to/stage1/llvm-profdata merge; -output=/path/to/output/profdata.prof path/to/stage2/profiles/*.profraw``. 4. Now, build your final, PGO-optimized Clang. To do this, you'll want to pass; the following additional arguments to CMake. - ``-DLLVM_PROFDATA_FILE=/path/to/output/profdata.prof`` - Use the PGO; profile from the previous step.; - ``-DCMAKE_C_COMPILER=/path/to/stage1/clang`` - Use the Clang we built in; step 1.; - ``-DCMAKE_CXX_COMPILER=/path/to/stage1/clang++`` - Same as above. From here, you can build whatever targets you need. .. note::; You may see warnings about a mismatched profile in the build output. These; are generally harmless. To silence them, you can add; ``-DCMAKE_C_FLAGS='-Wno-backend-plugin'; -DCMAKE_CXX_FLAGS='-Wno-backend-plugin'`` to your CMake invocation. Congrats! You now have a Clang built with profile-guided optimizations, and you; can delete all but the final build directory if you'd like. If this worked well for you and you plan on doing it often, there's a slight; optimization that can be made: LLVM and Clang have a tool called tblgen that's; built and run during the build process. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst:6007,optimiz,optimized,6007,interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,1,['optimiz'],['optimized']
Performance,"; }; uint32 : Padding (only if required to align to 8 byte); }. The first byte of each location encodes a type that indicates how to; interpret the ``RegNum`` and ``Offset`` fields as follows:. ======== ========== =================== ===========================; Encoding Type Value Description; -------- ---------- ------------------- ---------------------------; 0x1 Register Reg Value in a register; 0x2 Direct Reg + Offset Frame index value; 0x3 Indirect [Reg + Offset] Spilled value; 0x4 Constant Offset Small constant; 0x5 ConstIndex Constants[Offset] Large constant; ======== ========== =================== ===========================. In the common case, a value is available in a register, and the; ``Offset`` field will be zero. Values spilled to the stack are encoded; as ``Indirect`` locations. The runtime must load those values from a; stack address, typically in the form ``[BP + Offset]``. If an; ``alloca`` value is passed directly to a stack map intrinsic, then; LLVM may fold the frame index into the stack map as an optimization to; avoid allocating a register or stack slot. These frame indices will be; encoded as ``Direct`` locations in the form ``BP + Offset``. LLVM may; also optimize constants by emitting them directly in the stack map,; either in the ``Offset`` of a ``Constant`` location or in the constant; pool, referred to by ``ConstantIndex`` locations. At each callsite, a ""liveout"" register list is also recorded. These; are the registers that are live across the stackmap and therefore must; be saved by the runtime. This is an important optimization when the; patchpoint intrinsic is used with a calling convention that by default; preserves most registers as callee-save. Each entry in the liveout register list contains a DWARF register; number and size in bytes. The stackmap format deliberately omits; specific subregister information. Instead the runtime must interpret; this information conservatively. For example, if the stackmap reports; one byte at ``%r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:14111,optimiz,optimization,14111,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['optimiz'],['optimization']
Performance,";. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14198,cache,cached,14198,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['cache'],['cached']
Performance,";; // return a slice of the second column from; // (0,1): c2[0] = m(0,1); c2[1] = m(1,1); SVector2 c2 = m.SubCol<SVector2> (1,0);. // return a sub-matrix 2x2 with the upper left corner at(1,1); SMatrix22 subM = m.Sub<SMatrix22> (1,1);. // return the diagonal element in a SVector; SVector3 diag = m.Diagonal();. // return the upper(lower) block of the matrix m; SVector6 vub = m.UpperBlock(); // vub = [ 1, 2, 3, 5, 6, 9 ]; SVector6 vlb = m.LowerBlock(); // vlb = [ 1, 4, 5, 7, 8, 9 ]; ```. #### Linear Algebra Matrix Functions (Inversion, Determinant). Only limited linear algebra functionality is available for `SMatrix`. It; is possible for squared matrices `NxN`, to find the inverse or to; calculate the determinant. Different inversion algorithms are used if; the matrix is smaller than `6x6` or if it is symmetric. In the case of a; small matrix, a faster direct inversion is used. For a large; `(N>6) `symmetric matrix the Bunch-Kaufman diagonal pivoting method is; used while for a large `(N>6)` general matrix an LU factorization is; performed using the same algorithm as in the CERNLIB routine `dinv`. ``` {.cpp}; // Invert a NxN matrix.; // The inverted matrix replaces the existing one if the; // result is successful; bool ret = m.Invert(); // return the inverse matrix of m. // If the inversion fails ifail is different than zero ???; int ifail = 0;; ifail = m.Inverse(ifail);. // determinant of a square matrix - calculate the determinant; // modyfing the matrix content and returns it if the calculation; // was successful; double det;; bool ret = m.Det(det);. // calculate determinant by using a temporary matrix; preserves; // matrix content; bool ret = n.Det2(det);; ```. ### Example: Matrix and Vector Functions and Operators. #### Matrix and Vector Operators. The **`ROOT::Math::SVector`** and **`ROOT::Math::SMatrix`** classes; define the following operators described below. The `m1`, `m2`, `m3` are; vectors or matrices of the same type (and size) and `a` is a scalar; value:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:112375,perform,performed,112375,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['performed']
Performance,";; movswl %ax, %eax -> not needed because eax is already sext'd; 	addl	$12, %esp; 	ret. //===----------------------------------------------------------------------===//; // What we have right now.; //===----------------------------------------------------------------------===//. Currently, these sorts of things are modelled by compiling a function to return; the small type and a signext/zeroext marker is used. For example, we compile; Z into:. define i32 @z() nounwind {; entry:; 	%0 = tail call signext i16 (...)* @y() nounwind; 	%1 = sext i16 %0 to i32; 	ret i32 %1; }. and b into:. define signext i16 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind		; <i32> [#uses=1]; 	%retval12 = trunc i32 %0 to i16		; <i16> [#uses=1]; 	ret i16 %retval12; }. This has some problems: 1) the actual precise semantics are really poorly; defined (see PR3779). 2) some targets might want the caller to extend, some; might want the callee to extend 3) the mid-level optimizer doesn't know the; size of the GPR, so it doesn't know that %0 is sign extended up to 32-bits ; here, and even if it did, it could not eliminate the sext. 4) the code; generator has historically assumed that the result is extended to i32, which is; a problem on PIC16 (and is also probably wrong on alpha and other 64-bit; targets). //===----------------------------------------------------------------------===//; // The proposal; //===----------------------------------------------------------------------===//. I suggest that we have the front-end fully lower out the ABI issues here to; LLVM IR. This makes it 100% explicit what is going on and means that there is; no cause for confusion. For example, the cases above should compile into:. define i32 @z() nounwind {; entry:; %0 = tail call i32 (...)* @y() nounwind; 	%1 = trunc i32 %0 to i16; %2 = sext i16 %1 to i32; ret i32 %2; }; define i32 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind; 	%retval12 = trunc i32 %0 to i16; 	%tmp = sext i16 %r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:2595,optimiz,optimizer,2595,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,1,['optimiz'],['optimizer']
Performance,";; void (*copy_helper)(struct __block_literal_5 *dst, struct __block_literal_5 *src);; void (*dispose_helper)(struct __block_literal_5 *);; } __block_descriptor_5 = { 0, sizeof(struct __block_literal_5) __block_copy_5, __block_dispose_5 };. and:. .. code-block:: c. struct _block_byref_i i = {( .isa=NULL, .forwarding=&i, .flags=0, .size=sizeof(struct _block_byref_i), .captured_i=2 )};; struct __block_literal_5 _block_literal = {; &_NSConcreteStackBlock,; (1<<25)|(1<<29), <uninitialized>,; __block_invoke_5,; &__block_descriptor_5,; &i,; };. Importing ``__attribute__((NSObject))`` ``__block`` variables; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. A ``__block`` variable that is also marked ``__attribute__((NSObject))`` should; have ``byref_keep`` and ``byref_dispose`` helper functions that use; ``_Block_object_assign`` and ``_Block_object_dispose``. ``__block`` escapes; ^^^^^^^^^^^^^^^^^^^. Because ``Blocks`` referencing ``__block`` variables may have ``Block_copy()``; performed upon them the underlying storage for the variables may move to the; heap. In Objective-C Garbage Collection Only compilation environments the heap; used is the garbage collected one and no further action is required. Otherwise; the compiler must issue a call to potentially release any heap storage for; ``__block`` variables at all escapes or terminations of their scope. The call; should be:. .. code-block:: c. _Block_object_dispose(&_block_byref_foo, BLOCK_FIELD_IS_BYREF);. Nesting; ^^^^^^^. ``Blocks`` may contain ``Block`` literal expressions. Any variables used within; inner blocks are imported into all enclosing ``Block`` scopes even if the; variables are not used. This includes ``const`` imports as well as ``__block``; variables. Objective C Extensions to ``Blocks``; ====================================. Importing Objects; -----------------. Objects should be treated as ``__attribute__((NSObject))`` variables; all; ``copy_helper``, ``dispose_helper``, ``byref_keep``, and `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst:17820,perform,performed,17820,interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst,1,['perform'],['performed']
Performance,";; void f(int* a) {; int xtemp = x;; for (int i = 0; i < 100; i++) {; if (a[i]); xtemp += 1;; }; x = xtemp;; }. However, LLVM is not allowed to transform the former to the latter: it could; indirectly introduce undefined behavior if another thread can access ``x`` at; the same time. That thread would read `undef` instead of the value it was; expecting, which can lead to undefined behavior down the line. (This example is; particularly of interest because before the concurrency model was implemented,; LLVM would perform this transformation.). Note that speculative loads are allowed; a load which is part of a race returns; ``undef``, but does not have undefined behavior. Atomic instructions; ===================. For cases where simple loads and stores are not sufficient, LLVM provides; various atomic instructions. The exact guarantees provided depend on the; ordering; see `Atomic orderings`_. ``load atomic`` and ``store atomic`` provide the same basic functionality as; non-atomic loads and stores, but provide additional guarantees in situations; where threads and signals are involved. ``cmpxchg`` and ``atomicrmw`` are essentially like an atomic load followed by an; atomic store (where the store is conditional for ``cmpxchg``), but no other; memory operation can happen on any thread between the load and store. A ``fence`` provides Acquire and/or Release ordering which is not part; of another operation; it is normally used along with Monotonic memory; operations. A Monotonic load followed by an Acquire fence is roughly; equivalent to an Acquire load, and a Monotonic store following a; Release fence is roughly equivalent to a Release; store. SequentiallyConsistent fences behave as both an Acquire and a; Release fence, and additionally provide a total ordering with some; complicated guarantees, see the C++ standard for details. Frontends generating atomic instructions generally need to be aware of the; target to some degree; atomic instructions are guaranteed to be lock-fr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:4373,load,load,4373,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],"['load', 'loads']"
Performance,"<""fcomi"", (COM_FIr ST1)>;. // Simple alias.; def : InstAlias<""fcomi $reg"", (COM_FIr RST:$reg)>;. Instruction aliases can also have a Requires clause to make them subtarget; specific. If the back-end supports it, the instruction printer can automatically emit the; alias rather than what's being aliased. It typically leads to better, more; readable code. If it's better to print out what's being aliased, then pass a '0'; as the third parameter to the InstAlias definition. Instruction Matching; --------------------. .. note::. To Be Written. .. _Implementations of the abstract target description interfaces:; .. _implement the target description:. Target-specific Implementation Notes; ====================================. This section of the document explains features or design decisions that are; specific to the code generator for a particular target. .. _tail call section:. Tail call optimization; ----------------------. Tail call optimization, callee reusing the stack of the caller, is currently; supported on x86/x86-64, PowerPC, AArch64, and WebAssembly. It is performed on; x86/x86-64, PowerPC, and AArch64 if:. * Caller and callee have the calling convention ``fastcc``, ``cc 10`` (GHC; calling convention), ``cc 11`` (HiPE calling convention), ``tailcc``, or; ``swifttailcc``. * The call is a tail call - in tail position (ret immediately follows call and; ret uses value of call or is void). * Option ``-tailcallopt`` is enabled or the calling convention is ``tailcc``. * Platform-specific constraints are met. x86/x86-64 constraints:. * No variable argument lists are used. * On x86-64 when generating GOT/PIC code only module-local calls (visibility =; hidden or protected) are supported. PowerPC constraints:. * No variable argument lists are used. * No byval parameters are used. * On ppc32/64 GOT/PIC only module-local calls (visibility = hidden or protected); are supported. WebAssembly constraints:. * No variable argument lists are used. * The 'tail-call' target attribute ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:86251,optimiz,optimization,86251,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"</td>`; :raw-html:`<td>Saved LR</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>24</td>`; :raw-html:`<td>Reserved</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>32</td>`; :raw-html:`<td>Reserved</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>40</td>`; :raw-html:`<td>Saved FP (r31)</td>`; :raw-html:`</tr>`; :raw-html:`</table>`. The *parameter area* is used to store arguments being passed to a callee; function. Following the PowerPC ABI, the first few arguments are actually; passed in registers, with the space in the parameter area unused. However, if; there are not enough registers or the callee is a thunk or vararg function,; these register arguments can be spilled into the parameter area. Thus, the; parameter area must be large enough to store all the parameters for the largest; call sequence made by the caller. The size must also be minimally large enough; to spill registers r3-r10. This allows callees blind to the call signature,; such as thunks and vararg functions, enough space to cache the argument; registers. Therefore, the parameter area is minimally 32 bytes (64 bytes in 64; bit mode.) Also note that since the parameter area is a fixed offset from the; top of the frame, that a callee can access its split arguments using fixed; offsets from the stack pointer (or base pointer.). Combining the information about the linkage, parameter areas and alignment. A; stack frame is minimally 64 bytes in 32 bit mode and 128 bytes in 64 bit mode. The *dynamic area* starts out as size zero. If a function uses dynamic alloca; then space is added to the stack, the linkage and parameter areas are shifted to; top of stack, and the new space is available immediately below the linkage and; parameter areas. The cost of shifting the linkage and parameter areas is minor; since only the link value needs to be copied. The link value can be easily; fetched by adding the original frame size to the base pointer. Note that; allocations in the dynamic s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:99234,cache,cache,99234,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['cache'],['cache']
Performance,"<16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fadd.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fadd.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point addition of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fadd``' intrinsic performs floating-point addition (:ref:`fadd <i_fadd>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fadd.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fadd <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fsub:. '``llvm.vp.fsub.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fsub.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fsub.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fsub.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point subtraction of two vecto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:733447,perform,performed,733447,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"<16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fdiv.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fdiv.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point division of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fdiv``' intrinsic performs floating-point division (:ref:`fdiv <i_fdiv>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fdiv.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fdiv <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_frem:. '``llvm.vp.frem.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.frem.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.frem.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.frem.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point remainder of two vectors",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:738217,perform,performed,738217,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"<4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fneg:. '``llvm.vp.fneg.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fneg.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fneg.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fneg.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point negation of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of floating-point type.; The second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fneg``' intrinsic performs floating-point negation (:ref:`fneg <i_fneg>`); of the first vector operand on each enabled lane. The result on disabled lanes; is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fneg.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fneg <4 x float> %a; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fabs:. '``llvm.vp.fabs.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fabs.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fabs.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fabs.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point absolute value of a vector of floating-point v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:741072,perform,performs,741072,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"<< 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===--------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24669,optimiz,optimized,24669,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimized']
Performance,"<env>``. However, in order to standardize outputs for tools that consume bitcode; bundles, bundles written by the bundler internally use only the 4-field; target triple:. ``<arch><sub>-<vendor>-<sys>-<env>``. **target-id**; The canonical target ID of the code object. Present only if the target; supports a target ID. See :ref:`clang-target-id`. .. _code-object-composition:. Bundled Code Object Composition; -------------------------------. * Each entry of a bundled code object must have a different bundle entry ID.; * There can be multiple entries for the same processor provided they differ; in target feature settings.; * If there is an entry with a target feature specified as *Any*, then all; entries must specify that target feature as *Any* for the same processor. There may be additional target specific restrictions. .. _compatibility-bundle-entry-id:. Compatibility Rules for Bundle Entry ID; ---------------------------------------. A code object, specified using its Bundle Entry ID, can be loaded and; executed on a target processor, if:. * Their offload kinds are the same.; * Their target triples are compatible.; * Their Target IDs are compatible as defined in :ref:`compatibility-target-id`. .. _clang-target-id:. Target ID; =========. A target ID is used to indicate the processor and optionally its configuration,; expressed by a set of target features, that affect ISA generation. It is target; specific if a target ID is supported, or if the target triple alone is; sufficient to specify the ISA generation. It is used with the ``-mcpu=<target-id>`` and ``--offload-arch=<target-id>``; Clang compilation options to specify the kind of code to generate. It is also used as part of the bundle entry ID to identify the code object. See; :ref:`clang-bundle-entry-id`. Target ID syntax is defined by the following BNF syntax:. .. code::. <target-id> ::== <processor> ( "":"" <target-feature> ( ""+"" | ""-"" ) )*. Where:. **processor**; Is a the target specific processor or any alternat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst:10709,load,loaded,10709,interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,1,['load'],['loaded']
Performance,"<img src=""https://root-forum.cern.ch/uploads/default/original/2X/3/3fb82b650635bc6d61461f3c47f41786afad4548.png"" align=""right"" height=""50""/>. ## About. ROOT is a unified software package for the storage, processing, and analysis of ; scientific data: from its acquisition to the final visualization in form of highly ; customizable, publication-ready plots. It is reliable, performant and well supported,; easy to use and obtain, and strives to maximize the quantity and impact of scientific ; results obtained per unit cost, both of human effort and computing resources. ROOT provides a very efficient storage system for data models, ; that demonstrated to scale at the Large Hadron Collider experiments: Exabytes ; of scientific data are written in columnar ROOT format.; ROOT comes with histogramming capabilities in an arbitrary number of ; dimensions, curve fitting, statistical modelling, minimization, to allow; the easy setup of a data analysis system that can query and process the data; interactively or in batch mode, as well as a general parallel processing; framework, RDataFrame, that can considerably speed up an analysis, taking ; full advantage of multi-core and distributed systems. ROOT is performance critical software written in C++ and enables rapid prototyping ; powered by a unique C++ compliant interpreter called Cling. ; Cling also enables performant C++ type introspection which is a building block of automatic ; interoperability with Python. Thanks to PyROOT, leveraging the cppyy technology, ; ROOT offers efficient, on-demand C++/Python interoperability in a uniform cross-language ; execution environment. ROOT fully embraces open-source, it's made with passion by its community,; for the benefit of its community. [![License: LGPL v2.1+](https://img.shields.io/badge/License-LGPL%20v2.1+-blue.svg)](https://www.gnu.org/licenses/lgpl.html); [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5060/badge)](https://bestpractices.coreinfrastruct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README.md:374,perform,performant,374,README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README.md,1,['perform'],['performant']
Performance,"= ""[8]"" \; source = ""Int_t *fArray; Int_t fN;"" \; target = ""fArray"" \; code = ""{ fArray = new Char_t[onfile.fN]; Char_t* gtc=fArray; Int_t* gti=onfile.fArray; \; for(Int_t i=0; i<onfile.fN; i++) *(gtc+i) = *(gti+i)+10; }""; #pragma read sourceClass = ""ACache"" targetClass = ""ACache"" version = ""[8]"" \; source = ""float fValues[3]"" \; target = ""fValues"" \; code = ""{ for(Int_t i=0; i<3; i++) fValues[i] = 1+onfile.fValues[i]; }"". Allow the seamless schema evolution from map<a,b> to vector<pair<a,b> >.; Avoid dropping information when reading a long written on a 64 bits platforms; and being read into a long long on a 32 bits platform (previously the higher; bits were lost due to passing through a 32 bits temporary long).; Migrate the functionality of TStreamerInfo::TagFile to a new interface TBuffer::TagStreamerInfo; so that TMessage can customize the behavior. TMessage now relies on this new interface; instead of TBuffer::IncrementLevel.; New option to hadd, -O requesting the (re)optimization of the basket size (by avoid the fast merge technique). The equivalent in TFileMerger is to call; merger->SetFastMethod(kFALSE); To make sure that the class emulation layer of ROOT does not double delete an object,; tell the StreamerElement representing one of the pointers pointing to the object; to never delete the object. For example:. TClass::AddRule(""HepMC::GenVertex m_event attributes=NotOwner"");. The handling of memory by the collection proxy has been improved in the case of a; collection of pointers which can now become owner of its content. The default, for backward compatibility reasons and to avoid double delete (at the expense; of memory leaks), the container of pointers are still not owning their content; unless they are a free standing container (i.e. itself not contained in another; object).; To make a container of pointers become owner of its content do something like:. TClass::AddRule(""ObjectVector<LHCb::MCRichDigitSummary> m_vector options=Owner"");. Added TKey::Reset ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html:10921,optimiz,optimization,10921,io/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html,1,['optimiz'],['optimization']
Performance,"= ======= ============= ======= =====. See also :ref:`langext-__builtin_shufflevector`, :ref:`langext-__builtin_convertvector`. .. [#] ternary operator(?:) has different behaviors depending on condition; operand's vector type. If the condition is a GNU vector (i.e. __vector_size__),; a NEON vector or an SVE vector, it's only available in C++ and uses normal bool; conversions (that is, != 0).; If it's an extension (OpenCL) vector, it's only available in C and OpenCL C.; And it selects base on signedness of the condition operands (OpenCL v1.1 s6.3.9).; .. [#] sizeof can only be used on vector length specific SVE types.; .. [#] Clang does not allow the address of an element to be taken while GCC; allows this. This is intentional for vectors with a boolean element type and; not implemented otherwise. Vector Builtins; ---------------. **Note: The implementation of vector builtins is work-in-progress and incomplete.**. In addition to the operators mentioned above, Clang provides a set of builtins; to perform additional operations on certain scalar and vector types. Let ``T`` be one of the following types:. * an integer type (as in C23 6.2.5p22), but excluding enumerated types and ``bool``; * the standard floating types float or double; * a half-precision floating point type, if one is supported on the target; * a vector type. For scalar types, consider the operation applied to a vector with a single element. *Vector Size*; To determine the number of elements in a vector, use ``__builtin_vectorelements()``.; For fixed-sized vectors, e.g., defined via ``__attribute__((vector_size(N)))`` or ARM; NEON's vector types (e.g., ``uint16x8_t``), this returns the constant number of; elements at compile-time. For scalable vectors, e.g., SVE or RISC-V V, the number of; elements is not known at compile-time and is determined at runtime. This builtin can; be used, e.g., to increment the loop-counter in vector-type agnostic loops. *Elementwise Builtins*. Each builtin returns a vector equi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:20915,perform,perform,20915,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['perform']
Performance,"= B[0];; }. The runtime condition (``rtc``) checks that the array ``A`` and the; element `B[0]` do not alias. Currently, this transformation does not support followup-attributes. Loop Interchange; ----------------. Currently, the ``LoopInterchange`` pass does not use any metadata. Ambiguous Transformation Order; ==============================. If there multiple transformations defined, the order in which they are; executed depends on the order in LLVM's pass pipeline, which is subject; to change. The default optimization pipeline (anything higher than; ``-O0``) has the following order. When using the legacy pass manager:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - VersioningLICM (if enabled); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). When using the legacy pass manager with LTO:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopVectorizer; - LoopUnroll (partial and runtime unrolling). When using the new pass manager:. - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). Leftover Transformations; ========================. Forced transformations that have not been applied after the last; transformation pass should be reported to the user. The transformation; passes themselves cannot be responsible for this reporting because they; might not be in the pipeline, there might be multiple passes able to; apply a transformation (e.g. ``LoopInterchange`` and Polly) or a; transformation attribute may be 'hidden' inside another passes' followup; attribute. The pass ``-transform-warning`` (``WarnMissedTransformationsPass``); emits such warnings. It should be placed after the last transformation; pass. The current pass pipeline has a fixed order in which transformations; pa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:14162,perform,performs,14162,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,1,['perform'],['performs']
Performance,"= [this]() { return cc; };; return l();; }; };; lambdaExpr(hasAnyCapture(lambdaCapture(capturesThis()))); matches `[this]() { return cc; }`. Matcher<LambdaCapture>isImplicit; Matches an entity that has been implicitly added by the compiler (e.g.; implicit default/copy constructors). Matcher<MemberExpr>isArrow; Matches member expressions that are called with '->' as opposed; to '.'. Member calls on the implicit this pointer match as called with '->'. Given; class Y {; void x() { this->x(); x(); Y y; y.x(); a; this->b; Y::b; }; template <class T> void f() { this->f<T>(); f<T>(); }; int a;; static int b;; };; template <class T>; class Z {; void x() { this->m; }; };; memberExpr(isArrow()); matches this->x, x, y.x, a, this->b; cxxDependentScopeMemberExpr(isArrow()); matches this->m; unresolvedMemberExpr(isArrow()); matches this->f<T>, f<T>. Matcher<NamedDecl>hasAnyNameStringRef, ..., StringRef; Matches NamedDecl nodes that have any of the specified names. This matcher is only provided as a performance optimization of hasName.; hasAnyName(a, b, c); is equivalent to, but faster than; anyOf(hasName(a), hasName(b), hasName(c)). Matcher<NamedDecl>hasExternalFormalLinkage; Matches a declaration that has external formal linkage. Example matches only z (matcher = varDecl(hasExternalFormalLinkage())); void f() {; int x;; static int y;; }; int z;. Example matches f() because it has external formal linkage despite being; unique to the translation unit as though it has internal likage; (matcher = functionDecl(hasExternalFormalLinkage())). namespace {; void f() {}; }. Matcher<NamedDecl>hasNameStringRef Name; Matches NamedDecl nodes that have the specified name. Supports specifying enclosing namespaces or classes by prefixing the name; with '<enclosing>::'.; Does not match typedefs of an underlying type with the given name. Example matches X (Name == ""X""); class X;. Example matches X (Name is one of ""::a::b::X"", ""a::b::X"", ""b::X"", ""X""); namespace a { namespace b { class X; } }. Matche",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:100149,perform,performance,100149,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"= i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimize",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23248,optimiz,optimized,23248,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimized']
Performance,"= select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_bitreverse:. '``llvm.vp.bitreverse.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.bitreverse.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.bitreverse.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.bitreverse.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated bitreverse of a vector of integers. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of integer type. The; second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.bitreverse``' intrinsic performs bitreverse (:ref:`bitreverse <int_bitreverse>`) of the first operand on each; enabled lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.bitreverse.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.bitreverse.v4i32(<4 x i32> %a); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_bswap:. '``llvm.vp.bswap.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.bswap.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.bswap.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.bswap.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated bswap of a vector of integers. Arguments:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:832209,perform,performs,832209,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"=//; // The proposal; //===----------------------------------------------------------------------===//. I suggest that we have the front-end fully lower out the ABI issues here to; LLVM IR. This makes it 100% explicit what is going on and means that there is; no cause for confusion. For example, the cases above should compile into:. define i32 @z() nounwind {; entry:; %0 = tail call i32 (...)* @y() nounwind; 	%1 = trunc i32 %0 to i16; %2 = sext i16 %1 to i32; ret i32 %2; }; define i32 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind; 	%retval12 = trunc i32 %0 to i16; 	%tmp = sext i16 %retval12 to i32; 	ret i32 %tmp; }. In this model, no functions will return an i1/i8/i16 (and on a x86-64 target; that extends results to i64, no i32). This solves the ambiguity issue, allows us ; to fully describe all possible ABIs, and now allows the optimizers to reason; about and eliminate these extensions. The one thing that is missing is the ability for the front-end and optimizer to; specify/infer the guarantees provided by the ABI to allow other optimizations.; For example, in the y/z case, since y is known to return a sign extended value,; the trunc/sext in z should be eliminable. This can be done by introducing new sext/zext attributes which mean ""I know; that the result of the function is sign extended at least N bits. Given this,; and given that it is stuck on the y function, the mid-level optimizer could; easily eliminate the extensions etc with existing functionality. The major disadvantage of doing this sort of thing is that it makes the ABI; lowering stuff even more explicit in the front-end, and that we would like to; eventually move to having the code generator do more of this work. However,; the sad truth of the matter is that this is a) unlikely to happen anytime in; the near future, and b) this is no worse than we have now with the existing; attributes. C compilers fundamentally have to reason about the target in many ways. ; This is ugly and horrib",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:4002,optimiz,optimizer,4002,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,2,['optimiz'],"['optimizations', 'optimizer']"
Performance,"=1 abbrevid=4 op0=5 op1=2/>; <Remark version codeid=2 abbrevid=5 op0=30/>; <String table codeid=3 abbrevid=6/> blob data = 'pass\\x00remark\\x00function\\x00path\\x00key\\x00value\\x00argpath\\x00'; </Meta>; <Remark BlockID=9 NumWords=8 BlockCodeSize=4>; <Remark header codeid=5 abbrevid=4 op0=2 op1=1 op2=0 op3=2/>; <Remark debug location codeid=6 abbrevid=5 op0=3 op1=99 op2=55/>; <Remark hotness codeid=7 abbrevid=6 op0=999999999/>; <Argument with debug location codeid=8 abbrevid=7 op0=4 op1=5 op2=6 op3=11 op4=66/>; </Remark>. opt-viewer; ==========. The ``opt-viewer`` directory contains a collection of tools that visualize and; summarize serialized remarks. The tools only support the ``yaml`` format. .. _optviewerpy:. opt-viewer.py; -------------. Output a HTML page which gives visual feedback on compiler interactions with; your program. :Examples:. ::. $ opt-viewer.py my_yaml_file.opt.yaml. ::. $ opt-viewer.py my_build_dir/. opt-stats.py; ------------. Output statistics about the optimization remarks in the input set. :Example:. ::. $ opt-stats.py my_yaml_file.opt.yaml. Total number of remarks 3. Top 10 remarks by pass:; inline 33%; asm-printer 33%; prologepilog 33%. Top 10 remarks:; asm-printer/InstructionCount 33%; inline/NoDefinition 33%; prologepilog/StackSize 33%. opt-diff.py; -----------. Produce a new YAML file which contains all of the changes in optimizations; between two YAML files. Typically, this tool should be used to do diffs between:. * new compiler + fixed source vs old compiler + fixed source; * fixed compiler + new source vs fixed compiler + old source. This diff file can be displayed using :ref:`opt-viewer.py <optviewerpy>`. :Example:. ::. $ opt-diff.py my_opt_yaml1.opt.yaml my_opt_yaml2.opt.yaml -o my_opt_diff.opt.yaml; $ opt-viewer.py my_opt_diff.opt.yaml. .. _remarkssection:. Emitting remark diagnostics in the object file; ==============================================. A section containing metadata on remark diagnostics will be emitted for the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:15215,optimiz,optimization,15215,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimization']
Performance,"=19219); Write of size 4 at 0x7fcf47b21bc0 by thread T1:; #0 Thread1 tiny_race.c:4 (exe+0x00000000a360). Previous write of size 4 at 0x7fcf47b21bc0 by main thread:; #0 main tiny_race.c:10 (exe+0x00000000a3b4). Thread T1 (running) created at:; #0 pthread_create tsan_interceptors.cc:705 (exe+0x00000000c790); #1 main tiny_race.c:9 (exe+0x00000000a3a4). ``__has_feature(thread_sanitizer)``; ------------------------------------. In some cases one may need to execute different code depending on whether; ThreadSanitizer is enabled.; :ref:`\_\_has\_feature <langext-__has_feature-__has_extension>` can be used for; this purpose. .. code-block:: c. #if defined(__has_feature); # if __has_feature(thread_sanitizer); // code that builds only under ThreadSanitizer; # endif; #endif. ``__attribute__((no_sanitize(""thread"")))``; -----------------------------------------------. Some code should not be instrumented by ThreadSanitizer. One may use the; function attribute ``no_sanitize(""thread"")`` to disable instrumentation of plain; (non-atomic) loads/stores in a particular function. ThreadSanitizer still; instruments such functions to avoid false positives and provide meaningful stack; traces. This attribute may not be supported by other compilers, so we suggest; to use it together with ``__has_feature(thread_sanitizer)``. ``__attribute__((disable_sanitizer_instrumentation))``; --------------------------------------------------------. The ``disable_sanitizer_instrumentation`` attribute can be applied to functions; to prevent all kinds of instrumentation. As a result, it may introduce false; positives and incorrect stack traces. Therefore, it should be used with care,; and only if absolutely required; for example for certain code that cannot; tolerate any instrumentation and resulting side-effects. This attribute; overrides ``no_sanitize(""thread"")``. Ignorelist; ----------. ThreadSanitizer supports ``src`` and ``fun`` entity types in; :doc:`SanitizerSpecialCaseList`, that can be used to sup",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSanitizer.rst:2655,load,loads,2655,interpreter/llvm-project/clang/docs/ThreadSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSanitizer.rst,1,['load'],['loads']
Performance,"== ======= ==================; Memory Space Name HSA Segment Hardware Address NULL Value; Name Name Size; ================= =========== ======== ======= ==================; Private private scratch 32 0x00000000; Local group LDS 32 0xFFFFFFFF; Global global global 64 0x0000000000000000; Constant constant *same as 64 0x0000000000000000; global*; Generic flat flat 64 0x0000000000000000; Region N/A GDS 32 *not implemented; for AMDHSA*; ================= =========== ======== ======= ==================. The global and constant memory spaces both use global virtual addresses, which; are the same virtual address space used by the CPU. However, some virtual; addresses may only be accessible to the CPU, some only accessible by the GPU,; and some by both. Using the constant memory space indicates that the data will not change during; the execution of the kernel. This allows scalar read instructions to be; used. The vector and scalar L1 caches are invalidated of volatile data before; each kernel dispatch execution to allow constant memory to change values between; kernel dispatches. The local memory space uses the hardware Local Data Store (LDS) which is; automatically allocated when the hardware creates work-groups of wavefronts, and; freed when all the wavefronts of a work-group have terminated. The data store; (DS) instructions can be used to access it. The private memory space uses the hardware scratch memory support. If the kernel; uses scratch, then the hardware allocates memory that is accessed using; wavefront lane dword (4 byte) interleaving. The mapping used from private; address to physical address is:. ``wavefront-scratch-base +; (private-address * wavefront-size * 4) +; (wavefront-lane-id * 4)``. There are different ways that the wavefront scratch base address is determined; by a wavefront (see :ref:`amdgpu-amdhsa-initial-kernel-execution-state`). This; memory can be accessed in an interleaved manner using buffer instruction with; the scratch buffer descriptor and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:153941,cache,caches,153941,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"==. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <std",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1153,optimiz,optimizer,1153,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,2,['optimiz'],"['optimizations', 'optimizer']"
Performance,"==//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:18198,load,loaded,18198,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,1,['load'],['loaded']
Performance,"==; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:1005,load,load,1005,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['load'],['load']
Performance,"===----------------------------------------------------------------------===//. I suggest that we have the front-end fully lower out the ABI issues here to; LLVM IR. This makes it 100% explicit what is going on and means that there is; no cause for confusion. For example, the cases above should compile into:. define i32 @z() nounwind {; entry:; %0 = tail call i32 (...)* @y() nounwind; 	%1 = trunc i32 %0 to i16; %2 = sext i16 %1 to i32; ret i32 %2; }; define i32 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind; 	%retval12 = trunc i32 %0 to i16; 	%tmp = sext i16 %retval12 to i32; 	ret i32 %tmp; }. In this model, no functions will return an i1/i8/i16 (and on a x86-64 target; that extends results to i64, no i32). This solves the ambiguity issue, allows us ; to fully describe all possible ABIs, and now allows the optimizers to reason; about and eliminate these extensions. The one thing that is missing is the ability for the front-end and optimizer to; specify/infer the guarantees provided by the ABI to allow other optimizations.; For example, in the y/z case, since y is known to return a sign extended value,; the trunc/sext in z should be eliminable. This can be done by introducing new sext/zext attributes which mean ""I know; that the result of the function is sign extended at least N bits. Given this,; and given that it is stuck on the y function, the mid-level optimizer could; easily eliminate the extensions etc with existing functionality. The major disadvantage of doing this sort of thing is that it makes the ABI; lowering stuff even more explicit in the front-end, and that we would like to; eventually move to having the code generator do more of this work. However,; the sad truth of the matter is that this is a) unlikely to happen anytime in; the near future, and b) this is no worse than we have now with the existing; attributes. C compilers fundamentally have to reason about the target in many ways. ; This is ugly and horrible, but a fact of life. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:4435,optimiz,optimizer,4435,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,1,['optimiz'],['optimizer']
Performance,"===. This scheme checks that virtual calls take place using a vptr of the correct; dynamic type; that is, the dynamic type of the called object must be a; derived class of the static type of the object used to make the call.; This CFI scheme can be enabled on its own using ``-fsanitize=cfi-vcall``. For this scheme to work, all translation units containing the definition; of a virtual member function (whether inline or not), other than members; of :ref:`ignored <cfi-ignorelist>` types or types with public :doc:`LTO; visibility <LTOVisibility>`, must be compiled with ``-flto`` or ``-flto=thin``; enabled and be statically linked into the program. Performance; -----------. A performance overhead of less than 1% has been measured by running the; Dromaeo benchmark suite against an instrumented version of the Chromium; web browser. Another good performance benchmark for this mechanism is the; virtual-call-heavy SPEC 2006 xalancbmk. Note that this scheme has not yet been optimized for binary size; an increase; of up to 15% has been observed for Chromium. Bad Cast Checking; =================. This scheme checks that pointer casts are made to an object of the correct; dynamic type; that is, the dynamic type of the object must be a derived class; of the pointee type of the cast. The checks are currently only introduced; where the class being casted to is a polymorphic class. Bad casts are not in themselves control flow integrity violations, but they; can also create security vulnerabilities, and the implementation uses many; of the same mechanisms. There are two types of bad cast that may be forbidden: bad casts; from a base class to a derived class (which can be checked with; ``-fsanitize=cfi-derived-cast``), and bad casts from a pointer of; type ``void*`` or another unrelated type (which can be checked with; ``-fsanitize=cfi-unrelated-cast``). The difference between these two types of casts is that the first is defined; by the C++ standard to produce an undefined value, whil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst:5096,optimiz,optimized,5096,interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,1,['optimiz'],['optimized']
Performance,"====. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:1165,perform,performance,1165,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,1,['perform'],['performance']
Performance,"====. Atomic operations are represented in the SelectionDAG with ``ATOMIC_*`` opcodes.; On architectures which use barrier instructions for all atomic ordering (like; ARM), appropriate fences can be emitted by the AtomicExpand Codegen pass if; ``shouldInsertFencesForAtomic()`` returns true. The MachineMemOperand for all atomic operations is currently marked as volatile;; this is not correct in the IR sense of volatile, but CodeGen handles anything; marked volatile very conservatively. This should get fixed at some point. One very important property of the atomic operations is that if your backend; supports any inline lock-free atomic operations of a given size, you should; support *ALL* operations of that size in a lock-free manner. When the target implements atomic ``cmpxchg`` or LL/SC instructions (as most do); this is trivial: all the other operations can be implemented on top of those; primitives. However, on many older CPUs (e.g. ARMv5, SparcV8, Intel 80386) there; are atomic load and store instructions, but no ``cmpxchg`` or LL/SC. As it is; invalid to implement ``atomic load`` using the native instruction, but; ``cmpxchg`` using a library call to a function that uses a mutex, ``atomic; load`` must *also* expand to a library call on such architectures, so that it; can remain atomic with regards to a simultaneous ``cmpxchg``, by using the same; mutex. AtomicExpandPass can help with that: it will expand all atomic operations to the; proper ``__atomic_*`` libcalls for any size above the maximum set by; ``setMaxAtomicSizeInBitsSupported`` (which defaults to 0). On x86, all atomic loads generate a ``MOV``. SequentiallyConsistent stores; generate an ``XCHG``, other stores generate a ``MOV``. SequentiallyConsistent; fences generate an ``MFENCE``, other fences do not cause any code to be; generated. ``cmpxchg`` uses the ``LOCK CMPXCHG`` instruction. ``atomicrmw xchg``; uses ``XCHG``, ``atomicrmw add`` and ``atomicrmw sub`` use ``XADD``, and all; other ``atomicrmw`` ope",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:19130,load,load,19130,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['load']
Performance,"====. The allocator will output an error message, and potentially terminate the; process, when an unexpected behavior is detected. The output usually starts with; ``""Scudo ERROR:""`` followed by a short summary of the problem that occurred as; well as the pointer(s) involved. Once again, Scudo is meant to be a mitigation,; and might not be the most useful of tools to help you root-cause the issue,; please consider `ASan <https://github.com/google/sanitizers/wiki/AddressSanitizer>`_; for this purpose. Here is a list of the current error messages and their potential cause:. - ``""corrupted chunk header""``: the checksum verification of the chunk header; has failed. This is likely due to one of two things: the header was; overwritten (partially or totally), or the pointer passed to the function is; not a chunk at all;. - ``""race on chunk header""``: two different threads are attempting to manipulate; the same header at the same time. This is usually symptomatic of a; race-condition or general lack of locking when performing operations on that; chunk;. - ``""invalid chunk state""``: the chunk is not in the expected state for a given; operation, eg: it is not allocated when trying to free it, or it's not; quarantined when trying to recycle it, etc. A double-free is the typical; reason this error would occur;. - ``""misaligned pointer""``: we strongly enforce basic alignment requirements, 8; bytes on 32-bit platforms, 16 bytes on 64-bit platforms. If a pointer passed; to our functions does not fit those, something is definitely wrong. - ``""allocation type mismatch""``: when the optional deallocation type mismatch; check is enabled, a deallocation function called on a chunk has to match the; type of function that was called to allocate it. Security implications of such; a mismatch are not necessarily obvious but situational at best;. - ``""invalid sized delete""``: when the C++14 sized delete operator is used, and; the optional check enabled, this indicates that the size passed when;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:15567,perform,performing,15567,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,1,['perform'],['performing']
Performance,"===== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_load; glc=1 dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:344587,load,load,344587,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"=====; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; nt=1. - volatile. 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. GFX940, GFX941; - constant buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. - !volatile & nontemporal. 1. GFX940, GFX941; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic sc0=1; load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic sc1=1; load atomic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:293517,load,load,293517,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"====== ==============================================================; unordered *none*; monotonic *none*; acquire - If a load atomic/atomicrmw then no following load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved before the acquire.; - If a fence then same as load atomic, plus no preceding; associated fence-paired-atomic can be moved after the fence.; release - If a store atomic/atomicrmw then no preceding load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved after the release.; - If a fence then same as store atomic, plus no following; associated fence-paired-atomic can be moved before the; fence.; acq_rel Same constraints as both acquire and release.; seq_cst - If a load atomic then same constraints as acquire, plus no; preceding sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved after the; seq_cst.; - If a store atomic then the same constraints as release, plus; no following sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved before the; seq_cst.; - If an atomicrmw/fence then same constraints as acq_rel.; ============ ==============================================================. The code sequences used to implement the memory model are defined in the; following sections:. * :ref:`amdgpu-amdhsa-memory-model-gfx6-gfx9`; * :ref:`amdgpu-amdhsa-memory-model-gfx90a`; * :ref:`amdgpu-amdhsa-memory-model-gfx942`; * :ref:`amdgpu-amdhsa-memory-model-gfx10-gfx11`. .. _amdgpu-amdhsa-memory-model-gfx6-gfx9:. Memory Model GFX6-GFX9; ++++++++++++++++++++++. For GFX6-GFX9:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a C",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:205640,load,load,205640,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"=======; Modules; =======. .. contents::; :local:. Introduction; ============; Most software is built using a number of software libraries, including libraries supplied by the platform, internal libraries built as part of the software itself to provide structure, and third-party libraries. For each library, one needs to access both its interface (API) and its implementation. In the C family of languages, the interface to a library is accessed by including the appropriate header files(s):. .. code-block:: c. #include <SomeLib.h>. The implementation is handled separately by linking against the appropriate library. For example, by passing ``-lSomeLib`` to the linker. Modules provide an alternative, simpler way to use software libraries that provides better compile-time scalability and eliminates many of the problems inherent to using the C preprocessor to access the API of a library. Problems with the current model; -------------------------------; The ``#include`` mechanism provided by the C preprocessor is a very poor way to access the API of a library, for a number of reasons:. * **Compile-time scalability**: Each time a header is included, the; compiler must preprocess and parse the text in that header and every; header it includes, transitively. This process must be repeated for; every translation unit in the application, which involves a huge; amount of redundant work. In a project with *N* translation units; and *M* headers included in each translation unit, the compiler is; performing *M x N* work even though most of the *M* headers are; shared among multiple translation units. C++ is particularly bad,; because the compilation model for templates forces a huge amount of; code into headers. * **Fragility**: ``#include`` directives are treated as textual; inclusion by the preprocessor, and are therefore subject to any; active macro definitions at the time of inclusion. If any of the; active macro definitions happens to collide with a name in the; library, it can b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:777,scalab,scalability,777,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['scalab'],['scalability']
Performance,"=======; Remarks; =======. .. contents::; :local:. Introduction to the LLVM remark diagnostics; ===========================================. LLVM is able to emit diagnostics from passes describing whether an optimization; has been performed or missed for a particular reason, which should give more; insight to users about what the compiler did during the compilation pipeline. There are three main remark types:. ``Passed``. Remarks that describe a successful optimization performed by the compiler. :Example:. ::. foo inlined into bar with (cost=always): always inline attribute. ``Missed``. Remarks that describe an attempt to an optimization by the compiler that; could not be performed. :Example:. ::. foo not inlined into bar because it should never be inlined; (cost=never): noinline function attribute. ``Analysis``. Remarks that describe the result of an analysis, that can bring more; information to the user regarding the generated code. :Example:. ::. 16 stack bytes in function. ::. 10 instructions in function. Enabling optimization remarks; =============================. There are two modes that are supported for enabling optimization remarks in; LLVM: through remark diagnostics, or through serialized remarks. Remark diagnostics; ------------------. Optimization remarks can be emitted as diagnostics. These diagnostics will be; propagated to front-ends if desired, or emitted by tools like :doc:`llc; <CommandGuide/llc>` or :doc:`opt <CommandGuide/opt>`. .. option:: -pass-remarks=<regex>. Enables optimization remarks from passes whose name match the given (POSIX); regular expression. .. option:: -pass-remarks-missed=<regex>. Enables missed optimization remarks from passes whose name match the given; (POSIX) regular expression. .. option:: -pass-remarks-analysis=<regex>. Enables optimization analysis remarks from passes whose name match the given; (POSIX) regular expression. Serialized remarks; ------------------. While diagnostics are useful during development, it is oft",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:208,optimiz,optimization,208,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,6,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"=======; ThinLTO; =======. .. contents::; :local:. Introduction; ============. *ThinLTO* compilation is a new type of LTO that is both scalable and; incremental. *LTO* (Link Time Optimization) achieves better; runtime performance through whole-program analysis and cross-module; optimization. However, monolithic LTO implements this by merging all; input into a single module, which is not scalable; in time or memory, and also prevents fast incremental compiles. In ThinLTO mode, as with regular LTO, clang emits LLVM bitcode after the; compile phase. The ThinLTO bitcode is augmented with a compact summary; of the module. During the link step, only the summaries are read and; merged into a combined summary index, which includes an index of function; locations for later cross-module function importing. Fast and efficient; whole-program analysis is then performed on the combined summary index. However, all transformations, including function importing, occur; later when the modules are optimized in fully parallel backends.; By default, linkers_ that support ThinLTO are set up to launch; the ThinLTO backends in threads. So the usage model is not affected; as the distinction between the fast serial thin link step and the backends; is transparent to the user. For more information on the ThinLTO design and current performance,; see the LLVM blog post `ThinLTO: Scalable and Incremental LTO; <http://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html>`_.; While tuning is still in progress, results in the blog post show that; ThinLTO already performs well compared to LTO, in many cases matching; the performance improvement. Current Status; ==============. Clang/LLVM; ----------; .. _compiler:. The 3.9 release of clang includes ThinLTO support. However, ThinLTO; is under active development, and new features, improvements and bugfixes; are being added for the next release. For the latest ThinLTO support,; `build a recent version of clang and LLVM; <https://llvm.org/docs/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:135,scalab,scalable,135,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,6,"['optimiz', 'perform', 'scalab']","['optimization', 'optimized', 'performance', 'performed', 'scalable']"
Performance,"======== ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx90a:. Memory Model GFX90A; +++++++++++++++++++. For GFX90A:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for cohe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:234991,queue,queues,234991,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"======== ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx942:. Memory Model GFX942; +++++++++++++++++++. For GFX942:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for cohe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:285139,queue,queues,285139,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"========. .. .. table:: AMDGPU Trap Handler for AMDHSA OS Code Object V4 and Above; :name: amdgpu-trap-handler-for-amdhsa-os-v4-onwards-table. =================== =============== ================ ================= =======================================; Usage Code Sequence GFX6-GFX8 Inputs GFX9-GFX11 Inputs Description; =================== =============== ================ ================= =======================================; reserved ``s_trap 0x00`` Reserved by hardware.; debugger breakpoint ``s_trap 0x01`` *none* *none* Reserved for debugger to use for; breakpoints. Causes wave to be halted; with the PC at the trap instruction.; The debugger is responsible to resume; the wave, including the instruction; that the breakpoint overwrote.; ``llvm.trap`` ``s_trap 0x02`` ``SGPR0-1``: *none* Causes wave to be halted with the PC at; ``queue_ptr`` the trap instruction. The associated; queue is signalled to put it into the; error state. When the queue is put in; the error state, the waves executing; dispatches on the queue will be; terminated.; ``llvm.debugtrap`` ``s_trap 0x03`` *none* *none* - If debugger not enabled then behaves; as a no-operation. The trap handler; is entered and immediately returns to; continue execution of the wavefront.; - If the debugger is enabled, causes; the debug trap to be reported by the; debugger and the wavefront is put in; the halt state with the PC at the; instruction. The debugger must; increment the PC and resume the wave.; reserved ``s_trap 0x04`` Reserved.; reserved ``s_trap 0x05`` Reserved.; reserved ``s_trap 0x06`` Reserved.; reserved ``s_trap 0x07`` Reserved.; reserved ``s_trap 0x08`` Reserved.; reserved ``s_trap 0xfe`` Reserved.; reserved ``s_trap 0xff`` Reserved.; =================== =============== ================ ================= =======================================. .. _amdgpu-amdhsa-function-call-convention:. Call Convention; ~~~~~~~~~~~~~~~. .. note::. This section is currently incomplete and has inaccuracies. It is WI",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:384919,queue,queue,384919,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['queue'],['queue']
Performance,"========; GWP-ASan; ========. .. contents::; :local:; :depth: 2. Introduction; ============. GWP-ASan is a sampled allocator framework that assists in finding use-after-free; and heap-buffer-overflow bugs in production environments. It informally is a; recursive acronym, ""**G**\WP-ASan **W**\ill **P**\rovide **A**\llocation; **SAN**\ity"". GWP-ASan is based on the classic; `Electric Fence Malloc Debugger <https://linux.die.net/man/3/efence>`_, with a; key adaptation. Notably, we only choose a very small percentage of allocations; to sample, and apply guard pages to these sampled allocations only. The sampling; is small enough to allow us to have very low performance overhead. There is a small, tunable memory overhead that is fixed for the lifetime of the; process. This is approximately ~40KiB per process using the default settings,; depending on the average size of your allocations. GWP-ASan vs. ASan; =================. Unlike `AddressSanitizer <https://clang.llvm.org/docs/AddressSanitizer.html>`_,; GWP-ASan does not induce a significant performance overhead. ASan often requires; the use of dedicated canaries to be viable in production environments, and as; such is often impractical. GWP-ASan is only capable of finding a subset of the memory issues detected by; ASan. Furthermore, GWP-ASan's bug detection capabilities are only probabilistic.; As such, we recommend using ASan over GWP-ASan in testing, as well as anywhere; else that guaranteed error detection is more valuable than the 2x execution; slowdown/binary size bloat. For the majority of production environments, this; impact is too high, and GWP-ASan proves extremely useful. Design; ======. **Please note:** The implementation of GWP-ASan is largely in-flux, and these; details are subject to change. There are currently other implementations of; GWP-ASan, such as the implementation featured in; `Chromium <https://cs.chromium.org/chromium/src/components/gwp_asan/>`_. The; long-term support goal is to ensure feature-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GwpAsan.rst:662,perform,performance,662,interpreter/llvm-project/llvm/docs/GwpAsan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GwpAsan.rst,1,['perform'],['performance']
Performance,"=========. The SYCL specification represents pointers to disjoint memory regions using C++; wrapper classes on an accelerator to enable compilation with a standard C++; toolchain and a SYCL compiler toolchain. Section 3.8.2 of SYCL 2020; specification defines; `memory model <https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#_sycl_device_memory_model>`_\ ,; section 4.7.7 - `address space classes <https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#_address_space_classes>`_; and section 5.9 covers `address space deduction <https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#_address_space_deduction>`_.; The SYCL specification allows two modes of address space deduction: ""generic as; default address space"" (see section 5.9.3) and ""inferred address space"" (see; section 5.9.4). Current implementation supports only ""generic as default address; space"" mode. SYCL borrows its memory model from OpenCL however SYCL doesn't perform; the address space qualifier inference as detailed in; `OpenCL C v3.0 6.7.8 <https://www.khronos.org/registry/OpenCL/specs/3.0-unified/html/OpenCL_C.html#addr-spaces-inference>`_. The default address space is ""generic-memory"", which is a virtual address space; that overlaps the global, local, and private address spaces. SYCL mode enables; following conversions:. - explicit conversions to/from the default address space from/to the address; space-attributed type; - implicit conversions from the address space-attributed type to the default; address space; - explicit conversions to/from the global address space from/to the; ``__attribute__((opencl_global_device))`` or; ``__attribute__((opencl_global_host))`` address space-attributed type; - implicit conversions from the ``__attribute__((opencl_global_device))`` or; ``__attribute__((opencl_global_host))`` address space-attributed type to the; global address space. All named address spaces are disjoint and sub-sets of default address space",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SYCLSupport.rst:1515,perform,perform,1515,interpreter/llvm-project/clang/docs/SYCLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SYCLSupport.rst,1,['perform'],['perform']
Performance,"=========. This document is the central repository for all information pertaining to debug; information in LLVM. It describes the :ref:`actual format that the LLVM debug; information takes <format>`, which is useful for those interested in creating; front-ends or dealing directly with the information. Further, this document; provides specific examples of what debug information for C/C++ looks like. Philosophy behind LLVM debugging information; --------------------------------------------. The idea of the LLVM debugging information is to capture how the important; pieces of the source-language's Abstract Syntax Tree map onto LLVM code.; Several design aspects have shaped the solution that appears here. The; important ones are:. * Debugging information should have very little impact on the rest of the; compiler. No transformations, analyses, or code generators should need to; be modified because of debugging information. * LLVM optimizations should interact in :ref:`well-defined and easily described; ways <intro_debugopt>` with the debugging information. * Because LLVM is designed to support arbitrary programming languages,; LLVM-to-LLVM tools should not need to know anything about the semantics of; the source-level-language. * Source-level languages are often **widely** different from one another.; LLVM should not put any restrictions of the flavor of the source-language,; and the debugging information should work with any language. * With code generator support, it should be possible to use an LLVM compiler; to compile a program to native machine code and standard debugging; formats. This allows compatibility with traditional machine-code level; debuggers, like GDB or DBX. The approach used by the LLVM implementation is to use a small set of; :ref:`intrinsic functions <format_common_intrinsics>` to define a mapping; between LLVM program objects and the source-level objects. The description of; the source-level program is maintained in LLVM metadata in an; :ref:`impl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:1083,optimiz,optimizations,1083,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizations']
Performance,"=========; SafeStack; =========. .. contents::; :local:. Introduction; ============. SafeStack is an instrumentation pass that protects programs against attacks; based on stack buffer overflows, without introducing any measurable performance; overhead. It works by separating the program stack into two distinct regions:; the safe stack and the unsafe stack. The safe stack stores return addresses,; register spills, and local variables that are always accessed in a safe way,; while the unsafe stack stores everything else. This separation ensures that; buffer overflows on the unsafe stack cannot be used to overwrite anything; on the safe stack. SafeStack is a part of the `Code-Pointer Integrity (CPI) Project; <https://dslab.epfl.ch/research/cpi/>`_. Performance; -----------. The performance overhead of the SafeStack instrumentation is less than 0.1% on; average across a variety of benchmarks (see the `Code-Pointer Integrity; <https://dslab.epfl.ch/pubs/cpi.pdf>`__ paper for details). This is mainly; because most small functions do not have any variables that require the unsafe; stack and, hence, do not need unsafe stack frames to be created. The cost of; creating unsafe stack frames for large functions is amortized by the cost of; executing the function. In some cases, SafeStack actually improves the performance. Objects that end up; being moved to the unsafe stack are usually large arrays or variables that are; used through multiple stack frames. Moving such objects away from the safe; stack increases the locality of frequently accessed values on the stack, such; as register spills, return addresses, and small local variables. Compatibility; -------------. Most programs, static libraries, or individual files can be compiled; with SafeStack as is. SafeStack requires basic runtime support, which, on most; platforms, is implemented as a compiler-rt library that is automatically linked; in when the program is compiled with SafeStack. Linking a DSO with SafeStack is not curr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SafeStack.rst:230,perform,performance,230,interpreter/llvm-project/clang/docs/SafeStack.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SafeStack.rst,2,['perform'],['performance']
Performance,"==========. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compatible runtime; allocator for a memory region with the kernarg property for the kernel agent; that will execute the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Programmer's Language; Reference* [HSA]_. For AMDGPU the kernel execution directly accesses the; kernel argument memory in the same way constant memory",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149823,perform,performed,149823,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"===========+========================================================================+; | void* | addr | Address of global symbol within device image (function or global) |; +---------+------------+------------------------------------------------------------------------+; | char* | name | Name of the symbol |; +---------+------------+------------------------------------------------------------------------+; | size_t | size | Size of the entry info (0 if it is a function) |; +---------+------------+------------------------------------------------------------------------+; | int32_t | flags | Flags associated with the entry (see :ref:`table-offload_entry_flags`) |; +---------+------------+------------------------------------------------------------------------+; | int32_t | reserved | Reserved, to be used by the runtime library. |; +---------+------------+------------------------------------------------------------------------+. The address of the global symbol will be set to the device pointer value by the; runtime once the device image is loaded. The flags are set to indicate the; handling required for the offloading entry. If the offloading entry is an entry; to a target region it can have one of the following :ref:`entry flags; <table-offload_entry_flags>`. .. table:: Target Region Entry Flags; :name: table-offload_entry_flags. +----------------------------------+-------+-----------------------------------------+; | Name | Value | Description |; +==================================+=======+=========================================+; | OMPTargetRegionEntryTargetRegion | 0x00 | Mark the entry as generic target region |; +----------------------------------+-------+-----------------------------------------+; | OMPTargetRegionEntryCtor | 0x02 | Mark the entry as a global constructor |; +----------------------------------+-------+-----------------------------------------+; | OMPTargetRegionEntryDtor | 0x04 | Mark the entry as a global destructor |; +----------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:4819,load,loaded,4819,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['load'],['loaded']
Performance,"===========. In the common case, a value is available in a register, and the; ``Offset`` field will be zero. Values spilled to the stack are encoded; as ``Indirect`` locations. The runtime must load those values from a; stack address, typically in the form ``[BP + Offset]``. If an; ``alloca`` value is passed directly to a stack map intrinsic, then; LLVM may fold the frame index into the stack map as an optimization to; avoid allocating a register or stack slot. These frame indices will be; encoded as ``Direct`` locations in the form ``BP + Offset``. LLVM may; also optimize constants by emitting them directly in the stack map,; either in the ``Offset`` of a ``Constant`` location or in the constant; pool, referred to by ``ConstantIndex`` locations. At each callsite, a ""liveout"" register list is also recorded. These; are the registers that are live across the stackmap and therefore must; be saved by the runtime. This is an important optimization when the; patchpoint intrinsic is used with a calling convention that by default; preserves most registers as callee-save. Each entry in the liveout register list contains a DWARF register; number and size in bytes. The stackmap format deliberately omits; specific subregister information. Instead the runtime must interpret; this information conservatively. For example, if the stackmap reports; one byte at ``%rax``, then the value may be in either ``%al`` or; ``%ah``. It doesn't matter in practice, because the runtime will; simply save ``%rax``. However, if the stackmap reports 16 bytes at; ``%ymm0``, then the runtime can safely optimize by saving only; ``%xmm0``. The stack map format is a contract between an LLVM SVN revision and; the runtime. It is currently experimental and may change in the short; term, but minimizing the need to update the runtime is; important. Consequently, the stack map design is motivated by; simplicity and extensibility. Compactness of the representation is; secondary because the runtime is expected to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:14649,optimiz,optimization,14649,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['optimiz'],['optimization']
Performance,"============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx10-gfx11:. Memory Model GFX10-GFX11; ++++++++++++++++++++++++. For GFX10-GFX11:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple work-group processors (WGP).; * Each WGP has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same; WGP. In CU wavefront execution mode the wavefronts may be executed by; different SIMDs in the same CU. In WGP wavefront execution mode the; wavefronts may be executed by different SIMDs in different CUs in the same; WGP.; * Each WGP has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a WGP are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; WGP. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:335981,queue,queues,335981,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,============ ==============================================================; Vendor Description; ============ ==============================================================; ``amd`` Can be used for all AMD GPU usage.; ``mesa3d`` Can be used if the OS is ``mesa3d``.; ============ ==============================================================. .. table:: AMDGPU Operating Systems; :name: amdgpu-os. ============== ============================================================; OS Description; ============== ============================================================; *<empty>* Defaults to the *unknown* OS.; ``amdhsa`` Compute kernels executed on HSA [HSA]_ compatible runtimes; such as:. - AMD's ROCm™ runtime [AMD-ROCm]_ using the *rocm-amdhsa*; loader on Linux. See *AMD ROCm Platform Release Notes*; [AMD-ROCm-Release-Notes]_ for supported hardware and; software.; - AMD's PAL runtime using the *pal-amdhsa* loader on; Windows. ``amdpal`` Graphic shaders and compute kernels executed on AMD's PAL; runtime using the *pal-amdpal* loader on Windows and Linux; Pro.; ``mesa3d`` Graphic shaders and compute kernels executed on AMD's Mesa; 3D runtime using the *mesa-mesa3d* loader on Linux.; ============== ============================================================. .. table:: AMDGPU Environments; :name: amdgpu-environment-table. ============ ==============================================================; Environment Description; ============ ==============================================================; *<empty>* Default.; ============ ==============================================================. .. _amdgpu-processors:. Processors; ----------. Use the Clang options ``-mcpu=<target-id>`` or ``--offload-arch=<target-id>`` to; specify the AMDGPU processor together with optional target features. See; :ref:`amdgpu-target-id` and :ref:`amdgpu-target-features` for AMD GPU target; specific information. Every processor supports every OS ABI (see :ref:`amdgpu-os`) with the following excep,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:2700,load,loader,2700,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"=============. The HLSL language is insufficiently documented, and not formally specified.; Documentation is available on `Microsoft's website; <https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl>`_.; The language syntax is similar enough to C and C++ that carefully written C and; C++ code is valid HLSL. HLSL has some key differences from C & C++ which we will; need to handle in Clang. HLSL is not a conforming or valid extension or superset of C or C++. The; language has key incompatibilities with C and C++, both syntactically and; semantically. An Aside on GPU Languages; -------------------------. Due to HLSL being a GPU targeted language HLSL is a Single Program Multiple Data; (SPMD) language relying on the implicit parallelism provided by GPU hardware.; Some language features in HLSL enable programmers to take advantage of the; parallel nature of GPUs in a hardware abstracted language. HLSL also prohibits some features of C and C++ which can have catastrophic; performance or are not widely supportable on GPU hardware or drivers. As an; example, register spilling is often excessively expensive on GPUs, so HLSL; requires all functions to be inlined during code generation, and does not; support a runtime calling convention. Pointers & References; ---------------------. HLSL does not support referring to values by address. Semantically all variables; are value-types and behave as such. HLSL disallows the pointer dereference; operators (unary ``*``, and ``->``), as well as the address of operator (unary; &). While HLSL disallows pointers and references in the syntax, HLSL does use; reference types in the AST, and we intend to use pointer decay in the AST in; the Clang implementation. HLSL ``this`` Keyword; ---------------------. HLSL does support member functions, and (in HLSL 2021) limited operator; overloading. With member function support, HLSL also has a ``this`` keyword. The; ``this`` keyword is an example of one of the places where HLSL",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst:6025,perform,performance,6025,interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst,1,['perform'],['performance']
Performance,"=============; Calling Convention Description; =============================== ==========================================================; ``ccc`` The C calling convention. Used by default.; See :ref:`amdgpu-amdhsa-function-call-convention-non-kernel-functions`; for more details. ``fastcc`` The fast calling convention. Mostly the same as the ``ccc``. ``coldcc`` The cold calling convention. Mostly the same as the ``ccc``. ``amdgpu_cs`` Used for Mesa/AMDPAL compute shaders.; ..TODO::; Describe. ``amdgpu_cs_chain`` Similar to ``amdgpu_cs``, with differences described below. Functions with this calling convention cannot be called directly. They must; instead be launched via the ``llvm.amdgcn.cs.chain`` intrinsic. Arguments are passed in SGPRs, starting at s0, if they have the ``inreg``; attribute, and in VGPRs otherwise, starting at v8. Using more SGPRs or VGPRs; than available in the subtarget is not allowed. On subtargets that use; a scratch buffer descriptor (as opposed to ``scratch_{load,store}_*`` instructions),; the scratch buffer descriptor is passed in s[48:51]. This limits the; SGPR / ``inreg`` arguments to the equivalent of 48 dwords; using more; than that is not allowed. The return type must be void.; Varargs, sret, byval, byref, inalloca, preallocated are not supported. Values in scalar registers as well as v0-v7 are not preserved. Values in; VGPRs starting at v8 are not preserved for the active lanes, but must be; saved by the callee for inactive lanes when using WWM. Wave scratch is ""empty"" at function boundaries. There is no stack pointer input; or output value, but functions are free to use scratch starting from an initial; stack pointer. Calls to ``amdgpu_gfx`` functions are allowed and behave like they; do in ``amdgpu_cs`` functions. All counters (``lgkmcnt``, ``vmcnt``, ``storecnt``, etc.) are presumed in an; unknown state at function entry. A function may have multiple exits (e.g. one chain exit and one plain ``ret void``; for when the wave ends), but",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:53198,load,load,53198,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"=============; Hwreg Value Syntax Description; ==================================== ===============================================================================; hwreg({0..63}) All bits of a register indicated by the register *id*.; hwreg(<*name*>) All bits of a register indicated by the register *name*.; hwreg({0..63}, {0..31}, {1..32}) Register bits indicated by the register *id*, first bit *offset* and *size*.; hwreg(<*name*>, {0..31}, {1..32}) Register bits indicated by the register *name*, first bit *offset* and *size*.; ==================================== ===============================================================================. Numeric values may be specified as positive :ref:`integer numbers<amdgpu_synid_integer_number>`; or :ref:`absolute expressions<amdgpu_synid_absolute_expression>`. Predefined register *names* include:. ============================== ==========================================; Name Description; ============================== ==========================================; HW_REG_MODE Shader writable mode bits.; HW_REG_STATUS Shader read-only status.; HW_REG_TRAPSTS Trap status.; HW_REG_HW_ID1 Id of wave, simd, compute unit, etc.; HW_REG_HW_ID2 Id of queue, pipeline, etc.; HW_REG_GPR_ALLOC Per-wave SGPR and VGPR allocation.; HW_REG_LDS_ALLOC Per-wave LDS allocation.; HW_REG_IB_STS Counters of outstanding instructions.; HW_REG_SH_MEM_BASES Memory aperture.; HW_REG_FLAT_SCR_LO flat_scratch_lo register.; HW_REG_FLAT_SCR_HI flat_scratch_hi register.; ============================== ==========================================. Examples:. .. parsed-literal::. reg = 1; offset = 2; size = 4; hwreg_enc = reg | (offset << 6) | ((size - 1) << 11). s_getreg_b32 s2, 0x1881; s_getreg_b32 s2, hwreg_enc // the same as above; s_getreg_b32 s2, hwreg(1, 2, 4) // the same as above; s_getreg_b32 s2, hwreg(reg, offset, size) // the same as above. s_getreg_b32 s2, hwreg(15); s_getreg_b32 s2, hwreg(51, 1, 31); s_getreg_b32 s2, hwreg(HW_REG_LDS_ALLOC, 0, 1); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_hwreg.rst:2139,queue,queue,2139,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_hwreg.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_hwreg.rst,1,['queue'],['queue']
Performance,"=============; bin/clang-proto-fuzzer CORPUS_DIR. Arguments can be specified after -ignore_remaining_args=1 to modify the compiler; invocation. For example, the following command line will fuzz LLVM with a; custom optimization level and target triple:; bin/clang-proto-fuzzer CORPUS_DIR -ignore_remaining_args=1 -O3 -triple \; arm64apple-ios9. To translate a clang-proto-fuzzer corpus output to C++:; bin/clang-proto-to-cxx CORPUS_OUTPUT_FILE. ===================; llvm-proto-fuzzer; ===================; Like, clang-proto-fuzzer, llvm-proto-fuzzer is also a protobuf-mutator based; fuzzer. It receives as input a cxx_loop_proto which it then converts into a; string of valid LLVM IR: a function with either a single loop or two nested; loops. It then creates a new string of IR by running optimization passes over; the original IR. Currently, it only runs a loop-vectorize pass but more passes; can easily be added to the fuzzer. Once there are two versions of the input; function (optimized and not), llvm-proto-fuzzer uses LLVM's JIT Engine to; compile both functions. Lastly, it runs both functions on a suite of inputs and; checks that both functions behave the same on all inputs. In this way,; llvm-proto-fuzzer can find not only compiler crashes, but also miscompiles; originating from LLVM's optimization passes. llvm-proto-fuzzer is built very similarly to clang-proto-fuzzer. You can run the; fuzzer with the following command:; bin/clang-llvm-proto-fuzzer CORPUS_DIR. To translate a cxx_loop_proto file into LLVM IR do:; bin/clang-loop-proto-to-llvm CORPUS_OUTPUT_FILE; To translate a cxx_loop_proto file into C++ do:; bin/clang-loop-proto-to-cxx CORPUS_OUTPUT_FILE. Note: To get a higher number of executions per second with llvm-proto-fuzzer it; helps to build it without ASan instrumentation and with the -O2 flag. Because; the fuzzer is not only compiling code, but also running it, as the inputs get; large, the time necessary to fuzz one input can get very high.; Example:; cmake .. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt:4635,optimiz,optimized,4635,interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,1,['optimiz'],['optimized']
Performance,"==============. .. .. table:: compute_pgm_rsrc3 for GFX12; :name: amdgpu-amdhsa-compute_pgm_rsrc3-gfx12-table. ======= ======= =============================== ===========================================================================; Bits Size Field Name Description; ======= ======= =============================== ===========================================================================; 3:0 4 bits RESERVED Reserved, must be 0.; 11:4 8 bits INST_PREF_SIZE Number of instruction bytes to prefetch, starting at the kernel's entry; point instruction, before wavefront starts execution. The value is 0..255; with a granularity of 128 bytes.; 12 1 bit RESERVED Reserved, must be 0.; 13 1 bit GLG_EN If 1, group launch guarantee will be enabled for this dispatch; 30:14 17 bits RESERVED Reserved, must be 0.; 31 1 bit IMAGE_OP If 1, the kernel execution contains image instructions. If executed as; part of a graphics pipeline, image read instructions will stall waiting; for any necessary ``WAIT_SYNC`` fence to be performed in order to; indicate that earlier pipeline stages have completed writing to the; image. Not used for compute kernels that are not part of a graphics pipeline and; must be 0.; 32 **Total size 4 bytes.**; ======= ===================================================================================================================. .. .. table:: Floating Point Rounding Mode Enumeration Values; :name: amdgpu-amdhsa-floating-point-rounding-mode-enumeration-values-table. ====================================== ===== ==============================; Enumeration Name Value Description; ====================================== ===== ==============================; FLOAT_ROUND_MODE_NEAR_EVEN 0 Round Ties To Even; FLOAT_ROUND_MODE_PLUS_INFINITY 1 Round Toward +infinity; FLOAT_ROUND_MODE_MINUS_INFINITY 2 Round Toward -infinity; FLOAT_ROUND_MODE_ZERO 3 Round Toward 0; ====================================== ===== ==============================. .. table:: Extended FLT_ROUNDS En",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:179333,perform,performed,179333,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"==============; LTO Visibility; ==============. *LTO visibility* is a property of an entity that specifies whether it can be; referenced from outside the current LTO unit. A *linkage unit* is a set of; translation units linked together into an executable or DSO, and a linkage; unit's *LTO unit* is the subset of the linkage unit that is linked together; using link-time optimization; in the case where LTO is not being used, the; linkage unit's LTO unit is empty. Each linkage unit has only a single LTO unit. The LTO visibility of a class is used by the compiler to determine which; classes the whole-program devirtualization (``-fwhole-program-vtables``) and; control flow integrity (``-fsanitize=cfi-vcall`` and ``-fsanitize=cfi-mfcall``); features apply to. These features use whole-program information, so they; require the entire class hierarchy to be visible in order to work correctly. If any translation unit in the program uses either of the whole-program; devirtualization or control flow integrity features, it is effectively an ODR; violation to define a class with hidden LTO visibility in multiple linkage; units. A class with public LTO visibility may be defined in multiple linkage; units, but the tradeoff is that the whole-program devirtualization and; control flow integrity features can only be applied to classes with hidden LTO; visibility. A class's LTO visibility is treated as an ODR-relevant property; of its definition, so it must be consistent between translation units. In translation units built with LTO, LTO visibility is based on the; class's symbol visibility as expressed at the source level (i.e. the; ``__attribute__((visibility(""..."")))`` attribute, or the ``-fvisibility=``; flag) or, on the Windows platform, the dllimport and dllexport attributes. When; targeting non-Windows platforms, classes with a visibility other than hidden; visibility receive public LTO visibility. When targeting Windows, classes; with dllimport or dllexport attributes receive publ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LTOVisibility.rst:371,optimiz,optimization,371,interpreter/llvm-project/clang/docs/LTOVisibility.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LTOVisibility.rst,1,['optimiz'],['optimization']
Performance,"===============. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 2 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1232,optimiz,optimization,1232,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimization']
Performance,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:707,load,load,707,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,4,['load'],['load']
Performance,"===============; ShadowCallStack; ===============. .. contents::; :local:. Introduction; ============. ShadowCallStack is an instrumentation pass, currently only implemented for; aarch64, that protects programs against return address overwrites; (e.g. stack buffer overflows.) It works by saving a function's return address; to a separately allocated 'shadow call stack' in the function prolog in; non-leaf functions and loading the return address from the shadow call stack; in the function epilog. The return address is also stored on the regular stack; for compatibility with unwinders, but is otherwise unused. The aarch64 implementation is considered production ready, and; an `implementation of the runtime`_ has been added to Android's libc; (bionic). An x86_64 implementation was evaluated using Chromium and was found; to have critical performance and security deficiencies--it was removed in; LLVM 9.0. Details on the x86_64 implementation can be found in the; `Clang 7.0.1 documentation`_. .. _`implementation of the runtime`: https://android.googlesource.com/platform/bionic/+/808d176e7e0dd727c7f929622ec017f6e065c582/libc/bionic/pthread_create.cpp#128; .. _`Clang 7.0.1 documentation`: https://releases.llvm.org/7.0.1/tools/clang/docs/ShadowCallStack.html. Comparison; ----------. To optimize for memory consumption and cache locality, the shadow call; stack stores only an array of return addresses. This is in contrast to other; schemes, like :doc:`SafeStack`, that mirror the entire stack and trade-off; consuming more memory for shorter function prologs and epilogs with fewer; memory accesses. `Return Flow Guard`_ is a pure software implementation of shadow call stacks; on x86_64. Like the previous implementation of ShadowCallStack on x86_64, it is; inherently racy due to the architecture's use of the stack for calls and; returns. Intel `Control-flow Enforcement Technology`_ (CET) is a proposed hardware; extension that would add native support to use a shadow stack to store/c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ShadowCallStack.rst:421,load,loading,421,interpreter/llvm-project/clang/docs/ShadowCallStack.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ShadowCallStack.rst,2,"['load', 'perform']","['loading', 'performance']"
Performance,"================; LeakSanitizer; ================. .. contents::; :local:. Introduction; ============. LeakSanitizer is a run-time memory leak detector. It can be combined with; :doc:`AddressSanitizer` to get both memory error and leak detection, or; used in a stand-alone mode. LSan adds almost no performance overhead; until the very end of the process, at which point there is an extra leak; detection phase. Usage; =====. :doc:`AddressSanitizer`: integrates LeakSanitizer and enables it by default on; supported platforms. .. code-block:: console. $ cat memory-leak.c; #include <stdlib.h>; void *p;; int main() {; p = malloc(7);; p = 0; // The memory is leaked here.; return 0;; }; % clang -fsanitize=address -g memory-leak.c ; ASAN_OPTIONS=detect_leaks=1 ./a.out; ==23646==ERROR: LeakSanitizer: detected memory leaks; Direct leak of 7 byte(s) in 1 object(s) allocated from:; #0 0x4af01b in __interceptor_malloc /projects/compiler-rt/lib/asan/asan_malloc_linux.cc:52:3; #1 0x4da26a in main memory-leak.c:4:7; #2 0x7f076fd9cec4 in __libc_start_main libc-start.c:287; SUMMARY: AddressSanitizer: 7 byte(s) leaked in 1 allocation(s). To use LeakSanitizer in stand-alone mode, link your program with; ``-fsanitize=leak`` flag. Make sure to use ``clang`` (not ``ld``) for the; link step, so that it would link in proper LeakSanitizer run-time library; into the final executable. Supported Platforms; ===================. * Android aarch64/i386/x86_64; * Fuchsia aarch64/x86_64; * Linux arm/aarch64/mips64/ppc64/ppc64le/riscv64/s390x/i386/x86\_64; * macOS aarch64/i386/x86\_64; * NetBSD i386/x86_64. More Information; ================. `<https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer>`_; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LeakSanitizer.rst:299,perform,performance,299,interpreter/llvm-project/clang/docs/LeakSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LeakSanitizer.rst,1,['perform'],['performance']
Performance,"================; MemorySanitizer; ================. .. contents::; :local:. Introduction; ============. MemorySanitizer is a detector of uninitialized reads. It consists of a; compiler instrumentation module and a run-time library. Typical slowdown introduced by MemorySanitizer is **3x**. How to build; ============. Build LLVM/Clang with `CMake <https://llvm.org/docs/CMake.html>`_. Usage; =====. Simply compile and link your program with ``-fsanitize=memory`` flag.; The MemorySanitizer run-time library should be linked to the final; executable, so make sure to use ``clang`` (not ``ld``) for the final; link step. When linking shared libraries, the MemorySanitizer run-time; is not linked, so ``-Wl,-z,defs`` may cause link errors (don't use it; with MemorySanitizer). To get a reasonable performance add ``-O1`` or; higher. To get meaningful stack traces in error messages add; ``-fno-omit-frame-pointer``. To get perfect stack traces you may need; to disable inlining (just use ``-O1``) and tail call elimination; (``-fno-optimize-sibling-calls``). .. code-block:: console. % cat umr.cc; #include <stdio.h>. int main(int argc, char** argv) {; int* a = new int[10];; a[5] = 0;; if (a[argc]); printf(""xx\n"");; return 0;; }. % clang -fsanitize=memory -fno-omit-frame-pointer -g -O2 umr.cc. If a bug is detected, the program will print an error message to; stderr and exit with a non-zero exit code. .. code-block:: console. % ./a.out; WARNING: MemorySanitizer: use-of-uninitialized-value; #0 0x7f45944b418a in main umr.cc:6; #1 0x7f45938b676c in __libc_start_main libc-start.c:226. By default, MemorySanitizer exits on the first detected error. If you; find the error report hard to understand, try enabling; :ref:`origin tracking <msan-origins>`. ``__has_feature(memory_sanitizer)``; ------------------------------------. In some cases one may need to execute different code depending on; whether MemorySanitizer is enabled. :ref:`\_\_has\_feature; <langext-__has_feature-__has_extension>` can b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MemorySanitizer.rst:795,perform,performance,795,interpreter/llvm-project/clang/docs/MemorySanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MemorySanitizer.rst,2,"['optimiz', 'perform']","['optimize-sibling-calls', 'performance']"
Performance,"==================. * Use a high resolution timer, e.g. perf under linux. * Run the benchmark multiple times to be able to recognize noise. * Disable as many processes or services as possible on the target system. * Disable frequency scaling, turbo boost and address space; randomization (see OS specific section). * Static link if the OS supports it. That avoids any variation that; might be introduced by loading dynamic libraries. This can be done; by passing ``-DLLVM_BUILD_STATIC=ON`` to cmake. * Try to avoid storage. On some systems you can use tmpfs. Putting the; program, inputs and outputs on tmpfs avoids touching a real storage; system, which can have a pretty big variability. To mount it (on linux and freebsd at least)::. mount -t tmpfs -o size=<XX>g none dir_to_mount. Linux; =====. * Disable address space randomization::. echo 0 > /proc/sys/kernel/randomize_va_space. * Set scaling_governor to performance::. for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do; echo performance > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; done. * Use https://github.com/lpechacek/cpuset to reserve cpus for just the; program you are benchmarking. If using perf, leave at least 2 cores; so that perf runs in one and your program in another::. cset shield -c N1,N2 -k on. This will move all threads out of N1 and N2. The ``-k on`` means; that even kernel threads are moved out. * Disable the SMT pair of the cpus you will use for the benchmark. The; pair of cpu N can be found in; ``/sys/devices/system/cpu/cpuN/topology/thread_siblings_list`` and; disabled with::. echo 0 > /sys/devices/system/cpu/cpuX/online. * Run the program with::. cset shield --exec -- perf stat -r 10 <cmd>. This will run the command after ``--`` in the isolated cpus. The; particular perf command runs the ``<cmd>`` 10 times and reports; statistics. With these in place you can expect perf variations of less than 0.1%. Linux Intel; -----------. * Disable turbo mode::. echo 1 > /sys/devices/syst",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Benchmarking.rst:1451,perform,performance,1451,interpreter/llvm-project/llvm/docs/Benchmarking.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Benchmarking.rst,1,['perform'],['performance']
Performance,"==================. Clang supports additional attributes that are useful for documenting program; invariants and rules for static analysis tools, such as the `Clang Static; Analyzer <https://clang-analyzer.llvm.org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitizer)`` to check if the code is being built; with :doc:`AddressSanitizer`. Use ``__has_feature(thread_sanitizer)`` to check if the code is being built; with :doc:`ThreadSanitizer`. Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; with :doc:`MemorySanitizer`. Use ``__has_feature(dataflow_sanitizer)`` to check if the code is being built; with :doc:`DataFlowSanitizer`. Use ``__has_feature(safe_stack)`` to check if the code is being built; with :doc:`SafeStack`. Extensions for selectively disabling optimization; =================================================. Clang provides a mechanism for selectively disabling optimizations in functions; and methods. To disable optimizations in a single function definition, the GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This funct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:158491,optimiz,optimization,158491,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"==================; Available Checkers; ==================. The analyzer performs checks that are categorized into families or ""checkers"". The default set of checkers covers a variety of checks targeted at finding security and API usage bugs,; dead code, and other logic errors. See the :ref:`default-checkers` checkers list below. In addition to these, the analyzer contains a number of :ref:`alpha-checkers` (aka *alpha* checkers).; These checkers are under development and are switched off by default. They may crash or emit a higher number of false positives. The :ref:`debug-checkers` package contains checkers for analyzer developers for debugging purposes. .. contents:: Table of Contents; :depth: 4. .. _default-checkers:. Default Checkers; ----------------. .. _core-checkers:. core; ^^^^; Models core language features and contains general-purpose checkers such as division by zero,; null pointer dereference, usage of uninitialized values, etc.; *These checkers must be always switched on as other checker rely on them.*. .. _core-BitwiseShift:. core.BitwiseShift (C, C++); """""""""""""""""""""""""""""""""""""""""""""""""""". Finds undefined behavior caused by the bitwise left- and right-shift operator; operating on integer types. By default, this checker only reports situations when the right operand is; either negative or larger than the bit width of the type of the left operand;; these are logically unsound. Moreover, if the pedantic mode is activated by; ``-analyzer-config core.BitwiseShift:Pedantic=true``, then this checker also; reports situations where the _left_ operand of a shift operator is negative or; overflow occurs during the right shift of a signed value. (Most compilers; handle these predictably, but the C standard and the C++ standards before C++20; say that they're undefined behavior. In the C++20 standard these constructs are; well-defined, so activating pedantic mode in C++20 has no effect.). **Examples**. .. code-block:: cpp. static_assert(sizeof(int) == 4, ""assuming 32-bit in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst:73,perform,performs,73,interpreter/llvm-project/clang/docs/analyzer/checkers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst,1,['perform'],['performs']
Performance,"==================; Matrix Types; ==================. .. contents::; :local:. .. _matrixtypes:. Clang provides a C/C++ language extension that allows users to directly express; fixed-size 2-dimensional matrices as language values and perform arithmetic on; them. This feature is currently experimental, and both its design and its; implementation are in flux. Draft Specification; ===================. Matrix Type; -----------. A matrix type is a scalar type with an underlying *element type*, a constant; number of *rows*, and a constant number of *columns*. Matrix types with the same; element type, rows, and columns are the same type. A value of a matrix type; includes storage for ``rows * columns`` values of the *element type*. The; internal layout, overall size and alignment are implementation-defined. The maximum of the product of the number of rows and columns is; implementation-defined. If that implementation-defined limit is exceeded, the; program is ill-formed. Currently, the element type of a matrix is only permitted to be one of the; following types:. * an integer type (as in C23 6.2.5p22), but excluding enumerated types and ``bool``; * the standard floating types ``float`` or ``double``; * a half-precision floating point type, if one is supported on the target. Other types may be supported in the future. Matrix Type Attribute; ---------------------. Matrix types can be declared by adding the ``matrix_type`` attribute to the; declaration of a *typedef* (or a C++ alias declaration). The underlying type; of the *typedef* must be a valid matrix element type. The; attribute takes two arguments, both of which must be integer constant; expressions that evaluate to a value greater than zero. The first specifies the; number of rows, and the second specifies the number of columns. The underlying; type of the *typedef* becomes a matrix type with the given dimensions and an; element type of the former underlying type. If a declaration of a *typedef-name* has a ``matrix_typ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MatrixTypes.rst:234,perform,perform,234,interpreter/llvm-project/clang/docs/MatrixTypes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MatrixTypes.rst,1,['perform'],['perform']
Performance,"==================; Vectorization Plan; ==================. .. contents::; :local:. Abstract; ========; The vectorization transformation can be rather complicated, involving several; potential alternatives, especially for outer-loops [1]_ but also possibly for; innermost loops. These alternatives may have significant performance impact,; both positive and negative. A cost model is therefore employed to identify the; best alternative, including the alternative of avoiding any transformation; altogether. The Vectorization Plan is an explicit model for describing vectorization; candidates. It serves for both optimizing candidates including estimating their; cost reliably, and for performing their final translation into IR. This; facilitates dealing with multiple vectorization candidates. High-level Design; =================. Vectorization Workflow; ----------------------; VPlan-based vectorization involves three major steps, taking a ""scenario-based; approach"" to vectorization planning:. 1. Legal Step: check if a loop can be legally vectorized; encode constraints and; artifacts if so.; 2. Plan Step:. a. Build initial VPlans following the constraints and decisions taken by; Legal Step 1, and compute their cost.; b. Apply optimizations to the VPlans, possibly forking additional VPlans.; Prune sub-optimal VPlans having relatively high cost.; 3. Execute Step: materialize the best VPlan. Note that this is the only step; that modifies the IR. Design Guidelines; -----------------; In what follows, the term ""input IR"" refers to code that is fed into the; vectorizer whereas the term ""output IR"" refers to code that is generated by the; vectorizer. The output IR contains code that has been vectorized or ""widened""; according to a loop Vectorization Factor (VF), and/or loop unroll-and-jammed; according to an Unroll Factor (UF).; The design of VPlan follows several high-level guidelines:. 1. Analysis-like: building and manipulating VPlans must not modify the input IR.; In particular,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst:319,perform,performance,319,interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,3,"['optimiz', 'perform']","['optimizing', 'performance', 'performing']"
Performance,"=================== ================================. neg; ~~~. See a description :ref:`here<amdgpu_synid_neg>`. .. _amdgpu_synid_wait_exp:. wait_exp; ~~~~~~~~. Specifies a wait on the EXP counter before issuing the current instruction.; The counter must be less than or equal to this value before the instruction is issued.; If set to 7, no wait is performed. The default value is zero. This is a safe value, but it may be suboptimal. ================ ======================================================; Syntax Description; ================ ======================================================; wait_exp:{0..7} An additional wait on the EXP counter before; issuing this instruction.; ================ ======================================================. .. _amdgpu_synid_wait_vdst:. wait_vdst; ~~~~~~~~~. Specifies a wait on the VA_VDST counter before issuing the current instruction.; The counter must be less than or equal to this value before the instruction is issued.; If set to 15, no wait is performed. The default value is zero. This is a safe value, but it may be suboptimal. ================== ======================================================; Syntax Description; ================== ======================================================; wait_vdst:{0..15} An additional wait on the VA_VDST counter before; issuing this instruction.; ================== ======================================================. DPP8 Modifiers; --------------. .. _amdgpu_synid_dpp8_sel:. dpp8_sel; ~~~~~~~~. Selects which lanes to pull data from, within a group of 8 lanes. This is a mandatory modifier.; There is no default value. The *dpp8_sel* modifier must specify exactly 8 values.; The first value selects which lane to read from to supply data into lane 0.; The second value controls lane 1 and so on. Each value may be specified as either; an :ref:`integer number<amdgpu_synid_integer_number>` or; an :ref:`absolute expression<amdgpu_synid_absolute_expression>`. =======================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:36634,perform,performed,36634,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['perform'],['performed']
Performance,"=================== =========================================================; Syntax Description; =============================== =========================================================; dim:SQ_RSRC_IMG_1D One-dimensional image.; dim:SQ_RSRC_IMG_2D Two-dimensional image.; dim:SQ_RSRC_IMG_3D Three-dimensional image.; dim:SQ_RSRC_IMG_CUBE Cubemap array.; dim:SQ_RSRC_IMG_1D_ARRAY One-dimensional image array.; dim:SQ_RSRC_IMG_2D_ARRAY Two-dimensional image array.; dim:SQ_RSRC_IMG_2D_MSAA Two-dimensional multi-sample auto-aliasing image.; dim:SQ_RSRC_IMG_2D_MSAA_ARRAY Two-dimensional multi-sample auto-aliasing image array.; =============================== =========================================================. dlc; ~~~. See a description :ref:`here<amdgpu_synid_dlc>`. Miscellaneous Modifiers; -----------------------. .. _amdgpu_synid_dlc:. dlc; ~~~. Controls device level cache policy for memory operations. Used for synchronization.; When specified, forces operation to bypass device level cache, making the operation device; level coherent. By default, instructions use device level cache. ======================================== ================================================; Syntax Description; ======================================== ================================================; dlc Bypass device level cache.; ======================================== ================================================. .. _amdgpu_synid_glc:. glc; ~~~. For atomic opcodes, this modifier indicates that the instruction returns the value from memory; before the operation. For other opcodes, it is used together with :ref:`slc<amdgpu_synid_slc>`; to specify cache policy. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== ================================================; glc Set glc bit to 1.; ======================================== =========================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:17176,cache,cache,17176,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['cache'],['cache']
Performance,"===================; HLSL Resource Types; ===================. .. contents::; :local:. Introduction; ============. HLSL Resources are runtime-bound data that is provided as input, output or both; to shader programs written in HLSL. Resource Types in HLSL provide key user; abstractions for reading and writing resource data. Implementation Details; ======================. In Clang resource types are forward declared by the ``HLSLExternalSemaSource``; on initialization. They are then lazily completed when ``requiresCompleteType``; is called later in Sema. Resource types are templated class declarations. The template parameter; specifies the expected return type of resource loads, and the expected parameter; type for stores. In Clang's AST and code generation, resource types are classes that store a; pointer of the template parameter type. The pointer is populated from a call to; ``__builtin_hlsl_create_handle``, and treated as a pointer to an array of typed; data through until lowering in the backend. Resource types are annotated with the ``HLSLResource`` attribute, which drives; code generation for resource binding metadata. The ``hlsl`` metadata nodes are; transformed in the backend to the binding information expected by the target; runtime.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/ResourceTypes.rst:679,load,loads,679,interpreter/llvm-project/clang/docs/HLSL/ResourceTypes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/ResourceTypes.rst,1,['load'],['loads']
Performance,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect diagnostics are intended to help developers identify and address; these situations, by comparing the branch weights added by the ``llvm.expect``; intrinsic to those collected through profiling. Whenever these values are; mismatched, a diagnostic is surfaced to the user. Details on how the checks; operate in the LLVM backed can be found in LLVM's documentation. By default MisExpect checking is quite strict, because the use of the; ``llvm.expect`` intrinsic is designed for specialized cases, where the outcome; of a condition is severely skewed. As a result, the optimizer can be extremely; aggressive, which can result in performance degradation if the outcome is less; predictable than the annotation suggests. Even when the annotation is correct; 90% of the time, it may be beneficial to either remove the annotation or to use; a different intrinsic that can communicate the probability more directly. Because this may be too strict, MisExpect diagnostics are not enabled by; default, and support an additional flag to tolerate some deviation from the; exact thresholds. The ``-fdiagnostic-misexpect-tolerance=N`` accepts; deviations when comparing branch weights within ``N%`` of the expected values.; So passing ``-fdiagnostic-misexpect-tolerance=5`` will not report diagnosti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst:280,optimiz,optimizer,280,interpreter/llvm-project/clang/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst,2,['optimiz'],['optimizer']
Performance,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:280,optimiz,optimizer,280,interpreter/llvm-project/llvm/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst,2,['optimiz'],['optimizer']
Performance,"====================; Constant Interpreter; ====================. .. contents::; :local:. Introduction; ============. The constexpr interpreter aims to replace the existing tree evaluator in; clang, improving performance on constructs which are executed inefficiently; by the evaluator. The interpreter is activated using the following flags:. * ``-fexperimental-new-constant-interpreter`` enables the interpreter,; emitting an error if an unsupported feature is encountered. Bytecode Compilation; ====================. Bytecode compilation is handled in ``ByteCodeStmtGen.h`` for statements; and ``ByteCodeExprGen.h`` for expressions. The compiler has two different; backends: one to generate bytecode for functions (``ByteCodeEmitter``) and; one to directly evaluate expressions as they are compiled, without; generating bytecode (``EvalEmitter``). All functions are compiled to; bytecode, while toplevel expressions used in constant contexts are directly; evaluated since the bytecode would never be reused. This mechanism aims to; pave the way towards replacing the evaluator, improving its performance on; functions and loops, while being just as fast on single-use toplevel; expressions. The interpreter relies on stack-based, strongly-typed opcodes. The glue; logic between the code generator, along with the enumeration and; description of opcodes, can be found in ``Opcodes.td``. The opcodes are; implemented as generic template methods in ``Interp.h`` and instantiated; with the relevant primitive types by the interpreter loop or by the; evaluating emitter. Primitive Types; ---------------. * ``PT_{U|S}int{8|16|32|64}``. Signed or unsigned integers of a specific bit width, implemented using; the ```Integral``` type. * ``PT_{U|S}intFP``. Signed or unsigned integers of an arbitrary, but fixed width used to; implement integral types which are required by the target, but are not; supported by the host. Under the hood, they rely on APValue. The; ``Integral`` specialisation for these typ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ConstantInterpreter.rst:209,perform,performance,209,interpreter/llvm-project/clang/docs/ConstantInterpreter.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ConstantInterpreter.rst,1,['perform'],['performance']
Performance,"====================; The LLVM gold plugin; ====================. Introduction; ============. Building with link time optimization requires cooperation from; the system linker. LTO support on Linux systems is available via the; `gold linker`_ which supports LTO via plugins. This is the same mechanism; used by the `GCC LTO`_ project. The LLVM gold plugin implements the gold plugin interface on top of; :ref:`libLTO`. The same plugin can also be used by other tools such as; ``ar`` and ``nm``. Note that ld.bfd from binutils version 2.21.51.0.2; and above also supports LTO via plugins. However, usage of the LLVM; gold plugin with ld.bfd is not tested and therefore not officially; supported or recommended. As of LLVM 15, the gold plugin will ignore bitcode from the ``.llvmbc``; section inside of ELF object files. However, LTO with bitcode files; is still supported. .. _`gold linker`: http://sourceware.org/binutils; .. _`GCC LTO`: http://gcc.gnu.org/wiki/LinkTimeOptimization; .. _`gold plugin interface`: http://gcc.gnu.org/wiki/whopr/driver. .. _lto-how-to-build:. How to build it; ===============. You need to have gold with plugin support and build the LLVMgold plugin.; The gold linker is installed as ld.gold. To see whether gold is the default; on your system, run ``/usr/bin/ld -v``. It will report ""GNU; gold"" or else ""GNU ld"" if not. If gold is already installed at; ``/usr/bin/ld.gold``, one option is to simply make that the default by; backing up your existing ``/usr/bin/ld`` and creating a symbolic link; with ``ln -s /usr/bin/ld.gold /usr/bin/ld``. Alternatively, you can build; with clang's ``-fuse-ld=gold`` or add ``-fuse-ld=gold`` to LDFLAGS, which will; cause the clang driver to invoke ``/usr/bin/ld.gold`` directly. If you have gold installed, check for plugin support by running; ``/usr/bin/ld.gold -plugin``. If it complains ""missing argument"" then; you have plugin support. If not, and you get an error such as ""unknown option"",; then you will either need to build gol",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GoldPlugin.rst:118,optimiz,optimization,118,interpreter/llvm-project/llvm/docs/GoldPlugin.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GoldPlugin.rst,1,['optimiz'],['optimization']
Performance,"====================; Writing an LLVM Pass; ====================. .. program:: opt. .. contents::; :local:. Introduction --- What is a pass?; ================================. The LLVM Pass Framework is an important part of the LLVM system, because LLVM; passes are where most of the interesting parts of the compiler exist. Passes; perform the transformations and optimizations that make up the compiler, they; build the analysis results that are used by these transformations, and they; are, above all, a structuring technique for compiler code. All LLVM passes are subclasses of the `Pass; <https://llvm.org/doxygen/classllvm_1_1Pass.html>`_ class, which implement; functionality by overriding virtual methods inherited from ``Pass``. Depending; on how your pass works, you should inherit from the :ref:`ModulePass; <writing-an-llvm-pass-ModulePass>` , :ref:`CallGraphSCCPass; <writing-an-llvm-pass-CallGraphSCCPass>`, :ref:`FunctionPass; <writing-an-llvm-pass-FunctionPass>` , or :ref:`LoopPass; <writing-an-llvm-pass-LoopPass>`, or :ref:`RegionPass; <writing-an-llvm-pass-RegionPass>` classes, which gives the system more; information about what your pass does, and how it can be combined with other; passes. One of the main features of the LLVM Pass Framework is that it; schedules passes to run in an efficient way based on the constraints that your; pass meets (which are indicated by which class they derive from). We start by showing you how to construct a pass, everything from setting up the; code, to compiling, loading, and executing it. After the basics are down, more; advanced features are discussed. .. warning::; This document deals with the legacy pass manager. LLVM uses the new pass; manager for the optimization pipeline (the codegen pipeline; still uses the legacy pass manager), which has its own way of defining; passes. For more details, see :doc:`WritingAnLLVMNewPMPass` and; :doc:`NewPassManager`. Quick Start --- Writing hello world; ===================================. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:333,perform,perform,333,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"====================; Writing an LLVM Pass; ====================. .. program:: opt. .. contents::; :local:. Introduction --- What is a pass?; ================================. The LLVM pass framework is an important part of the LLVM system, because LLVM; passes are where most of the interesting parts of the compiler exist. Passes; perform the transformations and optimizations that make up the compiler, they; build the analysis results that are used by these transformations, and they; are, above all, a structuring technique for compiler code. Unlike passes under the legacy pass manager where the pass interface is; defined via inheritance, passes under the new pass manager rely on; concept-based polymorphism, meaning there is no explicit interface (see; comments in ``PassManager.h`` for more details). All LLVM passes inherit from; the CRTP mix-in ``PassInfoMixin<PassT>``. The pass should have a ``run()``; method which returns a ``PreservedAnalyses`` and takes in some unit of IR; along with an analysis manager. For example, a function pass would have a; ``PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);`` method. We start by showing you how to construct a pass, from setting up the build,; creating the pass, to executing and testing it. Looking at existing passes is; always a great way to learn details. .. warning::; This document deals with the new pass manager. LLVM uses the legacy pass; manager for the codegen pipeline. For more details, see; :doc:`WritingAnLLVMPass` and :doc:`NewPassManager`. Quick Start --- Writing hello world; ===================================. Here we describe how to write the ""hello world"" of passes. The ""HelloWorld""; pass is designed to simply print out the name of non-external functions that; exist in the program being compiled. It does not modify the program at all,; it just inspects it. The code below already exists; feel free to create a pass with a different; name alongside the HelloWorld source files. .. _writing-an-llvm-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst:333,perform,perform,333,interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"=====================; Clang Offload Bundler; =====================. .. contents::; :local:. .. _clang-offload-bundler:. Introduction; ============. For heterogeneous single source programming languages, use one or more; ``--offload-arch=<target-id>`` Clang options to specify the target IDs of the; code to generate for the offload code regions. The tool chain may perform multiple compilations of a translation unit to; produce separate code objects for the host and potentially multiple offloaded; devices. The ``clang-offload-bundler`` tool may be used as part of the tool; chain to combine these multiple code objects into a single bundled code object. The tool chain may use a bundled code object as an intermediate step so that; each tool chain step consumes and produces a single file as in traditional; non-heterogeneous tool chains. The bundled code object contains the code objects; for the host and all the offload devices. A bundled code object may also be used to bundle just the offloaded code; objects, and embedded as data into the host code object. The host compilation; includes an ``init`` function that will use the runtime corresponding to the; offload kind (see :ref:`clang-offload-kind-table`) to load the offload code; objects appropriate to the devices present when the host program is executed. :program:`clang-offload-bundler` is located in; `clang/tools/clang-offload-bundler`. .. code-block:: console. $ clang-offload-bundler -help; OVERVIEW: A tool to bundle several input files of the specified type <type>; referring to the same source file but different targets into a single; one. The resulting file can also be unbundled into different files by; this tool if -unbundle is provided. USAGE: clang-offload-bundler [options]. OPTIONS:. Generic Options:. --help - Display available options (--help-hidden for more); --help-list - Display list of available options (--help-list-hidden for more); --version - Display the version of this program. clang-offload-bundler opti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst:366,perform,perform,366,interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,1,['perform'],['perform']
Performance,"====================== ============== ========= ================================. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compatible runtime; allocator for a memory region with the kernarg property for the kernel agent; that will execute the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Programmer's Language; Reference* [HSA]_. For AMDGPU the kernel execution directl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149761,queue,queue,149761,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"======================. In the example above, every variable assignment uniquely corresponds to a; memory store to the variable's position on the stack. However in heavily; optimized code LLVM promotes most variables into SSA values, which can; eventually be placed in physical registers or memory locations. To track SSA; values through compilation, when objects are promoted to SSA values an; ``llvm.dbg.value`` intrinsic is created for each assignment, recording the; variable's new location. Compared with the ``llvm.dbg.declare`` intrinsic:. * A dbg.value terminates the effect of any preceding dbg.values for (any; overlapping fragments of) the specified variable.; * The dbg.value's position in the IR defines where in the instruction stream; the variable's value changes.; * Operands can be constants, indicating the variable is assigned a; constant value. Care must be taken to update ``llvm.dbg.value`` intrinsics when optimization; passes alter or move instructions and blocks -- the developer could observe such; changes reflected in the value of variables when debugging the program. For any; execution of the optimized program, the set of variable values presented to the; developer by the debugger should not show a state that would never have existed; in the execution of the unoptimized program, given the same input. Doing so; risks misleading the developer by reporting a state that does not exist,; damaging their understanding of the optimized program and undermining their; trust in the debugger. Sometimes perfectly preserving variable locations is not possible, often when a; redundant calculation is optimized out. In such cases, a ``llvm.dbg.value``; with operand ``poison`` should be used, to terminate earlier variable locations; and let the debugger present ``optimized out`` to the developer. Withholding; these potentially stale variable values from the developer diminishes the; amount of available debug information, but increases the reliability of the; remaining inf",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:18768,optimiz,optimization,18768,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimization']
Performance,"======================. The inputs for a fuzz target are generated via random mutations of a; :ref:`corpus <libfuzzer-corpus>`. There are a few options for the kinds of; mutations that a fuzzer in LLVM might want. .. _fuzzing-llvm-generic:. Generic Random Fuzzing; ----------------------. The most basic form of input mutation is to use the built in mutators of; LibFuzzer. These simply treat the input corpus as a bag of bits and make random; mutations. This type of fuzzer is good for stressing the surface layers of a; program, and is good at testing things like lexers, parsers, or binary; protocols. Some of the in-tree fuzzers that use this type of mutator are `clang-fuzzer`_,; `clang-format-fuzzer`_, `llvm-as-fuzzer`_, `llvm-dwarfdump-fuzzer`_,; `llvm-mc-assemble-fuzzer`_, and `llvm-mc-disassemble-fuzzer`_. .. _fuzzing-llvm-protobuf:. Structured Fuzzing using ``libprotobuf-mutator``; ------------------------------------------------. We can use libprotobuf-mutator_ in order to perform structured fuzzing and; stress deeper layers of programs. This works by defining a protobuf class that; translates arbitrary data into structurally interesting input. Specifically, we; use this to work with a subset of the C++ language and perform mutations that; produce valid C++ programs in order to exercise parts of clang that are more; interesting than parser error handling. To build this kind of fuzzer you need `protobuf`_ and its dependencies; installed, and you need to specify some extra flags when configuring the build; with :doc:`CMake <CMake>`. For example, `clang-proto-fuzzer`_ can be enabled by; adding ``-DCLANG_ENABLE_PROTO_FUZZER=ON`` to the flags described in; :ref:`building-fuzzers`. The only in-tree fuzzer that uses ``libprotobuf-mutator`` today is; `clang-proto-fuzzer`_. .. _libprotobuf-mutator: https://github.com/google/libprotobuf-mutator; .. _protobuf: https://github.com/google/protobuf. .. _fuzzing-llvm-ir:. Structured Fuzzing of LLVM IR; ---------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst:6502,perform,perform,6502,interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,1,['perform'],['perform']
Performance,"======================; Control Flow Integrity; ======================. .. toctree::; :hidden:. ControlFlowIntegrityDesign. .. contents::; :local:. Introduction; ============. Clang includes an implementation of a number of control flow integrity (CFI); schemes, which are designed to abort the program upon detecting certain forms; of undefined behavior that can potentially allow attackers to subvert the; program's control flow. These schemes have been optimized for performance,; allowing developers to enable them in release builds. To enable Clang's available CFI schemes, use the flag ``-fsanitize=cfi``.; You can also enable a subset of available :ref:`schemes <cfi-schemes>`.; As currently implemented, all schemes rely on link-time optimization (LTO);; so it is required to specify ``-flto``, and the linker used must support LTO,; for example via the `gold plugin`_. To allow the checks to be implemented efficiently, the program must; be structured such that certain object files are compiled with CFI; enabled, and are statically linked into the program. This may preclude; the use of shared libraries in some cases. The compiler will only produce CFI checks for a class if it can infer hidden; LTO visibility for that class. LTO visibility is a property of a class that; is inferred from flags and attributes. For more details, see the documentation; for :doc:`LTO visibility <LTOVisibility>`. The ``-fsanitize=cfi-{vcall,nvcall,derived-cast,unrelated-cast}`` flags; require that a ``-fvisibility=`` flag also be specified. This is because the; default visibility setting is ``-fvisibility=default``, which would disable; CFI checks for classes without visibility attributes. Most users will want; to specify ``-fvisibility=hidden``, which enables CFI checks for such classes. Experimental support for :ref:`cross-DSO control flow integrity; <cfi-cross-dso>` exists that does not require classes to have hidden LTO; visibility. This cross-DSO support has unstable ABI at this time. .. _g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst:456,optimiz,optimized,456,interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,3,"['optimiz', 'perform']","['optimization', 'optimized', 'performance']"
Performance,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:678,optimiz,optimized,678,interpreter/llvm-project/llvm/docs/Packaging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst,2,['optimiz'],['optimized']
Performance,"========================; Debugging C++ Coroutines; ========================. .. contents::; :local:. Introduction; ============. For performance and other architectural reasons, the C++ Coroutines feature in; the Clang compiler is implemented in two parts of the compiler. Semantic; analysis is performed in Clang, and Coroutine construction and optimization; takes place in the LLVM middle-end. However, this design forces us to generate insufficient debugging information.; Typically, the compiler generates debug information in the Clang frontend, as; debug information is highly language specific. However, this is not possible; for Coroutine frames because the frames are constructed in the LLVM middle-end. To mitigate this problem, the LLVM middle end attempts to generate some debug; information, which is unfortunately incomplete, since much of the language; specific information is missing in the middle end. This document describes how to use this debug information to better debug; coroutines. Terminology; ===========. Due to the recent nature of C++20 Coroutines, the terminology used to describe; the concepts of Coroutines is not settled. This section defines a common,; understandable terminology to be used consistently throughout this document. coroutine type; --------------. A `coroutine function` is any function that contains any of the Coroutine; Keywords `co_await`, `co_yield`, or `co_return`. A `coroutine type` is a; possible return type of one of these `coroutine functions`. `Task` and; `Generator` are commonly referred to coroutine types. coroutine; ---------. By technical definition, a `coroutine` is a suspendable function. However,; programmers typically use `coroutine` to refer to an individual instance.; For example:. .. code-block:: c++. std::vector<Task> Coros; // Task is a coroutine type.; for (int i = 0; i < 3; i++); Coros.push_back(CoroTask()); // CoroTask is a coroutine function, which; // would return a coroutine type 'Task'. In practice, we typical",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:134,perform,performance,134,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,3,"['optimiz', 'perform']","['optimization', 'performance', 'performed']"
Performance,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:338,perform,performance,338,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,1,['perform'],['performance']
Performance,"========================; Scudo Hardened Allocator; ========================. .. contents::; :local:; :depth: 2. Introduction; ============. The Scudo Hardened Allocator is a user-mode allocator, originally based on LLVM; Sanitizers'; `CombinedAllocator <https://github.com/llvm/llvm-project/blob/main/compiler-rt/lib/sanitizer_common/sanitizer_allocator_combined.h>`_.; It aims at providing additional mitigation against heap based vulnerabilities,; while maintaining good performance. Scudo is currently the default allocator in; `Fuchsia <https://fuchsia.dev/>`_, and in `Android <https://www.android.com/>`_; since Android 11. The name ""Scudo"" comes from the Italian word for; `shield <https://www.collinsdictionary.com/dictionary/italian-english/scudo>`_; (and Escudo in Spanish). Design; ======. Allocator; ---------; Scudo was designed with security in mind, but aims at striking a good balance; between security and performance. It was designed to be highly tunable and; configurable, and while we provide some default configurations, we encourage; consumers to come up with the parameters that will work best for their use; cases. The allocator combines several components that serve distinct purposes:. - the Primary allocator: fast and efficient, it services smaller allocation; sizes by carving reserved memory regions into blocks of identical size. There; are currently two Primary allocators implemented, specific to 32 and 64 bit; architectures. It is configurable via compile time options. - the Secondary allocator: slower, it services larger allocation sizes via the; memory mapping primitives of the underlying operating system. Secondary backed; allocations are surrounded by Guard Pages. It is also configurable via compile; time options. - the thread specific data Registry: defines how local caches operate for each; thread. There are currently two models implemented: the exclusive model where; each thread holds its own caches (using the ELF TLS); or the shared model; where t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:474,perform,performance,474,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,2,['perform'],['performance']
Performance,"=========================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default make",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:1068,optimiz,optimizations,1068,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['optimiz'],['optimizations']
Performance,"=========================; Dependence Graphs in LLVM; =========================. .. contents::; :local:. Introduction; ============; Dependence graphs are useful tools in compilers for analyzing relationships; between various program elements to help guide optimizations. The ideas; behind these graphs are described in papers [1]_ and [2]_. The implementation of these ideas in LLVM may be slightly different than; what is mentioned in the papers. These differences are documented in; the `implementation details <implementation-details_>`_. .. _DataDependenceGraph:. Data Dependence Graph; =====================; In its simplest form the Data Dependence Graph (or DDG) represents data; dependencies between individual instructions. Each node in such a graph; represents a single instruction and is referred to as an ""atomic"" node.; It is also possible to combine some atomic nodes that have a simple; def-use dependency between them into larger nodes that contain multiple-; instructions. As described in [1]_ the DDG uses graph abstraction to group nodes; that are part of a strongly connected component of the graph; into special nodes called pi-blocks. pi-blocks represent cycles of data; dependency that prevent reordering transformations. Since any strongly; connected component of the graph is a maximal subgraph of all the nodes; that form a cycle, pi-blocks are at most one level deep. In other words,; no pi-blocks are nested inside another pi-block, resulting in a; hierarchical representation that is at most one level deep. For example, consider the following:. .. code-block:: c++. for (int i = 1; i < n; i++) {; b[i] = c[i] + b[i-1];; }. This code contains a statement that has a loop carried dependence on; itself creating a cycle in the DDG. The figure below illustrates; how the cycle of dependency is carried through multiple def-use relations; and a memory access dependency. .. image:: cycle.png. The DDG corresponding to this example would have a pi-block that contains; all the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DependenceGraphs/index.rst:257,optimiz,optimizations,257,interpreter/llvm-project/llvm/docs/DependenceGraphs/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DependenceGraphs/index.rst,1,['optimiz'],['optimizations']
Performance,"==========================; Auto-Vectorization in LLVM; ==========================. .. contents::; :local:. LLVM has two vectorizers: The :ref:`Loop Vectorizer <loop-vectorizer>`,; which operates on Loops, and the :ref:`SLP Vectorizer; <slp-vectorizer>`. These vectorizers; focus on different optimization opportunities and use different techniques.; The SLP vectorizer merges multiple scalars that are found in the code into; vectors while the Loop Vectorizer widens instructions in loops; to operate on multiple consecutive iterations. Both the Loop Vectorizer and the SLP Vectorizer are enabled by default. .. _loop-vectorizer:. The Loop Vectorizer; ===================. Usage; -----. The Loop Vectorizer is enabled by default, but it can be disabled; through clang using the command line flag:. .. code-block:: console. $ clang ... -fno-vectorize file.c. Command line flags; ^^^^^^^^^^^^^^^^^^. The loop vectorizer uses a cost model to decide on the optimal vectorization factor; and unroll factor. However, users of the vectorizer can force the vectorizer to use; specific values. Both 'clang' and 'opt' support the flags below. Users can control the vectorization SIMD width using the command line flag ""-force-vector-width"". .. code-block:: console. $ clang -mllvm -force-vector-width=8 ...; $ opt -loop-vectorize -force-vector-width=8 ... Users can control the unroll factor using the command line flag ""-force-vector-interleave"". .. code-block:: console. $ clang -mllvm -force-vector-interleave=2 ...; $ opt -loop-vectorize -force-vector-interleave=2 ... Pragma loop hint directives; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``#pragma clang loop`` directive allows loop vectorization hints to be; specified for the subsequent for, while, do-while, or c++11 range-based for; loop. The directive allows vectorization and interleaving to be enabled or; disabled. Vector width as well as interleave count can also be manually; specified. The following example explicitly enables vectorization and; interl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:293,optimiz,optimization,293,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['optimiz'],['optimization']
Performance,"==========================; Exception Handling in LLVM; ==========================. .. contents::; :local:. Introduction; ============. This document is the central repository for all information pertaining to; exception handling in LLVM. It describes the format that LLVM exception; handling information takes, which is useful for those interested in creating; front-ends or dealing directly with the information. Further, this document; provides specific examples of what exception handling information is used for in; C and C++. Itanium ABI Zero-cost Exception Handling; ----------------------------------------. Exception handling for most programming languages is designed to recover from; conditions that rarely occur during general use of an application. To that end,; exception handling should not interfere with the main flow of an application's; algorithm by performing checkpointing tasks, such as saving the current pc or; register state. The Itanium ABI Exception Handling Specification defines a methodology for; providing outlying data in the form of exception tables without inlining; speculative exception handling code in the flow of an application's main; algorithm. Thus, the specification is said to add ""zero-cost"" to the normal; execution of an application. A more complete description of the Itanium ABI exception handling runtime; support of can be found at `Itanium C++ ABI: Exception Handling; <http://itanium-cxx-abi.github.io/cxx-abi/abi-eh.html>`_. A description of the; exception frame format can be found at `Exception Frames; <http://refspecs.linuxfoundation.org/LSB_3.0.0/LSB-Core-generic/LSB-Core-generic/ehframechpt.html>`_,; with details of the DWARF 4 specification at `DWARF 4 Standard; <http://dwarfstd.org/Dwarf4Std.php>`_. A description for the C++ exception; table formats can be found at `Exception Handling Tables; <http://itanium-cxx-abi.github.io/cxx-abi/exceptions.pdf>`_. Setjmp/Longjmp Exception Handling; ---------------------------------. Setjmp/Lon",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExceptionHandling.rst:869,perform,performing,869,interpreter/llvm-project/llvm/docs/ExceptionHandling.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExceptionHandling.rst,1,['perform'],['performing']
Performance,"==========================; Offload Kind Description; ============= ==============================================================; host Host code object. ``clang-offload-bundler`` always includes; this entry as the first bundled code object entry. For an; embedded bundled code object this entry is not used by the; runtime and so is generally an empty code object. hip Offload code object for the HIP language. Used for all; HIP language offload code objects when the; ``clang-offload-bundler`` is used to bundle code objects as; intermediate steps of the tool chain. Also used for AMD GPU; code objects before ABI version V4 when the; ``clang-offload-bundler`` is used to create a *fat binary*; to be loaded by the HIP runtime. The fat binary can be; loaded directly from a file, or be embedded in the host code; object as a data section with the name ``.hip_fatbin``. hipv4 Offload code object for the HIP language. Used for AMD GPU; code objects with at least ABI version V4 when the; ``clang-offload-bundler`` is used to create a *fat binary*; to be loaded by the HIP runtime. The fat binary can be; loaded directly from a file, or be embedded in the host code; object as a data section with the name ``.hip_fatbin``. openmp Offload code object for the OpenMP language extension.; ============= ==============================================================. **target-triple**; The target triple of the code object. See `Target Triple; <https://clang.llvm.org/docs/CrossCompilation.html#target-triple>`_. The bundler accepts target triples with or without the optional environment; field:. ``<arch><sub>-<vendor>-<sys>``, or; ``<arch><sub>-<vendor>-<sys>-<env>``. However, in order to standardize outputs for tools that consume bitcode; bundles, bundles written by the bundler internally use only the 4-field; target triple:. ``<arch><sub>-<vendor>-<sys>-<env>``. **target-id**; The canonical target ID of the code object. Present only if the target; supports a target ID. See :ref:`clang-target",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst:9098,load,loaded,9098,interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,1,['load'],['loaded']
Performance,"==========================; Vector Predication Roadmap; ==========================. .. contents:: Table of Contents; :depth: 3; :local:. Motivation; ==========. This proposal defines a roadmap towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/In",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:554,load,loads,554,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,1,['load'],['loads']
Performance,"===========================; ORC Design and Implementation; ===============================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; sessio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:999,perform,performance,999,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['perform'],['performance']
Performance,"============================ ================================================; lwe Enables LOD warning.; ======================================== ================================================. .. _amdgpu_synid_da:. da; ~~. Specifies if an array index must be sent to TA. By default, the array index is not sent. ======================================== ================================================; Syntax Description; ======================================== ================================================; da Send an array index to TA.; ======================================== ================================================. .. _amdgpu_synid_d16:. d16; ~~~. Specifies data size: 16 or 32 bits (32 bits by default). ======================================== ================================================; Syntax Description; ======================================== ================================================; d16 Enables 16-bits data mode. On loads, convert data in memory to 16-bit; format before storing it in VGPRs. For stores, convert 16-bit data in VGPRs to; 32 bits before writing the values to memory. Note that GFX8.0 does not support data packing.; Each 16-bit data element occupies 1 VGPR. GFX8.1 and GFX9+ support data packing.; Each pair of 16-bit data elements; occupies 1 VGPR.; ======================================== ================================================. .. _amdgpu_synid_a16:. a16; ~~~. Specifies the size of image address components: 16 or 32 bits (32 bits by default). ======================================== ================================================; Syntax Description; ======================================== ================================================; a16 Enables 16-bits image address components.; ======================================== ================================================. .. _amdgpu_synid_dim:. dim; ~~~. Specifies surface dimension. This is a mandatory modifier. There is no default value. ===================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:14397,load,loads,14397,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['load'],['loads']
Performance,"============================; Clang Compiler User's Manual; ============================. .. include:: <isonum.txt>. .. contents::; :local:. Introduction; ============. The Clang Compiler is an open-source compiler for the C family of; programming languages, aiming to be the best in class implementation of; these languages. Clang builds on the LLVM optimizer and code generator,; allowing it to provide high-quality optimization and code generation; support for many targets. For more general information, please see the; `Clang Web Site <https://clang.llvm.org>`_ or the `LLVM Web; Site <https://llvm.org>`_. This document describes important notes about using Clang as a compiler; for an end-user, documenting the supported features, command line; options, etc. If you are interested in using Clang to build a tool that; processes code, please see :doc:`InternalsManual`. If you are interested in the; `Clang Static Analyzer <https://clang-analyzer.llvm.org>`_, please see its web; page. Clang is one component in a complete toolchain for C family languages.; A separate document describes the other pieces necessary to; :doc:`assemble a complete toolchain <Toolchain>`. Clang is designed to support the C family of programming languages,; which includes :ref:`C <c>`, :ref:`Objective-C <objc>`, :ref:`C++ <cxx>`, and; :ref:`Objective-C++ <objcxx>` as well as many dialects of those. For; language-specific information, please see the corresponding language; specific section:. - :ref:`C Language <c>`: K&R C, ANSI C89, ISO C90, ISO C94 (C89+AMD1), ISO; C99 (+TC1, TC2, TC3).; - :ref:`Objective-C Language <objc>`: ObjC 1, ObjC 2, ObjC 2.1, plus; variants depending on base language.; - :ref:`C++ Language <cxx>`; - :ref:`Objective C++ Language <objcxx>`; - :ref:`OpenCL Kernel Language <opencl>`: OpenCL C 1.0, 1.1, 1.2, 2.0, 3.0,; and C++ for OpenCL 1.0 and 2021. In addition to these base languages and their dialects, Clang supports a; broad variety of language extensions, which are documente",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:351,optimiz,optimizer,351,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"============================; Global Instruction Selection; ============================. .. warning::; This document is a work in progress. It reflects the current state of the; implementation, as well as open design and implementation issues. .. contents::; :local:; :depth: 1. Introduction; ============. GlobalISel is a framework that provides a set of reusable passes and utilities; for instruction selection --- translation from LLVM IR to target-specific; Machine IR (MIR). GlobalISel is intended to be a replacement for SelectionDAG and FastISel, to; solve three major problems:. * **Performance** --- SelectionDAG introduces a dedicated intermediate; representation, which has a compile-time cost. GlobalISel directly operates on the post-isel representation used by the; rest of the code generator, MIR.; It does require extensions to that representation to support arbitrary; incoming IR: :ref:`gmir`. * **Granularity** --- SelectionDAG and FastISel operate on individual basic; blocks, losing some global optimization opportunities. GlobalISel operates on the whole function. * **Modularity** --- SelectionDAG and FastISel are radically different and share; very little code. GlobalISel is built in a way that enables code reuse. For instance, both the; optimized and fast selectors share the :ref:`pipeline`, and targets can; configure that pipeline to better suit their needs. Design and Implementation Reference; ===================================. More information on the design and implementation of GlobalISel can be found in; the following sections. .. toctree::; :maxdepth: 1. GMIR; GenericOpcode; MIRPatterns; Pipeline; Porting; Resources. More information on specific passes can be found in the following sections:. .. toctree::; :maxdepth: 1. IRTranslator; Legalizer; RegBankSelect; InstructionSelect; KnownBits. .. _progress:. Progress and Future Work; ========================. The initial goal is to replace FastISel on AArch64. The next step will be to; replace SelectionDA",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst:1017,optimiz,optimization,1017,interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,1,['optimiz'],['optimization']
Performance,"=============================; Advanced Build Configurations; =============================. .. contents::; :local:. Introduction; ============. `CMake <http://www.cmake.org/>`_ is a cross-platform build-generator tool. CMake; does not build the project, it generates the files needed by your build tool; (GNU make, Visual Studio, etc.) for building LLVM. If **you are a new contributor**, please start with the :doc:`GettingStarted` or; :doc:`CMake` pages. This page is intended for users doing more complex builds. Many of the examples below are written assuming specific CMake Generators.; Unless otherwise explicitly called out these commands should work with any CMake; generator. Many of the build configurations mentioned on this documentation page can be; utilized by using a CMake cache. A CMake cache is essentially a configuration; file that sets the necessary flags for a specific build configuration. The caches; for Clang are located in :code:`/clang/cmake/caches` within the monorepo. They; can be passed to CMake using the :code:`-C` flag as demonstrated in the examples; below along with additional configuration flags. Bootstrap Builds; ================. The Clang CMake build system supports bootstrap (aka multi-stage) builds. At a; high level a multi-stage build is a chain of builds that pass data from one; stage into the next. The most common and simple version of this is a traditional; bootstrap build. In a simple two-stage bootstrap build, we build clang using the system compiler,; then use that just-built clang to build clang again. In CMake this simplest form; of a bootstrap build can be configured with a single option,; CLANG_ENABLE_BOOTSTRAP. .. code-block:: console. $ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release \; -DCLANG_ENABLE_BOOTSTRAP=On \; -DLLVM_ENABLE_PROJECTS=""clang"" \; <path to source>/llvm; $ ninja stage2. This command itself isn't terribly useful because it assumes default; configurations for each stage. The next series of examples utilize CMake cac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:790,cache,cache,790,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,4,['cache'],"['cache', 'caches']"
Performance,"=============================; Offloading Design & Internals; =============================. .. contents::; :local:. Introduction; ============. This document describes the Clang driver and code generation steps for creating; offloading applications. Clang supports offloading to various architectures; using programming models like CUDA, HIP, and OpenMP. The purpose of this; document is to illustrate the steps necessary to create an offloading; application using Clang. OpenMP Offloading; =================. Clang supports OpenMP target offloading to several different architectures such; as NVPTX, AMDGPU, X86_64, Arm, and PowerPC. Offloading code is generated by; Clang and then executed using the ``libomptarget`` runtime and the associated; plugin for the target architecture, e.g. ``libomptarget.rtl.cuda``. This section; describes the steps necessary to create a functioning device image that can be; loaded by the OpenMP runtime. More information on the OpenMP runtimes can be; found at the `OpenMP documentation page <https://openmp.llvm.org>`__. .. _Offloading Overview:. Offloading Overview; -------------------. The goal of offloading compilation is to create an executable device image that; can be run on the target device. OpenMP offloading creates executable images by; compiling the input file for both the host and the target device. The output; from the device phase then needs to be embedded into the host to create a fat; object. A special tool then needs to extract the device code from the fat; objects, run the device linking step, and embed the final image in a symbol the; host runtime library can use to register the library and access the symbols on; the device. Compilation Process; ^^^^^^^^^^^^^^^^^^^. The compiler performs the following high-level actions to generate OpenMP; offloading code:. * Compile the input file for the host to produce a bitcode file. Lower ``#pragma; omp target`` declarations to :ref:`offloading entries <Generating Offloading; Entries>` and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:910,load,loaded,910,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['load'],['loaded']
Performance,"==============================; "".name"" string Kernel argument name.; "".type_name"" string Kernel argument type name.; "".size"" integer Required Kernel argument size in bytes.; "".offset"" integer Required Kernel argument offset in; bytes. The offset must be a; multiple of the alignment; required by the argument.; "".value_kind"" string Required Kernel argument kind that; specifies how to set up the; corresponding argument.; Values include:. ""by_value""; The argument is copied; directly into the kernarg. ""global_buffer""; A global address space pointer; to the buffer data is passed; in the kernarg. ""dynamic_shared_pointer""; A group address space pointer; to dynamically allocated LDS; is passed in the kernarg. ""sampler""; A global address space; pointer to a S# is passed in; the kernarg. ""image""; A global address space; pointer to a T# is passed in; the kernarg. ""pipe""; A global address space pointer; to an OpenCL pipe is passed in; the kernarg. ""queue""; A global address space pointer; to an OpenCL device enqueue; queue is passed in the; kernarg. ""hidden_global_offset_x""; The OpenCL grid dispatch; global offset for the X; dimension is passed in the; kernarg. ""hidden_global_offset_y""; The OpenCL grid dispatch; global offset for the Y; dimension is passed in the; kernarg. ""hidden_global_offset_z""; The OpenCL grid dispatch; global offset for the Z; dimension is passed in the; kernarg. ""hidden_none""; An argument that is not used; by the kernel. Space needs to; be left for it, but it does; not need to be set up. ""hidden_printf_buffer""; A global address space pointer; to the runtime printf buffer; is passed in kernarg. Mutually; exclusive with; ""hidden_hostcall_buffer""; before Code Object V5. ""hidden_hostcall_buffer""; A global address space pointer; to the runtime hostcall buffer; is passed in kernarg. Mutually; exclusive with; ""hidden_printf_buffer""; before Code Object V5. ""hidden_default_queue""; A global address space pointer; to the OpenCL device enqueue; queue that should be use",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:137422,queue,queue,137422,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['queue'],['queue']
Performance,"==============================; LLVM Language Reference Manual; ==============================. .. contents::; :local:; :depth: 3. Abstract; ========. This document is a reference manual for the LLVM assembly language. LLVM; is a Static Single Assignment (SSA) based representation that provides; type safety, low-level operations, flexibility, and the capability of; representing 'all' high-level languages cleanly. It is the common code; representation used throughout all phases of the LLVM compilation; strategy. Introduction; ============. The LLVM code representation is designed to be used in three different; forms: as an in-memory compiler IR, as an on-disk bitcode representation; (suitable for fast loading by a Just-In-Time compiler), and as a human; readable assembly language representation. This allows LLVM to provide a; powerful intermediate representation for efficient compiler; transformations and analysis, while providing a natural means to debug; and visualize the transformations. The three different forms of LLVM are; all equivalent. This document describes the human readable; representation and notation. The LLVM representation aims to be light-weight and low-level while; being expressive, typed, and extensible at the same time. It aims to be; a ""universal IR"" of sorts, by being at a low enough level that; high-level ideas may be cleanly mapped to it (similar to how; microprocessors are ""universal IR's"", allowing many source languages to; be mapped to them). By providing type information, LLVM can be used as; the target of optimizations: for example, through pointer analysis, it; can be proven that a C automatic variable is never accessed outside of; the current function, allowing it to be promoted to a simple SSA value; instead of a memory location. .. _wellformed:. Well-Formedness; ---------------. It is important to note that this document describes 'well formed' LLVM; assembly language. There is a difference between what the parser accepts; and what is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:710,load,loading,710,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"===============================; Building a Distribution of LLVM; ===============================. .. contents::; :local:. Introduction; ============. This document is geared toward people who want to build and package LLVM and any; combination of LLVM sub-project tools for distribution. This document covers; useful features of the LLVM build system as well as best practices and general; information about packaging LLVM. If you are new to CMake you may find the :doc:`CMake` or :doc:`CMakePrimer`; documentation useful. Some of the things covered in this document are the inner; workings of the builds described in the :doc:`AdvancedBuilds` document. General Distribution Guidance; =============================. When building a distribution of a compiler it is generally advised to perform a; bootstrap build of the compiler. That means building a ""stage 1"" compiler with; your host toolchain, then building the ""stage 2"" compiler using the ""stage 1""; compiler. This is done so that the compiler you distribute benefits from all the; bug fixes, performance optimizations and general improvements provided by the; new compiler. In deciding how to build your distribution there are a few trade-offs that you; will need to evaluate. The big two are:. #. Compile time of the distribution against performance of the built compiler. #. Binary size of the distribution against performance of the built compiler. The guidance for maximizing performance of the generated compiler is to use LTO,; PGO, and statically link everything. This will result in an overall larger; distribution, and it will take longer to generate, but it provides the most; opportunity for the compiler to optimize. The guidance for minimizing distribution size is to dynamically link LLVM and; Clang libraries into the tools to reduce code duplication. This will come at a; substantial performance penalty to the generated binary both because it reduces; optimization opportunity, and because dynamic linking requires resolving s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:787,perform,perform,787,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['perform'],['perform']
Performance,"===============================; How To Use Instruction Mappings; ===============================. .. contents::; :local:. Introduction; ============. This document contains information about adding instruction mapping support; for a target. The motivation behind this feature comes from the need to switch; between different instruction formats during various optimizations. One approach; could be to use switch cases which list all the instructions along with formats; they can transition to. However, it has large maintenance overhead; because of the hardcoded instruction names. Also, whenever a new instruction is; added in the .td files, all the relevant switch cases should be modified; accordingly. Instead, the same functionality could be achieved with TableGen and; some support from the .td files for a fraction of maintenance cost. ``InstrMapping`` Class Overview; ===============================. TableGen uses relationship models to map instructions with each other. These; models are described using ``InstrMapping`` class as a base. Each model sets; various fields of the ``InstrMapping`` class such that they can uniquely; describe all the instructions using that model. TableGen parses all the relation; models and uses the information to construct relation tables which relate; instructions with each other. These tables are emitted in the; ``XXXInstrInfo.inc`` file along with the functions to query them. Following; is the definition of ``InstrMapping`` class defined in Target.td file:. .. code-block:: text. class InstrMapping {; // Used to reduce search space only to the instructions using this; // relation model.; string FilterClass;. // List of fields/attributes that should be same for all the instructions in; // a row of the relation table. Think of this as a set of properties shared; // by all the instructions related by this relationship.; list<string> RowFields = [];. // List of fields/attributes that are same for all the instructions; // in a column of the relat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUseInstrMappings.rst:361,optimiz,optimizations,361,interpreter/llvm-project/llvm/docs/HowToUseInstrMappings.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUseInstrMappings.rst,1,['optimiz'],['optimizations']
Performance,"===============================; MCJIT Design and Implementation; ===============================. Introduction; ============. This document describes the internal workings of the MCJIT execution; engine and the RuntimeDyld component. It is intended as a high level; overview of the implementation, showing the flow and interactions of; objects throughout the code generation and dynamic loading process. Engine Creation; ===============. In most cases, an EngineBuilder object is used to create an instance of; the MCJIT execution engine. The EngineBuilder takes an llvm::Module; object as an argument to its constructor. The client may then set various; options that we control the later be passed along to the MCJIT engine,; including the selection of MCJIT as the engine type to be created.; Of particular interest is the EngineBuilder::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:388,load,loading,388,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['loading']
Performance,"================================; Fuzzing LLVM libraries and tools; ================================. .. contents::; :local:; :depth: 2. Introduction; ============. The LLVM tree includes a number of fuzzers for various components. These are; built on top of :doc:`LibFuzzer <LibFuzzer>`. In order to build and run these; fuzzers, see :ref:`building-fuzzers`. Available Fuzzers; =================. clang-fuzzer; ------------. A |generic fuzzer| that tries to compile textual input as C++ code. Some of the; bugs this fuzzer has reported are `on bugzilla`__ and `on OSS Fuzz's; tracker`__. __ https://llvm.org/pr23057; __ https://bugs.chromium.org/p/oss-fuzz/issues/list?q=proj-llvm+clang-fuzzer. clang-proto-fuzzer; ------------------. A |protobuf fuzzer| that compiles valid C++ programs generated from a protobuf; class that describes a subset of the C++ language. This fuzzer accepts clang command line options after `ignore_remaining_args=1`.; For example, the following command will fuzz clang with a higher optimization; level:. .. code-block:: shell. % bin/clang-proto-fuzzer <corpus-dir> -ignore_remaining_args=1 -O3. clang-format-fuzzer; -------------------. A |generic fuzzer| that runs clang-format_ on C++ text fragments. Some of the; bugs this fuzzer has reported are `on bugzilla`__; and `on OSS Fuzz's tracker`__. .. _clang-format: https://clang.llvm.org/docs/ClangFormat.html; __ https://llvm.org/pr23052; __ https://bugs.chromium.org/p/oss-fuzz/issues/list?q=proj-llvm+clang-format-fuzzer. llvm-as-fuzzer; --------------. A |generic fuzzer| that tries to parse text as :doc:`LLVM assembly <LangRef>`.; Some of the bugs this fuzzer has reported are `on bugzilla`__. __ https://llvm.org/pr24639. llvm-dwarfdump-fuzzer; ---------------------. A |generic fuzzer| that interprets inputs as object files and runs; :doc:`llvm-dwarfdump <CommandGuide/llvm-dwarfdump>` on them. Some of the bugs; this fuzzer has reported are `on OSS Fuzz's tracker`__. __ https://bugs.chromium.org/p/oss-fuzz/i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst:1013,optimiz,optimization,1013,interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,1,['optimiz'],['optimization']
Performance,"==================================; Benchmarking tips; ==================================. Introduction; ============. For benchmarking a patch we want to reduce all possible sources of; noise as much as possible. How to do that is very OS dependent. Note that low noise is required, but not sufficient. It does not; exclude measurement bias. See; https://www.cis.upenn.edu/~cis501/papers/producing-wrong-data.pdf for; example. General; ================================. * Use a high resolution timer, e.g. perf under linux. * Run the benchmark multiple times to be able to recognize noise. * Disable as many processes or services as possible on the target system. * Disable frequency scaling, turbo boost and address space; randomization (see OS specific section). * Static link if the OS supports it. That avoids any variation that; might be introduced by loading dynamic libraries. This can be done; by passing ``-DLLVM_BUILD_STATIC=ON`` to cmake. * Try to avoid storage. On some systems you can use tmpfs. Putting the; program, inputs and outputs on tmpfs avoids touching a real storage; system, which can have a pretty big variability. To mount it (on linux and freebsd at least)::. mount -t tmpfs -o size=<XX>g none dir_to_mount. Linux; =====. * Disable address space randomization::. echo 0 > /proc/sys/kernel/randomize_va_space. * Set scaling_governor to performance::. for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do; echo performance > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; done. * Use https://github.com/lpechacek/cpuset to reserve cpus for just the; program you are benchmarking. If using perf, leave at least 2 cores; so that perf runs in one and your program in another::. cset shield -c N1,N2 -k on. This will move all threads out of N1 and N2. The ``-k on`` means; that even kernel threads are moved out. * Disable the SMT pair of the cpus you will use for the benchmark. The; pair of cpu N can be found in; ``/sys/devices/system/cpu/cpuN/topology/t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Benchmarking.rst:858,load,loading,858,interpreter/llvm-project/llvm/docs/Benchmarking.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Benchmarking.rst,1,['load'],['loading']
Performance,"==================================; Stack Safety Analysis; ==================================. Introduction; ============. The Stack Safety Analysis determines if stack allocated variables can be; considered 'safe' from memory access bugs. The primary purpose of the analysis is to be used by sanitizers to avoid; unnecessary instrumentation of 'safe' variables. SafeStack is going to be the; first user. 'safe' variables can be defined as variables that can not be used out-of-scope; (e.g. use-after-return) or accessed out of bounds. In the future it can be; extended to track other variable properties. E.g. we plan to extend; implementation with a check to make sure that variable is always initialized; before every read to optimize use-of-uninitialized-memory checks. How it works; ============. The analysis is implemented in two stages:. The intra-procedural, or 'local', stage performs a depth-first search inside; functions to collect all uses of each alloca, including loads/stores and uses as; arguments functions. After this stage we know which parts of the alloca are used; by functions itself but we don't know what happens after it is passed as; an argument to another function. The inter-procedural, or 'global', stage, resolves what happens to allocas after; they are passed as function arguments. This stage performs a depth-first search; on function calls inside a single module and propagates allocas usage through; functions calls. When used with ThinLTO, the global stage performs a whole program analysis over; the Module Summary Index. Testing; =======. The analysis is covered with lit tests. We expect that users can tolerate false classification of variables as; 'unsafe' when in-fact it's 'safe'. This may lead to inefficient code. However, we; can't accept false 'safe' classification which may cause sanitizers to miss actual; bugs in instrumented code. To avoid that we want additional validation tool. AddressSanitizer may help with this validation. We can instrument ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackSafetyAnalysis.rst:729,optimiz,optimize,729,interpreter/llvm-project/llvm/docs/StackSafetyAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackSafetyAnalysis.rst,3,"['load', 'optimiz', 'perform']","['loads', 'optimize', 'performs']"
Performance,"====================================; Getting Started with the LLVM System; ====================================. .. contents::; :local:. Overview; ========. Welcome to the LLVM project!. The LLVM project has multiple components. The core of the project is; itself called ""LLVM"". This contains all of the tools, libraries, and header; files needed to process intermediate representations and converts it into; object files. Tools include an assembler, disassembler, bitcode analyzer, and; bitcode optimizer. It also contains basic regression tests. C-like languages use the `Clang <https://clang.llvm.org/>`_ front end. This; component compiles C, C++, Objective C, and Objective C++ code into LLVM bitcode; -- and from there into object files, using LLVM. Other components include:; the `libc++ C++ standard library <https://libcxx.llvm.org>`_,; the `LLD linker <https://lld.llvm.org>`_, and more. Getting the Source Code and Building LLVM; =========================================. #. Check out LLVM (including subprojects like Clang):. * ``git clone https://github.com/llvm/llvm-project.git``; * Or, on windows:. ``git clone --config core.autocrlf=false; https://github.com/llvm/llvm-project.git``; * To save storage and speed-up the checkout time, you may want to do a; `shallow clone <https://git-scm.com/docs/git-clone#Documentation/git-clone.txt---depthltdepthgt>`_.; For example, to get the latest revision of the LLVM project, use. ``git clone --depth 1 https://github.com/llvm/llvm-project.git``. * You are likely only interested in the main branch moving forward, if; you don't want `git fetch` (or `git pull`) to download user branches, use:. ``sed 's#fetch = +refs/heads/\*:refs/remotes/origin/\*#fetch = +refs/heads/main:refs/remotes/origin/main#' -i llvm-project/.git/config``. #. Configure and build LLVM and Clang:. * ``cd llvm-project``; * ``cmake -S llvm -B build -G <generator> [options]``. Some common build system generators are:. * ``Ninja`` --- for generating `Ninja <https://",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:497,optimiz,optimizer,497,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimizer']
Performance,"====================================; LLVM bugpoint tool: design and usage; ====================================. .. contents::; :local:. Description; ===========. ``bugpoint`` narrows down the source of problems in LLVM tools and passes. It; can be used to debug three types of failures: optimizer crashes, miscompilations; by optimizers, or bad native code generation (including problems in the static; and JIT compilers). It aims to reduce large test cases to small, useful ones.; For example, if ``opt`` crashes while optimizing a file, it will identify the; optimization (or combination of optimizations) that causes the crash, and reduce; the file down to a small example which triggers the crash. For detailed case scenarios, such as debugging ``opt``, or one of the LLVM code; generators, see :doc:`HowToSubmitABug`. Design Philosophy; =================. ``bugpoint`` is designed to be a useful tool without requiring any hooks into; the LLVM infrastructure at all. It works with any and all LLVM passes and code; generators, and does not need to ""know"" how they work. Because of this, it may; appear to do stupid things or miss obvious simplifications. ``bugpoint`` is; also designed to trade off programmer time for computer time in the; compiler-debugging process; consequently, it may take a long period of; (unattended) time to reduce a test case, but we feel it is still worth it. Note; that ``bugpoint`` is generally very quick unless debugging a miscompilation; where each test of the program (which requires executing it) takes a long time. Automatic Debugger Selection; ----------------------------. ``bugpoint`` reads each ``.bc`` or ``.ll`` file specified on the command line; and links them together into a single module, called the test program. If any; LLVM passes are specified on the command line, it runs these passes on the test; program. If any of the passes crash, or if they produce malformed output (which; causes the verifier to abort), ``bugpoint`` starts the `crash d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst:289,optimiz,optimizer,289,interpreter/llvm-project/llvm/docs/Bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst,5,['optimiz'],"['optimization', 'optimizations', 'optimizer', 'optimizers', 'optimizing']"
Performance,"====================================; LLVM's Analysis and Transform Passes; ====================================. .. contents::; :local:. Introduction; ============. This document serves as a high level summary of the optimization features that; LLVM provides. Optimizations are implemented as Passes that traverse some; portion of a program to either collect information or transform the program.; The table below divides the passes that LLVM provides into three categories.; Analysis passes compute information that other passes can use or for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:218,optimiz,optimization,218,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimization']
Performance,"====================================; ``threadId`` ``@llvm.nvvm.read.ptx.sreg.tid.*``; ``blockIdx`` ``@llvm.nvvm.read.ptx.sreg.ctaid.*``; ``blockDim`` ``@llvm.nvvm.read.ptx.sreg.ntid.*``; ``gridDim`` ``@llvm.nvvm.read.ptx.sreg.nctaid.*``; ============ =====================================. Barriers; --------. '``llvm.nvvm.barrier0``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare void @llvm.nvvm.barrier0(). Overview:; """""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Linking with Libdevice; ======================. The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` that; implements many common mathematical functions. This library can be used as a; high-performance math library for any compilers using the LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:7740,perform,performance,7740,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['perform'],['performance']
Performance,"=====================================; Garbage Collection with LLVM; =====================================. .. contents::; :local:. Abstract; ========. This document covers how to integrate LLVM into a compiler for a language which; supports garbage collection. **Note that LLVM itself does not provide a; garbage collector.** You must provide your own. Quick Start; ============. First, you should pick a collector strategy. LLVM includes a number of built; in ones, but you can also implement a loadable plugin with a custom definition.; Note that the collector strategy is a description of how LLVM should generate; code such that it interacts with your collector and runtime, not a description; of the collector itself. Next, mark your generated functions as using your chosen collector strategy.; From c++, you can call:. .. code-block:: c++. F.setGC(<collector description name>);. This will produce IR like the following fragment:. .. code-block:: llvm. define void @foo() gc ""<collector description name>"" { ... }. When generating LLVM IR for your functions, you will need to:. * Use ``@llvm.gcread`` and/or ``@llvm.gcwrite`` in place of standard load and; store instructions. These intrinsics are used to represent load and store; barriers. If you collector does not require such barriers, you can skip; this step. * Use the memory allocation routines provided by your garbage collector's; runtime library. * If your collector requires them, generate type maps according to your; runtime's binary interface. LLVM is not involved in the process. In; particular, the LLVM type system is not suitable for conveying such; information though the compiler. * Insert any coordination code required for interacting with your collector.; Many collectors require running application code to periodically check a; flag and conditionally call a runtime function. This is often referred to; as a safepoint poll. You will need to identify roots (i.e. references to heap objects your collector; needs to kno",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:497,load,loadable,497,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['loadable']
Performance,"=====================================; Performance Tips for Frontend Authors; =====================================. .. contents::; :local:; :depth: 2. Abstract; ========. The intended audience of this document is developers of language frontends; targeting LLVM IR. This document is home to a collection of tips on how to; generate IR that optimizes well. IR Best Practices; =================. As with any optimizer, LLVM has its strengths and weaknesses. In some cases,; surprisingly small changes in the source IR can have a large effect on the; generated code. Beyond the specific items on the list below, it's worth noting that the most; mature frontend for LLVM is Clang. As a result, the further your IR gets from; what Clang might emit, the less likely it is to be effectively optimized. It; can often be useful to write a quick C program with the semantics you're trying; to model and see what decisions Clang's IRGen makes about what IR to emit.; Studying Clang's CodeGen directory can also be a good source of ideas. Note; that Clang and LLVM are explicitly version locked so you'll need to make sure; you're using a Clang built from the same git revision or release as the LLVM; library you're using. As always, it's *strongly* recommended that you track; tip of tree development, particularly during bring up of a new project. The Basics; ^^^^^^^^^^^. #. Make sure that your Modules contain both a data layout specification and; target triple. Without these pieces, non of the target specific optimization; will be enabled. This can have a major effect on the generated code quality. #. For each function or global emitted, use the most private linkage type; possible (private, internal or linkonce_odr preferably). Doing so will; make LLVM's inter-procedural optimizations much more effective. #. Avoid high in-degree basic blocks (e.g. basic blocks with dozens or hundreds; of predecessors). Among other issues, the register allocator is known to; perform badly with confronted with suc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:341,optimiz,optimizes,341,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,3,['optimiz'],"['optimized', 'optimizer', 'optimizes']"
Performance,==========================================. .. table:: AMDGPU Vendors; :name: amdgpu-vendor-table. ============ ==============================================================; Vendor Description; ============ ==============================================================; ``amd`` Can be used for all AMD GPU usage.; ``mesa3d`` Can be used if the OS is ``mesa3d``.; ============ ==============================================================. .. table:: AMDGPU Operating Systems; :name: amdgpu-os. ============== ============================================================; OS Description; ============== ============================================================; *<empty>* Defaults to the *unknown* OS.; ``amdhsa`` Compute kernels executed on HSA [HSA]_ compatible runtimes; such as:. - AMD's ROCm™ runtime [AMD-ROCm]_ using the *rocm-amdhsa*; loader on Linux. See *AMD ROCm Platform Release Notes*; [AMD-ROCm-Release-Notes]_ for supported hardware and; software.; - AMD's PAL runtime using the *pal-amdhsa* loader on; Windows. ``amdpal`` Graphic shaders and compute kernels executed on AMD's PAL; runtime using the *pal-amdpal* loader on Windows and Linux; Pro.; ``mesa3d`` Graphic shaders and compute kernels executed on AMD's Mesa; 3D runtime using the *mesa-mesa3d* loader on Linux.; ============== ============================================================. .. table:: AMDGPU Environments; :name: amdgpu-environment-table. ============ ==============================================================; Environment Description; ============ ==============================================================; *<empty>* Default.; ============ ==============================================================. .. _amdgpu-processors:. Processors; ----------. Use the Clang options ``-mcpu=<target-id>`` or ``--offload-arch=<target-id>`` to; specify the AMDGPU processor together with optional target features. See; :ref:`amdgpu-target-id` and :ref:`amdgpu-target-features` for AMD GPU target; specifi,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:2579,load,loader,2579,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"==========================================; Design and Usage of the InAlloca Attribute; ==========================================. Introduction; ============. The :ref:`inalloca <attr_inalloca>` attribute is designed to allow; taking the address of an aggregate argument that is being passed by; value through memory. Primarily, this feature is required for; compatibility with the Microsoft C++ ABI. Under that ABI, class; instances that are passed by value are constructed directly into; argument stack memory. Prior to the addition of inalloca, calls in LLVM; were indivisible instructions. There was no way to perform intermediate; work, such as object construction, between the first stack adjustment; and the final control transfer. With inalloca, all arguments passed in; memory are modelled as a single alloca, which can be stored to prior to; the call. Unfortunately, this complicated feature comes with a large; set of restrictions designed to bound the lifetime of the argument; memory around the call. For now, it is recommended that frontends and optimizers avoid producing; this construct, primarily because it forces the use of a base pointer.; This feature may grow in the future to allow general mid-level; optimization, but for now, it should be regarded as less efficient than; passing by value with a copy. Intended Usage; ==============. The example below is the intended LLVM IR lowering for some C++ code; that passes two default-constructed ``Foo`` objects to ``g`` in the; 32-bit Microsoft C++ ABI. .. code-block:: c++. // Foo is non-trivial.; struct Foo { int a, b; Foo(); ~Foo(); Foo(const Foo &); };; void g(Foo a, Foo b);; void f() {; g(Foo(), Foo());; }. .. code-block:: text. %struct.Foo = type { i32, i32 }; declare void @Foo_ctor(%struct.Foo* %this); declare void @Foo_dtor(%struct.Foo* %this); declare void @g(<{ %struct.Foo, %struct.Foo }>* inalloca %memargs). define void @f() {; entry:; %base = call i8* @llvm.stacksave(); %memargs = alloca <{ %struct.Foo, %struc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst:615,perform,perform,615,interpreter/llvm-project/llvm/docs/InAlloca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst,1,['perform'],['perform']
Performance,==========================================; ``amd`` Can be used for all AMD GPU usage.; ``mesa3d`` Can be used if the OS is ``mesa3d``.; ============ ==============================================================. .. table:: AMDGPU Operating Systems; :name: amdgpu-os. ============== ============================================================; OS Description; ============== ============================================================; *<empty>* Defaults to the *unknown* OS.; ``amdhsa`` Compute kernels executed on HSA [HSA]_ compatible runtimes; such as:. - AMD's ROCm™ runtime [AMD-ROCm]_ using the *rocm-amdhsa*; loader on Linux. See *AMD ROCm Platform Release Notes*; [AMD-ROCm-Release-Notes]_ for supported hardware and; software.; - AMD's PAL runtime using the *pal-amdhsa* loader on; Windows. ``amdpal`` Graphic shaders and compute kernels executed on AMD's PAL; runtime using the *pal-amdpal* loader on Windows and Linux; Pro.; ``mesa3d`` Graphic shaders and compute kernels executed on AMD's Mesa; 3D runtime using the *mesa-mesa3d* loader on Linux.; ============== ============================================================. .. table:: AMDGPU Environments; :name: amdgpu-environment-table. ============ ==============================================================; Environment Description; ============ ==============================================================; *<empty>* Default.; ============ ==============================================================. .. _amdgpu-processors:. Processors; ----------. Use the Clang options ``-mcpu=<target-id>`` or ``--offload-arch=<target-id>`` to; specify the AMDGPU processor together with optional target features. See; :ref:`amdgpu-target-id` and :ref:`amdgpu-target-features` for AMD GPU target; specific information. Every processor supports every OS ABI (see :ref:`amdgpu-os`) with the following exceptions:. * ``amdhsa`` is not supported in ``r600`` architecture (see :ref:`amdgpu-architecture-table`). .. table:: AMDGPU Processo,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:2841,load,loader,2841,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"=============================================; Building a JIT: Per-function Lazy Compilation; =============================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:931,optimiz,optimized,931,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,1,['optimiz'],['optimized']
Performance,"==============================================; JSON Compilation Database Format Specification; ==============================================. This document describes a format for specifying how to replay single; compilations independently of the build system. Background; ==========. Tools based on the C++ Abstract Syntax Tree need full information how to; parse a translation unit. Usually this information is implicitly; available in the build system, but running tools as part of the build; system is not necessarily the best solution:. - Build systems are inherently change driven, so running multiple tools; over the same code base without changing the code does not fit into; the architecture of many build systems.; - Figuring out whether things have changed is often an IO bound; process; this makes it hard to build low latency end user tools based; on the build system.; - Build systems are inherently sequential in the build graph, for; example due to generated source code. While tools that run; independently of the build still need the generated source code to; exist, running tools multiple times over unchanging source does not; require serialization of the runs according to the build dependency; graph. Supported Systems; =================. Clang has the ability to generate compilation database fragments via; ``-MJ argument <clang -MJ\<arg>>``. You can concatenate those; fragments together between ``[`` and ``]`` to create a compilation database. Currently `CMake <https://cmake.org>`_ (since 2.8.5) supports generation; of compilation databases for Unix Makefile builds (Ninja builds in the; works) with the option ``CMAKE_EXPORT_COMPILE_COMMANDS``. For projects on Linux, there is an alternative to intercept compiler; calls with a tool called `Bear <https://github.com/rizsotto/Bear>`_. `Bazel <https://bazel.build>`_ can export a compilation database via; `this extractor extension; <https://github.com/hedronvision/bazel-compile-commands-extractor>`_.; Bazel is otherwise",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/JSONCompilationDatabase.rst:832,latency,latency,832,interpreter/llvm-project/clang/docs/JSONCompilationDatabase.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/JSONCompilationDatabase.rst,1,['latency'],['latency']
Performance,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:464,optimiz,optimizer,464,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,2,['optimiz'],"['optimizations', 'optimizer']"
Performance,"==============================================; LLVM Atomic Instructions and Concurrency Guide; ==============================================. .. contents::; :local:. Introduction; ============. LLVM supports instructions which are well-defined in the presence of threads and; asynchronous signals. The atomic instructions are designed specifically to provide readable IR and; optimized code generation for the following:. * The C++ ``<atomic>`` header and C ``<stdatomic.h>`` headers. These; were originally added in C++11 and C11. The memory model has been; subsequently adjusted to correct errors in the initial; specification, so LLVM currently intends to implement the version; specified by C++20. (See the `C++20 draft standard; <https://isocpp.org/files/papers/N4860.pdf>`_ or the unofficial; `latest C++ draft <https://eel.is/c++draft/>`_. A `C2x draft; <https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3047.pdf>`_ is; also available, though the text has not yet been updated with the; errata corrected by C++20.). * Proper semantics for Java-style memory, for both ``volatile`` and regular; shared variables. (`Java Specification; <http://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html>`_). * gcc-compatible ``__sync_*`` builtins. (`Description; <https://gcc.gnu.org/onlinedocs/gcc/_005f_005fsync-Builtins.html>`_). * Other scenarios with atomic semantics, including ``static`` variables with; non-trivial constructors in C++. Atomic and volatile in the IR are orthogonal; ""volatile"" is the C/C++ volatile,; which ensures that every volatile load and store happens and is performed in the; stated order. A couple examples: if a SequentiallyConsistent store is; immediately followed by another SequentiallyConsistent store to the same; address, the first store can be erased. This transformation is not allowed for a; pair of volatile stores. On the other hand, a non-volatile non-atomic load can; be moved across a volatile load freely, but not an Acquire load. This document is int",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:378,optimiz,optimized,378,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['optimiz'],['optimized']
Performance,"==============================================; Using ARM NEON instructions in big endian mode; ==============================================. .. contents::; :local:. Introduction; ============. Generating code for big endian ARM processors is for the most part straightforward. NEON loads and stores however have some interesting properties that make code generation decisions less obvious in big endian mode. The aim of this document is to explain the problem with NEON loads and stores, and the solution that has been implemented in LLVM. In this document the term ""vector"" refers to what the ARM ABI calls a ""short vector"", which is a sequence of items that can fit in a NEON register. This sequence can be 64 or 128 bits in length, and can constitute 8, 16, 32 or 64 bit items. This document refers to A64 instructions throughout, but is almost applicable to the A32/ARMv7 instruction sets also. The ABI format for passing vectors in A32 is slightly different to A64. Apart from that, the same concepts apply. Example: C-level intrinsics -> assembly; ---------------------------------------. It may be helpful first to illustrate how C-level ARM NEON intrinsics are lowered to instructions. This trivial C function takes a vector of four ints and sets the zero'th lane to the value ""42""::. #include <arm_neon.h>; int32x4_t f(int32x4_t p) {; return vsetq_lane_s32(42, p, 0);; }. arm_neon.h intrinsics generate ""generic"" IR where possible (that is, normal IR instructions not ``llvm.arm.neon.*`` intrinsic calls). The above generates::. define <4 x i32> @f(<4 x i32> %p) {; %vset_lane = insertelement <4 x i32> %p, i32 42, i32 0; ret <4 x i32> %vset_lane; }. Which then becomes the following trivial assembly::. f: // @f; movz	w8, #0x2a; ins 	v0.s[0], w8; ret. Problem; =======. The main problem is how vectors are represented in memory and in registers. First, a recap. The ""endianness"" of an item affects its representation in memory only. In a register, a number is just a sequence of bits - 64",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:285,load,loads,285,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,2,['load'],['loads']
Performance,"===============================================. The code sequences used to implement the memory model are defined in the; following sections:. * :ref:`amdgpu-amdhsa-memory-model-gfx6-gfx9`; * :ref:`amdgpu-amdhsa-memory-model-gfx90a`; * :ref:`amdgpu-amdhsa-memory-model-gfx942`; * :ref:`amdgpu-amdhsa-memory-model-gfx10-gfx11`. .. _amdgpu-amdhsa-memory-model-gfx6-gfx9:. Memory Model GFX6-GFX9; ++++++++++++++++++++++. For GFX6-GFX9:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that for GFX7-GFX9 ``flat_load/store/atomic`` instructions can report out of; vector memory order if they access LDS memory, and out of LDS operation order; if they access global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore, no special action is requir",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:206818,queue,queues,206818,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"==================================================. Introduction and Warning; ========================. During the course of using LLVM, you may wish to customize it for your research; project or for experimentation. At this point, you may realize that you need to; add something to LLVM, whether it be a new fundamental type, a new intrinsic; function, or a whole new instruction. When you come to this realization, stop and think. Do you really need to extend; LLVM? Is it a new fundamental capability that LLVM does not support at its; current incarnation or can it be synthesized from already pre-existing LLVM; elements? If you are not sure, ask on the `LLVM forums; <https://discourse.llvm.org>`_. The reason is that; extending LLVM will get involved as you need to update all the different passes; that you intend to use with your extension, and there are ``many`` LLVM analyses; and transformations, so it may be quite a bit of work. Adding an `intrinsic function`_ is far easier than adding an; instruction, and is transparent to optimization passes. If your added; functionality can be expressed as a function call, an intrinsic function is the; method of choice for LLVM extension. Before you invest a significant amount of effort into a non-trivial extension,; **ask on the list** if what you are looking to do can be done with; already-existing infrastructure, or if maybe someone else is already working on; it. You will save yourself a lot of time and effort by doing so. .. _intrinsic function:. Adding a new intrinsic function; ===============================. Adding a new intrinsic function to LLVM is much easier than adding a new; instruction. Almost all extensions to LLVM should start as an intrinsic; function and then be turned into an instruction if warranted. #. ``llvm/docs/LangRef.html``:. Document the intrinsic. Decide whether it is code generator specific and; what the restrictions are. Talk to other people about it so that you are; sure it's a good idea. #. ``llvm/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst:1173,optimiz,optimization,1173,interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst,1,['optimiz'],['optimization']
Performance,"==================================================; Kaleidoscope: Extending the Language: Control Flow; ==================================================. .. contents::; :local:. Chapter 5 Introduction; ======================. Welcome to Chapter 5 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Parts 1-4 described the implementation of; the simple Kaleidoscope language and included support for generating; LLVM IR, followed by optimizations and a JIT compiler. Unfortunately, as; presented, Kaleidoscope is mostly useless: it has no control flow other; than call and return. This means that you can't have conditional; branches in the code, significantly limiting its power. In this episode; of ""build that compiler"", we'll extend Kaleidoscope to have an; if/then/else expression plus a simple 'for' loop. If/Then/Else; ============. Extending Kaleidoscope to support if/then/else is quite straightforward.; It basically requires adding support for this ""new"" concept to the; lexer, parser, AST, and LLVM code emitter. This example is nice, because; it shows how easy it is to ""grow"" a language over time, incrementally; extending it as new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:453,optimiz,optimizations,453,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,1,['optimiz'],['optimizations']
Performance,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:87,optimiz,optimization,87,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,6,"['optimiz', 'perform']","['optimization', 'optimizations', 'perform', 'performing']"
Performance,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:254,optimiz,optimizations,254,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,6,"['optimiz', 'perform']","['optimization', 'optimizations', 'optimizer', 'performed']"
Performance,"=======================================================; Building a JIT: Starting out with KaleidoscopeJIT; =======================================================. .. contents::; :local:. Chapter 1 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:821,concurren,concurrent,821,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,2,"['concurren', 'optimiz']","['concurrent', 'optimization']"
Performance,"=======================================================; Kaleidoscope: Extending the Language: Mutable Variables; =======================================================. .. contents::; :local:. Chapter 7 Introduction; ======================. Welcome to Chapter 7 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 6, we've built a; very respectable, albeit simple, `functional programming; language <http://en.wikipedia.org/wiki/Functional_programming>`_. In our; journey, we learned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:620,optimiz,optimize,620,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimize']
Performance,"=============================================================; How To Build Clang and LLVM with Profile-Guided Optimizations; =============================================================. Introduction; ============. PGO (Profile-Guided Optimization) allows your compiler to better optimize code; for how it actually runs. Users report that applying this to Clang and LLVM can; decrease overall compile time by 20%. This guide walks you through how to build Clang with PGO, though it also applies; to other subprojects, such as LLD. If you want to build other software with PGO, see the `end-user documentation; for PGO <https://clang.llvm.org/docs/UsersManual.html#profile-guided-optimization>`_. Using preconfigured CMake caches; ================================. See https://llvm.org/docs/AdvancedBuilds.html#multi-stage-pgo. Using the script; ================. We have a script at ``utils/collect_and_build_with_pgo.py``. This script is; tested on a few Linux flavors, and requires a checkout of LLVM, Clang, and; compiler-rt. Despite the name, it performs four clean builds of Clang, so it; can take a while to run to completion. Please see the script's ``--help`` for; more information on how to run it, and the different options available to you.; If you want to get the most out of PGO for a particular use-case (e.g. compiling; a specific large piece of software), please do read the section below on; 'benchmark' selection. Please note that this script is only tested on a few Linux distros. Patches to; add support for other platforms, as always, are highly appreciated. :). This script also supports a ``--dry-run`` option, which causes it to print; important commands instead of running them. Selecting 'benchmarks'; ======================. PGO does best when the profiles gathered represent how the user plans to use the; compiler. Notably, highly accurate profiles of llc building x86_64 code aren't; incredibly helpful if you're going to be targeting ARM. By default, the script above ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst:282,optimiz,optimize,282,interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,3,"['cache', 'optimiz']","['caches', 'optimization', 'optimize']"
Performance,"==================================================================; Getting Started with the LLVM System using Microsoft Visual Studio; ==================================================================. .. contents::; :local:. Overview; ========; Welcome to LLVM on Windows! This document only covers LLVM on Windows using; Visual Studio, not WSL, mingw or cygwin. In order to get started, you first need; to know some basic information. There are many different projects that compose LLVM. The first piece is the; LLVM suite. This contains all of the tools, libraries, and header files needed; to use LLVM. It contains an assembler, disassembler, bitcode analyzer and; bitcode optimizer. It also contains basic regression tests that can be used to; test the LLVM tools and the Clang front end. The second piece is the `Clang <https://clang.llvm.org/>`_ front end. This; component compiles C, C++, Objective C, and Objective C++ code into LLVM; bitcode. Clang typically uses LLVM libraries to optimize the bitcode and emit; machine code. LLVM fully supports the COFF object file format, which is; compatible with all other existing Windows toolchains. There are more LLVM projects which this document does not discuss. Requirements; ============; Before you begin to use the LLVM system, review the requirements given; below. This may save you some trouble by knowing ahead of time what hardware; and software you will need. Hardware; --------; Any system that can adequately run Visual Studio 2019 is fine. The LLVM; source tree including the git index consumes approximately 3GB.; Object files, libraries and executables consume approximately 5GB in; Release mode and much more in Debug mode. SSD drive and >16GB RAM are; recommended. Software; --------; You will need `Visual Studio <https://visualstudio.microsoft.com/>`_ 2019 or; later, with the latest Update installed. Visual Studio Community Edition; suffices. You will also need the `CMake <http://www.cmake.org/>`_ build system since it; ge",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedVS.rst:679,optimiz,optimizer,679,interpreter/llvm-project/llvm/docs/GettingStartedVS.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedVS.rst,2,['optimiz'],"['optimize', 'optimizer']"
Performance,"===================================================================; Cross-compilation using Clang; ===================================================================. Introduction; ============. This document will guide you in choosing the right Clang options; for cross-compiling your code to a different architecture. It assumes you; already know how to compile the code in question for the host architecture,; and that you know how to choose additional include and library paths. However, this document is *not* a ""how to"" and won't help you setting your; build system or Makefiles, nor choosing the right CMake options, etc.; Also, it does not cover all the possible options, nor does it contain; specific examples for specific architectures. For a concrete example, the; `instructions for cross-compiling LLVM itself; <https://llvm.org/docs/HowToCrossCompileLLVM.html>`_ may be of interest. After reading this document, you should be familiar with the main issues; related to cross-compilation, and what main compiler options Clang provides; for performing cross-compilation. Cross compilation issues; ========================. In GCC world, every host/target combination has its own set of binaries,; headers, libraries, etc. So, it's usually simple to download a package; with all files in, unzip to a directory and point the build system to; that compiler, that will know about its location and find all it needs to; when compiling your code. On the other hand, Clang/LLVM is natively a cross-compiler, meaning that; one set of programs can compile to all targets by setting the ``-target``; option. That makes it a lot easier for programmers wishing to compile to; different platforms and architectures, and for compiler developers that; only have to maintain one build system, and for OS distributions, that; need only one set of main packages. But, as is true to any cross-compiler, and given the complexity of; different architectures, OS's and options, it's not always easy finding; the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CrossCompilation.rst:1053,perform,performing,1053,interpreter/llvm-project/clang/docs/CrossCompilation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CrossCompilation.rst,1,['perform'],['performing']
Performance,"=ON \; -DTEST_SUITE_USE_IR_PGO=ON \; -DTEST_SUITE_RUN_TYPE=train \; ../test-suite; % make; % llvm-lit .; # Use the profile data for compilation and actual benchmark run:; % cmake -DTEST_SUITE_PROFILE_GENERATE=OFF \; -DTEST_SUITE_PROFILE_USE=ON \; -DTEST_SUITE_RUN_TYPE=ref \; .; % make; % llvm-lit -o result.json .; ```. To use Clang frontend's PGO instead of LLVM IR PGO, set `-DTEST_SUITE_USE_IR_PGO=OFF`. The `TEST_SUITE_RUN_TYPE` setting only affects the SPEC benchmark suites. Cross Compilation and External Devices; --------------------------------------. ### Compilation. CMake allows to cross compile to a different target via toolchain files. More; information can be found here:. - [https://llvm.org/docs/lnt/tests.html#cross-compiling](https://llvm.org/docs/lnt/tests.html#cross-compiling). - [https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html](https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html). Cross compilation from macOS to iOS is possible with the; `test-suite/cmake/caches/target-target-*-iphoneos-internal.cmake` CMake cache; files; this requires an internal iOS SDK. ### Running. There are two ways to run the tests in a cross compilation setting:. - Via SSH connection to an external device: The `TEST_SUITE_REMOTE_HOST` option; should be set to the SSH hostname. The executables and data files need to be; transferred to the device after compilation. This is typically done via the; `rsync` make target. After this, the lit runner can be used on the host; machine. It will prefix the benchmark and verification command lines with an; `ssh` command. Example:. ```bash; % cmake -G Ninja -D CMAKE_C_COMPILER=path/to/clang \; -C ../test-suite/cmake/caches/target-arm64-iphoneos-internal.cmake \; -D CMAKE_BUILD_TYPE=Release \; -D TEST_SUITE_REMOTE_HOST=mydevice \; ../test-suite; % ninja; % ninja rsync; % llvm-lit -j1 -o result.json .; ```. - You can specify a simulator for the target machine with the; `TEST_SUITE_RUN_UNDER` setting. The lit run",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:11408,cache,caches,11408,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['cache'],['caches']
Performance,"=init; src:bad/init/files/*=init. Suppressing memory leaks; ------------------------. Memory leak reports produced by :doc:`LeakSanitizer` (if it is run as a part; of AddressSanitizer) can be suppressed by a separate file passed as. .. code-block:: bash. LSAN_OPTIONS=suppressions=MyLSan.supp. which contains lines of the form `leak:<pattern>`. Memory leak will be; suppressed if pattern matches any function name, source file name, or; library name in the symbolized stack trace of the leak report. See; `full documentation; <https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer#suppressions>`_; for more details. Code generation control; =======================. Instrumentation code outlining; ------------------------------. By default AddressSanitizer inlines the instrumentation code to improve the; run-time performance, which leads to increased binary size. Using the; (clang flag ``-fsanitize-address-outline-instrumentation` default: ``false``); flag forces all code instrumentation to be outlined, which reduces the size; of the generated code, but also reduces the run-time performance. Limitations; ===========. * AddressSanitizer uses more real memory than a native run. Exact overhead; depends on the allocations sizes. The smaller the allocations you make the; bigger the overhead is.; * AddressSanitizer uses more stack memory. We have seen up to 3x increase.; * On 64-bit platforms AddressSanitizer maps (but not reserves) 16+ Terabytes of; virtual address space. This means that tools like ``ulimit`` may not work as; usually expected.; * Static linking of executables is not supported. Supported Platforms; ===================. AddressSanitizer is supported on:. * Linux i386/x86\_64 (tested on Ubuntu 12.04); * macOS 10.7 - 10.11 (i386/x86\_64); * iOS Simulator; * Android ARM; * NetBSD i386/x86\_64; * FreeBSD i386/x86\_64 (tested on FreeBSD 11-current); * Windows 8.1+ (i386/x86\_64). Ports to various other platforms are in progress. Current Status; ========",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst:12188,perform,performance,12188,interpreter/llvm-project/clang/docs/AddressSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst,1,['perform'],['performance']
Performance,"> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.load.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.load <int_mload>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is a; vector of boolean values with the same number of elements as the return type.; The third is the explicit vector length of the operation. The return type and; underlying type of the base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.vp.load``' intrinsic reads a vector from memory in the same way as; the '``llvm.masked.load``' intrinsic, where the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way.; Certain '``llvm.masked.load``' operands do not have corresponding operands in; '``llvm.vp.load``': the '``passthru``' operand is implicitly ``poison``; the; '``alignment``' operand is taken as the ``align`` parameter attribute, if; provided. The default alignment is taken as the ABI alignment of the return; type as specified by the :ref:`datalayout string<langref_datalayout>`. Examples:; """""""""""""""""". .. code-block:: text. %r = call <8 x i8> @llvm.vp.load.v8i8.p0(ptr align 2 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %also.r = call <8 x i8> @llvm.masked.load.v8i8.p0(ptr %ptr, i32 2, <8 x i1> %mask, <8 x i8> poison). .. _int_vp_store:. '``llvm.vp.store``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare void @llvm.vp.store.v4f32.p0(<4 x float> %val, ptr %ptr, <4 x i1> %mask, i32 %evl); declare void @llvm.vp.store.nxv2i16.p0(<vscale x 2 x i16> %val, ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare void @llvm.vp.store.v8f32.p1(<8 x float> %val, ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare void @llvm.vp.s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:784315,load,load,784315,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"> asi = 0; // asi not currently used; bits<5> rs2;; let op = opVal;; let op3 = op3val;; let Inst{13} = 0; // i field = 0; let Inst{12-5} = asi; // address space identifier; let Inst{4-0} = rs2;; }. ``F3_1`` assigns a value to ``op`` and ``op3`` fields, and defines the ``rs2``; field. Therefore, a ``F3_1`` format instruction will require a definition for; ``rd``, ``rs1``, and ``rs2`` in order to fully specify the instruction encoding. The ``XNORrr`` instruction then provides those three operands in its; OutOperandList and InOperandList, which bind to the corresponding fields, and; thus complete the instruction encoding. For some instructions, a single operand may contain sub-operands. As shown; earlier, the instruction ``LDrr`` uses an input operand of type ``MEMrr``. This; operand type contains two register sub-operands, defined by the; ``MIOperandInfo`` value to be ``(ops IntRegs, IntRegs)``. .. code-block:: text. def LDrr : F3_1 <3, 0b000000, (outs IntRegs:$rd), (ins (MEMrr $rs1, $rs2):$addr),; ""ld [$addr], $dst"",; [(set i32:$dst, (load ADDRrr:$addr))]>;. As this instruction is also the ``F3_1`` format, it will expect operands named; ``rd``, ``rs1``, and ``rs2`` as well. In order to allow this, a complex operand; can optionally give names to each of its sub-operands. In this example; ``MEMrr``'s first sub-operand is named ``$rs1``, the second ``$rs2``, and the; operand as a whole is also given the name ``$addr``. When a particular instruction doesn't use all the operands that the instruction; format defines, a constant value may instead be bound to one or all. For; example, the ``RDASR`` instruction only takes a single register operand, so we; assign a constant zero to ``rs2``:. .. code-block:: text. let rs2 = 0 in; def RDASR : F3_1<2, 0b101000,; (outs IntRegs:$rd), (ins ASRRegs:$rs1),; ""rd $rs1, $rd"", []>;. Instruction Operand Name Mapping; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. TableGen will also generate a function called getNamedOperandIdx() which; can be used to lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:39202,load,load,39202,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['load']
Performance,"> cppyy.cppdef(r""""""\; ... void process_data(double) { std::cerr << ""processing double\n""; }; ... void process_data(int32_t) { std::cerr << ""processing int\n""; }""""""); True; >>> cppyy.gbl.process_data(2**32) # too large for int32_t type; processing double; >>>. There are two rounds to run-time overload resolution.; The first round considers all overloads in sorted order, with promotion but; no implicit conversion allowed.; The sorting is based on priority scores of each overload.; Higher priority is given to overloads with argument types that can be; promoted or align better with Python types.; E.g. ``int`` is preferred over ``double`` and ``double`` is preferred over; ``float``.; If argument conversion fails for all overloads during this round *and* at; least one argument converter has indicated that it can do implicit; conversion, a second round is tried where implicit conversion, including; instantiation of temporaries, is allowed.; The implicit creation of temporaries, although convenient, can be costly in; terms of run-time performance. During some template calls, implicit conversion is not allowed, giving; preference to new instantiations (as is the case in C++).; If, however, a previously instantiated overload is available and would match; with promotion, it is preferred over a (costly) new instantiation, unless a; template overload is explicitly selected using template arguments.; For example:. .. code-block:: python. >>> cppyy.cppdef(r""""""\; ... template<typename T>; ... T process_T(T t) { return t; }""""""); True; >>> type(cppyy.gbl.process_T(1.0)); <class 'float'>; >>> type(cppyy.gbl.process_T(1)) # selects available ""double"" overload; <class 'float'>; >>> type(cppyy.gbl.process_T[int](1)) # explicit selection of ""int"" overload; <class 'int'>; >>>. The template parameters used for instantiation can depend on the argument; values.; For example, if the type of an argument is Python ``int``, but its value is; too large for a 4-byte C++ ``int``, the template may be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst:7607,perform,performance,7607,bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,1,['perform'],['performance']
Performance,"> poison. .. _int_vp_nearbyint:. '``llvm.vp.nearbyint.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.nearbyint.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.nearbyint.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.nearbyint.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point nearbyint of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of floating-point type.; The second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.nearbyint``' intrinsic performs floating-point nearbyint; (:ref:`nearbyint <int_nearbyint>`) of the first vector operand on each enabled lane.; The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.nearbyint.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.nearbyint.v4f32(<4 x float> %a); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_round:. '``llvm.vp.round.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.round.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.round.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.round.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floatin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:826439,perform,performs,826439,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"> {; };; public:; my_stream2() {; this->init(new my_streambuf);; }; };. void test() {; my_stream1<char> *p1 = new my_stream1<char>;; my_stream2<char> *p2 = new my_stream2<char>;; p1->narrow('a', 'b'); // warn; p2->narrow('a', 'b'); // ok; }. undefbehavior.MinusOnePosType; (C++); Undefined behavior: passing -1 to any streambuf/; istream/ostream member that accepts a value of; type traits::pos_type result in undefined behavior.; Source: C++03 27.4.3.2p3; C++11 27.5.4.2p3. #include <fstream>. class my_streambuf : public std::streambuf {; void f() {; seekpos(-1); // warn; }; };. #include <fstream>. void test() {; std::filebuf fb;; std::istream in(&fb);; std::filebuf::off_type pos(-1);; in.seekg(pos); // warn; }. different. Name, DescriptionExampleProgress. different.SuccessiveAssign; (C); Successive assign to a variable. int test() {; int i;; i=1;; i=2; // warn; return i;; }. different.NullDerefStmtOrder; (C); Dereferencing of the null pointer might take place. Checking the pointer for; null should be performed first.; Note: possibly an enhancement to ; core.NullDereference. struct S {; int x;; };. struct S* f();. void test() {; struct S *p1 = f();; int x1 = p1->x; // warn; if (p1) {};. struct S *p2 = f();; int x2 = p2->x; // ok; }. different.NullDerefCondOrder; (C); Dereferencing of the null pointer might take place. Checking the pointer for; null should be performed first.; Note: possibly an enhancement to ; core.NullDereference. struct S {int i;};. struct S* f();. void test() {; struct S *p = f();; if (p->i && p) {}; // warn; }. different.MultipleAccessors; (C++); Identical accessor bodies. Possibly a misprint. class A {; int i;; int j;; public:; int getI() { return i; }; int getJ() { return i; } // warn; };. class A {; int i;; int j;; public:; void setI(int& ii) { i = ii; }; void setJ(int& jj) { i = jj; } // warn; };. different.AccessorsForPublic; (C++); Accessors exist for a public class field. Should this field really be; public?. class A {; public:; int i; // war",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:19923,perform,performed,19923,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,1,['perform'],['performed']
Performance,">' must be a pointer to; that type. If the ``cmpxchg`` is marked as ``volatile``, then the; optimizer is not allowed to modify the number or order of execution of; this ``cmpxchg`` with other :ref:`volatile operations <volatile>`. The success and failure :ref:`ordering <ordering>` arguments specify how this; ``cmpxchg`` synchronizes with other atomic operations. Both ordering parameters; must be at least ``monotonic``, the failure ordering cannot be either; ``release`` or ``acq_rel``. A ``cmpxchg`` instruction can also take an optional; "":ref:`syncscope <syncscope>`"" argument. Note: if the alignment is not greater or equal to the size of the `<value>`; type, the atomic operation is likely to require a lock and have poor; performance. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. If unspecified, the alignment is assumed to be equal to the; size of the '<value>' type. Note that this default alignment assumption is; different from the alignment used for the load/store instructions when align; isn't specified. The pointer passed into cmpxchg must have alignment greater than or; equal to the size in memory of the operand. Semantics:; """""""""""""""""""". The contents of memory at the location specified by the '``<pointer>``' operand; is read and compared to '``<cmp>``'; if the values are equal, '``<new>``' is; written to the location. The original value at the location is returned,; together with a flag indicating success (true) or failure (false). If the cmpxchg operation is marked as ``weak`` then a spurious failure is; permitted: the operation may not write ``<new>`` even if the comparison; matched. If the cmpxchg operation is strong (the default), the i1 value is 1 if and only; if the value loaded equals ``cmp``. A successful ``cmpxchg`` is a read-modify-write instruction for the purpose of; identifying release sequences. A failed ``cmpxchg`` is equivalent to an atomic; load with an ordering parameter determined the second ord",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:427518,load,load,427518,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,">* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%esp); 	movaps	%xmm0, 64(%esp); 	movl	60(%esp), %eax; 	movl	%eax, 124(%esp); 	movl	56(%esp), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit copies>; 	movaps	(%esp), %xmm0; 	movaps	16(%esp), %xmm1; 	movaps	32(%esp), %xmm2; 	movaps	48(%esp), %xmm3; 	addl	$140, %esp; 	ret. On Nehalem, it may even be cheaper to just use movups when unaligned than to; fall back to lower-granularity chunks. //===---------------------------------------------------------------------===//. Implement processor-specific optimizations for parity with GCC on these; processors. GCC does two optimizations:. 1. ix86_pad_returns inserts a noop before ret instructions if immediately; preceded by a conditional branch or is the target of a jump.; 2. ix86_avoid_jump_misspredicts inserts noops in cases where a 16-byte block of; code contains more than 3 branches.; ; The first one is done for all AMDs, Core2, and ""Generic""; The second one is done for: Atom, Pentium Pro, all AMDs, Pentium 4, Nocona,; Core 2, and ""Generic"". //===---------------------------------------------------------------------===//; Testcase:; int x(int a) { return (a&0xf0)>>4; }. Current output:; 	movl	4(%esp), %eax; 	shrl	$4, %eax; 	andl	$15, %eax; 	ret. Ideal output:; 	movzbl	4(%esp), %eax; 	shrl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. Re-implement atomic builtins __sync_add_and_fetch() and __sync_sub_and_fetch; properly. When the return value is not used (i.e. only care about the value in the; memory), x86 does not have to use add to implement these. In",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:32831,optimiz,optimizations,32831,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,1,['optimiz'],['optimizations']
Performance,">, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.or.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``OR`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.or``' intrinsic performs the integer ``OR`` reduction; (:ref:`llvm.vector.reduce.or <int_vector_reduce_or>`) of the vector operand; ``val`` on each enabled lane, performing an '``or``' of that with the scalar; ``start_value``. Disabled lanes are treated as containing the neutral value; ``0`` (i.e. having no effect on the reduction operation). If the vector length; is zero, the result is the start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.or.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> <i32 0, i32 0, i32 0, i32 0>; %reduction = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %masked.a); %also.r = or i32 %reduction, %start. .. _int_vp_reduce_xor:. '``llvm.vp.reduce.xor.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.xor.v4i32(i32 <start_value>, <4 x i32",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:759917,perform,performing,759917,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performing']
Performance,">. void test() {; STARTUPINFO si;; PROCESS_INFORMATION pi;; CreateProcess(NULL, TEXT(""C:\\Program Files\\App -L -S""),; NULL, NULL, TRUE, 0, NULL, NULL, &si, π);; // warn; }. WinAPI.LoadLibrary; (C); The SearchPath() function is used to retrieve a path to a DLL for; a subsequent LoadLibrary() call.; Source: ; MSDN: LoadLibrary function, Security Remarks. #include <windows.h>. HINSTANCE test() {; char filePath[100];; SearchPath(NULL, ""file.dll"", NULL, 100, filePath, NULL);; return LoadLibrary(filePath); // warn; }. WinAPI.WideCharToMultiByte; (C); Buffer overrun while calling WideCharToMultiByte(). The size of; the input buffer equals the number of characters in the Unicode string, while; the size of the output buffer equals the number of bytes.; Source: ; MSDN: WideCharToMultiByte function. #include <windows.h>. void test() {; wchar_t ws[] = L""abc"";; char s[3];; WideCharToMultiByte(CP_UTF8, 0, ws, -1, s,; 3, NULL, NULL); // warn; }. optimization. Name, DescriptionExampleProgress. optimization.PassConstObjByValue; (C, C++); Optimization: It is more effective to pass constant parameter by reference to; avoid unnecessary object copying. struct A {};. void f(const struct A a); // warn. optimization.PostfixIncIter; (C++); Optimization: It is more effective to use prefix increment operator with; iterator.; Source: Scott Meyers ""More Effective C++"", item 6:; Distinguish between prefix and postfix forms of increment and decrement; operators. #include <vector>. void test() {; std::vector<int> v;; std::vector<int>::const_iterator it;; for(it = v.begin();; it != v.end(); it++) {}; // warn; }. optimization.MultipleCallsStrlen; (C); Optimization: multiple calls to strlen() for a string in an; expression. It is more effective to hold a value returned; from strlen() in a temporary variable. #include <string.h>. void test(const char* s) {; if (strlen(s) > 0 &&; strlen(s) < 7) {}; // warn; }. optimization.StrLengthCalculation; (C++); Optimization: it is more efficient to use string::",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:27264,optimiz,optimization,27264,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,1,['optimiz'],['optimization']
Performance,">> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27474,optimiz,optimized,27474,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['optimiz'],['optimized']
Performance,">registerCallbacks(*ThePIC, TheMAM.get());; ... After initializing the global module ``TheModule`` and the FunctionPassManager,; we need to initialize other parts of the framework. The four AnalysisManagers; allow us to add analysis passes that run across the four levels of the IR; hierarchy. PassInstrumentationCallbacks and StandardInstrumentations are; required for the pass instrumentation framework, which allows developers to; customize what happens between passes. Once these managers are set up, we use a series of ""addPass"" calls to add a; bunch of LLVM transform passes:. .. code-block:: c++. // Add transform passes.; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->addPass(InstCombinePass());; // Reassociate expressions.; TheFPM->addPass(ReassociatePass());; // Eliminate Common SubExpressions.; TheFPM->addPass(GVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; TheFPM->addPass(SimplifyCFGPass());. In this case, we choose to add four optimization passes.; The passes we choose here are a pretty standard set; of ""cleanup"" optimizations that are useful for a wide variety of code. I won't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:7427,optimiz,optimization,7427,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimization']
Performance,"? Description; ================= ============== ========= ================================; ""Name"" string Kernel argument name.; ""TypeName"" string Kernel argument type name.; ""Size"" integer Required Kernel argument size in bytes.; ""Align"" integer Required Kernel argument alignment in; bytes. Must be a power of two.; ""ValueKind"" string Required Kernel argument kind that; specifies how to set up the; corresponding argument.; Values include:. ""ByValue""; The argument is copied; directly into the kernarg. ""GlobalBuffer""; A global address space pointer; to the buffer data is passed; in the kernarg. ""DynamicSharedPointer""; A group address space pointer; to dynamically allocated LDS; is passed in the kernarg. ""Sampler""; A global address space; pointer to a S# is passed in; the kernarg. ""Image""; A global address space; pointer to a T# is passed in; the kernarg. ""Pipe""; A global address space pointer; to an OpenCL pipe is passed in; the kernarg. ""Queue""; A global address space pointer; to an OpenCL device enqueue; queue is passed in the; kernarg. ""HiddenGlobalOffsetX""; The OpenCL grid dispatch; global offset for the X; dimension is passed in the; kernarg. ""HiddenGlobalOffsetY""; The OpenCL grid dispatch; global offset for the Y; dimension is passed in the; kernarg. ""HiddenGlobalOffsetZ""; The OpenCL grid dispatch; global offset for the Z; dimension is passed in the; kernarg. ""HiddenNone""; An argument that is not used; by the kernel. Space needs to; be left for it, but it does; not need to be set up. ""HiddenPrintfBuffer""; A global address space pointer; to the runtime printf buffer; is passed in kernarg. Mutually; exclusive with; ""HiddenHostcallBuffer"". ""HiddenHostcallBuffer""; A global address space pointer; to the runtime hostcall buffer; is passed in kernarg. Mutually; exclusive with; ""HiddenPrintfBuffer"". ""HiddenDefaultQueue""; A global address space pointer; to the OpenCL device enqueue; queue that should be used by; the kernel by default is; passed in the kernarg. ""HiddenComp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:124183,queue,queue,124183,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"@bb.exitStub; mov r0, #3; bx lr; LBB1_8:; .align 2; LCPI1_0:; .long 642. gcc compiles to:. 	cmp	r0, #9; 	@ lr needed for prologue; 	bhi	L2; 	ldr	r3, L11; 	mov	r2, #1; 	mov	r1, r2, asl r0; 	ands	r0, r3, r2, asl r0; 	movne	r0, #2; 	bxne	lr; 	tst	r1, #13; 	beq	L9; L3:; 	mov	r0, r2; 	bx	lr; L9:; 	tst	r1, #256; 	movne	r0, #3; 	bxne	lr; L2:; 	mov	r0, #0; 	bx	lr; L12:; 	.align 2; L11:; 	.long	642; . GCC is doing a couple of clever things here:; 1. It is predicating one of the returns. This isn't a clear win though: in; cases where that return isn't taken, it is replacing one condbranch with; two 'ne' predicated instructions.; 2. It is sinking the shift of ""1 << i"" into the tst, and using ands instead of; tst. This will probably require whole function isel.; 3. GCC emits:; 	tst	r1, #256; we emit:; mov r1, #1; lsl r1, r1, #8; tst r2, r1. //===---------------------------------------------------------------------===//. When spilling in thumb mode and the sp offset is too large to fit in the ldr /; str offset field, we load the offset from a constpool entry and add it to sp:. ldr r2, LCPI; add r2, sp; ldr r2, [r2]. These instructions preserve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:4100,load,load,4100,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,1,['load'],['load']
Performance,"@llvm.gcwrite(i8* %value, i8* %object, i8** %derived). For write barriers, LLVM provides the ``llvm.gcwrite`` intrinsic function. It; has exactly the same semantics as a non-volatile ``store`` to the derived; pointer (the third argument). The exact code generated is specified by the; Function's selected :ref:`GC strategy <plugin>`. Many important algorithms require write barriers, including generational and; concurrent collectors. Additionally, write barriers could be used to implement; reference counting. Read barrier: ``llvm.gcread``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. i8* @llvm.gcread(i8* %object, i8** %derived). For read barriers, LLVM provides the ``llvm.gcread`` intrinsic function. It has; exactly the same semantics as a non-volatile ``load`` from the derived pointer; (the second argument). The exact code generated is specified by the Function's; selected :ref:`GC strategy <plugin>`. Read barriers are needed by fewer algorithms than write barriers, and may have a; greater performance impact since pointer reads are more frequent than writes. .. _plugin:. .. _builtin-gc-strategies:. Built In GC Strategies; ======================. LLVM includes built in support for several varieties of garbage collectors. The Shadow Stack GC; ----------------------. To use this collector strategy, mark your functions with:. .. code-block:: c++. F.setGC(""shadow-stack"");. Unlike many GC algorithms which rely on a cooperative code generator to compile; stack maps, this algorithm carefully maintains a linked list of stack roots; [:ref:`Henderson2002 <henderson02>`]. This so-called ""shadow stack"" mirrors the; machine stack. Maintaining this data structure is slower than using a stack map; compiled into the executable as constant data, but has a significant portability; advantage because it requires no special support from the target code generator,; and does not require tricky platform-specific code to crawl the machine stack. The tradeoff for this simplicity and por",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:15737,perform,performance,15737,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['perform'],['performance']
Performance,"@print(i32 %n.addr) #4; %2 = call i8 @llvm.coro.suspend(token none, i1 false); switch i8 %2, label %suspend [i8 0, label %loop.resume; i8 1, label %cleanup]; loop.resume:; %inc = add nsw i32 %n.addr, 1; %sub = xor i32 %n.addr, -1; call void @print(i32 %sub); %3 = call i8 @llvm.coro.suspend(token none, i1 false); switch i8 %3, label %suspend [i8 0, label %loop; i8 1, label %cleanup]. In this case, the coroutine frame would include a suspend index that will; indicate at which suspend point the coroutine needs to resume. .. code-block:: llvm. %f.frame = type { ptr, ptr, i32, i32 }. The resume function will use an index to jump to an appropriate basic block and will look; as follows:. .. code-block:: llvm. define internal fastcc void @f.Resume(ptr %FramePtr) {; entry.Resume:; %index.addr = getelementptr inbounds %f.Frame, ptr %FramePtr, i64 0, i32 2; %index = load i8, ptr %index.addr, align 1; %switch = icmp eq i8 %index, 0; %n.addr = getelementptr inbounds %f.Frame, ptr %FramePtr, i64 0, i32 3; %n = load i32, ptr %n.addr, align 4. br i1 %switch, label %loop.resume, label %loop. loop.resume:; %sub = sub nsw i32 0, %n; call void @print(i32 %sub); br label %suspend; loop:; %inc = add nsw i32 %n, 1; store i32 %inc, ptr %n.addr, align 4; tail call void @print(i32 %inc); br label %suspend. suspend:; %storemerge = phi i8 [ 0, %loop ], [ 1, %loop.resume ]; store i8 %storemerge, ptr %index.addr, align 1; ret void; }. If different cleanup code needs to get executed for different suspend points,; a similar switch will be in the `f.destroy` function. .. note ::. Using suspend index in a coroutine state and having a switch in `f.resume` and; `f.destroy` is one of the possible implementation strategies. We explored; another option where a distinct `f.resume1`, `f.resume2`, etc. are created for; every suspend point, and instead of storing an index, the resume and destroy; function pointers are updated at every suspend. Early testing showed that the; current approach is easier on the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:19418,load,load,19418,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['load'],['load']
Performance,"A specific shape can be created; stand-alone:. ``` {.cpp}; TGeoBBox *box = new TGeoBBox(""s_box"",halfX,halfY,halfZ); // named; TGeoTube *tub = new TGeoTube(rmin,rmax,halfZ); // no name; //... (See all specific shape constructors); ```. Sometimes it is much easier to create a volume having a given shape in; one step, since shapes are not directly linked in the geometrical tree; but volumes are:. ``` {.cpp}; TGeoVolume *vol_box = gGeoManager->MakeBox(""BOX_VOL"",pmed,halfX,; halfY,halfZ);; TGeoVolume *vol_tub = gGeoManager->MakeTube(""TUB_VOL"",pmed,rmin,; rmax,halfZ);; // ...(See MakeXXX() utilities in TGeoManager class); ```. ### Dividing Shapes. Shapes can generally be divided along a given axis. Supported axes are:; `X`, `Y`, `Z`, `Rxy`, `Phi`, `Rxyz`. A given shape cannot be divided; however on any axis. The general rule is that that divisions are; possible on whatever axis that produces still known shapes as slices.; The division of shapes are performed by the call `TGeoShape::Divide()`,; but this operation can be done only via `TGeoVolume::Divide()` method.; In other words, the algorithm for dividing a specific shape is known by; the shape object, but is always invoked in a generic way from the volume; level. Details on how to do that can be found in the paragraph ‘Dividing; volumes'. One can see how all division options are interpreted and which; their result inside specific shape classes is. ### Parametric Shapes. Shapes generally have a set of parameters that is well defined at build; time. In fact, when the final geometrical hierarchy is assembled and the; geometry is closed, all constituent shapes `MUST`**have well defined and; valid parameters. In order to ease-up geometry creation, some; parameterizations are however allowed. For instance let's suppose that we need to define several volumes having; exactly the same properties but different sizes. A way to do this would; be to create as many different volumes and shapes. The modeller allows; however the definit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:53950,perform,performed,53950,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"AG-based instruction selection consists of the following steps:. #. `Build initial DAG`_ --- This stage performs a simple translation from the; input LLVM code to an illegal SelectionDAG. #. `Optimize SelectionDAG`_ --- This stage performs simple optimizations on the; SelectionDAG to simplify it, and recognize meta instructions (like rotates; and ``div``/``rem`` pairs) for targets that support these meta operations.; This makes the resultant code more efficient and the `select instructions; from DAG`_ phase (below) simpler. #. `Legalize SelectionDAG Types`_ --- This stage transforms SelectionDAG nodes; to eliminate any types that are unsupported on the target. #. `Optimize SelectionDAG`_ --- The SelectionDAG optimizer is run to clean up; redundancies exposed by type legalization. #. `Legalize SelectionDAG Ops`_ --- This stage transforms SelectionDAG nodes to; eliminate any operations that are unsupported on the target. #. `Optimize SelectionDAG`_ --- The SelectionDAG optimizer is run to eliminate; inefficiencies introduced by operation legalization. #. `Select instructions from DAG`_ --- Finally, the target instruction selector; matches the DAG operations to target instructions. This process translates; the target-independent input DAG into another DAG of target instructions. #. `SelectionDAG Scheduling and Formation`_ --- The last phase assigns a linear; order to the instructions in the target-instruction DAG and emits them into; the MachineFunction being compiled. This step uses traditional prepass; scheduling techniques. After all of these steps are complete, the SelectionDAG is destroyed and the; rest of the code generation passes are run. One of the most common ways to debug these steps is using ``-debug-only=isel``,; which prints out the DAG, along with other information like debug info,; after each of these steps. Alternatively, ``-debug-only=isel-dump`` shows only; the DAG dumps, but the results can be filtered by function names using; ``-filter-print-funcs=",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:37852,optimiz,optimizer,37852,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimizer']
Performance,"ARMv8.2a and above); * AMDGPU (natively); * SPIR (natively); * X86 (if SSE2 is available; natively if AVX512-FP16 is also available); * RISC-V (natively if Zfh or Zhinx is available). * ``__bf16`` is supported on the following targets (currently never natively):. * 32-bit ARM; * 64-bit ARM (AArch64); * RISC-V; * X86 (when SSE2 is available). (For X86, SSE2 is available on 64-bit and all recent 32-bit processors.). ``__fp16`` and ``_Float16`` both use the binary16 format from IEEE; 754-2008, which provides a 5-bit exponent and an 11-bit significand; (counting the implicit leading 1). ``__bf16`` uses the `bfloat16; <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>`_ format,; which provides an 8-bit exponent and an 8-bit significand; this is the same; exponent range as `float`, just with greatly reduced precision. ``_Float16`` and ``__bf16`` follow the usual rules for arithmetic; floating-point types. Most importantly, this means that arithmetic operations; on operands of these types are formally performed in the type and produce; values of the type. ``__fp16`` does not follow those rules: most operations; immediately promote operands of type ``__fp16`` to ``float``, and so; arithmetic operations are defined to be performed in ``float`` and so result in; a value of type ``float`` (unless further promoted because of other operands).; See below for more information on the exact specifications of these types. When compiling arithmetic on ``_Float16`` and ``__bf16`` for a target without; native support, Clang will perform the arithmetic in ``float``, inserting; extensions and truncations as necessary. This can be done in a way that; exactly matches the operation-by-operation behavior of native support,; but that can require many extra truncations and extensions. By default,; when emulating ``_Float16`` and ``__bf16`` arithmetic using ``float``, Clang; does not truncate intermediate operands back to their true type unless the; operand is the result of an explic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:31793,perform,performed,31793,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['performed']
Performance,"ARS)`. Going; on and opening our packs, we will obviously find out some empty ones,; otherwise, something is very wrong... We will call these leaves (by; analogy with a tree structure). On the other hand, any volume is a small world by itself - what we need; to do is to take it out and to ignore all the rest since it is a; self-contained object. In fact, the modeller can act like this,; considering a given volume as temporary MARS, but we will describe this; feature later on. Let us focus on the biggest pack - it is mandatory to; define one. Consider the simplest geometry that is made of a single box.; Here is an example on how to build it:. ### Example 1: Creating the World. We first need to load the geometry library. This is not needed if one; does `make map` in root folder. ``` {.cpp}; root[] gSystem->Load(""libGeom"");; ```. Second, we have to create an instance of the geometry manager class.; This takes care of all the modeller components, performing several tasks; to insure geometry validity and containing the user interface for; building and interacting with the geometry. After its creation, the; geometry manager class can be accessed with the global; ***`gGeoManager`***:. ``` {.cpp}; root[] new TGeoManager(""world"", ""the simplest geometry"");; ```. We want to create a single volume in our geometry, but since any volume; needs to have an associated medium, we will create a dummy one. You can; safely ignore the following lines for the time being, since materials; and media will be explained in detail later on. ``` {.cpp}; root[] TGeoMaterial *mat = new TGeoMaterial(""Vacuum"",0,0,0);; root[] TGeoMedium *med = new TGeoMedium(""Vacuum"",1,mat);; ```. We can finally make our volume having a box shape. Note that the world; volume does not need to be a box - it can be any other shape. Generally,; boxes and tubes are the most recommendable shapes for this purpose due; to their fast navigation algorithms. ``` {.cpp}; root[] TGeoVolume *top=gGeoManager->MakeBox(""Top"",med,10.,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:2384,perform,performing,2384,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performing']
Performance,"Activation function is now supported in the SOFIE Keras parser. ## 2D Graphics Libraries. - Introduce `TAxis::ChangeLabelByValue` to set custom label defined by axis value. It works also; when axis zooming changes and position and index of correspondent axis label changes as well.; `TAxis::ChangeLabel` method to change axis label by index works as before. - Introduce `TCanvas::SaveAll` method. Allows to store several pads at once into different image file formats.; File name can include printf qualifier to code pad number. Also allows to store all pads in single PDF; or single ROOT file. Significantly improves performance when creating many image files using web graphics. - Introduce `TCanvas::UpdateAsync` method. In case of web-based canvas triggers update of the canvas on the client side,; but does not wait that real update is completed. Avoids blocking of caller thread.; Have to be used if called from other web-based widget to avoid logical dead-locks.; In case of normal canvas just canvas->Update() is performed. - The Delaunay triangles (used by TGraph2D) were computed by the external package `triangle.c`; included in the ROOT distribution. This package had several issues:; - It was not maintained anymore.; - Its license was not compatible with LGPL; This code is now replaced by the [CDT package](https://github.com/artem-ogre/CDT) which is; properly maintained and has a license (MLP) compatible with LGPL. It will appear in 6.03.02. ## Machine Learning integration. - ROOT now offers functionality to extract batches of events out of a dataset for use in common ML training workflows. For example, one can generate PyTorch tensors from a TTree. The functionality is available through the `RBatchGenerator` class and can be seamlessly integrated in user code, for example:; ```python; # Returns two generators that return training and validation batches as PyTorch tensors.; gen_train, gen_validation = ROOT.TMVA.Experimental.CreatePyTorchGenerators(; tree_name, file_name, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v630/index.md:20213,perform,performed,20213,README/ReleaseNotes/v630/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v630/index.md,1,['perform'],['performed']
Performance,"Additional; memory dependence analysis is required to make that determination. As a fail; safe mechanism, this causes loops that were originally parallel to be considered; sequential (if optimization passes that are unaware of the parallel semantics; insert new memory instructions into the loop body). Example of a loop that is considered parallel due to its correct use of; both ``llvm.access.group`` and ``llvm.loop.parallel_accesses``; metadata types. .. code-block:: llvm. for.body:; ...; %val0 = load i32, ptr %arrayidx, !llvm.access.group !1; ...; store i32 %val0, ptr %arrayidx1, !llvm.access.group !1; ...; br i1 %exitcond, label %for.end, label %for.body, !llvm.loop !0. for.end:; ...; !0 = distinct !{!0, !{!""llvm.loop.parallel_accesses"", !1}}; !1 = distinct !{}. It is also possible to have nested parallel loops:. .. code-block:: llvm. outer.for.body:; ...; %val1 = load i32, ptr %arrayidx3, !llvm.access.group !4; ...; br label %inner.for.body. inner.for.body:; ...; %val0 = load i32, ptr %arrayidx1, !llvm.access.group !3; ...; store i32 %val0, ptr %arrayidx2, !llvm.access.group !3; ...; br i1 %exitcond, label %inner.for.end, label %inner.for.body, !llvm.loop !1. inner.for.end:; ...; store i32 %val1, ptr %arrayidx4, !llvm.access.group !4; ...; br i1 %exitcond, label %outer.for.end, label %outer.for.body, !llvm.loop !2. outer.for.end: ; preds = %for.body; ...; !1 = distinct !{!1, !{!""llvm.loop.parallel_accesses"", !3}} ; metadata for the inner loop; !2 = distinct !{!2, !{!""llvm.loop.parallel_accesses"", !3, !4}} ; metadata for the outer loop; !3 = distinct !{} ; access group for instructions in the inner loop (which are implicitly contained in outer loop as well); !4 = distinct !{} ; access group for instructions in the outer, but not the inner loop. .. _langref_llvm_loop_mustprogress:. '``llvm.loop.mustprogress``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``llvm.loop.mustprogress`` metadata indicates that this loop is required to; terminate, unwind, or inter",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:313681,load,load,313681,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:16921,optimiz,optimization,16921,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimization']
Performance,"By Chris:. LLVM has been designed with two primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:378,perform,performance,378,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,2,['perform'],['performance']
Performance,"C++.; It uses LLVM along with a C++ interpreter (e.g., Cling) to enable features like; run-time instantiation of C++ templates, cross-inheritance, callbacks,; auto-casting, transparent use of smart pointers, etc. In a nutshell, this feature enables a new way of developing code, paving the; way for language interoperability and easier interactive programming. Implementation Details; ======================. Interpreter as a REPL vs. as a Library; --------------------------------------. 1 - If we're using the interpreter in interactive (REPL) mode, it will dump; the value (i.e., value printing). .. code-block:: console. if (LastValue.isValid()) {; if (!V) {; LastValue.dump();; LastValue.clear();; } else; *V = std::move(LastValue);; }. 2 - If we're using the interpreter as a library, then it will pass the value; to the user. Incremental AST Consumer; ------------------------. The ``IncrementalASTConsumer`` class wraps the original code generator; ``ASTConsumer`` and it performs a hook, to traverse all the top-level decls, to; look for expressions to synthesize, based on the ``isSemiMissing()`` condition. If this condition is found to be true, then ``Interp.SynthesizeExpr()`` will be; invoked. **Note:** Following is a sample code snippet. Actual code may vary over time. .. code-block:: console. for (Decl *D : DGR); if (auto *TSD = llvm::dyn_cast<TopLevelStmtDecl>(D);; TSD && TSD->isSemiMissing()); TSD->setStmt(Interp.SynthesizeExpr(cast<Expr>(TSD->getStmt())));. return Consumer->HandleTopLevelDecl(DGR);. The synthesizer will then choose the relevant expression, based on its type. Communication between Compiled Code and Interpreted Code; --------------------------------------------------------. In Clang-Repl there is **interpreted code**, and this feature adds a 'value'; runtime that can talk to the **compiled code**. Following is an example where the compiled code interacts with the interpreter; code. The execution results of an expression are stored in the object 'V' of;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangRepl.rst:12013,perform,performs,12013,interpreter/llvm-project/clang/docs/ClangRepl.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangRepl.rst,1,['perform'],['performs']
Performance,"C64, it returns a; compile-time-known constant value. The return value type of :ref:`llvm.get.dynamic.area.offset <int_get_dynamic_area_offset>`; must match the target's default address space's (address space 0) pointer type. '``llvm.prefetch``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.prefetch(ptr <address>, i32 <rw>, i32 <locality>, i32 <cache type>). Overview:; """""""""""""""""". The '``llvm.prefetch``' intrinsic is a hint to the code generator to; insert a prefetch instruction if supported; otherwise, it is a noop.; Prefetches have no effect on the behavior of the program but can change; its performance characteristics. Arguments:; """""""""""""""""""". ``address`` is the address to be prefetched, ``rw`` is the specifier; determining if the fetch should be for a read (0) or write (1), and; ``locality`` is a temporal locality specifier ranging from (0) - no; locality, to (3) - extremely local keep in cache. The ``cache type``; specifies whether the prefetch is performed on the data (1) or; instruction (0) cache. The ``rw``, ``locality`` and ``cache type``; arguments must be constant integers. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. In; particular, prefetches cannot trap and do not produce a value. On; targets that support this intrinsic, the prefetch can provide hints to; the processor cache for better performance. '``llvm.pcmarker``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.pcmarker(i32 <id>). Overview:; """""""""""""""""". The '``llvm.pcmarker``' intrinsic is a method to export a Program; Counter (PC) in a region of code to simulators and other tools. The; method is target specific, but it is expected that the marker will use; exported symbols to transmit the PC of the marker. The marker makes no; guarantees that it will remain with any specific instruction after; optimizations. It is possible that the presence of a marker will inhibit; optimizations. The intende",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:523588,cache,cache,523588,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,"['cache', 'perform']","['cache', 'performed']"
Performance,"C; Rint.Logon: rootlogon.C; Rint.Logoff: rootlogoff.C; ...; Rint.Canvas.MoveOpaque: false; Rint.Canvas.HighLightColor: 5; ```. The various options are explained in `$ROOTSYS/etc/system.rootrc`. The; `.rootrc` file contents are combined. For example, if the flag to use; true type fonts is set to true in the `system.rootrc` file, you have; to set explicitly it false in your local `.rootrc` file if you do not; want to use true type fonts. Removing the `UseTTFonts `statement in; the local `.rootrc` file will not disable true fonts. The value of the; environment variable `ROOTDEBUG` overrides the value in the `.rootrc`; file at startup. Its value is used to set ***`gDebug`*** and helps for; quick turn on debug mode in **`TROOT`** startup. ROOT looks for scripts in the path specified in the `.rootrc` file in; the `Root.Macro.Path` variable. You can expand this path to hold your; own directories. ### Logon and Logoff Scripts. The `rootlogon.C` and `rootlogoff.C` files are scripts loaded and; executed at start-up and shutdown. The `rootalias.C` file is loaded; but not executed. It typically contains small utility functions. For; example, the `rootalias.C` script that comes with the ROOT; distributions (located in `$ROOTSYS/tutorials)` defines the function; `edit(char *file)`. This allows the user to call the editor from the; command line. This particular function will start the VI editor if the; environment variable `EDITOR` is not set. ``` {.cpp}; root[0] edit(""c1.C""); ```. For more details, see `$ROOTSYS/tutorials/rootalias.C`. ### History File. You can use the up and down arrow at the command line, to access the; previous and next command. The commands are recorded in the history; file `$HOME/.root_hist`. It is a text file, and you can edit, cut, and; paste from it. You can specify the history file in the `system.rootrc`; file, by setting the `Rint.History `option. You can also turn off the; command logging in the `system.rootrc` file with the option:; `Rint.History: -`. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md:37840,load,loaded,37840,documentation/users-guide/GettingStarted.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md,1,['load'],['loaded']
Performance,"CL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:265785,load,load,265785,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"CMake Caches; ============. This directory contains CMake cache scripts that pre-populate the CMakeCache in; a build directory with commonly used settings. You can use the caches files with the following CMake invocation:. cmake -G <build system>; -C <path to cache file>; [additional CMake options (i.e. -DCMAKE_INSTALL_PREFIX=<install path>)]; <path to llvm>. Options specified on the command line will override options in the cache files. The following cache files exist. Apple-stage1; ------------. The Apple stage1 cache configures a two stage build similar to how Apple builds; the clang shipped with Xcode. The build files generated from this invocation has; a target named ""stage2"" which performs an LTO build of clang. The Apple-stage2 cache can be used directly to match the build settings Apple; uses in shipping builds without doing a full bootstrap build. PGO; ---. The PGO CMake cache can be used to generate a multi-stage instrumented compiler.; You can configure your build directory with the following invocation of CMake:. cmake -G <generator> -C <path_to_clang>/cmake/caches/PGO.cmake <source dir>. After configuration the following additional targets will be generated:. stage2-instrumented:; Builds a stage1 x86 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. stage2-instrumented-generate-profdata:; Depends on ""stage2-instrumented"" and will use the instrumented compiler to; generate profdata based on the training files in <clang>/utils/perf-training. stage2:; Depends on ""stage2-instrumented-generate-profdata"" and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. stage2-check-llvm:; Depends on stage2 and runs check-llvm using the stage3 compiler. stage2-check-clang:; Depends on stage2 and runs check-clang using the stage3 compiler. stage2-check-all:; Depends on stage2 and runs check-all using the stage3 compiler. stage2-test-suite:; Depends on ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt:58,cache,cache,58,interpreter/llvm-project/clang/cmake/caches/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt,9,"['cache', 'perform']","['cache', 'caches', 'performs']"
Performance,"CSE/DSE and a few other optimizations are allowed, but Monotonic; operations are unlikely to be used in ways which would make those; optimizations useful. Notes for code generation; Code generation is essentially the same as that for unordered for loads and; stores. No fences are required. ``cmpxchg`` and ``atomicrmw`` are required; to appear as a single operation. Acquire; -------. Acquire provides a barrier of the sort necessary to acquire a lock to access; other memory with normal loads and stores. Relevant standard; This corresponds to the C++/C ``memory_order_acquire``. It should also be; used for C++/C ``memory_order_consume``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. It is; also possible to move stores from before an Acquire load or read-modify-write; operation to after it, and move non-Acquire loads from before an Acquire; operation to after it. Notes for code generation; Architectures with weak memory ordering (essentially everything relevant today; except x86 and SPARC) require some sort of fence to maintain the Acquire; semantics. The precise fences required varies widely by architecture, but for; a simple implementation, most architectures provide a barrier which is strong; enough for everything (``dmb`` on ARM, ``sync`` on PowerPC, etc.). Putting; such a fence after the equivalent Monotonic operation is sufficient to; maintain Acquire semantics for a memory operation. Release; -------. Release is similar to Acquire, but with a barrier of the sort necessary to; release a lock. Relevant standard; This corresponds to the C++/C ``memory_order_release``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Release only provides a semantic guarantee when paired with an Acquire; operation. Notes ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:12053,load,load,12053,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],"['load', 'loads']"
Performance,"CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atom",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:347578,load,load,347578,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address sp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:359535,load,load,359535,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"Cb` trigger. The program stress takes one argument, the number of events to process.; The default is 1000 events. Be aware that executing stress with 1000; events*will create several files consuming about 100 MB of disk space;*; running stress with 30 events will consume about 20 MB. The disk space; is released once stress is done. There are two ways to run `stress`:. From the system prompt or from the ROOT prompt using the interpreter. ``` {.cpp}; > cd $ROOTSYS/test; > stress // default 1000 events; > stress 30 // test with 30 events; ```. Start ROOT with the batch mode option (-b) to suppress the graphic; output. ``` {.cpp}; > root -b; root[] .L stress.cxx; root[] stress(1000)// test with 1000 events; root[] stress(30)// test with 30 events; ```. The output of stress includes a pass/fail conclusion for each test, the; total number of bytes read and written, and the elapsed real and CPU; time. It also calculates a performance index for your machine relative; to a reference machine a DELL Inspiron 7500 (Pentium III 600 MHz) with; 256 MB of memory and 18GB IDE disk in ROOTMARKS. Higher ROOTMARKS means; better performance. The reference machine has 200 ROOTMARKS, so the; sample run below with 53.7 ROOTMARKS is about four times slower than the; reference machine. Here is a sample run:. ``` {.cpp}; % root -b; root[] .x stress.cxx(30). Test 1 : Functions, Random Numbers, Histogram Fits............. OK; Test 2 : Check size & compression factor of a Root file........ OK; Test 3 : Purge, Reuse of gaps in TFile......................... OK; Test 4 : Test of 2-d histograms, functions, 2-d fits........... OK; Test 5 : Test graphics & PostScript ............................OK; Test 6 : Test subdirectories in a Root file.................... OK; Test 7 : TNtuple, selections, TCutG, TEventList.......... OK; Test 8 : Trees split and compression modes..................... OK; Test 9 : Analyze Event.root file of stress 8................... OK; Test 10 : Create 10 files starting from Ev",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/TutorialsandTests.md:14341,perform,performance,14341,documentation/users-guide/TutorialsandTests.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/TutorialsandTests.md,1,['perform'],['performance']
Performance,"Check that NSLocalizedString macros include a comment for context. .. code-block:: objc. - (void)test {; NSString *string = NSLocalizedString(@""LocalizedString"", nil); // warn; NSString *string2 = NSLocalizedString(@""LocalizedString"", @"" ""); // warn; NSString *string3 = NSLocalizedStringWithDefaultValue(; @""LocalizedString"", nil, [[NSBundle alloc] init], nil,@""""); // warn; }. .. _optin-osx-cocoa-localizability-NonLocalizedStringChecker:. optin.osx.cocoa.localizability.NonLocalizedStringChecker (ObjC); """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Warns about uses of non-localized NSStrings passed to UI methods expecting localized NSStrings. .. code-block:: objc. NSString *alarmText =; NSLocalizedString(@""Enabled"", @""Indicates alarm is turned on"");; if (!isEnabled) {; alarmText = @""Disabled"";; }; UILabel *alarmStateLabel = [[UILabel alloc] init];. // Warning: User-facing text should use localized string macro; [alarmStateLabel setText:alarmText];. .. _optin-performance-GCDAntipattern:. optin.performance.GCDAntipattern; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Check for performance anti-patterns when using Grand Central Dispatch. .. _optin-performance-Padding:. optin.performance.Padding; """"""""""""""""""""""""""""""""""""""""""""""""""; Check for excessively padded structs. .. _optin-portability-UnixAPI:. optin.portability.UnixAPI; """"""""""""""""""""""""""""""""""""""""""""""""""; Finds implementation-defined behavior in UNIX/Posix functions. .. _security-checkers:. security; ^^^^^^^^. Security related checkers. .. _security-cert-env-InvalidPtr:. security.cert.env.InvalidPtr; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". Corresponds to SEI CERT Rules `ENV31-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV31-C.+Do+not+rely+on+an+environment+pointer+following+an+operation+that+may+invalidate+it>`_ and `ENV34-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV34-C.+Do+not+store+pointers+returned+by+certain+functions>`_. * **ENV31-C**:; Rule is about the possible problem with ``main`` function's third argument",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst:19306,perform,performance-GCDAntipattern,19306,interpreter/llvm-project/clang/docs/analyzer/checkers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst,1,['perform'],['performance-GCDAntipattern']
Performance,"Clang generates an access function to access C++-style TLS. The access; function generally has an entry block, an exit block and an initialization; block that is run at the first time. The entry and exit blocks can access; a few TLS IR variables, each access will be lowered to a platform-specific; sequence. This calling convention aims to minimize overhead in the caller by; preserving as many registers as possible (all the registers that are; preserved on the fast path, composed of the entry and exit blocks). This calling convention behaves identical to the `C` calling convention on; how arguments and return values are passed, but it uses a different set of; caller/callee-saved registers. Given that each platform has its own lowering sequence, hence its own set; of preserved registers, we can't use the existing `PreserveMost`. - On X86-64 the callee preserves all general purpose registers, except for; RDI and RAX.; ""``tailcc``"" - Tail callable calling convention; This calling convention ensures that calls in tail position will always be; tail call optimized. This calling convention is equivalent to fastcc,; except for an additional guarantee that tail calls will be produced; whenever possible. `Tail calls can only be optimized when this, the fastcc,; the GHC or the HiPE convention is used. <CodeGenerator.html#tail-call-optimization>`_; This calling convention does not support varargs and requires the prototype of; all callees to exactly match the prototype of the function definition.; ""``swiftcc``"" - This calling convention is used for Swift language.; - On X86-64 RCX and R8 are available for additional integer returns, and; XMM2 and XMM3 are available for additional FP/vector returns.; - On iOS platforms, we use AAPCS-VFP calling convention.; ""``swifttailcc``""; This calling convention is like ``swiftcc`` in most respects, but also the; callee pops the argument area of the stack so that mandatory tail calls are; possible as in ``tailcc``.; ""``cfguard_checkcc``"" - Win",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:21026,optimiz,optimized,21026,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,"Cling interprets C++; ====================. .. figure:: images/fig1.jpeg. **Cling** is an interactive C++ interpreter built on top of `Clang; <https://clang.llvm.org/>`_ and `LLVM <https://llvm.org/>`_. It uses LLVM's; *Just-In-Time* (`JIT <https://en.wikipedia.org/wiki/Just-in-time_compilation>`_); compiler to provide a fast and optimized compilation pipeline. Cling uses the; `read-eval-print-loop; <https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop>`_; (**REPL**) approach, making rapid application development in C++ possible,; avoiding the classic edit-compile-run-debug cycle approach. Cling's last release, download instructions, dependencies, and any other useful; information for developers can be found on `Cling's GitHub webpage; <https://github.com/vgvassilev/cling>`_. Find out more about **Interpreting C++** on the `Compiler Research Group; <https://compiler-research.org/>`_'s webpage.; . Table of Contents; -----------------. .. toctree::; :numbered:; ; chapters/background; chapters/interactivity; chapters/why_interpreting; chapters/implementation; chapters/REPL; chapters/grammar; chapters/applications; chapters/conclusion; chapters/references; . .. note::. This project is under active development.; Cling has its documentation hosted on Read the Docs. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/index.rst:332,optimiz,optimized,332,interpreter/cling/docs/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/index.rst,1,['optimiz'],['optimized']
Performance,"Command Line; ============. Cling has its own command line, which looks like any other Unix shell. The; emacs-like command line editor is what we call interactive command line or; interactive shell. Once we start Cling it automatically includes several header files and its own; runtime universe. Thus it creates the minimal environment for the user to start. Grammar; -------. Cling is capable to parse everything that `Clang <https://clang.llvm.org/>`_ can; do. In addition, Cling can parse some interpreter-specific C++ extensions. Metaprocessor; -------------. Cling Metaprocessor provides convenient and easy to use interface for changing; the interpreter’s internal state or for executing handy commands. Cling provides; the following metaprocessor commands:. **syntax: .(command)**, where command is:. .. code:: bash. x filename.cxx; ; loads filename and calls void filename() if defined;. .. code:: bash. L library | filename.cxx; ; loads library or filename.cxx;. .. code:: bash. printAST; ; (DEBUG ONLY) shows the abstract syntax tree after each processed entity;. .. code:: bash. I path; ; adds an include path;. .. code:: bash. .@ . Cancels the multiline input;. .. code:: bash. .dynamicExtensions. Turns on cling's dynamic extensions. This in turn enables the dynamic lookup and; the late resolving of the identifier. With that option cling tries to heal the; compile-time failed lookups at runtime.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/grammar.rst:843,load,loads,843,interpreter/cling/docs/chapters/grammar.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/grammar.rst,2,['load'],['loads']
Performance,"Conclusion; ----------. Cling is not just an interpreter, and is not just a REPL: it is a C/C++; JIT-compiler that can be embedded to your software for efficient incremental; execution of C++. Cling allows you to decide how much you want to compile; statically and how much to defer for the target platform. Cling enables; reflection and introspection information in high-performance systems such as; ROOT, or Xeus Jupyter, where it provides efficient code for performance-critical; tasks where hot-spot regions can be annotated with specific optimization; levels. You ca find more information regarding Cling's internal architecture,; functionment, user-cases, and Cling's based project into the References Chapter.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/conclusion.rst:372,perform,performance,372,interpreter/cling/docs/chapters/conclusion.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/conclusion.rst,3,"['optimiz', 'perform']","['optimization', 'performance', 'performance-critical']"
Performance,"Connection to Other Vector Classes. The 3D and 4D vectors of the `GenVector` package can be constructed and; assigned from any vector which satisfies the following requisites:. - for 3D vectors implementing the `x()`, `y()` and `z()` methods. - for Lorentz vectors implementing the `x()`, `y()`, `z()` and `t()`; methods. ``` {.cpp}; CLHEP::Hep3Vector hv;; XYZVector v1(hv); //create 3D vector from; //CLHEP 3D Vector; HepGeom::Point3D hp;; XYZPoint p1(hp); //create a 3D p; ```. ## Linear Algebra: SMatrix Package. The ROOT Linear algebra package is documented in a separate chapter (see; ""Linear Algebra in ROOT""). `SMatrix` is a C++ package, for high; performance vector and matrix computations. It has been introduced in; ROOT v5.08. It is optimized for describing small matrices and vectors; and It can be used only in problems when the size of the matrices is; known at compile time, like in the tracking reconstruction of physics; experiments. It is based on a C++ technique, called expression; templates, to achieve an high level optimization. The C++ templates can; be used to implement vector and matrix expressions such that these; expressions can be transformed at compile time to code which is; equivalent to hand optimized code in a low-level language like FORTRAN; or C (see for example T. Veldhuizen, Expression Templates, C++ Report,; 1995). The `SMatrix` has been developed initially by T. Glebe in; Max-Planck-Institut, Heidelberg, as part of the `HeraB` analysis; framework. A subset of the original package has been now incorporated in; the ROOT distribution, with the aim to provide a stand-alone and high; performance matrix package. The API of the current package differs from; the original one, in order to be compliant to the ROOT coding; conventions. `SMatrix` contains the generic **`ROOT::Math::SMatrix`** and; **`ROOT::Math::SVector`** classes for describing matrices and vectors of; arbitrary dimensions and of arbitrary type. The classes are templated on; the scalar ty",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:99448,optimiz,optimization,99448,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['optimiz'],['optimization']
Performance,"D = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ``ObjectLinkingLayer``. Your custom ``MaterializationUnit`` will have two operations: ``materiali",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24347,perform,performed,24347,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['perform'],['performed']
Performance,D2; Position of attribute-specifier in declarator syntax; Unknown. 980; CD2; Explicit instantiation of a member of a class template; Unknown. 981; C++11; Constexpr constructor templates and literal types; Unknown. 982; NAD; Initialization with an empty initializer list; Unknown. 983; CD2; Ambiguous pointer-to-member constant; Unknown. 984; CD2; “Deduced type” is unclear in auto type deduction; Unknown. 985; C++11; Alternative tokens and user-defined literals; Unknown. 986; CD2; Transitivity of using-directives versus qualified lookup; Unknown. 987; CD4; Which declarations introduce namespace members?; Unknown. 988; CD2; Reference-to-reference collapsing with decltype; Unknown. 989; CD2; Misplaced list-initialization example; Unknown. 990; CD2; Value initialization with multiple initializer-list constructors; Clang 3.5. 991; CD2; Reference parameters of constexpr functions and constructors; Unknown. 992; NAD; Inheriting explicitness; Unknown. 993; C++11; Freedom to perform instantiation at the end of the translation unit; Unknown. 994; C++11; braced-init-list as a default argument; Unknown. 995; CD2; Incorrect example for using-declaration and explicit instantiation; Unknown. 996; C++11; Ambiguous partial specializations of member class templates; Unknown. 997; C++11; Argument-dependent lookup and dependent function template parameter types; Unknown. 998; dup; Function parameter transformations and template functions; Unknown. 999; CD2; “Implicit” or “implied” object argument/parameter?; Unknown. 1000; CD2; Mistaking member typedefs for constructors; Unknown. 1001; drafting; Parameter type adjustment in dependent parameter types; Not resolved. 1002; NAD; Pack expansion for function arguments; Unknown. 1003; CD3; Acceptable definitions of main; Unknown. 1004; C++11; Injected-class-names as arguments for template template parameters; Clang 5. 1005; NAD; Qualified name resolution in member functions of class templates; Unknown. 1006; C++11; std::nullptr_t as a non-type ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html:65280,perform,perform,65280,interpreter/llvm-project/clang/www/cxx_dr_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html,1,['perform'],['perform']
Performance,"DGPU_REL32_HI`` Static 11 ``word32`` (S + A - P) >> 32; *reserved* 12; ``R_AMDGPU_RELATIVE64`` Dynamic 13 ``word64`` B + A; ``R_AMDGPU_REL16`` Static 14 ``word16`` ((S + A - P) - 4) / 4; ========================== ======= ===== ========== ==============================. ``R_AMDGPU_ABS32_LO`` and ``R_AMDGPU_ABS32_HI`` are only supported by; the ``mesa3d`` OS, which does not support ``R_AMDGPU_ABS64``. There is no current OS loader support for 32-bit programs and so; ``R_AMDGPU_ABS32`` is not used. .. _amdgpu-loaded-code-object-path-uniform-resource-identifier:. Loaded Code Object Path Uniform Resource Identifier (URI); ---------------------------------------------------------. The AMD GPU code object loader represents the path of the ELF shared object from; which the code object was loaded as a textual Uniform Resource Identifier (URI).; Note that the code object is the in memory loaded relocated form of the ELF; shared object. Multiple code objects may be loaded at different memory; addresses in the same process from the same ELF shared object. The loaded code object path URI syntax is defined by the following BNF syntax:. .. code::. code_object_uri ::== file_uri | memory_uri; file_uri ::== ""file://"" file_path [ range_specifier ]; memory_uri ::== ""memory://"" process_id range_specifier; range_specifier ::== [ ""#"" | ""?"" ] ""offset="" number ""&"" ""size="" number; file_path ::== URI_ENCODED_OS_FILE_PATH; process_id ::== DECIMAL_NUMBER; number ::== HEX_NUMBER | DECIMAL_NUMBER | OCTAL_NUMBER. **number**; Is a C integral literal where hexadecimal values are prefixed by ""0x"" or ""0X"",; and octal values by ""0"". **file_path**; Is the file's path specified as a URI encoded UTF-8 string. In URI encoding,; every character that is not in the regular expression ``[a-zA-Z0-9/_.~-]`` is; encoded as two uppercase hexadecimal digits proceeded by ""%"". Directories in; the path are separated by ""/"". **offset**; Is a 0-based byte offset to the start of the code object. For a file URI, it; is f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:83117,load,loaded,83117,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"DS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relativ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:337373,cache,cache,337373,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"Date: Fri, 1 Jun 2001 16:38:17 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Interesting: GCC passes. Take a look at this document (which describes the order of optimizations; that GCC performs):. http://gcc.gnu.org/onlinedocs/gcc_17.html. The rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:217,optimiz,optimizations,217,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,4,"['optimiz', 'perform']","['optimization', 'optimizations', 'performs']"
Performance,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:321,optimiz,optimization,321,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,6,['optimiz'],"['optimization', 'optimizations']"
Performance,"Date: Fri, 6 Jul 2001 16:56:56 -0500; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: lowering the IR. BTW, I do think that we should consider lowering the IR as you said. I; didn't get time to raise it today, but it comes up with the SPARC; move-conditional instruction. I don't think we want to put that in the core; VM -- it is a little too specialized. But without a corresponding; conditional move instruction in the VM, it is pretty difficult to maintain a; close mapping between VM and machine code. Other architectures may have; other such instructions. What I was going to suggest was that for a particular processor, we define; additional VM instructions that match some of the unusual opcodes on the; processor but have VM semantics otherwise, i.e., all operands are in SSA; form and typed. This means that we can re-generate core VM code from the; more specialized code any time we want (so that portability is not lost). Typically, a static compiler like gcc would generate just the core VM, which; is relatively portable. Anyone (an offline tool, the linker, etc., or even; the static compiler itself if it chooses) can transform that into more; specialized target-specific VM code for a particular architecture. If the; linker does it, it can do it after all machine-independent optimizations.; This would be the most convenient, but not necessary. The main benefit of lowering will be that we will be able to retain a close; mapping between VM and machine code. --Vikram. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt:1338,optimiz,optimizations,1338,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt,1,['optimiz'],['optimizations']
Performance,"Date: Tue, 13 Feb 2001 13:29:52 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: LLVM Concerns... I've updated the documentation to include load store and allocation; instructions (please take a look and let me know if I'm on the right; track):. file:/home/vadve/lattner/llvm/docs/LangRef.html#memoryops. I have a couple of concerns I would like to bring up:. 1. Reference types; Right now, I've spec'd out the language to have a pointer type, which; works fine for lots of stuff... except that Java really has; references: constrained pointers that cannot be manipulated: added and; subtracted, moved, etc... Do we want to have a type like this? It; could be very nice for analysis (pointer always points to the start of; an object, etc...) and more closely matches Java semantics. The; pointer type would be kept for C++ like semantics. Through analysis,; C++ pointers could be promoted to references in the LLVM; representation. 2. Our ""implicit"" memory references in assembly language:; After thinking about it, this model has two problems:; A. If you do pointer analysis and realize that two stores are; independent and can share the same memory source object, there is; no way to represent this in either the bytecode or assembly.; B. When parsing assembly/bytecode, we effectively have to do a full; SSA generation/PHI node insertion pass to build the dependencies; when we don't want the ""pinned"" representation. This is not; cool.; I'm tempted to make memory references explicit in both the assembly and; bytecode to get around this... what do you think?. -Chris. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt:193,load,load,193,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt,1,['load'],['load']
Performance,"Date: Tue, 18 Sep 2001 00:38:37 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Idea for a simple, useful link time optimization. In C++ programs, exceptions suck, and here's why:. 1. In virtually all function calls, you must assume that the function; throws an exception, unless it is defined as 'nothrow'. This means; that every function call has to have code to invoke dtors on objects; locally if one is thrown by the function. Most functions don't throw; exceptions, so this code is dead [with all the bad effects of dead; code, including icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:170,optimiz,optimization,170,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,1,['optimiz'],['optimization']
Performance,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:457,optimiz,optimization,457,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,6,"['load', 'optimiz']","['loading', 'optimization', 'optimize']"
Performance,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:565,optimiz,optimization,565,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,3,['optimiz'],"['optimization', 'optimizations']"
Performance,"Debugify:. Mutation testing for IR-level transformations; ---------------------------------------------. An IR test case for a transformation can, in many cases, be automatically; mutated to test debug info handling within that transformation. This is a; simple way to test for proper debug info handling. The ``debugify`` utility pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``debugify`` testing utility is just a pair of passes: ``debugify`` and; ``check-debugify``. The first applies synthetic debug information to every instruction of the; module, and the second checks that this DI is still available after an; optimization has occurred, reporting any errors/warnings while doing so. The instructions are assigned sequentially increasing line locations, and are; immediately used by debug value intrinsics everywhere possible. For example, here is a module before:. .. code-block:: llvm. define void @f(i32* %x) {; entry:; %x.addr = alloca i32*, align 8; store i32* %x, i32** %x.addr, align 8; %0 = load i32*, i32** %x.addr, align 8; store i32 10, i32* %0, align 4; ret void; }. and after running ``opt -debugify``:. .. code-block:: llvm. define void @f(i32* %x) !dbg !6 {; entry:; %x.addr = alloca i32*, align 8, !dbg !12; call void @llvm.dbg.value(metadata i32** %x.addr, metadata !9, metadata !DIExpression()), !dbg !12; store i32* %x, i32** %x.addr, align 8, !dbg !13; %0 = load i32*, i32** %x.addr, align 8, !dbg !14; call void @llvm.dbg.value(metadata i32* %0, metadata !11, metadata !DIExpression()), !dbg !14; store i32 10, i32* %0, align 4, !dbg !15; ret void, !dbg !16; }. !llvm.dbg.cu = !{!0}; !llvm.debugify = !{!3, !4}; !llvm.module.flags = !{!5}. !0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: ""debugify"", isOptimized: true, runtimeVersion: 0, emissionKind: FullDebug, enums: !2); !1 = !DIFile(filename: ""debugify-sample.ll"", directory: ""/""); !2 = !{}; !3 = !{i32 5}; !4 = !{i32 2}; !5 = !{i32 2, !""Debug Info Version"", i32 3}; !6 = distinct !DISubprogram(name: ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst:10275,load,load,10275,interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,1,['load'],['load']
Performance,"DefMap.txt` to contain the name of the `.ast` files instead of the source files:. .. code-block:: bash. $ sed -i -e ""s/.cpp/.cpp.ast/g"" externalDefMap.txt. We still have to further modify the `externalDefMap.txt` file to contain relative paths:. .. code-block:: bash. $ sed -i -e ""s|$(pwd)/||g"" externalDefMap.txt. Now everything is available for the CTU analysis.; We have to feed Clang with CTU specific extra arguments:. .. code-block:: bash. $ pwd; /path/to/your/project; $ clang++ --analyze \; -Xclang -analyzer-config -Xclang experimental-enable-naive-ctu-analysis=true \; -Xclang -analyzer-config -Xclang ctu-dir=. \; -Xclang -analyzer-output=plist-multi-file \; main.cpp; main.cpp:5:12: warning: Division by zero; return 3 / foo();; ~~^~~~~~~; 1 warning generated.; $ # The plist file with the result is generated.; $ ls -F; compile_commands.json externalDefMap.txt foo.ast foo.cpp foo.cpp.ast main.cpp main.plist; $. This manual procedure is error-prone and not scalable, therefore to analyze real projects it is recommended to use; `CodeChecker` or `scan-build-py`. Automated CTU Analysis with CodeChecker; #######################################; The `CodeChecker <https://github.com/Ericsson/codechecker>`_ project fully supports automated CTU analysis with Clang.; Once we have set up the `PATH` environment variable and we activated the python `venv` then it is all it takes:. .. code-block:: bash. $ CodeChecker analyze --ctu compile_commands.json -o reports; $ ls -F; compile_commands.json foo.cpp foo.cpp.ast main.cpp reports/; $ tree reports; reports; ├── compile_cmd.json; ├── compiler_info.json; ├── foo.cpp_53f6fbf7ab7ec9931301524b551959e2.plist; ├── main.cpp_23db3d8df52ff0812e6e5a03071c8337.plist; ├── metadata.json; └── unique_compile_commands.json. 0 directories, 6 files; $. The `plist` files contain the results of the analysis, which may be viewed with the regular analysis tools.; E.g. one may use `CodeChecker parse` to view the results in command line:. .. code-block::",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/user-docs/CrossTranslationUnit.rst:4050,scalab,scalable,4050,interpreter/llvm-project/clang/docs/analyzer/user-docs/CrossTranslationUnit.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/user-docs/CrossTranslationUnit.rst,1,['scalab'],['scalable']
Performance,"Disable buffer security check; /GS Enable buffer security check (default); /Gs Use stack probes (default); /Gs<value> Set stack probe size (default 4096); /guard:<value> Enable Control Flow Guard with /guard:cf,; or only the table with /guard:cf,nochecks.; Enable EH Continuation Guard with /guard:ehcont; /Gv Set __vectorcall as a default calling convention; /Gw- Don't put each data item in its own section; /Gw Put each data item in its own section; /GX- Disable exception handling; /GX Enable exception handling; /Gy- Don't put each function in its own section (default); /Gy Put each function in its own section; /Gz Set __stdcall as a default calling convention; /help Display available options; /imsvc <dir> Add directory to system include search path, as if part of %INCLUDE%; /I <dir> Add directory to include search path; /J Make char type unsigned; /LDd Create debug DLL; /LD Create DLL; /link <options> Forward options to the linker; /MDd Use DLL debug run-time; /MD Use DLL run-time; /MTd Use static debug run-time; /MT Use static run-time; /O0 Disable optimization; /O1 Optimize for size (same as /Og /Os /Oy /Ob2 /GF /Gy); /O2 Optimize for speed (same as /Og /Oi /Ot /Oy /Ob2 /GF /Gy); /Ob0 Disable function inlining; /Ob1 Only inline functions which are (explicitly or implicitly) marked inline; /Ob2 Inline functions as deemed beneficial by the compiler; /Od Disable optimization; /Og No effect; /Oi- Disable use of builtin functions; /Oi Enable use of builtin functions; /Os Optimize for size; /Ot Optimize for speed; /Ox Deprecated (same as /Og /Oi /Ot /Oy /Ob2); use /O2 instead; /Oy- Disable frame pointer omission (x86 only, default); /Oy Enable frame pointer omission (x86 only); /O<flags> Set multiple /O flags at once; e.g. '/O2y-' for '/O2 /Oy-'; /o <file or directory> Set output file or directory (ends in / or \); /P Preprocess to file; /Qvec- Disable the loop vectorization passes; /Qvec Enable the loop vectorization passes; /showFilenames- Don't print the name of each ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:174019,optimiz,optimization,174019,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,['optimiz'],['optimization']
Performance,"E ""${CLANG_VENDOR} clang""); else(); set(TOOL_INFO_NAME ""clang""); endif(). set(TOOL_INFO_UTI ""${CLANG_VENDOR_UTI}""); set(TOOL_INFO_VERSION ""${CLANG_VERSION}""); set(TOOL_INFO_BUILD_VERSION ""${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}""). set(TOOL_INFO_PLIST_OUT ""${CMAKE_CURRENT_BINARY_DIR}/${TOOL_INFO_PLIST}""). if(LLVM_TOOL_LLVM_DRIVER_BUILD AND clang IN_LIST LLVM_DRIVER_TOOLS); set(TARGET_NAME llvm-driver); else(); set(TARGET_NAME clang); endif(). target_link_libraries(${TARGET_NAME}; PRIVATE; ""-Wl,-sectcreate,__TEXT,__info_plist,\""${TOOL_INFO_PLIST_OUT}\""""); configure_file(""${TOOL_INFO_PLIST}.in"" ""${TOOL_INFO_PLIST_OUT}"" @ONLY). set(TOOL_INFO_UTI); set(TOOL_INFO_NAME); set(TOOL_INFO_VERSION); set(TOOL_INFO_BUILD_VERSION); endif(). if(CLANG_ORDER_FILE AND; (LLVM_LINKER_IS_APPLE OR LLVM_LINKER_IS_GOLD OR LLVM_LINKER_IS_LLD)); include(LLVMCheckLinkerFlag). if (LLVM_LINKER_IS_APPLE OR (LLVM_LINKER_IS_LLD AND APPLE)); set(LINKER_ORDER_FILE_OPTION ""-Wl,-order_file,${CLANG_ORDER_FILE}""); elseif (LLVM_LINKER_IS_GOLD); set(LINKER_ORDER_FILE_OPTION ""-Wl,--section-ordering-file,${CLANG_ORDER_FILE}""); elseif (LLVM_LINKER_IS_LLD); set(LINKER_ORDER_FILE_OPTION ""-Wl,--symbol-ordering-file,${CLANG_ORDER_FILE}""); endif(). # This is a test to ensure the actual order file works with the linker.; llvm_check_linker_flag(CXX ${LINKER_ORDER_FILE_OPTION} LINKER_ORDER_FILE_WORKS). # Passing an empty order file disables some linker layout optimizations.; # To work around this and enable workflows for re-linking when the order file; # changes we check during configuration if the file is empty, and make it a; # configuration dependency.; file(READ ${CLANG_ORDER_FILE} ORDER_FILE LIMIT 20); if(""${ORDER_FILE}"" STREQUAL ""\n""); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${CLANG_ORDER_FILE}); elseif(LINKER_ORDER_FILE_WORKS); target_link_libraries(clang PRIVATE ${LINKER_ORDER_FILE_OPTION}); set_target_properties(clang PROPERTIES LINK_DEPENDS ${CLANG_ORDER_FILE}); endif(); endif(); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/driver/CMakeLists.txt:2797,optimiz,optimizations,2797,interpreter/llvm-project/clang/tools/driver/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/driver/CMakeLists.txt,1,['optimiz'],['optimizations']
Performance,"E RW and CC memory will; never be stale in L2 due to; the memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must hap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:253336,load,load,253336,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"E RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To ensure coherence of local memory reads of CUs in different agents a; ``buffer_inv sc0 sc1`` is required. It will invalidate non-local L2 cache; lines if configured to have multiple L2 caches. * PCIe access from the GPU to the CPU can be kept coherent by using the MTYPE; UC (uncached) which bypasses the L2. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector caches are; invalidated between kernel dispatches by CP since constant address space",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:289266,cache,caches,289266,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"ECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previously learnt) nor restart the learning phase.; The learning phase is restarted when a new cache is created, e.g. after having; removed a cache with SetCacheSize(0). ### TSelectorDraw. The axis titles in case of a `x:y:z` plot with the option `COLZ` were not correct. ### TParallelCoordVar. Change the format used to print the variables limit for ||-Coord to `%g`. It was; `%6.4f` before. ## Histogram Libraries. ### TFormula. - New version of the TFormula class based on Cling. Formula expressions are now used to create functions which are passed to Cling to be Just In Time compiled.; The expression is therefore compiled using Clang/LLVVM which will give execution time as compiled code and in addition correctness of the result obtained.; - This class is not 100% backward compatible with the old TFormula class, which is still available in ROOT as =ROOT::v5::TFormula=.; Some of the TFormula member funtions available in version 5, such as =Analyze= and =AnalyzeFunction= are not available in the new TFormula class.; On the other hand formula expressions which were valid ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:14856,cache,cache,14856,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cache']
Performance,"EDUCE_OR; * G_VECREDUCE_XOR; * G_VECREDUCE_SMAX; * G_VECREDUCE_SMIN; * G_VECREDUCE_UMAX; * G_VECREDUCE_UMIN. Integer reductions may have a result type larger than the vector element type.; However, the reduction is performed using the vector element type and the value; in the top bits is unspecified. Memory Operations; -----------------. G_LOAD, G_SEXTLOAD, G_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic load. Expects a MachineMemOperand in addition to explicit; operands. If the result size is larger than the memory size, the; high bits are undefined, sign-extended, or zero-extended respectively. Only G_LOAD is valid if the result is a vector type. If the result is larger; than the memory size, the high elements are undefined (i.e. this is not a; per-element, vector anyextload). Unlike in SelectionDAG, atomic loads are expressed with the same; opcodes as regular loads. G_LOAD, G_SEXTLOAD and G_ZEXTLOAD may all; have atomic memory operands. G_INDEXED_LOAD; ^^^^^^^^^^^^^^. Generic indexed load. Combines a GEP with a load. $newaddr is set to $base + $offset.; If $am is 0 (post-indexed), then the value is loaded from $base; if $am is 1 (pre-indexed); then the value is loaded from $newaddr. G_INDEXED_SEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is sign-extending, as with G_SEXTLOAD. G_INDEXED_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is zero-extending, as with G_ZEXTLOAD. G_STORE; ^^^^^^^. Generic store. Expects a MachineMemOperand in addition to explicit; operands. If the stored value size is greater than the memory size,; the high bits are implicitly truncated. If this is a vector store, the; high elements are discarded (i.e. this does not function as a per-lane; vector, truncating store). G_INDEXED_STORE; ^^^^^^^^^^^^^^^. Combines a store with a GEP. See description of G_INDEXED_LOAD for indexing behaviour. G_ATOMIC_CMPXCHG_WITH_SUCCESS; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic atomic c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst:16102,load,load,16102,interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,1,['load'],['load']
Performance,"EG_FS`` and ``__SEG_GS``; indicate their support. PowerPC Language Extensions; ---------------------------. Set the Floating Point Rounding Mode; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; PowerPC64/PowerPC64le supports the builtin function ``__builtin_setrnd`` to set; the floating point rounding mode. This function will use the least significant; two bits of integer argument to set the floating point rounding mode. .. code-block:: c++. double __builtin_setrnd(int mode);. The effective values for mode are:. - 0 - round to nearest; - 1 - round to zero; - 2 - round to +infinity; - 3 - round to -infinity. Note that the mode argument will modulo 4, so if the integer argument is greater; than 3, it will only use the least significant two bits of the mode.; Namely, ``__builtin_setrnd(102))`` is equal to ``__builtin_setrnd(2)``. PowerPC cache builtins; ^^^^^^^^^^^^^^^^^^^^^^. The PowerPC architecture specifies instructions implementing cache operations.; Clang provides builtins that give direct programmer access to these cache; instructions. Currently the following builtins are implemented in clang:. ``__builtin_dcbf`` copies the contents of a modified block from the data cache; to main memory and flushes the copy from the data cache. **Syntax**:. .. code-block:: c. void __dcbf(const void* addr); /* Data Cache Block Flush */. **Example of Use**:. .. code-block:: c. int a = 1;; __builtin_dcbf (&a);. Extensions for Static Analysis; ==============================. Clang supports additional attributes that are useful for documenting program; invariants and rules for static analysis tools, such as the `Clang Static; Analyzer <https://clang-analyzer.llvm.org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitizer)`` to check if the code is being built; with :doc:`AddressSanitizer`. Use ``__has_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:157075,cache,cache,157075,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['cache'],['cache']
Performance,"EV`` | ``LD1`` |; +-------------------------------+-------------------------------+---------------------+; | AAPCS | ``LDR`` | ``LD1 + REV`` |; +-------------------------------+-------------------------------+---------------------+; | Alignment for strict mode | ``LDR`` / ``LD1 + REV`` | ``LD1`` |; +-------------------------------+-------------------------------+---------------------+. Neither approach is perfect, and choosing one boils down to choosing the lesser of two evils. The issue with lane ordering, it was decided, would have to change target-agnostic compiler passes and would result in a strange IR in which lane indices were reversed. It was decided that this was worse than the changes that would have to be made to support ``LD1``, so ``LD1`` was chosen as the canonical vector load instruction (and by inference, ``ST1`` for vector stores). Implementation; ==============. There are 3 parts to the implementation:. 1. Predicate ``LDR`` and ``STR`` instructions so that they are never allowed to be selected to generate vector loads and stores. The exception is one-lane vectors [1]_ - these by definition cannot have lane ordering problems so are fine to use ``LDR``/``STR``. 2. Create code generation patterns for bitconverts that create ``REV`` instructions. 3. Make sure appropriate bitconverts are created so that vector values get passed over call boundaries as 1-element vectors (which is the same as if they were loaded with ``LDR``). Bitconverts; -----------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:9339,load,loads,9339,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loads']
Performance,"E_BUILD_TYPE=RelWithDebInfo`` instead of; ``-DCMAKE_BUILD_TYPE=Release``. This will grant better coverage of; debug info pieces of clang, but will take longer to complete and will; result in a much larger build directory. It's recommended to build the ``all`` target with your instrumented Clang,; since more coverage is often better. b. You should now have a few ``*.profraw`` files in; ``path/to/stage2/profiles/``. You need to merge these using; ``llvm-profdata`` (even if you only have one! The profile merge transforms; profraw into actual profile data, as well). This can be done with; ``/path/to/stage1/llvm-profdata merge; -output=/path/to/output/profdata.prof path/to/stage2/profiles/*.profraw``. 4. Now, build your final, PGO-optimized Clang. To do this, you'll want to pass; the following additional arguments to CMake. - ``-DLLVM_PROFDATA_FILE=/path/to/output/profdata.prof`` - Use the PGO; profile from the previous step.; - ``-DCMAKE_C_COMPILER=/path/to/stage1/clang`` - Use the Clang we built in; step 1.; - ``-DCMAKE_CXX_COMPILER=/path/to/stage1/clang++`` - Same as above. From here, you can build whatever targets you need. .. note::; You may see warnings about a mismatched profile in the build output. These; are generally harmless. To silence them, you can add; ``-DCMAKE_C_FLAGS='-Wno-backend-plugin'; -DCMAKE_CXX_FLAGS='-Wno-backend-plugin'`` to your CMake invocation. Congrats! You now have a Clang built with profile-guided optimizations, and you; can delete all but the final build directory if you'd like. If this worked well for you and you plan on doing it often, there's a slight; optimization that can be made: LLVM and Clang have a tool called tblgen that's; built and run during the build process. While it's potentially nice to build; this for coverage as part of step 3, none of your other builds should benefit; from building it. You can pass the CMake option; ``-DLLVM_NATIVE_TOOL_DIR=/path/to/stage1/bin``; to steps 2 and onward to avoid these useless rebuilds.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst:6719,optimiz,optimizations,6719,interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"Each scalar type has a parent type, which must also; be a scalar type or the TBAA root. Via this parent relation, scalar types; within a TBAA root form a tree. **Struct type descriptors** denote types; that contain a sequence of other type descriptors, at known offsets. These; contained type descriptors can either be struct type descriptors themselves; or scalar type descriptors. **Access tags** are metadata nodes attached to load and store instructions.; Access tags use type descriptors to describe the *location* being accessed; in terms of the type system of the higher level language. Access tags are; tuples consisting of a base type, an access type and an offset. The base; type is a scalar type descriptor or a struct type descriptor, the access; type is a scalar type descriptor, and the offset is a constant integer. The access tag ``(BaseTy, AccessTy, Offset)`` can describe one of two; things:. * If ``BaseTy`` is a struct type, the tag describes a memory access (load; or store) of a value of type ``AccessTy`` contained in the struct type; ``BaseTy`` at offset ``Offset``. * If ``BaseTy`` is a scalar type, ``Offset`` must be 0 and ``BaseTy`` and; ``AccessTy`` must be the same; and the access tag describes a scalar; access with scalar type ``AccessTy``. We first define an ``ImmediateParent`` relation on ``(BaseTy, Offset)``; tuples this way:. * If ``BaseTy`` is a scalar type then ``ImmediateParent(BaseTy, 0)`` is; ``(ParentTy, 0)`` where ``ParentTy`` is the parent of the scalar type as; described in the TBAA metadata. ``ImmediateParent(BaseTy, Offset)`` is; undefined if ``Offset`` is non-zero. * If ``BaseTy`` is a struct type then ``ImmediateParent(BaseTy, Offset)``; is ``(NewTy, NewOffset)`` where ``NewTy`` is the type contained in; ``BaseTy`` at offset ``Offset`` and ``NewOffset`` is ``Offset`` adjusted; to be relative within that inner type. A memory access with an access tag ``(BaseTy1, AccessTy1, Offset1)``; aliases a memory access with an access tag ``(BaseTy2",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:275104,load,load,275104,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"Enable debug output""), cl::Hidden, cl::location(DebugFlag));. In the above example, we specify ""``true``"" as the second argument to the; `cl::opt`_ template, indicating that the template should not maintain a copy of; the value itself. In addition to this, we specify the `cl::location`_; attribute, so that ``DebugFlag`` is automatically set. Option Attributes; -----------------. This section describes the basic attributes that you can specify on options. * The option name attribute (which is required for all options, except; `positional options`_) specifies what the option name is. This option is; specified in simple double quotes:. .. code-block:: c++. cl::opt<bool> Quiet(""quiet"");. .. _cl::desc(...):. * The **cl::desc** attribute specifies a description for the option to be; shown in the ``-help`` output for the program. This attribute supports; multi-line descriptions with lines separated by '\n'. .. _cl::value_desc:. * The **cl::value_desc** attribute specifies a string that can be used to; fine tune the ``-help`` output for a command line option. Look `here`_ for an; example. .. _cl::init:. * The **cl::init** attribute specifies an initial value for a `scalar`_; option. If this attribute is not specified then the command line option value; defaults to the value created by the default constructor for the; type. .. warning::. If you specify both **cl::init** and **cl::location** for an option, you; must specify **cl::location** first, so that when the command-line parser; sees **cl::init**, it knows where to put the initial value. (You will get an; error at runtime if you don't put them in the right order.). .. _cl::location:. * The **cl::location** attribute where to store the value for a parsed command; line option if using external storage. See the section on `Internal vs; External Storage`_ for more information. .. _cl::aliasopt:. * The **cl::aliasopt** attribute specifies which option a `cl::alias`_ option is; an alias for. .. _cl::values:. * The **cl::values",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:36141,tune,tune,36141,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,1,['tune'],['tune']
Performance,"Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store atomic/; atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; spa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:360892,load,load,360892,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Configuration options for MVA method :. Configuration options reference for MVA method: SVM. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Gamma No 1 − RBF kernel parameter: Gamma (size of the Kernel). C No 1 − Cost parameter. Tol No 0.01 − Tolerance parameter. MaxIter No 1000 − Maximum number of training loops. Configuration options for MVA method :. Configuration options reference for MVA method: CFMlpANN. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No Fa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:8135,perform,performance,8135,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,1,['perform'],['performance']
Performance,"Example; matcher = objcMessageExpr(isInstanceMessage()); matches; NSString *x = @""hello"";; [x containsString:@""h""];; but not; [NSString stringWithFormat:@""format""];. Matcher<ObjCMessageExpr>matchesSelectorStringRef RegExp, Regex::RegexFlags Flags = NoFlags; Matches ObjC selectors whose name contains; a substring matched by the given RegExp.; matcher = objCMessageExpr(matchesSelector(""loadHTMLStringmatches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTMLString:html baseURL:NULL];. If the matcher is used in clang-query, RegexFlags parameter; should be passed as a quoted string. e.g: ""NoFlags"".; Flags can be combined with '|' example ""IgnoreCase | BasicRegex"". Matcher<ObjCMessageExpr>numSelectorArgsunsigned N; Matches when the selector has the specified number of arguments. matcher = objCMessageExpr(numSelectorArgs(0));; matches self.bodyView in the code below. matcher = objCMessageExpr(numSelectorArgs(2));; matches the invocation of ""loadHTMLString:baseURL:"" but not that; of self.bodyView; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMethodDecl>isClassMethod; Returns true when the Objective-C method declaration is a class method. Example; matcher = objcMethodDecl(isClassMethod()); matches; @interface I + (void)foo; @end; but not; @interface I - (void)bar; @end. Matcher<ObjCMethodDecl>isDefinition; Matches if a declaration has a body attached. Example matches A, va, fa; class A {};; class B; // Doesn't match, as it has no body.; int va;; extern int vb; // Doesn't match, as it doesn't define the variable.; void fa() {}; void fb(); // Doesn't match, as it has no body.; @interface X; - (void)ma; // Doesn't match, interface is declaration.; @end; @implementation X; - (void)ma {}; @end. Usable as: Matcher<TagDecl>, Matcher<VarDecl>, Matcher<FunctionDecl>,; Matcher<ObjCMethodDecl>. Matcher<ObjCMethodDecl>isInstanceMethod; Returns true when the Objective-C method declaration is an instance m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:108620,load,loadHTMLString,108620,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,1,['load'],['loadHTMLString']
Performance,"Expression field of the original debug intrinsic. The second form, ``DBG_VALUE_LIST``, appears thus:. .. code-block:: text. DBG_VALUE_LIST !123, !DIExpression(DW_OP_LLVM_arg, 0, DW_OP_LLVM_arg, 1, DW_OP_plus), %1, %2. And has the following operands:; * The first operand is the Variable field of the original debug intrinsic.; * The second operand is the Expression field of the original debug intrinsic.; * Any number of operands, from the 3rd onwards, record a sequence of variable; location operands, which may take any of the same values as the first; operand of the ``DBG_VALUE`` instruction above. These variable location; operands are inserted into the final DWARF Expression in positions indicated; by the DW_OP_LLVM_arg operator in the `DIExpression; <LangRef.html#diexpression>`_. The position at which the DBG_VALUEs are inserted should correspond to the; positions of their matching ``llvm.dbg.value`` intrinsics in the IR block. As; with optimization, LLVM aims to preserve the order in which variable; assignments occurred in the source program. However SelectionDAG performs some; instruction scheduling, which can reorder assignments (discussed below).; Function parameter locations are moved to the beginning of the function if; they're not already, to ensure they're immediately available on function entry. To demonstrate variable locations during instruction selection, consider; the following example:. .. code-block:: llvm. define i32 @foo(i32* %addr) {; entry:; call void @llvm.dbg.value(metadata i32 0, metadata !3, metadata !DIExpression()), !dbg !5; br label %bb1, !dbg !5. bb1: ; preds = %bb1, %entry; %bar.0 = phi i32 [ 0, %entry ], [ %add, %bb1 ]; call void @llvm.dbg.value(metadata i32 %bar.0, metadata !3, metadata !DIExpression()), !dbg !5; %addr1 = getelementptr i32, i32 *%addr, i32 1, !dbg !5; call void @llvm.dbg.value(metadata i32 *%addr1, metadata !3, metadata !DIExpression()), !dbg !5; %loaded1 = load i32, i32* %addr1, !dbg !5; %addr2 = getelementptr i32, i32",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:28904,optimiz,optimization,28904,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimization']
Performance,"Extended SSA form, and; attach various forms of information to operands that dominate specific; uses. It is not meant for general use, only for building temporary; renaming forms that require value splits at certain points. .. _type.test:. '``llvm.type.test``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.type.test(ptr %ptr, metadata %type) nounwind memory(none). Arguments:; """""""""""""""""""". The first argument is a pointer to be tested. The second argument is a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return val",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:937970,load,load,937970,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"Extensions Documentation; <https://github.com/KhronosGroup/Khronosdotorg/blob/main/api/opencl/assets/OpenCL_LangExt.pdf>`_. OpenCL-Specific Attributes; --------------------------. OpenCL support in Clang contains a set of attribute taken directly from the; specification as well as additional attributes. See also :doc:`AttributeReference`. nosvm; ^^^^^. Clang supports this attribute to comply to OpenCL v2.0 conformance, but it; does not have any effect on the IR. For more details reffer to the specification; `section 6.7.2; <https://www.khronos.org/registry/cl/specs/opencl-2.0-openclc.pdf#49>`_. opencl_unroll_hint; ^^^^^^^^^^^^^^^^^^. The implementation of this feature mirrors the unroll hint for C.; More details on the syntax can be found in the specification; `section 6.11.5; <https://www.khronos.org/registry/cl/specs/opencl-2.0-openclc.pdf#61>`_. convergent; ^^^^^^^^^^. To make sure no invalid optimizations occur for single program multiple data; (SPMD) / single instruction multiple thread (SIMT) Clang provides attributes that; can be used for special functions that have cross work item semantics.; An example is the subgroup operations such as `intel_sub_group_shuffle; <https://www.khronos.org/registry/cl/extensions/intel/cl_intel_subgroups.txt>`_. .. code-block:: c. // Define custom my_sub_group_shuffle(data, c); // that makes use of intel_sub_group_shuffle; r1 = ...; if (r0) r1 = computeA();; // Shuffle data from r1 into r3; // of threads id r2.; r3 = my_sub_group_shuffle(r1, r2);; if (r0) r3 = computeB();. with non-SPMD semantics this is optimized to the following equivalent code:. .. code-block:: c. r1 = ...; if (!r0); // Incorrect functionality! The data in r1; // have not been computed by all threads yet.; r3 = my_sub_group_shuffle(r1, r2);; else {; r1 = computeA();; r3 = my_sub_group_shuffle(r1, r2);; r3 = computeB();; }. Declaring the function ``my_sub_group_shuffle`` with the convergent attribute; would prevent this:. .. code-block:: c. my_sub_group_shuff",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:149966,optimiz,optimizations,149966,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"F); option(LLVM_UNREACHABLE_OPTIMIZE ""Optimize llvm_unreachable() as undefined behavior (default), guaranteed trap when OFF"" ON). if( NOT uppercase_CMAKE_BUILD_TYPE STREQUAL ""DEBUG"" ); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" OFF); else(); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" ON); endif(). option(LLVM_ENABLE_EXPENSIVE_CHECKS ""Enable expensive checks"" OFF). # While adding scalable vector support to LLVM, we temporarily want to; # allow an implicit conversion of TypeSize to uint64_t, and to allow; # code to get the fixed number of elements from a possibly scalable vector.; # This CMake flag enables a more strict mode where it asserts that the type; # is not a scalable vector type.; #; # Enabling this flag makes it easier to find cases where the compiler makes; # assumptions on the size being 'fixed size', when building tests for; # SVE/SVE2 or other scalable vector architectures.; option(LLVM_ENABLE_STRICT_FIXED_SIZE_VECTORS; ""Enable assertions that type is not scalable in implicit conversion from TypeSize to uint64_t and calls to getNumElements"" OFF). set(LLVM_ABI_BREAKING_CHECKS ""WITH_ASSERTS"" CACHE STRING; ""Enable abi-breaking checks. Can be WITH_ASSERTS, FORCE_ON or FORCE_OFF.""). option(LLVM_FORCE_USE_OLD_TOOLCHAIN; ""Set to ON to force using an old, unsupported host toolchain."" OFF). set(LLVM_LOCAL_RPATH """" CACHE FILEPATH; ""If set, an absolute path added as rpath on binaries that do not already contain an executable-relative rpath.""). option(LLVM_TEMPORARILY_ALLOW_OLD_TOOLCHAIN; ""Set to ON to only warn when using a toolchain which is about to be deprecated, instead of emitting an error."" OFF). option(LLVM_USE_INTEL_JITEVENTS; ""Use Intel JIT API to inform Intel(R) VTune(TM) Amplifier XE 2011 about JIT code""; OFF). if( LLVM_USE_INTEL_JITEVENTS ); # Verify we are on a supported platform; if( NOT CMAKE_SYSTEM_NAME MATCHES ""Windows"" AND NOT CMAKE_SYSTEM_NAME MATCHES ""Linux"" ); message(FATAL_ERROR; ""Intel JIT API support is available on Linux and W",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:26091,scalab,scalable,26091,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['scalab'],['scalable']
Performance,"F1`; instances. Here, we used a macro, some sort of lightweight program, that the; interpreter distributed with ROOT, Cling, is able to execute. This is a; rather extraordinary situation, since `C++` is not natively an interpreted; language! There is much more to say: chapter is indeed dedicated to; macros. ## Controlling ROOT ##. One more remark at this point: as every command you type into ROOT is; usually interpreted by Cling, an ""escape character"" is needed to pass; commands to ROOT directly. This character is the dot at the beginning of; a line:. ``` {.cpp}; root [1] .<command>; ```. This is a selection of the most common commands. - **quit root**, simply type `.q` or `.quit` or `.exit`. - obtain the full **list of commands**, use `.?` or `.help`. - **access the shell** of the operating system, type `.!<OS_command>`;; try, e.g. `.!ls` or `.!pwd`. - **execute a macro**, enter `.x <file_name>`; in the above example,; you might have used `.x slits.C` at the ROOT prompt. - **load a macro**, type `.L <file_name>`; in the above example, you; might instead have used the command `.L slits.C` followed by the; function call `slits();`. Note that after loading a macro all; functions and procedures defined therein are available at the ROOT; prompt. - **compile a macro**, type `.L <file_name>+`; ROOT is able to manage; for you the `C++` compiler behind the scenes and to produce machine; code starting from your macro. One could decide to compile a macro; in order to obtain better performance or to get nearer to the; production environment. ## Plotting Measurements ##. To display measurements in ROOT, including errors, there exists a; powerful class `TGraphErrors` with different types of constructors. In; the example here, we use data from the file `ExampleData.txt` in text; format:. ``` {.cpp}; root [0] TGraphErrors gr(""ExampleData.txt"");; root [1] gr.Draw(""AP"");; ```. You should see the output shown in Figure [2.2](#f22). [f22]: figures/TGraphErrors_Example.png ""f22""; <a na",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md:8226,load,load,8226,documentation/primer/ROOT_as_calculator.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md,1,['load'],['load']
Performance,"FLAT_SCRATCH_LO is in units of 256 bytes, the offset must be right; shifted by 8 before moving into FLAT_SCRATCH_HI. FLAT_SCRATCH_HI corresponds to SGPRn-4 on GFX7, and SGPRn-6 on GFX8 (where; SGPRn is the highest numbered SGPR allocated to the wavefront).; FLAT_SCRATCH_HI is multiplied by 256 (as it is in units of 256 bytes) and; added to ``SH_HIDDEN_PRIVATE_BASE_VIMID`` to calculate the per wavefront; FLAT SCRATCH BASE in flat memory instructions that access the scratch; aperture.; 2. The second word of Flat Scratch Init is 32-bit byte size of a single; work-items scratch memory usage. CP obtains this from the runtime, and it is always a multiple of DWORD. CP; checks that the value in the kernel dispatch packet Private Segment Byte; Size is not larger and requests the runtime to increase the queue's scratch; size if necessary. CP directly loads from the kernel dispatch packet Private Segment Byte Size; field and rounds up to a multiple of DWORD. Having CP load it once avoids; loading it at the beginning of every wavefront. The kernel prolog code must move it to FLAT_SCRATCH_LO which is SGPRn-3 on; GFX7 and SGPRn-5 on GFX8. FLAT_SCRATCH_LO is used as the FLAT SCRATCH SIZE; in flat memory instructions. * If the *Target Properties* column of :ref:`amdgpu-processor-table`; specifies *Absolute flat scratch*:. If the kernel or any function it calls may use flat operations to access; scratch memory, the prolog code must set up the FLAT_SCRATCH register pair; (FLAT_SCRATCH_LO/FLAT_SCRATCH_HI which are in SGPRn-4/SGPRn-3). Initialization; uses Flat Scratch Init and Scratch Wavefront Offset SGPR registers (see; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`):. The Flat Scratch Init is the 64-bit address of the base of scratch backing; memory being managed by SPI for the queue executing the kernel dispatch. CP obtains this from the runtime. The kernel prolog must add the value of the wave's Scratch Wavefront Offset; and move the result as a 64-bit value to the FLAT_SCRAT",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:196167,load,load,196167,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],"['load', 'loading']"
Performance,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28838,optimiz,optimizes,28838,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimizes']
Performance,"FX941; - constant buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. - !volatile & nontemporal. 1. GFX940, GFX941; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic sc0=1; load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic sc1=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic monotonic - workgroup - global 1. buffer/global/flat_store; - generic sc0=1; store atomic monotonic - agent - global 1. buffer/global/flat_store; - generic sc1=1; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:294159,load,load,294159,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"File (#208); 3. Fix TGraph tooltips handling; 4. Fix TH2Poly tooltips handling. ## Changes in 6.0.0; 1. Major release with:; - incompatible changes in API; - heavy use of Promise class; - upgrade all used packages; 2. Use generic naming convention - all class names always starts from; capital letter like ""ObjectPainter"", all function names starts from small; letter like ""painter.getObjectHint()""; 3. Rename JSRootCore.js -> JSRoot.core.js, eliminate all URL parameters.; Loading of extra JSROOT functionality should be done via JSROOT.require() method; All other scripts uses similar naming convention.; 4. JSROOT.draw()/JSROOT.redraw() functions returns Promise, deprecate callback parameter; 5. Introduce JSROOT.httpRequest() function which returns Promise instance, deprecate; JSROOT.NewHttpRequest() function; 6. JSROOT.openFile() returns Promise with file instance, deprecate callback parameter; 7. Provide new code loader via JSROOT.require(); - introduces clean dependencies in JSROOT code; - by default uses plain script loading emulating require.js behavior; - can use require.js when available; - uses require() method when running inside node.js; - supports openui5 sap.ui.require loader if available before JSRoot.core.js; - deprecates old JSROOT.AssertPrerequisites() function; 8. Upgrade d3.js to v6.1.1, skip support of older versions; 9. Upgrade three.js to r121:; - SoftwareRenderer deprecated and removed; - let use WebGL for browser, batch and node.js (via headless-gl); - support r3d_gl, r3d_img, r3d_svg rendering options for TGeo and histograms; - keep support of SVGRendered as backup solution; 10. Upgrade MathJax.js to version 3.1.1; - reliably works in browser and node.js!; - all latex/mathjax related methods moved to special JSRoot.latex.js script, loaded on demand; 11. Update jquery to 3.5.1, openui5 to 1.82.2; 12. Use JS classes only in few places - performance is not good enough compared to Object.prototype; 13. Deprecate IE support; 14. Deprecate bower package",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:25531,load,loading,25531,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"FileCheck - Flexible pattern matching file verifier; ===================================================. .. program:: FileCheck. SYNOPSIS; --------. :program:`FileCheck` *match-filename* [*--check-prefix=XXX*] [*--strict-whitespace*]. DESCRIPTION; -----------. :program:`FileCheck` reads two files (one from standard input, and one; specified on the command line) and uses one to verify the other. This; behavior is particularly useful for the testsuite, which wants to verify that; the output of some tool (e.g. :program:`llc`) contains the expected information; (for example, a movsd from esp or whatever is interesting). This is similar to; using :program:`grep`, but it is optimized for matching multiple different; inputs in one file in a specific order. The ``match-filename`` file specifies the file that contains the patterns to; match. The file to verify is read from standard input unless the; :option:`--input-file` option is used. OPTIONS; -------. Options are parsed from the environment variable ``FILECHECK_OPTS``; and from the command line. .. option:: -help. Print a summary of command line options. .. option:: --check-prefix prefix. FileCheck searches the contents of ``match-filename`` for patterns to; match. By default, these patterns are prefixed with ""``CHECK:``"".; If you'd like to use a different prefix (e.g. because the same input; file is checking multiple different tool or options), the; :option:`--check-prefix` argument allows you to specify (without the trailing; ""``:``"") one or more prefixes to match. Multiple prefixes are useful for tests; which might change for different run options, but most lines remain the same. FileCheck does not permit duplicate prefixes, even if one is a check prefix; and one is a comment prefix (see :option:`--comment-prefixes` below). .. option:: --check-prefixes prefix1,prefix2,... An alias of :option:`--check-prefix` that allows multiple prefixes to be; specified as a comma separated list. .. option:: --comment-prefixes prefix",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:678,optimiz,optimized,678,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['optimiz'],['optimized']
Performance,"FisherCuts No False − Use multivariate splits using the Fisher criterion. MinLinCorrForFisher No 0.8 − The minimum linear correlation between two variables demanded for use in Fisher criterion in node splitting. UseExclusiveVars No False − Variables already used in fisher criterion are not anymore analysed individually for node splitting. DoPreselection No False − and and apply automatic pre-selection for 100% efficient signal (bkg) cuts prior to training. RenormByClass No False − Individually re-normalize each event class to the original size after boosting. SigToBkgFraction No 1 − Sig to Bkg ratio used in Training (similar to NodePurityLimit, which cannot be used in real adaboost. PruneMethod No NoPruning NoPruning, ExpectedError, CostComplexity Note: for BDTs use small trees (e.g.MaxDepth=3) and NoPruning: Pruning: Method used for pruning (removal) of statistically insignificant branches . PruneStrength No 0 − Pruning strength. PruningValFraction No 0.5 − Fraction of events to use for optimizing automatic pruning. nEventsMin No 0 − deprecated: Use MinNodeSize (in % of training events) instead. GradBaggingFraction No 0.6 − deprecated: Use *BaggedSampleFraction* instead: Defines the fraction of events to be used in each iteration, e.g. when UseBaggedGrad=kTRUE. . UseNTrainEvents No 0 − deprecated: Use *BaggedSampleFraction* instead: Number of randomly picked training events used in randomised (and bagged) trees. NNodesMax No 0 − deprecated: Use MaxDepth instead to limit the tree size. Configuration options for MVA method :. Configuration options reference for MVA method: Boost. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PC",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:15088,optimiz,optimizing,15088,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,1,['optimiz'],['optimizing']
Performance,"Flag(UInt_t(random+0.5));; event->SetTemperature(random+20.);; for(UChar_t m = 0; m < 10; m++) {; event->SetMeasure(m, Int_t(gRandom->Gaus(m,m+1)));; }; // continued...; // fill the matrix; for(UChar_t i0 = 0; i0 < 4; i0++) {; for(UChar_t i1 = 0; i1 < 4; i1++) {; event->SetMatrix(i0,i1,gRandom->Gaus(i0*i1,1));; }; }; // create and fill the Track objects; for (Int_t t = 0; t < ntrack; t++) event->AddTrack(random);; t4.Fill(); // Fill the tree; event->Clear(); // Clear before reloading event; }; f.Write(); // Write the file header; t4.Print(); // Print the tree contents; }; ```. ### Reading the Tree. First, we check if the shared library with the class definitions is; loaded. If not we load it. Then we read two branches, one for the number; of tracks and one for the entire event. We check the number of tracks; first, and if it meets our condition, we read the entire event. We show; the fist entry that meets the condition. ``` {.cpp}; void tree4r() {; // check if the event class is in the dictionary; // if it is not load the definition in libEvent.so; if (!TClassTable::GetDict(""Event"")) {; gSystem->Load(""$ROOTSYS/test/libEvent.so"");; }; // read the tree generated with tree4w. // note that we use ""new"" to create the TFile and TTree objects, because we; // want to keep these objects alive when we leave this function.; TFile *f = new TFile(""tree4.root"");; TTree *t4 = (TTree*)f->Get(""t4"");. // create a pointer to an event object for reading the branch values.; Event *event = new Event();; // get two branches and set the branch address; TBranch *bntrack = t4->GetBranch(""fNtrack"");; TBranch *branch = t4->GetBranch(""event_split"");; branch->SetAddress(&event);. Int_t nevent = t4->GetEntries();; Int_t nselected = 0;; Int_t nb = 0;; for (Int_t i=0; i<nevent; i++) {; //read branch ""fNtrack""only; bntrack->GetEntry(i);. // reject events with more than 587 tracks; if (event->GetNtrack() > 587)continue;. // read complete accepted event in memory; nb += t4->GetEntry(i);; nselected++;. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:64290,load,load,64290,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['load']
Performance,"FlushBaskets has been called.; This minimizes the number of cases where one has to seek backward when reading. This function may be called at the start of a program to change; the default value for fAutoFlush. CASE 1 : autof > 0. autof is the number of consecutive entries after which TTree::Fill will; flush all branch buffers to disk. CASE 2 : autof < 0. When filling the Tree the branch buffers will be flushed to disk when; more than autof bytes have been written to the file. At the first FlushBaskets; TTree::Fill will replace fAutoFlush by the current value of fEntries. Calling this function with autof < 0 is interesting when it is hard to estimate; the size of one entry. This value is also independent of the Tree. When calling SetAutoFlush with no arguments, the; default value is -30000000, ie that the first AutoFlush will be done when; 30 MBytes of data are written to the file. CASE 3 : autof = 0; The AutoFlush mechanism is disabled. Flushing the buffers at regular intervals optimize the location of; consecutive entries on the disk. Changed the default value of AutoSave from 10 to 30 MBytes. New class TTreePerfStats; This new class is an important tool to measure the I/O performance of a Tree.; It shows the locations in the file when reading a Tree. In particular it is easy; to see the performance of the Tree Cache. The results can be:. drawn in a canvas.; printed on standard output.; saved to a file for processing later. Example of use; {; TFile *f = TFile::Open(""RelValMinBias-GEN-SIM-RECO.root"");; T = (TTree*)f->Get(""Events"");; Long64_t nentries = T->GetEntries();; T->SetCacheSize(10000000);; T->AddBranchToCache(""*"");. TTreePerfStats *ps= new TTreePerfStats(""ioperf"",T);. for (Int_t i=0;i<nentries;i++) {; T->GetEntry(i);; }; ps->SaveAs(""atlas_perf.root"");; }. then, in a root interactive session, one can do:. root > TFile f(""atlas_perf.root"");; root > ioperf->Draw();; root > ioperf->Print();. The Draw or Print functions print the following information:. TreeCache ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:7051,optimiz,optimize,7051,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,1,['optimiz'],['optimize']
Performance,"For GFX6-GFX9:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that for GFX7-GFX9 ``flat_load/store/atomic`` instructions can report out of; vector memory order if they access LDS memory, and out of LDS operation order; if they access global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore, no special action is required for coherence between the; lanes of a single wavefront, or for coherence between wavefronts in the same; work-group. A ``buffer_wbinvl1_vol`` is required for coherence between; wavefronts executing in different work-groups as they may be executing on; different CUs.; * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coheren",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:207311,perform,performed,207311,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"For example, software pipelining; schedules multiple iterations in an interleaved fashion to allow the; instructions of one iteration to hide the latencies of the instructions of; another iteration. Another example is vectorization that can exploit SIMD; hardware to allow a single instruction to execute multiple iterations using; vector registers. Note that although this is similar to SIMT execution, the way a client debugger; uses the information is fundamentally different. In SIMT execution the debugger; needs to present the concurrent execution as distinct source language threads; that the user can list and switch focus between. With iteration concurrency; optimizations, such as software pipelining and vectorized SIMD, the debugger; must not present the concurrency as distinct source language threads. Instead,; it must inform the user that multiple loop iterations are executing in parallel; and allow the user to select between them. In general, SIMT execution fixes the number of concurrent executions per target; architecture thread. However, both software pipelining and SIMD vectorization; may vary the number of concurrent iterations for different loops executed by a; single source language thread. It is possible for the compiler to use both SIMT concurrency and iteration; concurrency techniques in the code of a single source language thread. Therefore, a DWARF operation is required to denote the current concurrent; iteration instance, much like ``DW_OP_push_object_address`` denotes the current; object. See ``DW_OP_LLVM_push_iteration`` in; :ref:`amdgpu-dwarf-literal-operations`. In addition, a way is needed for the compiler to communicate how many source; language loop iterations are executing concurrently. See; ``DW_AT_LLVM_iterations`` in :ref:`amdgpu-dwarf-low-level-information`. 2.20 DWARF Operation to Create Runtime Overlay Composite Location Description; -----------------------------------------------------------------------------. It is common in SIMD vec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:33960,concurren,concurrent,33960,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['concurren'],['concurrent']
Performance,"Fraser U) for suggesting; this.; ; Methods:. BDT: New Decision Tree Pruning algorithm: Cost; Complexity Pruning a la CART. Written by Doug Schouten; (Fraser U.). It replaces the old CostComplexity and; CostComplexity2 algorithms.; . BDT: New no splitting option (choosable with; NCuts<0) that finds best split point by first sorting the; events for each variable and then looping through all; events, placing the cuts always in the middle between two; of the sorted events, and finding the true possible; maximum separation gain in the training sample by cutting; on this variable.; . BDT, AdaBoost The beta parameter is now an; option (default is 1).; . BDT: The node purity at which a node is; classified as signal (respective background node) for; determining the error fraction in the pruning became a; parameter that can be set via the option NodePurityLimit; (default is 0.5).; . Dataset preparation:. First implementation of a new preprocessing method: transformation of the; variables first into a Gaussian distribution, then performing a decorrelation of; the ""Gaussianised"" variables. The transformation is again done by default such that; (by default) the signal distributions become Gaussian and are decorrelated. Note ; that simultaneous Gaussianisation and decorrelation of signal and background is ; only possible (and done) for methods, such as Likelihood, which test both hypotheses.; . Bug fixes:. Fix in Expected error pruning: Rather than multiplying both sides, the error on ; the node and the sub-tree, with the prune strength, now only the expected error ; of the sub-tree is scaled.; . Fix in FDA parsing of the input formula. There were problems when treating; more than 10 parameters (thanks to Hugh Skottowe for reporting this).; . Calculation of ""Separation"": fixed bin-shift and; normalisation bugs. Thanks to Dag Gillberg (Fraser U) for; spotting these.; . Fixed problem in ""SetSignal(Background)WeightExpression"":; signal (background weight expressions not existing in t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v522/index.html:1360,perform,performing,1360,tmva/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v522/index.html,1,['perform'],['performing']
Performance,"GC intrinsic lowering; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. As a part of lowering to the explicit model of relocations; RewriteStatepointsForGC performs GC specific lowering for the following; intrinsics:. * ``gc.get.pointer.base``; * ``gc.get.pointer.offset``; * ``llvm.memcpy.element.unordered.atomic.*``; * ``llvm.memmove.element.unordered.atomic.*``. There are two possible lowerings for the memcpy and memmove operations:; GC leaf lowering and GC parseable lowering. If a call is explicitly marked with; ""gc-leaf-function"" attribute the call is lowered to a GC leaf call to; '``__llvm_memcpy_element_unordered_atomic_*``' or; '``__llvm_memmove_element_unordered_atomic_*``' symbol. Such a call can not; take a safepoint. Otherwise, the call is made GC parseable by wrapping the; call into a statepoint. This makes it possible to take a safepoint during; copy operation. Note that a GC parseable copy operation is not required to; take a safepoint. For example, a short copy operation may be performed without; taking a safepoint. GC parseable calls to '``llvm.memcpy.element.unordered.atomic.*``',; '``llvm.memmove.element.unordered.atomic.*``' intrinsics are lowered to calls; to '``__llvm_memcpy_element_unordered_atomic_safepoint_*``',; '``__llvm_memmove_element_unordered_atomic_safepoint_*``' symbols respectively.; This way the runtime can provide implementations of copy operations with and; without safepoints. GC parseable lowering also involves adjusting the arguments for the call.; Memcpy and memmove intrinsics take derived pointers as source and destination; arguments. If a copy operation takes a safepoint it might need to relocate the; underlying source and destination objects. This requires the corresponding base; pointers to be available in the copy operation. In order to make the base; pointers available RewriteStatepointsForGC replaces derived pointers with base; pointer and offset pairs. For example:. .. code-block:: llvm. declare void @__llvm_memcpy_element_u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:30289,perform,performed,30289,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['perform'],['performed']
Performance,"GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This function will be decorated with optnone.; int foo() {; // ... code; }. // optnone conflicts with always_inline, so bar() will not be decorated.; __attribute__((always_inline)) int bar() {; // ... code; }; #pragma clang optimize on. If no ``on`` is found to close an ``off`` region, the end of the region is the; end of the compilation unit. Note that a stray ``#pragma clang optimize on`` does not selectively enable; additional optimizations when compiling at low optimization levels. This feature; can only be used to selectively disable optimizations. The pragma has an effect on functions only at the point of their definition; for; function templates, this means that the state of the pragma at the point of an; instantiation is not necessarily relevant. Consider the following example:. .. code-block:: c++. template<typename T> T twice(T t) {; return 2 * t;; }. #pragma clang optimize off; template<typename T> T thrice(T t) {; return 3 * t;; }. int container(int a, int b) {; return twice(a) + thrice(b);; }; #pragma clang optimize on. In this example, the definition of the template function ``twice`` is outside; the pragma region, whereas the definition of ``thrice`` is inside the region.; The ``conta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:159718,optimiz,optimize,159718,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimize']
Performance,"GetTopVolume()->Draw(""ogl"");; ~~~. For any questions or comments about the GDML->ROOT binding please contact ben.lloyd@cern.ch. ### ROOT->GDML. The TGeo to GDML converter allows to export ROOT geometries (TGeo; geometry trees) as GDML files. The writer module writes a GDML file; out of the 'in-memory' representation of the geometry. The actual; application-specific (ROOT) binding is implemented in ROOTwriter; module. It contains 'binding methods' for TGeo geometry classes which; can be exported in GDML format. Please refere to the comment part of; the ROOTwriter.py file for the list of presently supported TGeo; classes. The ROOTwriter class contains also three methods,; dumpMaterials, dumpSolids and examineVol which need to be called in; order to export materials, solids and geometry tree respectively. The TGeo to GDML converter is now interfaced to the; TGeoManager::Export method which automatically calls the appropriate; Python scripts whenever the geometry output file has the .gdml; extension. Alternatively, one can also use the ROOT->GDML converter directly from; the Python prompt (assuming the TGeo geometry has already been loaded; into memory in one or another way), for example:. ~~~ {.cpp}; from math import *. import ROOT; import writer; import ROOTwriter. # get TGeoManager and; # get the top volume of the existing (in-memory) geometry tree; geomgr = ROOT.gGeoManager; topV = geomgr.GetTopVolume(). # instanciate writer; gdmlwriter = writer.writer('mygeo.gdml'); binding = ROOTwriter.ROOTwriter(gdmlwriter). # dump materials; matlist = geomgr.GetListOfMaterials(); binding.dumpMaterials(matlist). # dump solids; shapelist = geomgr.GetListOfShapes(); binding.dumpSolids(shapelist). # dump geo tree; print 'Traversing geometry tree'; gdmlwriter.addSetup('default', '1.0', topV.GetName()); binding.examineVol(topV). # write file; gdmlwriter.writeFile(); ~~~. For all other functionality questions or comments, or even GDML in general,; please email Witold.Pokorski@cern.ch; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/gdml/doc/index.md:2818,load,loaded,2818,geom/gdml/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/gdml/doc/index.md,1,['load'],['loaded']
Performance,"Getting Started/Tutorials; =========================. For those new to the LLVM system. .. toctree::; :hidden:. CompilerWriterInfo; Frontend/PerformanceTips; GettingStarted; GettingStartedVS; ProgrammersManual; tutorial/index; MyFirstTypoFix. :doc:`GettingStarted`; Discusses how to get up and running quickly with the LLVM infrastructure.; Everything from unpacking and compilation of the distribution to execution; of some tools. :doc:`tutorial/index`; Tutorials about using LLVM. Includes a tutorial about making a custom; language with LLVM. :doc:`ProgrammersManual`; Introduction to the general layout of the LLVM sourcebase, important classes; and APIs, and some tips & tricks. :doc:`Frontend/PerformanceTips`; A collection of tips for frontend authors on how to generate IR; which LLVM is able to effectively optimize. :doc:`GettingStartedVS`; An addendum to the main Getting Started guide for those using Visual Studio; on Windows. :doc:`CompilerWriterInfo`; A list of helpful links for compiler writers. :doc:`MyFirstTypoFix`; This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedTutorials.rst:816,optimiz,optimize,816,interpreter/llvm-project/llvm/docs/GettingStartedTutorials.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedTutorials.rst,1,['optimiz'],['optimize']
Performance,"HF_ALLOC`` + ``SHF_WRITE``; ``.debug_``\ *\** ``SHT_PROGBITS`` *none*; ``.dynamic`` ``SHT_DYNAMIC`` ``SHF_ALLOC``; ``.dynstr`` ``SHT_PROGBITS`` ``SHF_ALLOC``; ``.dynsym`` ``SHT_PROGBITS`` ``SHF_ALLOC``; ``.got`` ``SHT_PROGBITS`` ``SHF_ALLOC`` + ``SHF_WRITE``; ``.hash`` ``SHT_HASH`` ``SHF_ALLOC``; ``.note`` ``SHT_NOTE`` *none*; ``.rela``\ *name* ``SHT_RELA`` *none*; ``.rela.dyn`` ``SHT_RELA`` *none*; ``.rodata`` ``SHT_PROGBITS`` ``SHF_ALLOC``; ``.shstrtab`` ``SHT_STRTAB`` *none*; ``.strtab`` ``SHT_STRTAB`` *none*; ``.symtab`` ``SHT_SYMTAB`` *none*; ``.text`` ``SHT_PROGBITS`` ``SHF_ALLOC`` + ``SHF_EXECINSTR``; ================== ================ =================================. These sections have their standard meanings (see [ELF]_) and are only generated; if needed. ``.debug``\ *\**; The standard DWARF sections. See :ref:`amdgpu-dwarf-debug-information` for; information on the DWARF produced by the AMDGPU backend. ``.dynamic``, ``.dynstr``, ``.dynsym``, ``.hash``; The standard sections used by a dynamic loader. ``.note``; See :ref:`amdgpu-note-records` for the note records supported by the AMDGPU; backend. ``.rela``\ *name*, ``.rela.dyn``; For relocatable code objects, *name* is the name of the section that the; relocation records apply. For example, ``.rela.text`` is the section name for; relocation records associated with the ``.text`` section. For linked shared code objects, ``.rela.dyn`` contains all the relocation; records from each of the relocatable code object's ``.rela``\ *name* sections. See :ref:`amdgpu-relocation-records` for the relocation records supported by; the AMDGPU backend. ``.text``; The executable machine code for the kernels and functions they call. Generated; as position independent code. See :ref:`amdgpu-code-conventions` for; information on conventions used in the isa generation. .. _amdgpu-note-records:. Note Records; ------------. The AMDGPU backend code object contains ELF note records in the ``.note``; section. The set of generated not",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:68316,load,loader,68316,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"HLSLShader`` attribute to the function with the; specified name. This allows code generation for entry functions to always key; off the presence of the ``HLSLShader`` attribute, regardless of what shader; profile you are compiling. In code generation, two functions are generated. One is the user defined; function, which is code generated as a mangled C++ function with internal; linkage following normal function code generation. The actual exported entry function which can be called by the GPU driver is a; ``void(void)`` function that isn't name mangled. In code generation we generate; the unmangled entry function to serve as the actual shader entry. The shader; entry function is annotated with the ``hlsl.shader`` function attribute; identifying the entry's pipeline stage. The body of the unmangled entry function contains first a call to execute global; constructors, then instantiations of the user-defined entry parameters with; their semantic values populated, and a call to the user-defined function.; After the call instruction the return value (if any) is saved using a; target-appropriate intrinsic for storing outputs (for DirectX, the; ``llvm.dx.store.output``). Lastly, any present global destructors will be called; immediately before the return. HLSL does not support C++ ``atexit``; registrations, instead calls to global destructors are compile-time generated. .. note::. HLSL support in Clang is currently focused on compute shaders, which do not; support output semantics. Support for output semantics will not be; implemented until other shader profiles are supported. Below is example IR that represents the planned implementation, subject to; change as the ``llvm.dx.store.output`` and ``llvm.dx.load.input`` intrinsics are; not yet implemented. .. code-block:: none. ; Function Attrs: norecurse; define void @main() #1 {; entry:; %0 = call i32 @llvm.dx.load.input.i32(...); %1 = call i32 @""?main@@YAXII@Z""(i32 %0); call @llvm.dx.store.output.i32(%1, ...); ret void; }. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/EntryFunctions.rst:2761,load,load,2761,interpreter/llvm-project/clang/docs/HLSL/EntryFunctions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/EntryFunctions.rst,2,['load'],['load']
Performance,"HttpServer(""http:127.0.0.1:8080""); If host has several network interfaces, one could select one for binding:; new THttpServer(""http:192.168.1.17:8080""). ### TNetXNGFileStager; Fixed ROOT-7703. This restores the behavior of Locate() to that found with; TXNetFileStager: Rather than return only the xrootd server's reply, the endpoint; hostname is looked up and Locate() returns the full url, including the path. ### TWebFile; Fixed ROOT-7809. Returns an error for a redirect which does not specify the new; URI, rather than going into a loop. Fixed ROOT-7817. Avoid a crash under some circumstances when trying to open an; invalid path. ## GUI Libraries. ## Montecarlo Libraries. ## Multi-processing. With this version we introduce a new module, core/multiproc, for multi-processing on multi-core machines. This module is based on fork technology and offers an interface inspired from Python multiprocessor module. The new interface, implemented in the class TProcPool, provides the possibility to perform in parallel a very generic set of tasks, described by macros, functions or lambdas. This illustrates the usage of lambdas:. ``` {.cpp}; {; TProcPool pool;; auto ten = pool.MapReduce([]() { return 1; }, 10, [](std::vector<int> v) { return std::accumulate(v.begin(), v.end(), 0); }); }; ```. And this how it can be used to generate ten histos and merge them:. ``` {.cpp}; {; TObject *CreateAndFillHists(); {. TH1F *h = new TH1F(""h"", """", 100, -3., 3.);; h->SetDirectory(0);; h->FillRandom(""gaus"", 1000);; return h;; }. TProcPool pool;; auto hist = pool.MapReduce(CreateAndFillHists, 10, PoolUtils::ReduceObjects);; hist->DrawClone();; }; ```. Tutorials illustrating other usages of the new class TProcPool are available under tutorials/multicore. ## Language Bindings. ### Notebooks; We provided integration of ROOT with the Jupyter technology, integrating ROOT with Python Notebooks and providing a ROOT Kernel like functionality - de facto an enhanced C++ web based shell. Tab completion, output a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md:21293,perform,perform,21293,README/ReleaseNotes/v606/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md,1,['perform'],['perform']
Performance,"ION 3.20.0); project(SimpleProject). find_package(LLVM REQUIRED CONFIG). message(STATUS ""Found LLVM ${LLVM_PACKAGE_VERSION}""); message(STATUS ""Using LLVMConfig.cmake in: ${LLVM_DIR}""). # Set your project compile flags.; # E.g. if using the C++ header files; # you will need to enable C++11 support; # for your compiler. include_directories(${LLVM_INCLUDE_DIRS}); separate_arguments(LLVM_DEFINITIONS_LIST NATIVE_COMMAND ${LLVM_DEFINITIONS}); add_definitions(${LLVM_DEFINITIONS_LIST}). # Now build our tools; add_executable(simple-tool tool.cpp). # Find the libraries that correspond to the LLVM components; # that we wish to use; llvm_map_components_to_libnames(llvm_libs support core irreader). # Link against LLVM libraries; target_link_libraries(simple-tool ${llvm_libs}). The ``find_package(...)`` directive when used in CONFIG mode (as in the above; example) will look for the ``LLVMConfig.cmake`` file in various locations (see; cmake manual for details). It creates a ``LLVM_DIR`` cache entry to save the; directory where ``LLVMConfig.cmake`` is found or allows the user to specify the; directory (e.g. by passing ``-DLLVM_DIR=/usr/lib/cmake/llvm`` to; the ``cmake`` command or by setting it directly in ``ccmake`` or ``cmake-gui``). This file is available in two different locations. * ``<LLVM_INSTALL_PACKAGE_DIR>/LLVMConfig.cmake`` where; ``<LLVM_INSTALL_PACKAGE_DIR>`` is the location where LLVM CMake modules are; installed as part of an installed version of LLVM. This is typically; ``cmake/llvm/`` within the lib directory. On Linux, this is typically; ``/usr/lib/cmake/llvm/LLVMConfig.cmake``. * ``<LLVM_BUILD_ROOT>/lib/cmake/llvm/LLVMConfig.cmake`` where; ``<LLVM_BUILD_ROOT>`` is the root of the LLVM build tree. **Note: this is only; available when building LLVM with CMake.**. If LLVM is installed in your operating system's normal installation prefix (e.g.; on Linux this is usually ``/usr/``) ``find_package(LLVM ...)`` will; automatically find LLVM if it is installed correctly. I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:42303,cache,cache,42303,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['cache'],['cache']
Performance,"IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4056,optimiz,optimizeModule,4056,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizeModule']
Performance,"IRgen optimization opportunities. //===---------------------------------------------------------------------===//. The common pattern of; --; short x; // or char, etc; (x == 10); --; generates an zext/sext of x which can easily be avoided. //===---------------------------------------------------------------------===//. Bitfields accesses can be shifted to simplify masking and sign; extension. For example, if the bitfield width is 8 and it is; appropriately aligned then is is a lot shorter to just load the char; directly. //===---------------------------------------------------------------------===//. It may be worth avoiding creation of alloca's for formal arguments; for the common situation where the argument is never written to or has; its address taken. The idea would be to begin generating code by using; the argument directly and if its address is taken or it is stored to; then generate the alloca and patch up the existing code. In theory, the same optimization could be a win for block local; variables as long as the declaration dominates all statements in the; block. NOTE: The main case we care about this for is for -O0 -g compile time; performance, and in that scenario we will need to emit the alloca; anyway currently to emit proper debug info. So this is blocked by; being able to emit debug information which refers to an LLVM; temporary, not an alloca. //===---------------------------------------------------------------------===//. We should try and avoid generating basic blocks which only contain; jumps. At -O0, this penalizes us all the way from IRgen (malloc &; instruction overhead), all the way down through code generation and; assembly time. On 176.gcc:expr.ll, it looks like over 12% of basic blocks are just; direct branches!. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/README.txt:6,optimiz,optimization,6,interpreter/llvm-project/clang/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/README.txt,4,"['load', 'optimiz', 'perform']","['load', 'optimization', 'performance']"
Performance,"IX ps c.ommand;; ```. With the `mhs3` example, you should be able to see a canvas with two; pads on it. Both pads keep histograms updated and filled by three; different threads. With the `CalcPi` example, you should be able to see; two threads calculating Pi with the given number of intervals as; precision. ### TThread in More Details. Cling is not thread safe yet, and it will block the execution of the; threads until it has finished executing. #### Asynchronous Actions. Different threads can work simultaneously with the same object. Some; actions can be dangerous. For example, when two threads create a; histogram object, ROOT allocates memory and puts them to the same; collection. If it happens at the same time, the results are; undetermined. To avoid this problem, the user has to synchronize these; actions with:. ``` {.cpp}; TThread::Lock() // Locking the following part of code; ... // Create an object, etc...; TThread::UnLock() // Unlocking; ```. The code between `Lock()` and `UnLock()` will be performed; uninterrupted. No other threads can perform actions or access; objects/collections while it is being executed. The methods; `TThread::Lock() `and **`TThread::UnLock()`** internally use a global; `TMutex` instance for locking. The user may also define their own **`TMutex`** `MyMutex` instance and may; locally protect their asynchronous actions by calling `MyMutex.Lock()` and; `MyMutex.UnLock().`. #### Synchronous Actions: TCondition. To synchronize the actions of different threads you can use the; **`TCondition`** class, which provides a signaling mechanism. The; **`TCondition`** instance must be accessible by all threads that need to; use it, i.e. it should be a global object (or a member of the class; which owns the threaded methods, see below). To create a; **`TCondition`** object, a **`TMutex`** instance is required for the; Wait and `TimedWait` locking methods. One can pass the address of an; external mutex to the **`TCondition`** constructor:. ``` {.cpp}; TM",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:7070,perform,performed,7070,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['perform'],['performed']
Performance,"I`` to; ``rootcling``/``genreflex`` during build time).; When relocating the shared library, move the .pcm with it.; Once support for C++ modules is fully fleshed out, access to the header file; will no longer be needed. .. _`rootcling manual`: https://root.cern.ch/root/html/guides/users-guide/AddingaClass.html#the-linkdef.h-file; .. _`helper script`: https://github.com/wlav/cppyy/blob/master/test/make_dict_win32.py. Class loader; ^^^^^^^^^^^^. Explicitly loading dictionaries is fine if this is hidden under the hood of; a Python package and thus transparently done on ``import``.; Otherwise, the automatic class loader is more convenient, as it allows direct; use without having to manually find and load dictionaries (assuming these are; locatable by the dynamic loader). The class loader utilizes so-called rootmap files, which by convention should; live alongside the dictionary shared library (and C++ module file).; These are simple text files, which map C++ entities (such as classes) to the; dictionaries and other libraries that need to be loaded for their use. With ``genreflex``, the mapping file can be automatically created with; ``--rootmap-lib=MyClassDict``, where ""MyClassDict"" is the name of the shared; library (without the extension) build from the dictionary file.; With ``rootcling``, create the same mapping file with; ``-rmf MyClassDict.rootmap -rml MyClassDict``.; It is necessary to provide the final library name explicitly, since it is; only in the separate linking step where these names are fixed and those names; may not match the default choice. With the mapping file in place, the above example can be rerun without; explicit loading of the dictionary:. .. code-block:: python. >>> import cppyy; >>> from cppyy.gbl import MyClass; >>> MyClass(42).get_int(); 42; >>>. .. _cppyy-generator:. Bindings collection; -------------------. ``cppyy-generator`` is a clang-based utility program which takes a set of C++; header files and generates a JSON output file describ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:8786,load,loaded,8786,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loaded']
Performance,"If one has a web server which already provides such JSON file, one could specify the URL to this file like:. <https://root.cern/js/latest/demo/update_draw.htm?addr=../httpserver.C/Canvases/c1/root.json.gz>. Here the same problem with [Cross-Origin Request](https://developer.mozilla.org/en/http_access_control) can appear. If the web server configuration cannot be changed, just copy JSROOT to the web server itself. ### Binary file-based monitoring (not recommended). Theoretically, one could use binary ROOT files to implement monitoring.; With such approach, a ROOT-based application creates and regularly updates content of a ROOT file, which can be accessed via normal web server. From the browser side, JSROOT could regularly read the specified objects and update their drawings. But such solution has three major caveats. First of all, one need to store the data of all objects, which only potentially could be displayed in the browser. In case of 10 objects it does not matter, but for 1000 or 100000 objects this will be a major performance penalty. With such big amount of data one will never achieve higher update rate. The second problem is I/O. To read the first object from the ROOT file, one need to perform several (about 5) file-reading operations via http protocol.; There is no http file locking mechanism (at least not for standard web servers),; therefore there is no guarantee that the file content is not changed/replaced between consequent read operations. Therefore, one should expect frequent I/O failures while trying to monitor data from ROOT binary files. There is a workaround for the problem - one could load the file completely and exclude many partial I/O operations by this. To achieve this with JSROOT, one should add ""+"" sign at the end of the file name. Of course, it only could work for small files. If somebody still wants to use monitoring of data from ROOT files, could try link like:. - <https://root.cern/js/latest/?nobrowser&file=../files/hsimple.root+&item",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:32766,perform,performance,32766,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['perform'],['performance']
Performance,"If we were not permitted in any event to shorten the lifetime of the; object in ``x``, then we would not be able to eliminate this retain; and release unless we could prove that the message send could not; modify ``_ivar`` (or deallocate ``self``). Since message sends are; opaque to the optimizer, this is not possible, and so ARC's hands; would be almost completely tied. ARC makes no guarantees about the execution of a computation history; which contains undefined behavior. In particular, ARC makes no; guarantees in the presence of race conditions. ARC may assume that any retainable object pointers it receives or; generates are instantaneously valid from that point until a point; which, by the concurrency model of the host language, happens-after; the generation of the pointer and happens-before a release of that; object (possibly via an aliasing pointer or indirectly due to; destruction of a different object). .. admonition:: Rationale. There is very little point in trying to guarantee correctness in the; presence of race conditions. ARC does not have a stack-scanning; garbage collector, and guaranteeing the atomicity of every load and; store operation would be prohibitive and preclude a vast amount of; optimization. ARC may assume that non-ARC code engages in sensible balancing; behavior and does not rely on exact or minimum retain count values; except as guaranteed by ``__strong`` object invariants or +1 transfer; conventions. For example, if an object is provably double-retained; and double-released, ARC may eliminate the inner retain and release;; it does not need to guard against code which performs an unbalanced; release followed by a ""balancing"" retain. .. _arc.optimization.liveness:. Object liveness; ---------------. ARC may not allow a retainable object ``X`` to be deallocated at a; time ``T`` in a computation history if:. * ``X`` is the value stored in a ``__strong`` object ``S`` with; :ref:`precise lifetime semantics <arc.optimization.precise>`, or. * ``X",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:77977,race condition,race conditions,77977,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['race condition'],['race conditions']
Performance,"InTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). FitMethod No GA GA, SA, MC, MCEvents, MINUIT, EventScan Minimisation Method (GA, SA, and MC are the primary methods to be used; the others have been introduced for testing purposes and are depreciated). EffMethod No EffSel EffSel, EffPDF Selection Method. CutRangeMin Yes -1 − Minimum of allowed cut range (set per variable). CutRangeMax Yes -1 − Maximum of allowed cut range (set per variable). VarProp Yes NotEnforced NotEnforced, FMax, FMin, FSmart Categorisation of cuts. Configuration options for MVA method :. Configuration options reference for MVA method: PDEFoam. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). SigBgSeparate No False − Separate foams for signal and background. TailCut No 0.001 − Fraction of outlier events that are excluded from the foam in each dimension. VolFrac No 0.0666667 − Size of sampling box, used for density calculation during foam build-up (maximum value: 1.0 is equivalent to volume of entire foam). nActiveCells No 500 − Maximum number of active cells to be created by the foam. nSampl No 2000 − Num",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:26030,perform,performed,26030,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,1,['perform'],['performed']
Performance,"Indicates that PAL will program this user-SGPR to contain the amount of LDS; space used for the ES/GS pseudo-ring-buffer for passing data between shader; stages.; 0x1000000B ViewId View id (32-bit unsigned integer) identifies a view of graphic; pipeline instancing.; 0x1000000C StreamOutTable 32-bit pointer to GPU memory containing the stream out target SRD table. This; can only appear for one shader stage per pipeline.; 0x1000000D PerShaderPerfData 32-bit pointer to GPU memory containing the per-shader performance data buffer.; 0x1000000F VertexBufferTable 32-bit pointer to GPU memory containing the vertex buffer SRD table. This can; only appear for one shader stage per pipeline.; 0x10000010 UavExportTable 32-bit pointer to GPU memory containing the UAV export SRD table. This can; only appear for one shader stage per pipeline (PS). These replace color targets; and are completely separate from any UAVs used by the shader. This is optional,; and only used by the PS when UAV exports are used to replace color-target; exports to optimize specific shaders.; 0x10000011 NggCullingData 64-bit pointer to GPU memory containing the hardware register data needed by; some NGG pipelines to perform culling. This value contains the address of the; first of two consecutive registers which provide the full GPU address.; 0x10000015 FetchShaderPtr 64-bit pointer to GPU memory containing the fetch shader subroutine.; ========== ================= ===============================================================================. .. _amdgpu-amdpal-code-object-metadata-user-data-per-shader-table-section:. Per-Shader Table; ################. Low 32 bits of the GPU address for an optional buffer in the ``.data``; section of the ELF. The high 32 bits of the address match the high 32 bits; of the shader's program counter. The buffer can be anything the shader compiler needs it for, and; allows each shader to have its own region of the ``.data`` section.; Typically, this could be a table of buffer ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:417880,optimiz,optimize,417880,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['optimiz'],['optimize']
